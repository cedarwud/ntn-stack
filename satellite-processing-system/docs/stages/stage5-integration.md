# ğŸ“ éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆèˆ‡æ··åˆå­˜å„²

[ğŸ”„ è¿”å›æ•¸æ“šæµç¨‹å°èˆª](../README.md) > éšæ®µäº”

## ğŸ“– éšæ®µæ¦‚è¿°

**ç›®æ¨™**ï¼šå°‡æ‰€æœ‰è™•ç†çµæœæ•´åˆä¸¦å»ºç«‹æ··åˆå­˜å„²æ¶æ§‹  
**è¼¸å…¥**ï¼šéšæ®µä¸‰çš„ä¿¡è™Ÿåˆ†ææ•¸æ“šï¼ˆå«ä»°è§’ï¼‰+ éšæ®µå››çš„å‹•ç•«æ•¸æ“šï¼ˆæ™ºèƒ½èåˆï¼‰  
**è¼¸å‡º**ï¼šPostgreSQLçµæ§‹åŒ–æ•¸æ“š + Docker Volumeæª”æ¡ˆå­˜å„² + åˆ†å±¤ä»°è§’æ•¸æ“š  
**å­˜å„²ç¸½é‡**ï¼š~575MB (PostgreSQL ~2MB + Volume ~573MB)  
**è™•ç†æ™‚é–“**ï¼šç´„ 1-2 åˆ†é˜  
**æ¶æ§‹é‡é»**ï¼šå°ˆæ³¨æ–¼ç´”æ•¸æ“šæ•´åˆåŠŸèƒ½ (Phase 2çµ„ä»¶å·²ç§»è‡³Stage 6)

### ğŸ¯ @doc/todo.md å°æ‡‰å¯¦ç¾
æœ¬éšæ®µæ”¯æ´ä»¥ä¸‹éœ€æ±‚ï¼š
- ğŸ”§ **åˆ†å±¤æ•¸æ“šæº–å‚™**: ç”Ÿæˆ5Â°/10Â°/15Â°ä»°è§’åˆ†å±¤æ•¸æ“šï¼Œæ”¯æ´ä¸åŒä»°è§’é–€æª»éœ€æ±‚
- ğŸ’¾ **æ··åˆå­˜å„²**: PostgreSQLå¿«é€ŸæŸ¥è©¢ + Volumeå¤§å®¹é‡å­˜å„²ï¼Œæ”¯æ´å¼·åŒ–å­¸ç¿’æ•¸æ“šå­˜å–
- ğŸ”— **APIæ¥å£æº–å‚™**: ç‚ºå‹•æ…‹æ± è¦åŠƒå’Œæ›æ‰‹æ±ºç­–æä¾›é«˜æ•ˆæ•¸æ“šè¨ªå•æ¥å£

### ğŸ†• **å¯¦ä½œæ–°å¢åŠŸèƒ½** (2025-09-16 ç™¼ç¾)

**æ™ºèƒ½æ•¸æ“šèåˆå¼•æ“** (`intelligent_data_fusion_engine.py`):
- **å¤šæºæ•¸æ“šæ•´åˆ**: æ™ºèƒ½èåˆStage3-4çš„å¤šç¶­åº¦æ•¸æ“š
- **æ•¸æ“šä¸€è‡´æ€§ä¿è­‰**: ç¢ºä¿è·¨éšæ®µæ•¸æ“šçš„æ™‚é–“æˆ³å’Œæ ¼å¼ä¸€è‡´æ€§
- **è¡çªè§£æ±ºæ©Ÿåˆ¶**: è‡ªå‹•è™•ç†æ•¸æ“šè¡çªå’Œç•°å¸¸å€¼

**æ›æ‰‹å ´æ™¯å¼•æ“** (`handover_scenario_engine.py`):
- **3GPPå ´æ™¯ç”Ÿæˆ**: è‡ªå‹•ç”ŸæˆA4/A5/D2æ›æ‰‹å ´æ™¯æ•¸æ“š
- **å ´æ™¯å¤šæ¨£åŒ–**: å‰µå»ºè±å¯Œçš„æ›æ‰‹è¨“ç·´å ´æ™¯ä¾›RLä½¿ç”¨
- **å ´æ™¯é©—è­‰**: ç¢ºä¿ç”Ÿæˆå ´æ™¯ç¬¦åˆ3GPPæ¨™æº–è¦æ±‚

**åˆ†å±¤æ•¸æ“šç”Ÿæˆå™¨** (`layered_data_generator.py`):
- **å¤šå±¤ä»°è§’æ•¸æ“š**: è‡ªå‹•ç”Ÿæˆ5Â°/10Â°/15Â°åˆ†å±¤æ•¸æ“šçµæ§‹
- **å‹•æ…‹é–€æª»èª¿æ•´**: æ”¯æ´ç’°å¢ƒå› å­çš„é–€æª»å‹•æ…‹èª¿æ•´
- **æ€§èƒ½å„ªåŒ–**: æ¸›å°‘é‡è¤‡è¨ˆç®—ï¼Œæå‡æ•¸æ“šç”Ÿæˆæ•ˆç‡

**è™•ç†ç·©å­˜ç®¡ç†å™¨** (`processing_cache_manager.py`):
- **æ™ºèƒ½ç·©å­˜ç­–ç•¥**: åŸºæ–¼ä½¿ç”¨é »ç‡çš„æ™ºèƒ½æ•¸æ“šç·©å­˜
- **å…§å­˜å„ªåŒ–**: é˜²æ­¢å¤§æ•¸æ“šé›†é€ æˆçš„å…§å­˜æº¢å‡º
- **ç·©å­˜ä¸€è‡´æ€§**: ç¢ºä¿ç·©å­˜æ•¸æ“šèˆ‡æºæ•¸æ“šåŒæ­¥

**è·¨éšæ®µé©—è­‰å™¨** (`cross_stage_validator.py`):
- **æ•¸æ“šå®Œæ•´æ€§é©—è­‰**: ç¢ºä¿Stage3-4æ•¸æ“šå®Œæ•´å‚³éåˆ°Stage5
- **æ ¼å¼ä¸€è‡´æ€§æª¢æŸ¥**: é©—è­‰è·¨éšæ®µæ•¸æ“šæ ¼å¼æ¨™æº–åŒ–
- **å­¸è¡“æ¨™æº–åˆè¦**: æ•´åˆå­¸è¡“æ¨™æº–é©—è­‰æ©Ÿåˆ¶

## ğŸ—ï¸ æ··åˆå­˜å„²æ¶æ§‹

### å­˜å„²ç­–ç•¥åˆ†å·¥
- **PostgreSQL**ï¼šçµæ§‹åŒ–æ•¸æ“šã€ç´¢å¼•æŸ¥è©¢ã€çµ±è¨ˆåˆ†æ
- **Docker Volume**ï¼šå¤§å‹æª”æ¡ˆã€æ™‚é–“åºåˆ—æ•¸æ“šã€å‰ç«¯è³‡æº

### æ•¸æ“šåˆ†é¡åŸå‰‡
```python
STORAGE_STRATEGY = {
    'postgresql': [
        'satellite_metadata',      # è¡›æ˜ŸåŸºæœ¬è³‡è¨Š
        'signal_statistics',       # ä¿¡è™Ÿçµ±è¨ˆæŒ‡æ¨™
        'event_summaries',         # 3GPPäº‹ä»¶æ‘˜è¦
        'performance_metrics'      # ç³»çµ±æ€§èƒ½æŒ‡æ¨™
    ],
    'volume_files': [
        'timeseries_data',         # å®Œæ•´æ™‚é–“åºåˆ—
        'animation_resources',     # å‰ç«¯å‹•ç•«æ•¸æ“š
        'signal_heatmaps',        # ä¿¡è™Ÿç†±åŠ›åœ–
        'orbit_trajectories'       # è»Œé“è»Œè·¡æ•¸æ“š
    ]
}
```

## ğŸš¨ å¼·åˆ¶é‹è¡Œæ™‚æª¢æŸ¥ (æ–°å¢)

**2025-09-09 é‡å¤§å¼·åŒ–**: æ–°å¢éšæ®µäº”å°ˆé–€çš„é‹è¡Œæ™‚æ¶æ§‹å®Œæ•´æ€§æª¢æŸ¥ç¶­åº¦ã€‚

### ğŸ”´ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥ (ä»»ä½•å¤±æ•—éƒ½æœƒåœæ­¢åŸ·è¡Œ)

#### 1. æ•¸æ“šæ•´åˆè™•ç†å™¨é¡å‹å¼·åˆ¶æª¢æŸ¥
```python
# ğŸš¨ åš´æ ¼æª¢æŸ¥å¯¦éš›ä½¿ç”¨çš„æ•¸æ“šæ•´åˆè™•ç†å™¨é¡å‹
assert isinstance(processor, DataIntegrationProcessor), f"éŒ¯èª¤æ•¸æ“šæ•´åˆè™•ç†å™¨: {type(processor)}"
assert isinstance(storage_manager, HybridStorageManager), f"éŒ¯èª¤å­˜å„²ç®¡ç†å™¨: {type(storage_manager)}"
# åŸå› : ç¢ºä¿ä½¿ç”¨å®Œæ•´çš„æ•¸æ“šæ•´åˆè™•ç†å™¨ï¼Œè€Œéç°¡åŒ–ç‰ˆæœ¬
# å½±éŸ¿: éŒ¯èª¤è™•ç†å™¨å¯èƒ½å°è‡´æ•¸æ“šæ•´åˆä¸å®Œæ•´æˆ–å­˜å„²ç­–ç•¥éŒ¯èª¤
```

#### 2. å¤šéšæ®µè¼¸å…¥æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥  
```python
# ğŸš¨ å¼·åˆ¶æª¢æŸ¥ä¾†è‡ªéšæ®µä¸‰å’Œéšæ®µå››çš„è¼¸å…¥æ•¸æ“šå®Œæ•´æ€§
assert 'signal_analysis_results' in stage3_input, "ç¼ºå°‘éšæ®µä¸‰ä¿¡è™Ÿåˆ†æçµæœ"
assert 'timeseries_data' in stage4_input, "ç¼ºå°‘éšæ®µå››æ™‚é–“åºåˆ—æ•¸æ“š"

# æª¢æŸ¥éšæ®µä¸‰æ•¸æ“š
stage3_satellites = stage3_input['signal_analysis_results']
assert len(stage3_satellites['starlink']) > 1000, f"Starlinkä¿¡è™Ÿæ•¸æ“šä¸è¶³: {len(stage3_satellites['starlink'])}"
assert len(stage3_satellites['oneweb']) > 100, f"OneWebä¿¡è™Ÿæ•¸æ“šä¸è¶³: {len(stage3_satellites['oneweb'])}"

# æª¢æŸ¥éšæ®µå››æ•¸æ“š
stage4_data = stage4_input['timeseries_data']
assert 'animation_enhanced_starlink' in stage4_data, "ç¼ºå°‘Starlinkå‹•ç•«æ•¸æ“š"
assert 'animation_enhanced_oneweb' in stage4_data, "ç¼ºå°‘OneWebå‹•ç•«æ•¸æ“š"
# åŸå› : ç¢ºä¿è·¨éšæ®µæ•¸æ“šæ•´åˆçš„è¼¸å…¥å®Œæ•´æ€§
# å½±éŸ¿: ä¸å®Œæ•´çš„è¼¸å…¥æœƒå°è‡´æ•¸æ“šæ•´åˆéŒ¯èª¤æˆ–åŠŸèƒ½ç¼ºå¤±
```

#### 3. æ··åˆå­˜å„²æ¶æ§‹å®Œæ•´æ€§æª¢æŸ¥
```python
# ğŸš¨ å¼·åˆ¶æª¢æŸ¥æ··åˆå­˜å„²æ¶æ§‹æ­£ç¢ºå¯¦æ–½
storage_config = storage_manager.get_storage_configuration()
assert 'postgresql' in storage_config, "ç¼ºå°‘PostgreSQLå­˜å„²é…ç½®"
assert 'volume_files' in storage_config, "ç¼ºå°‘Volumeæ–‡ä»¶å­˜å„²é…ç½®"

# æª¢æŸ¥PostgreSQLé€£æ¥å’Œè¡¨çµæ§‹
db_connection = storage_manager.get_database_connection()
assert db_connection.is_connected(), "PostgreSQLé€£æ¥å¤±æ•—"
required_tables = ['satellite_metadata', 'signal_quality_statistics', 'handover_events_summary']
existing_tables = db_connection.get_table_list()
for table in required_tables:
    assert table in existing_tables, f"ç¼ºå°‘å¿…éœ€çš„æ•¸æ“šè¡¨: {table}"

# æª¢æŸ¥Volumeå­˜å„²è·¯å¾‘
volume_path = storage_manager.get_volume_path()
assert os.path.exists(volume_path), f"Volumeå­˜å„²è·¯å¾‘ä¸å­˜åœ¨: {volume_path}"
assert os.access(volume_path, os.W_OK), f"Volumeè·¯å¾‘ç„¡å¯«å…¥æ¬Šé™: {volume_path}"
# åŸå› : ç¢ºä¿æ··åˆå­˜å„²æ¶æ§‹æ­£ç¢ºé…ç½®å’Œå¯ç”¨
# å½±éŸ¿: å­˜å„²æ¶æ§‹å•é¡Œæœƒå°è‡´æ•¸æ“šç„¡æ³•æ­£ç¢ºä¿å­˜æˆ–è®€å–
```

#### 4. åˆ†å±¤ä»°è§’æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
```python
# ğŸš¨ å¼·åˆ¶æª¢æŸ¥åˆ†å±¤ä»°è§’æ•¸æ“šç”Ÿæˆå®Œæ•´æ€§
layered_data = processor.get_layered_elevation_data()
required_layers = ['5deg', '10deg', '15deg']
for constellation in ['starlink', 'oneweb']:
    for layer in required_layers:
        layer_key = f"{constellation}_{layer}_enhanced"
        assert layer_key in layered_data, f"ç¼ºå°‘åˆ†å±¤æ•¸æ“š: {layer_key}"
        layer_satellites = layered_data[layer_key]
        assert len(layer_satellites) > 0, f"{layer_key}åˆ†å±¤æ•¸æ“šç‚ºç©º"
        
        # æª¢æŸ¥ä»°è§’é–€æª»æ­£ç¢ºæ€§
        expected_threshold = int(layer.replace('deg', ''))
        for satellite in layer_satellites[:3]:
            max_elevation = satellite.get('max_elevation_deg', 0)
            assert max_elevation >= expected_threshold, \
                f"{layer_key}è¡›æ˜Ÿæœ€å¤§ä»°è§’{max_elevation}Â°ä½æ–¼é–€æª»{expected_threshold}Â°"
# åŸå› : ç¢ºä¿åˆ†å±¤ä»°è§’æ•¸æ“šæ­£ç¢ºç”Ÿæˆå’Œç¬¦åˆé–€æª»è¦æ±‚
# å½±éŸ¿: éŒ¯èª¤çš„åˆ†å±¤æ•¸æ“šæœƒå½±éŸ¿æ›æ‰‹æ±ºç­–å’Œè¦†è“‹åˆ†æ
```

#### 5. æ•¸æ“šä¸€è‡´æ€§è·¨éšæ®µæª¢æŸ¥
```python
# ğŸš¨ å¼·åˆ¶æª¢æŸ¥è·¨éšæ®µæ•¸æ“šä¸€è‡´æ€§
stage3_satellite_ids = set(sat['satellite_id'] for constellation in stage3_input['signal_analysis_results'].values() 
                          for sat in constellation)
stage4_satellite_ids = set(stage4_input['timeseries_data']['satellite_ids'])
stage5_satellite_ids = set(processor.get_integrated_satellite_ids())

# æª¢æŸ¥è¡›æ˜ŸIDä¸€è‡´æ€§
common_satellites = stage3_satellite_ids.intersection(stage4_satellite_ids)
assert len(common_satellites) > 1000, f"è·¨éšæ®µå…±åŒè¡›æ˜Ÿæ•¸é‡ä¸è¶³: {len(common_satellites)}"
assert stage5_satellite_ids.issubset(common_satellites), "éšæ®µäº”åŒ…å«äº†æœªåœ¨å‰éšæ®µå‡ºç¾çš„è¡›æ˜Ÿ"

# æª¢æŸ¥æ•¸æ“šæ™‚é–“æˆ³ä¸€è‡´æ€§
stage3_timestamp = stage3_input['metadata']['processing_timestamp']
stage4_timestamp = stage4_input['metadata']['processing_timestamp']
timestamp_diff = abs((stage3_timestamp - stage4_timestamp).total_seconds())
assert timestamp_diff < 3600, f"éšæ®µä¸‰å››æ™‚é–“æˆ³å·®ç•°éå¤§: {timestamp_diff}ç§’"
# åŸå› : ç¢ºä¿è·¨éšæ®µæ•¸æ“šçš„ä¸€è‡´æ€§å’ŒåŒæ­¥æ€§
# å½±éŸ¿: æ•¸æ“šä¸ä¸€è‡´æœƒå°è‡´æ•´åˆçµæœéŒ¯èª¤æˆ–æ±ºç­–åå·®
```

#### 6. ç„¡ç°¡åŒ–æ•´åˆé›¶å®¹å¿æª¢æŸ¥
```python
# ğŸš¨ ç¦æ­¢ä»»ä½•å½¢å¼çš„ç°¡åŒ–æ•¸æ“šæ•´åˆ
forbidden_integration_modes = [
    "partial_integration", "simplified_storage", "mock_database",
    "estimated_statistics", "arbitrary_aggregation", "lossy_compression"
]
for mode in forbidden_integration_modes:
    assert mode not in str(processor.__class__).lower(), \
        f"æª¢æ¸¬åˆ°ç¦ç”¨çš„ç°¡åŒ–æ•´åˆ: {mode}"
    assert mode not in storage_manager.get_storage_methods(), \
        f"æª¢æ¸¬åˆ°ç¦ç”¨çš„å­˜å„²æ–¹æ³•: {mode}"
```

### ğŸ“‹ Runtime Check Integration Points

**æª¢æŸ¥æ™‚æ©Ÿ**: 
- **åˆå§‹åŒ–æ™‚**: é©—è­‰æ•¸æ“šæ•´åˆè™•ç†å™¨å’Œå­˜å„²ç®¡ç†å™¨é¡å‹
- **è¼¸å…¥è™•ç†æ™‚**: æª¢æŸ¥éšæ®µä¸‰å››æ•¸æ“šå®Œæ•´æ€§å’Œè·¨éšæ®µä¸€è‡´æ€§
- **å­˜å„²é…ç½®æ™‚**: é©—è­‰æ··åˆå­˜å„²æ¶æ§‹æ­£ç¢ºé…ç½®å’Œå¯ç”¨æ€§
- **æ•¸æ“šæ•´åˆæ™‚**: ç›£æ§åˆ†å±¤æ•¸æ“šç”Ÿæˆå’Œæ•¸æ“šä¸€è‡´æ€§
- **è¼¸å‡ºå‰**: åš´æ ¼æª¢æŸ¥æ•´åˆçµæœå®Œæ•´æ€§å’Œå­˜å„²æˆåŠŸæ€§

**å¤±æ•—è™•ç†**:
- **ç«‹å³åœæ­¢**: ä»»ä½•runtime checkå¤±æ•—éƒ½æœƒç«‹å³çµ‚æ­¢åŸ·è¡Œ
- **å­˜å„²æª¢æŸ¥**: é©—è­‰PostgreSQLå’ŒVolumeå­˜å„²æ­£ç¢ºé…ç½®
- **ä¸€è‡´æ€§é©—è­‰**: æª¢æŸ¥è·¨éšæ®µæ•¸æ“šæ™‚é–“æˆ³å’Œè¡›æ˜ŸIDä¸€è‡´æ€§
- **ç„¡é™ç´šè™•ç†**: çµ•ä¸å…è¨±ä½¿ç”¨ç°¡åŒ–æ•´åˆæˆ–ä¸å®Œæ•´å­˜å„²

### ğŸ›¡ï¸ å¯¦æ–½è¦æ±‚

- **è·¨éšæ®µä¸€è‡´æ€§å¼·åˆ¶åŸ·è¡Œ**: å¿…é ˆç¢ºä¿éšæ®µä¸‰å››äº”æ•¸æ“šå®Œå…¨ä¸€è‡´
- **æ··åˆå­˜å„²æ¶æ§‹å®Œæ•´æ€§**: PostgreSQLå’ŒVolumeå­˜å„²å¿…é ˆåŒæ™‚æ­£ç¢ºé…ç½®
- **åˆ†å±¤æ•¸æ“šæº–ç¢ºæ€§**: æ‰€æœ‰ä»°è§’å±¤æ•¸æ“šå¿…é ˆç¬¦åˆç›¸æ‡‰é–€æª»è¦æ±‚
- **æ•¸æ“šå®Œæ•´æ€§ä¿è­‰**: æ•´åˆéç¨‹ä¸­ä¸å¾—ä¸Ÿå¤±ä»»ä½•é—œéµæ•¸æ“š
- **æ€§èƒ½å½±éŸ¿æ§åˆ¶**: é‹è¡Œæ™‚æª¢æŸ¥é¡å¤–æ™‚é–“é–‹éŠ· <3%

## ğŸ“Š PostgreSQL æ•¸æ“šçµæ§‹

### æ ¸å¿ƒè³‡æ–™è¡¨è¨­è¨ˆ

#### 1. satellite_metadata
```sql
CREATE TABLE satellite_metadata (
    satellite_id VARCHAR(50) PRIMARY KEY,
    constellation VARCHAR(20) NOT NULL,
    norad_id INTEGER UNIQUE,
    tle_epoch TIMESTAMP WITH TIME ZONE,
    orbital_period_minutes NUMERIC(8,3),
    inclination_deg NUMERIC(6,3),
    mean_altitude_km NUMERIC(8,3),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ç´¢å¼•å„ªåŒ–
CREATE INDEX idx_satellite_constellation ON satellite_metadata(constellation);
CREATE INDEX idx_satellite_norad ON satellite_metadata(norad_id);
```

#### 2. signal_quality_statistics
```sql
CREATE TABLE signal_quality_statistics (
    id SERIAL PRIMARY KEY,
    satellite_id VARCHAR(50) REFERENCES satellite_metadata(satellite_id),
    analysis_period_start TIMESTAMP WITH TIME ZONE,
    analysis_period_end TIMESTAMP WITH TIME ZONE,
    mean_rsrp_dbm NUMERIC(6,2),
    std_rsrp_db NUMERIC(5,2),
    max_elevation_deg NUMERIC(5,2),
    total_visible_time_minutes INTEGER,
    handover_event_count INTEGER,
    signal_quality_grade VARCHAR(10), -- 'high', 'medium', 'low'
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- è¤‡åˆç´¢å¼•
CREATE INDEX idx_signal_satellite_period ON signal_quality_statistics(satellite_id, analysis_period_start);
CREATE INDEX idx_signal_quality_grade ON signal_quality_statistics(signal_quality_grade);
```

#### 3. handover_events_summary
```sql
CREATE TABLE handover_events_summary (
    id SERIAL PRIMARY KEY,
    event_type VARCHAR(10) NOT NULL, -- 'A4', 'A5', 'D2'
    serving_satellite_id VARCHAR(50) REFERENCES satellite_metadata(satellite_id),
    neighbor_satellite_id VARCHAR(50) REFERENCES satellite_metadata(satellite_id),
    event_timestamp TIMESTAMP WITH TIME ZONE,
    trigger_rsrp_dbm NUMERIC(6,2),
    handover_decision VARCHAR(20), -- 'trigger', 'hold', 'reject'
    processing_latency_ms INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- äº‹ä»¶æŸ¥è©¢ç´¢å¼•
CREATE INDEX idx_handover_event_type ON handover_events_summary(event_type);
CREATE INDEX idx_handover_timestamp ON handover_events_summary(event_timestamp);
CREATE INDEX idx_handover_serving ON handover_events_summary(serving_satellite_id);
```

## ğŸ“ Docker Volume æª”æ¡ˆçµæ§‹

### Volume çµ„ç¹”æ¶æ§‹
```bash
/app/data/
â”œâ”€â”€ timeseries_preprocessing_outputs/    # éšæ®µå››è¼¸å‡ºï¼šå‰ç«¯å‹•ç•«æ•¸æ“š (~60-75MB)
â”‚   â”œâ”€â”€ animation_enhanced_starlink.json
â”‚   â”œâ”€â”€ animation_enhanced_oneweb.json
â”‚   â””â”€â”€ conversion_statistics.json
â”‚
â”œâ”€â”€ layered_phase0_enhanced/      # åˆ†å±¤è™•ç†çµæœ (~85MB)
â”‚   â”œâ”€â”€ starlink_5deg_enhanced.json
â”‚   â”œâ”€â”€ starlink_10deg_enhanced.json
â”‚   â”œâ”€â”€ starlink_15deg_enhanced.json
â”‚   â”œâ”€â”€ oneweb_10deg_enhanced.json
â”‚   â”œâ”€â”€ oneweb_15deg_enhanced.json
â”‚   â””â”€â”€ oneweb_20deg_enhanced.json
â”‚
â”œâ”€â”€ handover_scenarios/           # æ›æ‰‹å ´æ™¯æ•¸æ“š (~55MB)
â”‚   â”œâ”€â”€ a4_events_enhanced.json
â”‚   â”œâ”€â”€ a5_events_enhanced.json
â”‚   â”œâ”€â”€ d2_events_enhanced.json
â”‚   â””â”€â”€ best_handover_windows.json
â”‚
â”œâ”€â”€ signal_quality_analysis/      # ä¿¡è™Ÿåˆ†æçµæœ (~65MB)
â”‚   â”œâ”€â”€ signal_heatmap_data.json
â”‚   â”œâ”€â”€ quality_metrics_summary.json
â”‚   â””â”€â”€ constellation_comparison.json
â”‚
â”œâ”€â”€ processing_cache/             # è™•ç†ç·©å­˜ (~35MB)
â”‚   â”œâ”€â”€ sgp4_calculation_cache.json
â”‚   â”œâ”€â”€ filtering_results_cache.json
â”‚   â””â”€â”€ gpp3_event_cache.json
â”‚
â””â”€â”€ status_files/                 # ç‹€æ…‹æ¨™è¨˜æª”æ¡ˆ (~1MB)
    â”œâ”€â”€ last_processing_time.txt
    â”œâ”€â”€ tle_checksum.txt
    â”œâ”€â”€ processing_status.json
    â””â”€â”€ health_check.json
```

## ğŸ”— æ™ºèƒ½æ•¸æ“šèåˆç­–ç•¥

### é›™æ•¸æ“šæºæ•´åˆè¨­è¨ˆ
éšæ®µäº”æ¡ç”¨å‰µæ–°çš„**æ™ºèƒ½æ•¸æ“šèåˆ**æ–¹æ³•ï¼ŒåŒæ™‚åˆ©ç”¨éšæ®µä¸‰å’Œéšæ®µå››çš„å„ªå‹¢ï¼š

#### æ•¸æ“šä¾†æºåˆ†å·¥
```python
DATA_FUSION_STRATEGY = {
    'stage3_data': {
        'source': '/app/data/signal_quality_analysis_output.json',
        'provides': [
            'position_timeseries',      # å®Œæ•´è»Œé“æ™‚åºæ•¸æ“š
            'elevation_deg',            # çœŸå¯¦ä»°è§’æ•¸æ“šï¼ˆä½æ–¼relative_to_observerï¼‰
            'signal_quality',           # è©³ç´°ä¿¡è™Ÿåˆ†æ
            'visibility_analysis',      # å¯è¦‹æ€§åˆ¤æ–·
            '3gpp_events'              # 3GPPæ¨™æº–äº‹ä»¶
        ],
        'purpose': 'æä¾›ç§‘å­¸è¨ˆç®—æ‰€éœ€çš„ç²¾ç¢ºæ•¸æ“š'
    },
    'stage4_data': {
        'source': '/app/data/timeseries_preprocessing_outputs/',
        'provides': [
            'track_points',            # å„ªåŒ–çš„è»Œè·¡å‹•ç•«é»
            'signal_timeline',         # å‰ç«¯ä¿¡è™Ÿå¯è¦–åŒ–
            'animation_metadata'       # å‹•ç•«æ€§èƒ½æ•¸æ“š
        ],
        'purpose': 'æä¾›å‰ç«¯å‹•ç•«å’Œå¯è¦–åŒ–å„ªåŒ–æ•¸æ“š'
    }
}
```

#### èåˆé‚è¼¯å¯¦ç¾
```python
async def _load_enhanced_timeseries(self) -> Dict[str, Any]:
    """æ™ºèƒ½æ•¸æ“šèåˆï¼šçµåˆéšæ®µä¸‰ç§‘å­¸æ•¸æ“šå’Œéšæ®µå››å‹•ç•«æ•¸æ“š"""
    
    # 1. è¼‰å…¥éšæ®µä¸‰æ•¸æ“šï¼ˆç§‘å­¸ç²¾ç¢ºæ•¸æ“šï¼‰
    stage3_data = self._load_stage3_signal_analysis()
    
    # 2. è¼‰å…¥éšæ®µå››æ•¸æ“šï¼ˆå‹•ç•«å„ªåŒ–æ•¸æ“šï¼‰ 
    stage4_data = self._load_stage4_animation_data()
    
    # 3. æŒ‰è¡›æ˜ŸIDé€²è¡Œæ™ºèƒ½èåˆ
    for constellation in ["starlink", "oneweb"]:
        for sat_id, stage3_sat in stage3_data[constellation]['satellites'].items():
            enhanced_satellite = {
                # éšæ®µä¸‰æä¾›ï¼šç§‘å­¸è¨ˆç®—æ•¸æ“š
                **stage3_sat,  # position_timeseries, signal_quality, etc.
                
                # éšæ®µå››æä¾›ï¼šå‹•ç•«å„ªåŒ–æ•¸æ“šï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                'signal_timeline': stage4_data.get(sat_id, {}).get('signal_timeline', []),
                'track_points': stage4_data.get(sat_id, {}).get('track_points', []),
                'summary': stage4_data.get(sat_id, {}).get('summary', {})
            }
            
            enhanced_data[constellation]['satellites'][sat_id] = enhanced_satellite
    
    return enhanced_data
```

### èåˆå„ªå‹¢
1. **ç§‘å­¸ç²¾ç¢ºæ€§** - ä½¿ç”¨éšæ®µä¸‰çš„çœŸå¯¦ä»°è§’æ•¸æ“šé€²è¡Œåˆ†å±¤æ¿¾æ³¢
2. **å‹•ç•«æµæš¢æ€§** - ä¿ç•™éšæ®µå››çš„å‰ç«¯å„ªåŒ–æ•¸æ“š
3. **åŠŸèƒ½å®Œæ•´æ€§** - åŒæ™‚æ»¿è¶³ç§‘å­¸è¨ˆç®—å’Œå¯è¦–åŒ–éœ€æ±‚
4. **æ¶æ§‹å½ˆæ€§** - å¯ç¨ç«‹æ›´æ–°å„æ•¸æ“šæºè€Œä¸å½±éŸ¿å…¶ä»–åŠŸèƒ½

## ğŸ”§ æ•´åˆè™•ç†å™¨å¯¦ç¾

### ä¸»è¦å¯¦ç¾ä½ç½®
```bash
# æ•¸æ“šæ•´åˆè™•ç†å™¨
/netstack/src/stages/data_integration_processor.py
â”œâ”€â”€ Stage5IntegrationProcessor.process_enhanced_timeseries()    # å¢å¼·æ™‚é–“åºåˆ—è™•ç†
â”œâ”€â”€ Stage5IntegrationProcessor._integrate_postgresql_data()     # PostgreSQLæ•¸æ“šæ•´åˆ
â”œâ”€â”€ Stage5IntegrationProcessor._generate_layered_data()         # åˆ†å±¤æ•¸æ“šç”Ÿæˆ
â”œâ”€â”€ Stage5IntegrationProcessor._generate_handover_scenarios()   # æ›æ‰‹å ´æ™¯ç”Ÿæˆ
â””â”€â”€ Stage5IntegrationProcessor._verify_mixed_storage_access()   # æ··åˆå­˜å„²é©—è­‰

# è³‡æ–™åº«é€£æ¥ç®¡ç†
/netstack/src/services/database/postgresql_manager.py
â”œâ”€â”€ PostgreSQLManager.setup_connection_pool()              # é€£æ¥æ± ç®¡ç†
â”œâ”€â”€ PostgreSQLManager.execute_batch_insert()               # æ‰¹æ¬¡æ’å…¥å„ªåŒ–
â””â”€â”€ PostgreSQLManager.create_indexes()                     # ç´¢å¼•å»ºç«‹
```

### æ ¸å¿ƒè™•ç†é‚è¼¯
```python
class Stage5IntegrationProcessor:
    
    async def process_enhanced_timeseries(self) -> Dict[str, Any]:
        """åŸ·è¡Œéšæ®µäº”å®Œæ•´æ•´åˆè™•ç† - æ™ºèƒ½æ•¸æ“šèåˆç‰ˆ"""
        
        results = {
            "stage": "stage5_integration",
            "start_time": datetime.now(timezone.utc).isoformat(),
            "postgresql_integration": {},
            "layered_data_enhancement": {},
            "handover_scenarios": {},
            "signal_quality_analysis": {},
            "processing_cache": {},
            "status_files": {},
            "mixed_storage_verification": {}
        }
        
        try:
            # 1. æ™ºèƒ½æ•¸æ“šèåˆï¼šåŒæ™‚è¼‰å…¥éšæ®µä¸‰ï¼ˆä»°è§’ï¼‰å’Œéšæ®µå››ï¼ˆå‹•ç•«ï¼‰æ•¸æ“š
            enhanced_data = await self._load_enhanced_timeseries()
            
            # 2. PostgreSQL æ•¸æ“šæ•´åˆ
            results["postgresql_integration"] = await self._integrate_postgresql_data(enhanced_data)
            
            # 3. ä½¿ç”¨çœŸå¯¦ä»°è§’ç”Ÿæˆåˆ†å±¤æ•¸æ“š
            results["layered_data_enhancement"] = await self._generate_layered_data(enhanced_data)
            
            # 4. ç”Ÿæˆæ›æ‰‹å ´æ™¯å°ˆç”¨æ•¸æ“š
            results["handover_scenarios"] = await self._generate_handover_scenarios(enhanced_data)
            
            # 5. å‰µå»ºä¿¡è™Ÿå“è³ªåˆ†æç›®éŒ„çµæ§‹
            results["signal_quality_analysis"] = await self._setup_signal_analysis_structure(enhanced_data)
            
            # 6. å‰µå»ºè™•ç†ç·©å­˜
            results["processing_cache"] = await self._create_processing_cache(enhanced_data)
            
            # 7. ç”Ÿæˆç‹€æ…‹æ–‡ä»¶
            results["status_files"] = await self._create_status_files()
            
            # 8. é©—è­‰æ··åˆå­˜å„²è¨ªå•æ¨¡å¼
            results["mixed_storage_verification"] = await self._verify_mixed_storage_access()
            
            results["success"] = True
            
        except Exception as e:
            logger.error(f"âŒ éšæ®µäº”è™•ç†å¤±æ•—: {e}")
            results["success"] = False
            results["error"] = str(e)
            
        return results
    
    async def _generate_layered_data(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†å±¤æ•¸æ“šå¢å¼· - ä½¿ç”¨çœŸå¯¦ä»°è§’æ•¸æ“š"""
        
        self.logger.info("ğŸ”„ ç”Ÿæˆåˆ†å±¤ä»°è§’æ•¸æ“šï¼ˆä½¿ç”¨éšæ®µä¸‰çœŸå¯¦ä»°è§’æ•¸æ“šï¼‰")
        
        layered_results = {}
        
        for threshold in self.config.elevation_thresholds:
            threshold_dir = Path(self.config.output_layered_dir) / f"elevation_{threshold}deg"
            threshold_dir.mkdir(parents=True, exist_ok=True)
            
            layered_results[f"elevation_{threshold}deg"] = {}
            
            for constellation, data in enhanced_data.items():
                if not data or 'satellites' not in data:
                    continue
                
                satellites_data = data.get('satellites', {})
                filtered_satellites = {}
                total_satellites = len(satellites_data)
                
                for sat_id, satellite in satellites_data.items():
                    # ä½¿ç”¨éšæ®µä¸‰çš„position_timeseriesæ•¸æ“šï¼ˆåŒ…å«çœŸå¯¦ä»°è§’ï¼‰
                    position_timeseries = satellite.get('position_timeseries', [])
                    
                    # ç¯©é¸ç¬¦åˆä»°è§’é–€æª»çš„æ™‚åºé»
                    filtered_timeseries = []
                    for point in position_timeseries:
                        if isinstance(point, dict):
                            # å¾relative_to_observerä¸­ç²å–çœŸå¯¦ä»°è§’æ•¸æ“š
                            relative_data = point.get('relative_to_observer', {})
                            if isinstance(relative_data, dict):
                                elevation_deg = relative_data.get('elevation_deg')
                                is_visible = relative_data.get('is_visible', False)
                                
                                # åªä¿ç•™å¯è¦‹ä¸”ç¬¦åˆä»°è§’é–€æª»çš„é»
                                if is_visible and elevation_deg is not None and elevation_deg >= threshold:
                                    filtered_timeseries.append(point)
                    
                    # å¦‚æœæœ‰ç¬¦åˆæ¢ä»¶çš„æ™‚åºé»ï¼Œä¿ç•™è©²è¡›æ˜Ÿ
                    if filtered_timeseries:
                        filtered_satellite = {
                            **satellite,  # ä¿ç•™æ‰€æœ‰åŸæœ‰æ•¸æ“š
                            'position_timeseries': filtered_timeseries,  # æ›´æ–°ç‚ºç¯©é¸å¾Œçš„æ™‚åºæ•¸æ“š
                            'satellite_id': sat_id,
                            'layered_stats': {
                                'elevation_threshold': threshold,
                                'filtered_points': len(filtered_timeseries),
                                'original_points': len(position_timeseries)
                            }
                        }
                        filtered_satellites[sat_id] = filtered_satellite
                
                # ç”Ÿæˆåˆ†å±¤æ•¸æ“šæª”æ¡ˆ
                retention_rate = round(len(filtered_satellites) / max(total_satellites, 1) * 100, 1)
                layered_data = {
                    "metadata": {
                        **data.get('metadata', {}),
                        "elevation_threshold_deg": threshold,
                        "total_input_satellites": total_satellites,
                        "filtered_satellites_count": len(filtered_satellites),
                        "filter_retention_rate": retention_rate,
                        "stage5_processing_time": datetime.now(timezone.utc).isoformat(),
                        "constellation": constellation,
                        "filtering_method": "real_elevation_data_from_position_timeseries"
                    },
                    "satellites": filtered_satellites
                }
                
                output_file = threshold_dir / f"{constellation}_with_3gpp_events.json"
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(layered_data, f, indent=2, ensure_ascii=False)
                
                file_size_mb = output_file.stat().st_size / (1024 * 1024)
                
                layered_results[f"elevation_{threshold}deg"][constellation] = {
                    "file_path": str(output_file),
                    "total_input_satellites": total_satellites,
                    "satellites_count": len(filtered_satellites),
                    "retention_rate_percent": retention_rate,
                    "file_size_mb": round(file_size_mb, 2),
                    "filtering_method": "real_elevation_from_position_timeseries"
                }
                
                self.logger.info(f"âœ… {constellation} {threshold}Â° é–€æª»: {len(filtered_satellites)}/{total_satellites} é¡†è¡›æ˜Ÿ ({retention_rate}%), {file_size_mb:.1f}MB")
        
        return layered_results
```

## ğŸš¨ **å­¸è¡“ç´šæ•¸æ“šæ•´åˆæ¨™æº–éµå¾ª** (Grade A/B ç­‰ç´š)

### ğŸŸ¢ **Grade A å¼·åˆ¶è¦æ±‚ï¼šæ•¸æ“šå®Œæ•´æ€§å„ªå…ˆ**

#### æ•¸æ“šæ•´åˆå®Œæ•´æ€§åŸå‰‡
- **ç„¡ææ•¸æ“šæ•´åˆ**ï¼šä»»ä½•éšæ®µçš„åŸå§‹æ•¸æ“šéƒ½ä¸å¾—åœ¨æ•´åˆéç¨‹ä¸­ä¸Ÿå¤±
- **æ™‚é–“åºåˆ—ä¸€è‡´æ€§**ï¼šä¿æŒæ‰€æœ‰æ•¸æ“šæºçš„æ™‚é–“æˆ³åŒæ­¥å’Œæº–ç¢ºæ€§
- **è¡€çµ±è¿½è¹¤ä¿æŒ**ï¼šç¶­æŒæ¯å€‹æ•¸æ“šé»çš„ä¾†æºå’Œè™•ç†æ­·å²

#### ğŸŸ¡ **Grade B å¯æ¥å—ï¼šåŸºæ–¼æ•ˆèƒ½åˆ†æçš„é…ç½®**

#### è³‡æ–™åº«é…ç½® (åŸºæ–¼è² è¼‰åˆ†æ)
```python
# âœ… æ­£ç¢ºï¼šåŸºæ–¼å¯¦éš›è² è¼‰å’Œç¡¬é«”èƒ½åŠ›åˆ†æ
POSTGRESQL_CONFIG = {
    'max_connections': calculate_optimal_connections(),  # åŸºæ–¼CPUæ ¸å¿ƒæ•¸åˆ†æ
    'connection_timeout': 30,                           # åŸºæ–¼ç¶²è·¯å»¶é²æ¸¬è©¦
    'query_timeout': calculate_query_timeout(),         # åŸºæ–¼æŸ¥è©¢è¤‡é›œåº¦åˆ†æ
    'batch_insert_size': optimize_batch_size(),         # åŸºæ–¼è¨˜æ†¶é«”å’ŒIOæ¸¬è©¦
    'enable_connection_pooling': True                   # æ¨™æº–æœ€ä½³å¯¦è¸
}

def calculate_optimal_connections():
    """åŸºæ–¼ç¡¬é«”è³‡æºè¨ˆç®—æœ€ä½³é€£æ¥æ•¸"""
    cpu_cores = os.cpu_count()
    available_memory_gb = psutil.virtual_memory().total / (1024**3)
    # PostgreSQLå»ºè­°ï¼š2-4å€CPUæ ¸å¿ƒæ•¸ï¼Œå—è¨˜æ†¶é«”é™åˆ¶
    return min(cpu_cores * 3, int(available_memory_gb / 0.1))  # æ¯é€£æ¥ç´„100MB

# âŒ éŒ¯èª¤ï¼šä»»æ„è¨­å®šæ•¸å€¼
ARBITRARY_CONFIG = {
    'max_connections': 20,      # ä»»æ„æ•¸å­—
    'batch_insert_size': 100,   # æœªç¶“æ¸¬è©¦çš„æ‰¹æ¬¡å¤§å°
}
```

#### ğŸ”´ **Grade C åš´æ ¼ç¦æ­¢é …ç›®** (é›¶å®¹å¿)
- **âŒ ä»»æ„æ‰¹æ¬¡å¤§å°è¨­å®š**ï¼šå¦‚"100ç­†æ‰¹æ¬¡"ç­‰æœªç¶“æ•ˆèƒ½æ¸¬è©¦çš„æ•¸å€¼
- **âŒ å›ºå®šé€£æ¥æ± é…ç½®**ï¼šä¸è€ƒæ…®ç¡¬é«”è³‡æºçš„å›ºå®šé€£æ¥æ•¸
- **âŒ æ•¸æ“šå®Œæ•´æ€§çŠ§ç‰²**ï¼šç‚ºæ•ˆèƒ½è€Œçœç•¥å¿…è¦çš„æ•¸æ“šé©—è­‰
- **âŒ æ™‚é–“æˆ³ä¸ä¸€è‡´**ï¼šæ•´åˆéç¨‹ä¸­æ”¹è®ŠåŸå§‹æ™‚é–“æˆ³
- **âŒ ä»»æ„å£“ç¸®è¨­å®š**ï¼šå¯èƒ½æå¤±ç²¾åº¦çš„å£“ç¸®åƒæ•¸

### ğŸ“Š **æ›¿ä»£æ–¹æ¡ˆï¼šåŸºæ–¼ç§‘å­¸åŸç†çš„æ•ˆèƒ½å„ªåŒ–**

#### å­¸è¡“ç´šè³‡æ–™åº«é…ç½®ç­–ç•¥
```python
# âœ… æ­£ç¢ºï¼šåŸºæ–¼è³‡æ–™åº«ç†è«–å’Œç¡¬é«”åˆ†æ
class AcademicDatabaseOptimizer:
    def __init__(self):
        self.system_resources = self.analyze_system_resources()
        self.data_characteristics = self.analyze_data_characteristics()
    
    def calculate_index_strategy(self):
        """åŸºæ–¼æŸ¥è©¢æ¨¡å¼åˆ†æè¨ˆç®—ç´¢å¼•ç­–ç•¥"""
        query_patterns = self.analyze_query_patterns()
        return {
            'primary_indexes': self.identify_primary_keys(),
            'composite_indexes': self.optimize_composite_indexes(query_patterns),
            'partial_indexes': self.calculate_selective_indexes()
        }
    
    def optimize_batch_processing(self):
        """åŸºæ–¼IOå’Œè¨˜æ†¶é«”ç‰¹æ€§å„ªåŒ–æ‰¹æ¬¡è™•ç†"""
        memory_available = self.system_resources['memory_mb']
        io_bandwidth = self.system_resources['io_mbps']
        data_row_size = self.data_characteristics['avg_row_size_bytes']
        
        # åŸºæ–¼è¨˜æ†¶é«”é™åˆ¶è¨ˆç®—æœ€ä½³æ‰¹æ¬¡å¤§å°
        max_batch_memory = memory_available * 0.1  # ä½¿ç”¨10%è¨˜æ†¶é«”
        optimal_batch_size = int(max_batch_memory * 1024 * 1024 / data_row_size)
        
        return min(optimal_batch_size, 10000)  # ä¸Šé™10Ké˜²æ­¢é•·äº‹å‹™
```

#### æ•¸æ“šå®Œæ•´æ€§é©—è­‰æ©Ÿåˆ¶
```python
# âœ… æ­£ç¢ºï¼šç¢ºä¿å­¸è¡“ç´šæ•¸æ“šå®Œæ•´æ€§
def verify_data_integration_integrity(source_data, integrated_data):
    """é©—è­‰æ•¸æ“šæ•´åˆéç¨‹çš„å®Œæ•´æ€§å’Œæº–ç¢ºæ€§"""
    
    integrity_checks = {
        'record_count_preservation': verify_record_counts(source_data, integrated_data),
        'time_series_continuity': verify_time_series_completeness(source_data, integrated_data),
        'measurement_accuracy': verify_measurement_values(source_data, integrated_data),
        'metadata_preservation': verify_metadata_completeness(source_data, integrated_data),
        'data_lineage_tracking': verify_lineage_information(integrated_data)
    }
    
    return integrity_checks

def calculate_required_precision_for_storage():
    """åŸºæ–¼æ¸¬é‡ä¸ç¢ºå®šåº¦è¨ˆç®—å­˜å„²ç²¾åº¦è¦æ±‚"""
    measurement_uncertainties = {
        'satellite_position': 1.0,      # SGP4 Â±1kmå…¸å‹ç²¾åº¦
        'signal_strength': 0.5,         # RSRP Â±0.5dBæ¸¬é‡ç²¾åº¦
        'elevation_angle': 0.1,         # Â±0.1åº¦è§’åº¦ç²¾åº¦
        'time_stamp': 0.001             # Â±1msæ™‚é–“ç²¾åº¦
    }
    
    # åŸºæ–¼æ¸¬é‡ä¸ç¢ºå®šåº¦è¨ˆç®—æ‰€éœ€æ•¸å€¼ç²¾åº¦
    storage_precision = {}
    for measurement, uncertainty in measurement_uncertainties.items():
        # å­˜å„²ç²¾åº¦æ‡‰è‡³å°‘æ¯”æ¸¬é‡ä¸ç¢ºå®šåº¦é«˜ä¸€å€‹æ•¸é‡ç´š
        required_precision = -int(math.floor(math.log10(uncertainty))) + 1
        storage_precision[measurement] = required_precision
    
    return storage_precision
```

## ğŸ“ˆ å­˜å„²çµ±è¨ˆèˆ‡ç›£æ§

### å­˜å„²ä½¿ç”¨åˆ†æ
```python
# é æœŸå­˜å„²åˆ†ä½ˆ
STORAGE_BREAKDOWN = {
    'postgresql_total_mb': 65,
    'postgresql_breakdown': {
        'satellite_metadata': 1.5,     # 391é¡†è¡›æ˜Ÿ Ã— åŸºæœ¬è³‡è¨Š
        'signal_statistics': 25,       # 391é¡† Ã— çµ±è¨ˆæ•¸æ“š
        'handover_events': 18,         # ~1,800å€‹æ›æ‰‹äº‹ä»¶
        'indexes_overhead': 9,         # ç´¢å¼•ç©ºé–“
        'system_metadata': 11.5        # PostgreSQLç³»çµ±é–‹éŠ·
    },
    'volume_total_mb': 300,
    'volume_breakdown': {
        'enhanced_timeseries': 75,     # å‰ç«¯å‹•ç•«æ•¸æ“š
        'layered_phase0': 85,          # åˆ†å±¤è™•ç†çµæœ  
        'handover_scenarios': 55,      # æ›æ‰‹å ´æ™¯
        'signal_analysis': 65,         # ä¿¡è™Ÿåˆ†æ
        'cache_files': 20             # ç·©å­˜æª”æ¡ˆ
    }
}
```

### å¥åº·æª¢æŸ¥æ©Ÿåˆ¶
```bash
# å­˜å„²å¥åº·æª¢æŸ¥è…³æœ¬
#!/bin/bash
echo "ğŸ“Š æ··åˆå­˜å„²å¥åº·æª¢æŸ¥"

# PostgreSQLé€£æ¥æª¢æŸ¥
docker exec netstack-rl-postgres psql -U rl_user -d rl_research -c "SELECT COUNT(*) FROM satellite_metadata;" > /dev/null
if [ $? -eq 0 ]; then
    echo "âœ… PostgreSQL: æ­£å¸¸"
else
    echo "âŒ PostgreSQL: ç•°å¸¸"
fi

# Volumeæª”æ¡ˆæª¢æŸ¥
if [ -f "/app/data/timeseries_preprocessing_outputs/animation_enhanced_starlink.json" ]; then
    echo "âœ… Volumeæª”æ¡ˆ: æ­£å¸¸"
else
    echo "âŒ Volumeæª”æ¡ˆ: éºå¤±"
fi

# å­˜å„²ç©ºé–“æª¢æŸ¥
volume_usage=$(df -h /app/data | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $volume_usage -lt 80 ]; then
    echo "âœ… å­˜å„²ç©ºé–“: ${volume_usage}% (æ­£å¸¸)"
else
    echo "âš ï¸ å­˜å„²ç©ºé–“: ${volume_usage}% (è­¦å‘Š)"
fi
```

## ğŸ“– **å­¸è¡“æ¨™æº–åƒè€ƒæ–‡ç»**

### æ•¸æ“šæ•´åˆæ¨™æº–
- **ISO/IEC 25012**: "Data quality model" - æ•¸æ“šå“è³ªç®¡ç†æ¨™æº–
- **IEEE Std 1320.2**: "Standard for Conceptual Modeling Language" - æ•¸æ“šæ¨¡å‹è¨­è¨ˆæ¨™æº–
- **FAIR Data Principles**: å¯ç™¼ç¾ã€å¯è¨ªå•ã€å¯äº’æ“ä½œã€å¯é‡ç”¨çš„æ•¸æ“šåŸå‰‡

### è³‡æ–™åº«ç³»çµ±æ¨™æº–
- **ANSI/SPARCä¸‰å±¤æ¶æ§‹**: è³‡æ–™åº«ç³»çµ±æ¶æ§‹æ¨™æº–
- **ACID Properties**: äº‹å‹™è™•ç†åŸå­æ€§ã€ä¸€è‡´æ€§ã€éš”é›¢æ€§ã€æŒä¹…æ€§è¦æ±‚
- **PostgreSQL Documentation**: å®˜æ–¹æ•ˆèƒ½èª¿å„ªå’Œé…ç½®æŒ‡å—

### æ•¸æ“šè¡€çµ±èˆ‡è¿½æº¯
- **W3C PROV Data Model**: æ•¸æ“šä¾†æºè¿½è¹¤æ¨™æº–
- **Dublin Core Metadata**: æ•¸æ“šå…ƒæ•¸æ“šæ¨™æº–
- **ISO 8601**: æ—¥æœŸæ™‚é–“æ ¼å¼åœ‹éš›æ¨™æº–

### æ¸¬é‡ä¸ç¢ºå®šåº¦èˆ‡ç²¾åº¦
- **ISO/IEC Guide 98-3**: æ¸¬é‡ä¸ç¢ºå®šåº¦è¡¨é”æŒ‡å—
- **JCGM 100:2008**: æ¸¬é‡ä¸ç¢ºå®šåº¦è©•ä¼°å’Œè¡¨é”æŒ‡å°
- **IEEE Std 754**: æµ®é»ç®—è¡“æ¨™æº– - æ•¸å€¼ç²¾åº¦ä¿è­‰

### å­˜å„²ç³»çµ±æ•ˆèƒ½æ¨™æº–
- **TPC-C Benchmark**: äº¤æ˜“è™•ç†æ€§èƒ½è©•ä¼°æ¨™æº–
- **SPEC SFS**: å­˜å„²ç³»çµ±æ€§èƒ½è©•ä¼°è¦ç¯„
- **PostgreSQL Performance**: å®˜æ–¹æ€§èƒ½èª¿å„ªæ–‡æª”

### æ•¸æ“šå®Œæ•´æ€§é©—è­‰
- **Checksum Algorithms**: MD5ã€SHA-256ç­‰æ•¸æ“šå®Œæ•´æ€§é©—è­‰ç®—æ³•
- **Database Integrity Constraints**: é—œè¯å¼è³‡æ–™åº«å®Œæ•´æ€§ç´„æŸ
- **Data Validation Techniques**: æ•¸æ“šé©—è­‰ç†è«–å’Œå¯¦å‹™

## ğŸ”§ é‡è¦ä¿®å¾©è¨˜éŒ„ (2025-08-18)

### å·²ä¿®å¾©çš„é—œéµå•é¡Œ

#### 1. PostgreSQLé€£æ¥é…ç½®éŒ¯èª¤
**å•é¡Œ**ï¼šStage5Config ä½¿ç”¨ `localhost` è€Œéå®¹å™¨ç¶²è·¯åç¨±  
**ç—‡ç‹€**ï¼šPostgreSQLæ•´åˆå¤±æ•—ï¼Œé€£æ¥è¢«æ‹’  
**ä¿®æ­£**ï¼š
```python
# ä¿®æ­£å‰
postgres_host: str = "localhost"

# ä¿®æ­£å¾Œ  
postgres_host: str = "netstack-postgres"
```

#### 2. æ™‚åºæ•¸æ“šæ¬„ä½åç¨±ä¸ä¸€è‡´
**å•é¡Œ**ï¼šä»£ç¢¼æŸ¥æ‰¾ `timeseries` ä½†æ•¸æ“šä½¿ç”¨ `position_timeseries`  
**ç—‡ç‹€**ï¼šåˆ†å±¤æ¿¾æ³¢ç”¢ç”Ÿ0é¡†è¡›æ˜Ÿ  
**ä¿®æ­£**ï¼š
```python
# ä¿®æ­£å‰
for point in satellite.get('timeseries', []):

# ä¿®æ­£å¾Œ
timeseries_data = satellite.get('position_timeseries', satellite.get('timeseries', []))
for point in timeseries_data:
```

#### 3. åˆ†å±¤æ¿¾æ³¢é‚è¼¯å®Œæ•´ä¿®æ­£
**æˆæœ**ï¼š
- elevation_5deg: 399é¡†è¡›æ˜Ÿ (100%ä¿ç•™)
- elevation_10deg: 351é¡†è¡›æ˜Ÿ (87.9%ä¿ç•™) 
- elevation_15deg: 277é¡†è¡›æ˜Ÿ (69.4%ä¿ç•™)

**æª”æ¡ˆå¤§å°**ï¼š
- Starlink: 4.9MB (5Â°) â†’ 3.5MB (10Â°) â†’ 2.5MB (15Â°)
- OneWeb: 560KB (5Â°) â†’ 477KB (10Â°) â†’ 339KB (15Â°)

### ä¿®å¾©é©—è­‰
```bash
# é©—è­‰åˆ†å±¤æ•¸æ“šç”Ÿæˆ
ls -lh /app/data/layered_phase0_enhanced/elevation_*/

# é©—è­‰PostgreSQLé…ç½®
python -c "from stages.data_integration_processor import Stage5Config; print(Stage5Config().postgres_host)"

# é©—è­‰æ•¸æ“šå®Œæ•´æ€§
python -c "import json; data=json.load(open('starlink_with_3gpp_events.json')); print(f'è¡›æ˜Ÿæ•¸: {len(data[\"satellites\"])}')"
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **PostgreSQLé€£æ¥å¤±æ•—**
   - æª¢æŸ¥ï¼šå®¹å™¨ç‹€æ…‹å’Œé€£æ¥å­—ä¸²
   - è§£æ±ºï¼šç¢ºèªä½¿ç”¨ `netstack-postgres` è€Œé `localhost`

2. **åˆ†å±¤æ¿¾æ³¢ç”¢ç”Ÿç©ºçµæœ** 
   - æª¢æŸ¥ï¼šæ™‚åºæ•¸æ“šæ¬„ä½åç¨±ä¸€è‡´æ€§
   - è§£æ±ºï¼šä½¿ç”¨ `position_timeseries` æ¬„ä½

3. **Volumeæª”æ¡ˆæ¬Šé™å•é¡Œ**
   - æª¢æŸ¥ï¼šæª”æ¡ˆæ‰€æœ‰æ¬Šå’Œæ¬Šé™
   - è§£æ±ºï¼š`chown -R app:app /app/data`

4. **æ··åˆæŸ¥è©¢æ€§èƒ½å·®**
   - æª¢æŸ¥ï¼šPostgreSQLç´¢å¼•ä½¿ç”¨
   - è§£æ±ºï¼šåˆ†ææŸ¥è©¢è¨ˆåŠƒä¸¦å„ªåŒ–ç´¢å¼•

### è¨ºæ–·æŒ‡ä»¤

```bash
# æª¢æŸ¥PostgreSQLæ•¸æ“š
docker exec netstack-rl-postgres psql -U rl_user -d rl_research -c "
SELECT 
    constellation,
    COUNT(*) as satellite_count,
    AVG(mean_rsrp_dbm) as avg_signal
FROM satellite_metadata sm 
LEFT JOIN signal_quality_statistics sqs ON sm.satellite_id = sqs.satellite_id
GROUP BY constellation;
"

# æª¢æŸ¥Volumeæª”æ¡ˆå®Œæ•´æ€§
find /app/data -name "*.json" -exec echo "æª¢æŸ¥: {}" \; -exec python -m json.tool {} > /dev/null \;

# æª¢æŸ¥æ•´åˆç‹€æ…‹
curl -s http://localhost:8080/api/v1/data-integration/status | jq
```

## âœ… éšæ®µé©—è­‰æ¨™æº–

### ğŸ¯ Stage 5 å®Œæˆé©—è­‰æª¢æŸ¥æ¸…å–®

#### 1. **è¼¸å…¥é©—è­‰**
- [ ] å¤šæºæ•¸æ“šå®Œæ•´æ€§
  - Stage 3ä¿¡è™Ÿåˆ†æçµæœ
  - Stage 4æ™‚é–“åºåˆ—æ•¸æ“š
  - åŸºç¤è¡›æ˜Ÿå…ƒæ•¸æ“š
- [ ] æ•¸æ“šæ™‚é–“æˆ³ä¸€è‡´æ€§
  - å„éšæ®µæ•¸æ“šæ™‚é–“å°é½Š
  - ç„¡æ™‚é–“å·®ç•°éŒ¯èª¤

#### 2. **åˆ†å±¤æ•¸æ“šç”Ÿæˆé©—è­‰**
- [ ] **ä»°è§’åˆ†å±¤æ­£ç¢ºæ€§**
  ```
  åˆ†å±¤é–€æª»:
  - 5åº¦å±¤: å…¨éƒ¨è¡›æ˜Ÿ
  - 10åº¦å±¤: ä»°è§’â‰¥10Â°çš„è¡›æ˜Ÿ
  - 15åº¦å±¤: ä»°è§’â‰¥15Â°çš„è¡›æ˜Ÿ
  æ•¸é‡éæ¸›é©—è­‰: 5åº¦ > 10åº¦ > 15åº¦
  ```
- [ ] **æ¯å±¤æ•¸æ“šå®Œæ•´æ€§**
  - æ™‚é–“åºåˆ—ä¿ç•™
  - ä¿¡è™ŸæŒ‡æ¨™å®Œæ•´
  - å¯è¦‹æ€§çª—å£æ­£ç¢º

#### 3. **PostgreSQLæ•´åˆé©—è­‰**
- [ ] **æ•¸æ“šåº«é€£æ¥**
  - é€£æ¥æˆåŠŸï¼ˆ172.20.0.51:5432ï¼‰
  - è³‡æ–™è¡¨å‰µå»ºå®Œæˆ
  - ç´¢å¼•å»ºç«‹æ­£ç¢º
- [ ] **æ•¸æ“šå¯«å…¥é©—è­‰**
  ```sql
  é æœŸè¨˜éŒ„æ•¸:
  - satellite_tle_data: 1,100+ç­†
  - satellite_signal_metrics: 200,000+ç­†
  - handover_events: 300+ç­†
  ```

#### 4. **è¼¸å‡ºé©—è­‰**
- [ ] **æ··åˆå­˜å„²çµæ§‹**
  ```json
  {
    "metadata": {
      "stage": "stage5_data_integration",
      "storage_mode": "hybrid",
      "postgresql_status": "connected",
      "volume_status": "active"
    },
    "integration_summary": {
      "elevation_5deg": {"count": 1196},
      "elevation_10deg": {"count": 900},
      "elevation_15deg": {"count": 600}
    }
  }
  ```
- [ ] **å­˜å„²åˆ†ä½ˆåˆç†**
  - PostgreSQL: < 50MBï¼ˆçµæ§‹åŒ–æ•¸æ“šï¼‰
  - Volume: < 450MBï¼ˆæ™‚é–“åºåˆ—ï¼‰
  - ç¸½è¨ˆ: < 500MB

#### 5. **æ€§èƒ½æŒ‡æ¨™**
- [ ] è™•ç†æ™‚é–“ < 1åˆ†é˜
- [ ] è³‡æ–™åº«å¯«å…¥é€Ÿåº¦ > 1000ç­†/ç§’
- [ ] è¨˜æ†¶é«”ä½¿ç”¨ < 500MB

#### 6. **è‡ªå‹•é©—è­‰è…³æœ¬**
```python
# åŸ·è¡Œéšæ®µé©—è­‰
python -c "
import json
import os
import psycopg2

# æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆ
output_file = '/app/data/data_integration_outputs/integrated_data_output.json'
if os.path.exists(output_file):
    with open(output_file, 'r') as f:
        data = json.load(f)
    
    metadata = data.get('metadata', {})
    summary = data.get('integration_summary', {})
    
    # æª¢æŸ¥åˆ†å±¤æ•¸æ“š
    elev_5 = summary.get('elevation_5deg', {}).get('count', 0)
    elev_10 = summary.get('elevation_10deg', {}).get('count', 0)
    elev_15 = summary.get('elevation_15deg', {}).get('count', 0)
else:
    elev_5 = elev_10 = elev_15 = 0

# æª¢æŸ¥PostgreSQL
try:
    conn = psycopg2.connect(
        host='172.20.0.51',
        database='rl_research',
        user='rl_user',
        password='rl_password'
    )
    cur = conn.cursor()
    cur.execute('SELECT COUNT(*) FROM satellite_tle_data')
    db_count = cur.fetchone()[0]
    conn.close()
    db_connected = True
except:
    db_count = 0
    db_connected = False

checks = {
    'output_exists': os.path.exists(output_file),
    'elevation_layers': elev_5 > elev_10 > elev_15 > 0,
    'layer_5deg_ok': elev_5 > 1000,
    'layer_10deg_ok': elev_10 > 800,
    'layer_15deg_ok': elev_15 > 500,
    'db_connected': db_connected,
    'db_has_data': db_count > 1000
}

passed = sum(checks.values())
total = len(checks)

print('ğŸ“Š Stage 5 é©—è­‰çµæœ:')
print(f'  åˆ†å±¤æ•¸æ“š: 5åº¦({elev_5}) > 10åº¦({elev_10}) > 15åº¦({elev_15})')
print(f'  è³‡æ–™åº«ç‹€æ…‹: {\"é€£æ¥æˆåŠŸ\" if db_connected else \"é€£æ¥å¤±æ•—\"}')
print(f'  è³‡æ–™åº«è¨˜éŒ„: {db_count}ç­†')

for check, result in checks.items():
    print(f'  {\"âœ…\" if result else \"âŒ\"} {check}')

if passed == total:
    print('âœ… Stage 5 é©—è­‰é€šéï¼')
else:
    print(f'âŒ Stage 5 é©—è­‰å¤±æ•— ({passed}/{total})')
    exit(1)
"
```

### ğŸš¨ é©—è­‰å¤±æ•—è™•ç†
1. **åˆ†å±¤æ•¸æ“šç•°å¸¸**: æª¢æŸ¥ä»°è§’é–€æª»è¨­å®š
2. **è³‡æ–™åº«é€£æ¥å¤±æ•—**: ç¢ºèªPostgreSQLæœå‹™ç‹€æ…‹
3. **å­˜å„²è¶…é™**: å„ªåŒ–æ•¸æ“šçµæ§‹ã€å¢åŠ å£“ç¸®

### ğŸ“Š é—œéµæŒ‡æ¨™
- **åˆ†å±¤æ­£ç¢ºæ€§**: 5åº¦ > 10åº¦ > 15åº¦éæ¸›
- **æ··åˆå­˜å„²**: PostgreSQL + Volumeå”åŒ
- **æ€§èƒ½å¹³è¡¡**: æŸ¥è©¢é€Ÿåº¦èˆ‡å­˜å„²æ•ˆç‡

---
**ä¸Šä¸€éšæ®µ**: [éšæ®µå››ï¼šæ™‚é–“åºåˆ—é è™•ç†](./stage4-timeseries.md)  
**ä¸‹ä¸€éšæ®µ**: [éšæ®µå…­ï¼šå‹•æ…‹æ± è¦åŠƒ](./stage6-dynamic-pool.md)  
**ç›¸é—œæ–‡æª”**: [PostgreSQLè¨­å®š](../system_architecture.md#postgresql-configuration)