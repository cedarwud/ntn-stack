#!/usr/bin/env python3
"""
Academic Compliance Validator - å­¸è¡“åˆè¦é©—è­‰å™¨

å°ˆè²¬å­¸è¡“æ¨™æº–å’Œç‰©ç†åˆè¦é©—è­‰ï¼š
- ç‰©ç†å®šå¾‹åˆè¦æ€§é©—è­‰
- å­¸è¡“æ¨™æº–æª¢æŸ¥
- å¯é æ€§è©•ä¼°
- æ•¸æ“šçœŸå¯¦æ€§é©—è­‰

å¾ ValidationEngine ä¸­æ‹†åˆ†å‡ºä¾†ï¼Œå°ˆæ³¨æ–¼å­¸è¡“åˆè¦åŠŸèƒ½
"""

import logging
from typing import Dict, List, Any, Optional
import numpy as np

class AcademicComplianceValidator:
    """
    å­¸è¡“åˆè¦é©—è­‰å™¨

    å°ˆè²¬ç‰©ç†å®šå¾‹å’Œå­¸è¡“æ¨™æº–çš„åˆè¦æ€§é©—è­‰
    """

    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–å­¸è¡“åˆè¦é©—è­‰å™¨"""
        self.logger = logging.getLogger(f"{__name__}.AcademicComplianceValidator")
        self.config = config or self._get_default_validation_config()

        self.validation_stats = {
            'physics_checks_performed': 0,
            'physics_checks_passed': 0,
            'academic_checks_performed': 0,
            'academic_checks_passed': 0,
            'reliability_assessments': 0
        }

        # å­¸è¡“é©—è­‰æ¨™æº–
        self.validation_standards = {
            "academic_grade_target": "A+",
            "physics_compliance_required": True,
            "data_authenticity_required": True,
            "calculation_method_verification": True,
            "reproducibility_required": True
        }

        self.logger.info("âœ… Academic Compliance Validator åˆå§‹åŒ–å®Œæˆ")

    def validate_physics_compliance(self, pool_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        é©—è­‰ç‰©ç†å®šå¾‹åˆè¦æ€§

        Args:
            pool_data: è¡›æ˜Ÿæ± æ•¸æ“š

        Returns:
            ç‰©ç†åˆè¦é©—è­‰çµæœ
        """
        try:
            self.logger.info("ğŸ” é–‹å§‹ç‰©ç†å®šå¾‹åˆè¦æ€§é©—è­‰...")

            physics_result = {
                "physics_validation": {
                    "passed": True,
                    "compliance_score": 0.0,
                    "physics_checks": {},
                    "violations": [],
                    "recommendations": []
                }
            }

            satellites = pool_data.get('satellites', [])
            if not satellites:
                physics_result["physics_validation"]["passed"] = False
                return physics_result

            # é©—è­‰RSRPç‰©ç†åˆç†æ€§
            rsrp_validation = self._validate_rsrp_physics(satellites)
            physics_result["physics_validation"]["physics_checks"]["rsrp_validation"] = rsrp_validation

            # é©—è­‰ä»°è§’ç‰©ç†åˆç†æ€§
            elevation_validation = self._validate_elevation_physics(satellites)
            physics_result["physics_validation"]["physics_checks"]["elevation_validation"] = elevation_validation

            # é©—è­‰è·é›¢è¨ˆç®—åˆç†æ€§
            distance_validation = self._validate_distance_physics(satellites)
            physics_result["physics_validation"]["physics_checks"]["distance_validation"] = distance_validation

            # é©—è­‰è»Œé“åƒæ•¸ä¸€è‡´æ€§
            orbital_consistency = self._validate_orbital_consistency(satellites)
            physics_result["physics_validation"]["physics_checks"]["orbital_consistency"] = orbital_consistency

            # è¨ˆç®—æ•´é«”ç‰©ç†åˆè¦åˆ†æ•¸
            compliance_score = self._calculate_physics_compliance_score([
                rsrp_validation, elevation_validation, distance_validation, orbital_consistency
            ])
            physics_result["physics_validation"]["compliance_score"] = compliance_score

            # æª¢æŸ¥æ˜¯å¦é€šé
            min_compliance_score = self.config.get('min_physics_compliance_score', 0.8)
            if compliance_score < min_compliance_score:
                physics_result["physics_validation"]["passed"] = False
                physics_result["physics_validation"]["violations"].append(
                    f"ç‰©ç†åˆè¦åˆ†æ•¸ {compliance_score:.3f} ä½æ–¼è¦æ±‚ {min_compliance_score}"
                )

            # æ›´æ–°çµ±è¨ˆ
            self.validation_stats['physics_checks_performed'] += 1
            if physics_result["physics_validation"]["passed"]:
                self.validation_stats['physics_checks_passed'] += 1

            self.logger.info(f"âœ… ç‰©ç†å®šå¾‹åˆè¦æ€§é©—è­‰å®Œæˆï¼Œåˆ†æ•¸: {compliance_score:.3f}")
            return physics_result

        except Exception as e:
            self.logger.error(f"âŒ ç‰©ç†å®šå¾‹åˆè¦æ€§é©—è­‰å¤±æ•—: {e}")
            return {
                "physics_validation": {
                    "passed": False,
                    "error": str(e)
                }
            }

    def validate_academic_standards(self, pool_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        é©—è­‰å­¸è¡“æ¨™æº–

        Args:
            pool_data: è¡›æ˜Ÿæ± æ•¸æ“š

        Returns:
            å­¸è¡“æ¨™æº–é©—è­‰çµæœ
        """
        try:
            self.logger.info("ğŸ” é–‹å§‹å­¸è¡“æ¨™æº–é©—è­‰...")

            academic_result = {
                "academic_validation": {
                    "passed": True,
                    "grade_achieved": "Unknown",
                    "standard_checks": {},
                    "compliance_areas": {},
                    "improvement_recommendations": []
                }
            }

            # æª¢æŸ¥æ•¸æ“šçœŸå¯¦æ€§
            authenticity_result = self._check_data_authenticity(pool_data)
            academic_result["academic_validation"]["standard_checks"]["data_authenticity"] = authenticity_result

            # æª¢æŸ¥è¨ˆç®—æ–¹æ³•
            calculation_result = self._analyze_calculation_methods(pool_data)
            academic_result["academic_validation"]["standard_checks"]["calculation_methods"] = calculation_result

            # æª¢æŸ¥å¯é‡ç¾æ€§
            reproducibility_result = self._assess_reproducibility(pool_data)
            academic_result["academic_validation"]["standard_checks"]["reproducibility"] = reproducibility_result

            # æª¢æŸ¥æ–‡æª”å®Œæ•´æ€§
            documentation_result = self._check_documentation_completeness(pool_data)
            academic_result["academic_validation"]["standard_checks"]["documentation"] = documentation_result

            # è©•ä¼°å­¸è¡“ç­‰ç´š
            grade_achieved = self._calculate_academic_grade([
                authenticity_result, calculation_result, reproducibility_result, documentation_result
            ])
            academic_result["academic_validation"]["grade_achieved"] = grade_achieved

            # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™ç­‰ç´š
            target_grade = self.validation_standards.get("academic_grade_target", "A+")
            if not self._is_grade_sufficient(grade_achieved, target_grade):
                academic_result["academic_validation"]["passed"] = False
                academic_result["academic_validation"]["improvement_recommendations"].append(
                    f"ç•¶å‰ç­‰ç´š {grade_achieved} æœªé”åˆ°ç›®æ¨™ {target_grade}"
                )

            # æ›´æ–°çµ±è¨ˆ
            self.validation_stats['academic_checks_performed'] += 1
            if academic_result["academic_validation"]["passed"]:
                self.validation_stats['academic_checks_passed'] += 1

            self.logger.info(f"âœ… å­¸è¡“æ¨™æº–é©—è­‰å®Œæˆï¼Œç­‰ç´š: {grade_achieved}")
            return academic_result

        except Exception as e:
            self.logger.error(f"âŒ å­¸è¡“æ¨™æº–é©—è­‰å¤±æ•—: {e}")
            return {
                "academic_validation": {
                    "passed": False,
                    "error": str(e)
                }
            }

    def assess_reliability(self, pool_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        è©•ä¼°å¯é æ€§

        Args:
            pool_data: è¡›æ˜Ÿæ± æ•¸æ“š

        Returns:
            å¯é æ€§è©•ä¼°çµæœ
        """
        try:
            self.logger.info("ğŸ” é–‹å§‹å¯é æ€§è©•ä¼°...")

            reliability_result = {
                "reliability_assessment": {
                    "overall_reliability": 0.0,
                    "reliability_factors": {},
                    "risk_assessment": {},
                    "recommendations": []
                }
            }

            # æ•¸æ“šä¸€è‡´æ€§å¯é æ€§
            data_consistency = self._assess_data_consistency(pool_data)
            reliability_result["reliability_assessment"]["reliability_factors"]["data_consistency"] = data_consistency

            # è¨ˆç®—æ–¹æ³•å¯é æ€§
            method_reliability = self._assess_method_reliability(pool_data)
            reliability_result["reliability_assessment"]["reliability_factors"]["method_reliability"] = method_reliability

            # çµæœé‡ç¾æ€§å¯é æ€§
            reproducibility_reliability = self._assess_reproducibility_reliability(pool_data)
            reliability_result["reliability_assessment"]["reliability_factors"]["reproducibility"] = reproducibility_reliability

            # è¨ˆç®—æ•´é«”å¯é æ€§
            overall_reliability = self._calculate_overall_reliability([
                data_consistency, method_reliability, reproducibility_reliability
            ])
            reliability_result["reliability_assessment"]["overall_reliability"] = overall_reliability

            # é¢¨éšªè©•ä¼°
            risk_assessment = self._perform_risk_assessment(overall_reliability)
            reliability_result["reliability_assessment"]["risk_assessment"] = risk_assessment

            # ç”Ÿæˆå¯é æ€§å»ºè­°
            recommendations = self._generate_reliability_recommendation(overall_reliability, risk_assessment)
            reliability_result["reliability_assessment"]["recommendations"] = recommendations

            # æ›´æ–°çµ±è¨ˆ
            self.validation_stats['reliability_assessments'] += 1

            self.logger.info(f"âœ… å¯é æ€§è©•ä¼°å®Œæˆï¼Œå¯é æ€§: {overall_reliability:.3f}")
            return reliability_result

        except Exception as e:
            self.logger.error(f"âŒ å¯é æ€§è©•ä¼°å¤±æ•—: {e}")
            return {
                "reliability_assessment": {
                    "overall_reliability": 0.0,
                    "error": str(e)
                }
            }

    # ===== ç§æœ‰æ–¹æ³• =====

    def _validate_rsrp_physics(self, satellites: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é©—è­‰RSRPç‰©ç†åˆç†æ€§"""
        try:
            result = {"passed": True, "violations": [], "metrics": {}}

            rsrp_values = [s.get('rsrp', -120) for s in satellites if 'rsrp' in s]
            if not rsrp_values:
                result["passed"] = False
                result["violations"].append("ç¼ºå°‘RSRPæ•¸æ“š")
                return result

            # æª¢æŸ¥RSRPç¯„åœåˆç†æ€§
            min_rsrp, max_rsrp = min(rsrp_values), max(rsrp_values)
            if min_rsrp < -140 or max_rsrp > -30:
                result["passed"] = False
                result["violations"].append(f"RSRPç¯„åœç•°å¸¸: {min_rsrp} ~ {max_rsrp} dBm")

            # æª¢æŸ¥RSRPåˆ†å¸ƒåˆç†æ€§
            rsrp_std = np.std(rsrp_values)
            if rsrp_std > 30:  # æ¨™æº–å·®éå¤§å¯èƒ½è¡¨ç¤ºæ•¸æ“šä¸ä¸€è‡´
                result["violations"].append(f"RSRPåˆ†å¸ƒç•°å¸¸ï¼Œæ¨™æº–å·®: {rsrp_std:.2f}")

            result["metrics"] = {
                "min_rsrp": min_rsrp,
                "max_rsrp": max_rsrp,
                "mean_rsrp": np.mean(rsrp_values),
                "std_rsrp": rsrp_std,
                "sample_count": len(rsrp_values)
            }

            return result

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _validate_elevation_physics(self, satellites: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é©—è­‰ä»°è§’ç‰©ç†åˆç†æ€§"""
        try:
            result = {"passed": True, "violations": [], "metrics": {}}

            elevations = [s.get('elevation', 0) for s in satellites if 'elevation' in s]
            if not elevations:
                result["passed"] = False
                result["violations"].append("ç¼ºå°‘ä»°è§’æ•¸æ“š")
                return result

            # æª¢æŸ¥ä»°è§’ç¯„åœ
            min_elevation, max_elevation = min(elevations), max(elevations)
            if min_elevation < 0 or max_elevation > 90:
                result["passed"] = False
                result["violations"].append(f"ä»°è§’ç¯„åœç•°å¸¸: {min_elevation}Â° ~ {max_elevation}Â°")

            # æª¢æŸ¥ä½ä»°è§’è¡›æ˜Ÿæ¯”ä¾‹ï¼ˆæ‡‰è©²è¼ƒå°‘ï¼‰
            low_elevation_count = sum(1 for e in elevations if e < 10)
            if low_elevation_count / len(elevations) > 0.3:
                result["violations"].append(f"ä½ä»°è§’è¡›æ˜Ÿæ¯”ä¾‹éé«˜: {low_elevation_count}/{len(elevations)}")

            result["metrics"] = {
                "min_elevation": min_elevation,
                "max_elevation": max_elevation,
                "mean_elevation": np.mean(elevations),
                "low_elevation_ratio": low_elevation_count / len(elevations)
            }

            return result

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _validate_distance_physics(self, satellites: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é©—è­‰è·é›¢è¨ˆç®—åˆç†æ€§"""
        try:
            result = {"passed": True, "violations": [], "metrics": {}}

            distances = [s.get('distance', 0) for s in satellites if 'distance' in s]
            if not distances:
                result["violations"].append("ç¼ºå°‘è·é›¢æ•¸æ“š")
                return result

            # LEOè¡›æ˜Ÿå…¸å‹è·é›¢ç¯„åœæª¢æŸ¥
            min_distance, max_distance = min(distances), max(distances)
            if min_distance < 500000 or max_distance > 2000000:  # 500-2000km
                result["violations"].append(f"è¡›æ˜Ÿè·é›¢ç¯„åœç•°å¸¸: {min_distance/1000:.1f} ~ {max_distance/1000:.1f} km")

            result["metrics"] = {
                "min_distance_km": min_distance / 1000,
                "max_distance_km": max_distance / 1000,
                "mean_distance_km": np.mean(distances) / 1000
            }

            return result

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _validate_orbital_consistency(self, satellites: List[Dict[str, Any]]) -> Dict[str, Any]:
        """é©—è­‰è»Œé“åƒæ•¸ä¸€è‡´æ€§"""
        try:
            result = {"passed": True, "violations": [], "metrics": {}}

            # æª¢æŸ¥ä¸åŒæ˜Ÿåº§çš„è»Œé“é«˜åº¦ä¸€è‡´æ€§
            starlink_elevations = [s.get('elevation', 0) for s in satellites
                                 if 'starlink' in s.get('constellation', '').lower()]
            oneweb_elevations = [s.get('elevation', 0) for s in satellites
                               if 'oneweb' in s.get('constellation', '').lower()]

            if starlink_elevations and oneweb_elevations:
                starlink_mean = np.mean(starlink_elevations)
                oneweb_mean = np.mean(oneweb_elevations)

                # åŒä¸€æ˜Ÿåº§çš„ä»°è§’æ‡‰è©²ç›¸å°é›†ä¸­
                if abs(starlink_mean - oneweb_mean) < 5:  # å¤ªæ¥è¿‘å¯èƒ½ä¸æ­£å¸¸
                    result["violations"].append("ä¸åŒæ˜Ÿåº§ä»°è§’åˆ†å¸ƒéæ–¼ç›¸ä¼¼")

            result["metrics"] = {
                "starlink_mean_elevation": np.mean(starlink_elevations) if starlink_elevations else 0,
                "oneweb_mean_elevation": np.mean(oneweb_elevations) if oneweb_elevations else 0,
                "starlink_count": len(starlink_elevations),
                "oneweb_count": len(oneweb_elevations)
            }

            return result

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _calculate_physics_compliance_score(self, validation_results: List[Dict[str, Any]]) -> float:
        """è¨ˆç®—ç‰©ç†åˆè¦åˆ†æ•¸"""
        try:
            passed_count = sum(1 for result in validation_results if result.get("passed", False))
            total_count = len(validation_results)

            if total_count == 0:
                return 0.0

            base_score = passed_count / total_count

            # æ ¹æ“šé•è¦æ•¸é‡èª¿æ•´åˆ†æ•¸
            total_violations = sum(len(result.get("violations", [])) for result in validation_results)
            violation_penalty = min(0.5, total_violations * 0.1)

            final_score = max(0.0, base_score - violation_penalty)
            return final_score

        except Exception as e:
            self.logger.warning(f"âš ï¸ ç‰©ç†åˆè¦åˆ†æ•¸è¨ˆç®—å¤±æ•—: {e}")
            return 0.0

    def _check_data_authenticity(self, pool_data: Dict[str, Any]) -> Dict[str, Any]:
        """æª¢æŸ¥æ•¸æ“šçœŸå¯¦æ€§"""
        try:
            result = {
                "passed": True,
                "authenticity_score": 0.0,
                "checks": {},
                "flags": []
            }

            satellites = pool_data.get('satellites', [])

            # æª¢æŸ¥æ•¸æ“šä¾†æºæ¨™è¨˜
            has_source_info = any('data_source' in s for s in satellites)
            result["checks"]["source_info"] = has_source_info
            if not has_source_info:
                result["flags"].append("ç¼ºå°‘æ•¸æ“šä¾†æºä¿¡æ¯")

            # æª¢æŸ¥æ™‚é–“æˆ³ä¸€è‡´æ€§
            timestamps = [s.get('timestamp') for s in satellites if 'timestamp' in s]
            timestamp_consistency = len(set(timestamps)) <= len(satellites) * 0.1  # æ™‚é–“æˆ³æ‡‰è©²ç›¸å°é›†ä¸­
            result["checks"]["timestamp_consistency"] = timestamp_consistency
            if not timestamp_consistency:
                result["flags"].append("æ™‚é–“æˆ³åˆ†å¸ƒç•°å¸¸")

            # æª¢æŸ¥æ˜¯å¦æœ‰æ˜é¡¯çš„æ¨¡æ“¬æ•¸æ“šç‰¹å¾µ
            rsrp_values = [s.get('rsrp', -120) for s in satellites if 'rsrp' in s]
            if rsrp_values:
                # æª¢æŸ¥æ˜¯å¦æœ‰éæ–¼è¦å¾‹çš„æ•¸å€¼
                rounded_values = [round(r) for r in rsrp_values]
                unique_ratio = len(set(rounded_values)) / len(rounded_values)
                if unique_ratio < 0.5:  # ç¨ç‰¹å€¼æ¯”ä¾‹éä½å¯èƒ½æ˜¯æ¨¡æ“¬æ•¸æ“š
                    result["flags"].append("æ•¸å€¼éæ–¼è¦å¾‹ï¼Œå¯èƒ½ç‚ºæ¨¡æ“¬æ•¸æ“š")

            # è¨ˆç®—çœŸå¯¦æ€§åˆ†æ•¸
            authenticity_score = len([c for c in result["checks"].values() if c]) / len(result["checks"])
            penalty = len(result["flags"]) * 0.2
            result["authenticity_score"] = max(0.0, authenticity_score - penalty)

            if result["authenticity_score"] < 0.7:
                result["passed"] = False

            return result

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _analyze_calculation_methods(self, pool_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¨ˆç®—æ–¹æ³•"""
        try:
            result = {
                "passed": True,
                "method_score": 0.0,
                "method_checks": {},
                "recommendations": []
            }

            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨æ¨™æº–ç®—æ³•
            metadata = pool_data.get('metadata', {})
            calculation_method = metadata.get('calculation_method', '')

            standard_methods = ['sgp4', 'friis', 'itu-r', '3gpp']
            uses_standard_methods = any(method in calculation_method.lower() for method in standard_methods)
            result["method_checks"]["standard_algorithms"] = uses_standard_methods

            if not uses_standard_methods:
                result["recommendations"].append("å»ºè­°ä½¿ç”¨æ¨™æº–è¨ˆç®—ç®—æ³• (SGP4, Friisç­‰)")

            # æª¢æŸ¥è¨ˆç®—åƒæ•¸çš„å®Œæ•´æ€§
            has_physics_constants = 'physics_constants' in metadata
            result["method_checks"]["physics_constants"] = has_physics_constants

            if not has_physics_constants:
                result["recommendations"].append("ç¼ºå°‘ç‰©ç†å¸¸æ•¸å®šç¾©")

            # è¨ˆç®—æ–¹æ³•åˆ†æ•¸
            passed_checks = sum(result["method_checks"].values())
            total_checks = len(result["method_checks"])
            result["method_score"] = passed_checks / total_checks if total_checks > 0 else 0

            if result["method_score"] < 0.8:
                result["passed"] = False

            return result

        except Exception as e:
            return {"passed": False, "error": str(e)}

    def _assess_reproducibility(self, pool_data: Dict[str, Any]) -> Dict[str, Any]:
        """è©•ä¼°å¯é‡ç¾æ€§"""
        try:
            result = {
                "reproducible": True,
                "reproducibility_score": 0.0,
                "factors": {}
            }

            metadata = pool_data.get('metadata', {})

            # æª¢æŸ¥é…ç½®åƒæ•¸çš„å®Œæ•´æ€§
            has_config = 'configuration' in metadata or 'config' in metadata
            result["factors"]["configuration_available"] = has_config

            # æª¢æŸ¥ç‰ˆæœ¬ä¿¡æ¯
            has_version = 'version' in metadata or 'algorithm_version' in metadata
            result["factors"]["version_info"] = has_version

            # æª¢æŸ¥éš¨æ©Ÿç¨®å­è¨­å®š
            has_seed = 'random_seed' in metadata or 'seed' in metadata
            result["factors"]["deterministic_seed"] = has_seed

            # è¨ˆç®—å¯é‡ç¾æ€§åˆ†æ•¸
            reproducibility_score = sum(result["factors"].values()) / len(result["factors"])
            result["reproducibility_score"] = reproducibility_score

            if reproducibility_score < 0.6:
                result["reproducible"] = False

            return result

        except Exception as e:
            return {"reproducible": False, "error": str(e)}

    def _check_documentation_completeness(self, pool_data: Dict[str, Any]) -> Dict[str, Any]:
        """æª¢æŸ¥æ–‡æª”å®Œæ•´æ€§"""
        try:
            result = {
                "complete": True,
                "completeness_score": 0.0,
                "missing_elements": []
            }

            metadata = pool_data.get('metadata', {})

            required_documentation = [
                'description', 'methodology', 'parameters', 'assumptions', 'limitations'
            ]

            available_elements = sum(1 for element in required_documentation if element in metadata)
            result["completeness_score"] = available_elements / len(required_documentation)

            for element in required_documentation:
                if element not in metadata:
                    result["missing_elements"].append(element)

            if result["completeness_score"] < 0.7:
                result["complete"] = False

            return result

        except Exception as e:
            return {"complete": False, "error": str(e)}

    def _calculate_academic_grade(self, validation_results: List[Dict[str, Any]]) -> str:
        """è¨ˆç®—å­¸è¡“ç­‰ç´š"""
        try:
            # è¨ˆç®—é€šéçš„æª¢æŸ¥æ¯”ä¾‹
            total_checks = len(validation_results)
            passed_checks = sum(1 for result in validation_results if result.get("passed", result.get("complete", result.get("reproducible", False))))

            if total_checks == 0:
                return "F"

            pass_rate = passed_checks / total_checks

            if pass_rate >= 0.95:
                return "A+"
            elif pass_rate >= 0.9:
                return "A"
            elif pass_rate >= 0.85:
                return "A-"
            elif pass_rate >= 0.8:
                return "B+"
            elif pass_rate >= 0.75:
                return "B"
            elif pass_rate >= 0.7:
                return "B-"
            elif pass_rate >= 0.65:
                return "C+"
            elif pass_rate >= 0.6:
                return "C"
            else:
                return "F"

        except Exception as e:
            self.logger.warning(f"âš ï¸ å­¸è¡“ç­‰ç´šè¨ˆç®—å¤±æ•—: {e}")
            return "F"

    def _is_grade_sufficient(self, achieved_grade: str, target_grade: str) -> bool:
        """æª¢æŸ¥ç­‰ç´šæ˜¯å¦è¶³å¤ """
        grade_values = {
            "A+": 100, "A": 95, "A-": 90, "B+": 85, "B": 80,
            "B-": 75, "C+": 70, "C": 65, "C-": 60, "F": 0
        }

        achieved_value = grade_values.get(achieved_grade, 0)
        target_value = grade_values.get(target_grade, 100)

        return achieved_value >= target_value

    def _assess_data_consistency(self, pool_data: Dict[str, Any]) -> float:
        """è©•ä¼°æ•¸æ“šä¸€è‡´æ€§"""
        try:
            satellites = pool_data.get('satellites', [])
            if not satellites:
                return 0.0

            # æª¢æŸ¥å¿…éœ€å­—æ®µçš„å®Œæ•´æ€§
            required_fields = ['satellite_id', 'constellation', 'rsrp', 'elevation']
            field_completeness = []

            for field in required_fields:
                complete_count = sum(1 for s in satellites if field in s and s[field] is not None)
                completeness = complete_count / len(satellites)
                field_completeness.append(completeness)

            return np.mean(field_completeness)

        except Exception as e:
            self.logger.warning(f"âš ï¸ æ•¸æ“šä¸€è‡´æ€§è©•ä¼°å¤±æ•—: {e}")
            return 0.0

    def _assess_method_reliability(self, pool_data: Dict[str, Any]) -> float:
        """è©•ä¼°æ–¹æ³•å¯é æ€§"""
        try:
            metadata = pool_data.get('metadata', {})

            reliability_factors = []

            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨å·²é©—è­‰çš„ç®—æ³•
            calculation_method = metadata.get('calculation_method', '').lower()
            verified_algorithms = ['sgp4', 'friis', 'itu-r']
            uses_verified = any(alg in calculation_method for alg in verified_algorithms)
            reliability_factors.append(1.0 if uses_verified else 0.5)

            # æª¢æŸ¥æ˜¯å¦æœ‰èª¤å·®åˆ†æ
            has_error_analysis = 'error_analysis' in metadata or 'uncertainty' in metadata
            reliability_factors.append(1.0 if has_error_analysis else 0.0)

            # æª¢æŸ¥æ˜¯å¦æœ‰äº¤å‰é©—è­‰
            has_validation = 'validation' in metadata or 'cross_validation' in metadata
            reliability_factors.append(1.0 if has_validation else 0.0)

            return np.mean(reliability_factors) if reliability_factors else 0.0

        except Exception as e:
            self.logger.warning(f"âš ï¸ æ–¹æ³•å¯é æ€§è©•ä¼°å¤±æ•—: {e}")
            return 0.0

    def _assess_reproducibility_reliability(self, pool_data: Dict[str, Any]) -> float:
        """è©•ä¼°é‡ç¾æ€§å¯é æ€§"""
        try:
            metadata = pool_data.get('metadata', {})

            reproducibility_factors = []

            # æª¢æŸ¥é…ç½®å®Œæ•´æ€§
            has_complete_config = all(key in metadata for key in ['configuration', 'parameters'])
            reproducibility_factors.append(1.0 if has_complete_config else 0.0)

            # æª¢æŸ¥éš¨æ©Ÿæ€§æ§åˆ¶
            has_seed_control = 'random_seed' in metadata or 'deterministic' in str(metadata)
            reproducibility_factors.append(1.0 if has_seed_control else 0.0)

            # æª¢æŸ¥ç’°å¢ƒä¿¡æ¯
            has_environment_info = 'environment' in metadata or 'system_info' in metadata
            reproducibility_factors.append(1.0 if has_environment_info else 0.0)

            return np.mean(reproducibility_factors) if reproducibility_factors else 0.0

        except Exception as e:
            self.logger.warning(f"âš ï¸ é‡ç¾æ€§å¯é æ€§è©•ä¼°å¤±æ•—: {e}")
            return 0.0

    def _calculate_overall_reliability(self, reliability_factors: List[float]) -> float:
        """è¨ˆç®—æ•´é«”å¯é æ€§"""
        try:
            if not reliability_factors:
                return 0.0
            return np.mean(reliability_factors)
        except Exception as e:
            self.logger.warning(f"âš ï¸ æ•´é«”å¯é æ€§è¨ˆç®—å¤±æ•—: {e}")
            return 0.0

    def _perform_risk_assessment(self, reliability_score: float) -> Dict[str, Any]:
        """åŸ·è¡Œé¢¨éšªè©•ä¼°"""
        try:
            if reliability_score >= 0.9:
                risk_level = "Low"
                risk_description = "é«˜å¯é æ€§ï¼Œé¢¨éšªæ¥µä½"
            elif reliability_score >= 0.7:
                risk_level = "Medium"
                risk_description = "ä¸­ç­‰å¯é æ€§ï¼Œå­˜åœ¨ä¸€å®šé¢¨éšª"
            elif reliability_score >= 0.5:
                risk_level = "High"
                risk_description = "ä½å¯é æ€§ï¼Œå­˜åœ¨è¼ƒé«˜é¢¨éšª"
            else:
                risk_level = "Critical"
                risk_description = "æ¥µä½å¯é æ€§ï¼Œé¢¨éšªæ¥µé«˜"

            return {
                "risk_level": risk_level,
                "risk_score": 1.0 - reliability_score,
                "description": risk_description
            }

        except Exception as e:
            return {"risk_level": "Unknown", "error": str(e)}

    def _generate_reliability_recommendation(self, reliability_score: float,
                                           risk_assessment: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆå¯é æ€§å»ºè­°"""
        try:
            recommendations = []

            if reliability_score < 0.7:
                recommendations.append("å»ºè­°æ”¹å–„æ•¸æ“šå®Œæ•´æ€§å’Œä¸€è‡´æ€§")

            if reliability_score < 0.6:
                recommendations.append("å»ºè­°ä½¿ç”¨ç¶“éé©—è­‰çš„æ¨™æº–ç®—æ³•")

            if reliability_score < 0.5:
                recommendations.append("å»ºè­°å¢åŠ èª¤å·®åˆ†æå’Œä¸ç¢ºå®šæ€§è©•ä¼°")

            if risk_assessment.get("risk_level") in ["High", "Critical"]:
                recommendations.append("å»ºè­°é€²è¡Œç¨ç«‹é©—è­‰å’Œäº¤å‰æª¢æŸ¥")

            return recommendations

        except Exception as e:
            return [f"å»ºè­°ç”Ÿæˆå¤±æ•—: {e}"]

    def _get_default_validation_config(self) -> Dict[str, Any]:
        """ç²å–é è¨­é©—è­‰é…ç½®"""
        return {
            "min_physics_compliance_score": 0.8,
            "academic_grade_target": "A+",
            "min_reliability_score": 0.7,
            "require_standard_algorithms": True,
            "require_error_analysis": True
        }

    def get_validation_statistics(self) -> Dict[str, Any]:
        """ç²å–é©—è­‰çµ±è¨ˆä¿¡æ¯"""
        return self.validation_stats.copy()