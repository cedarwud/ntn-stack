#!/usr/bin/env python3
"""
Stage 6: æŒä¹…åŒ–APIå±¤è™•ç†å™¨ (é‡æ§‹ç‰ˆæœ¬)

é‡æ§‹åŸå‰‡ï¼š
- å°ˆæ³¨æ•¸æ“šæŒä¹…åŒ–å’ŒAPIè¼¸å‡º
- ç§»é™¤å‹•æ…‹æ± è¦åŠƒåŠŸèƒ½ï¼ˆå·²ç§»è‡³Stage 4ï¼‰
- å¯¦ç¾é«˜æ•ˆçš„æ•¸æ“šå­˜å„²å’Œæª¢ç´¢
- æä¾›æ¨™æº–åŒ–çš„APIæ¥å£

åŠŸèƒ½è®ŠåŒ–ï¼š
- âœ… ä¿ç•™: æ•¸æ“šæŒä¹…åŒ–ã€APIæ ¼å¼åŒ–ã€è¼¸å‡ºç®¡ç†
- âœ… ä¿ç•™: æ•¸æ“šå“è³ªç›£æ§ã€æ€§èƒ½çµ±è¨ˆ
- âŒ ç§»é™¤: å‹•æ…‹æ± è¦åŠƒã€è³‡æºå„ªåŒ–ï¼ˆå·²ç§»è‡³Stage 4ï¼‰
- âœ… æ–°å¢: é«˜æ•ˆå­˜å„²ã€æ‰¹æ¬¡è™•ç†ã€APIæ¨™æº–åŒ–
"""

import logging
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List
import json
import os

# å…±äº«æ¨¡çµ„å°å…¥
from shared.base_processor import BaseStageProcessor
from shared.interfaces import ProcessingStatus, ProcessingResult, create_processing_result
from shared.validation_framework import ValidationEngine
from shared.utils import FileUtils, TimeUtils
from shared.constants import SystemConstantsManager

logger = logging.getLogger(__name__)


class Stage6PersistenceProcessor(BaseStageProcessor):
    """
    Stage 6: æŒä¹…åŒ–APIå±¤è™•ç†å™¨ (é‡æ§‹ç‰ˆæœ¬)

    å°ˆè·è²¬ä»»ï¼š
    1. å¤šæ ¼å¼æ•¸æ“šæŒä¹…åŒ–å’Œå­˜å„²ç®¡ç†
    2. æ¨™æº–åŒ–APIè¼¸å‡ºæ ¼å¼
    3. æ‰¹æ¬¡è™•ç†å’Œå¢é‡æ›´æ–°
    4. æ•¸æ“šæª¢ç´¢å’ŒæŸ¥è©¢æ¥å£
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__(stage_number=6, stage_name="persistence_api", config=config or {})

        # é…ç½®åƒæ•¸
        self.output_formats = self.config.get('output_formats', ['json', 'csv'])
        self.batch_size = self.config.get('batch_size', 1000)
        self.enable_compression = self.config.get('enable_compression', True)

        # å­˜å„²è·¯å¾‘é…ç½® - æ ¹æ“šç’°å¢ƒè‡ªå‹•èª¿æ•´
        import os
        if os.path.exists('/satellite-processing'):
            # å®¹å™¨ç’°å¢ƒ
            self.base_output_path = self.config.get('base_output_path', '/satellite-processing/data/final_outputs')
            self.api_output_path = self.config.get('api_output_path', '/satellite-processing/data/api_outputs')
        else:
            # ä¸»æ©Ÿç’°å¢ƒ
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
            self.base_output_path = self.config.get('base_output_path', f'{project_root}/data/final_outputs')
            self.api_output_path = self.config.get('api_output_path', f'{project_root}/data/api_outputs')

        # åˆå§‹åŒ–çµ„ä»¶
        self.validation_engine = ValidationEngine('stage6')
        self.system_constants = SystemConstantsManager()
        self.file_utils = FileUtils()
        self.time_utils = TimeUtils()

        # è™•ç†çµ±è¨ˆ
        self.processing_stats = {
            'total_records_persisted': 0,
            'files_generated': 0,
            'api_endpoints_updated': 0,
            'storage_operations': 0,
            'compression_savings': 0.0
        }

        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self._ensure_output_directories()

        self.logger.info("Stage 6 æŒä¹…åŒ–APIå±¤è™•ç†å™¨å·²åˆå§‹åŒ–")

    def _ensure_output_directories(self):
        """ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨"""
        directories = [
            self.base_output_path,
            self.api_output_path,
            f"{self.base_output_path}/json",
            f"{self.base_output_path}/csv",
            f"{self.api_output_path}/latest",
            f"{self.api_output_path}/archive"
        ]

        for directory in directories:
            os.makedirs(directory, exist_ok=True)

    def process(self, input_data: Any) -> ProcessingResult:
        """ä¸»è¦è™•ç†æ–¹æ³•"""
        start_time = datetime.now(timezone.utc)
        self.logger.info("ğŸš€ é–‹å§‹Stage 6æŒä¹…åŒ–APIè™•ç†...")

        try:
            # é©—è­‰è¼¸å…¥æ•¸æ“š
            if not self._validate_stage5_output(input_data):
                return create_processing_result(
                    status=ProcessingStatus.VALIDATION_FAILED,
                    data={},
                    message="Stage 5è¼¸å‡ºæ•¸æ“šé©—è­‰å¤±æ•—"
                )

            # åŸ·è¡Œæ•¸æ“šæŒä¹…åŒ–
            persistence_results = self._perform_data_persistence(input_data)

            # ç”ŸæˆAPIè¼¸å‡º
            api_results = self._generate_api_outputs(persistence_results)

            # æ§‹å»ºæœ€çµ‚çµæœ
            processing_time = datetime.now(timezone.utc) - start_time
            result_data = {
                'stage': 'stage6_persistence_api',
                'persistence_results': persistence_results,
                'api_results': api_results,
                'metadata': {
                    'processing_start_time': start_time.isoformat(),
                    'processing_end_time': datetime.now(timezone.utc).isoformat(),
                    'processing_duration_seconds': processing_time.total_seconds(),
                    'output_paths': {
                        'base_output': self.base_output_path,
                        'api_output': self.api_output_path
                    }
                },
                'processing_stats': self.processing_stats,
                'pipeline_complete': True
            }

            return create_processing_result(
                status=ProcessingStatus.SUCCESS,
                data=result_data,
                message=f"æˆåŠŸæŒä¹…åŒ–{self.processing_stats['total_records_persisted']}ç­†è¨˜éŒ„ï¼Œç”Ÿæˆ{self.processing_stats['files_generated']}å€‹è¼¸å‡ºæ–‡ä»¶"
            )

        except Exception as e:
            self.logger.error(f"âŒ Stage 6æŒä¹…åŒ–APIå¤±æ•—: {e}")
            return create_processing_result(
                status=ProcessingStatus.ERROR,
                data={},
                message=f"æŒä¹…åŒ–APIéŒ¯èª¤: {str(e)}"
            )

    def _validate_stage5_output(self, input_data: Any) -> bool:
        """é©—è­‰Stage 5çš„è¼¸å‡ºæ•¸æ“š"""
        if not isinstance(input_data, dict):
            return False

        required_fields = ['stage', 'integrated_data']
        for field in required_fields:
            if field not in input_data:
                return False

        return input_data.get('stage') == 'stage5_data_integration'

    def _perform_data_persistence(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œæ•¸æ“šæŒä¹…åŒ–"""
        persistence_results = {
            'storage_operations': [],
            'file_outputs': [],
            'performance_metrics': {}
        }

        try:
            # æå–æ•´åˆæ•¸æ“š
            integrated_data = input_data.get('integrated_data', {})
            satellites_data = integrated_data.get('satellites', {})

            # ç”Ÿæˆæ™‚é–“æˆ³
            timestamp = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')

            # æŒä¹…åŒ–JSONæ ¼å¼
            if 'json' in self.output_formats:
                json_output = self._save_json_format(satellites_data, timestamp)
                persistence_results['file_outputs'].append(json_output)
                self.processing_stats['files_generated'] += 1

            # æŒä¹…åŒ–CSVæ ¼å¼
            if 'csv' in self.output_formats:
                csv_output = self._save_csv_format(satellites_data, timestamp)
                persistence_results['file_outputs'].append(csv_output)
                self.processing_stats['files_generated'] += 1

            # æ›´æ–°çµ±è¨ˆ
            self.processing_stats['total_records_persisted'] = len(satellites_data)
            self.processing_stats['storage_operations'] += len(persistence_results['file_outputs'])

            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            persistence_results['performance_metrics'] = {
                'records_processed': len(satellites_data),
                'storage_efficiency': self._calculate_storage_efficiency(),
                'write_performance': self._calculate_write_performance()
            }

        except Exception as e:
            self.logger.error(f"æ•¸æ“šæŒä¹…åŒ–å¤±æ•—: {e}")

        return persistence_results

    def _save_json_format(self, satellites_data: Dict[str, Any], timestamp: str) -> Dict[str, Any]:
        """ä¿å­˜JSONæ ¼å¼æ•¸æ“š"""
        json_filename = f"satellite_processing_results_{timestamp}.json"
        json_filepath = f"{self.base_output_path}/json/{json_filename}"

        # æ§‹å»ºè¼¸å‡ºæ•¸æ“šçµæ§‹
        output_data = {
            'metadata': {
                'generation_time': datetime.now(timezone.utc).isoformat(),
                'total_satellites': len(satellites_data),
                'data_format': 'json',
                'stage6_processor_version': '2.0.0'
            },
            'satellites': satellites_data,
            'processing_summary': self.processing_stats
        }

        # å¯«å…¥æ–‡ä»¶
        with open(json_filepath, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)

        return {
            'format': 'json',
            'filename': json_filename,
            'filepath': json_filepath,
            'size_bytes': os.path.getsize(json_filepath),
            'record_count': len(satellites_data)
        }

    def _save_csv_format(self, satellites_data: Dict[str, Any], timestamp: str) -> Dict[str, Any]:
        """ä¿å­˜CSVæ ¼å¼æ•¸æ“š"""
        csv_filename = f"satellite_processing_results_{timestamp}.csv"
        csv_filepath = f"{self.base_output_path}/csv/{csv_filename}"

        # æ§‹å»ºCSVæ•¸æ“š
        csv_rows = []
        headers = ['satellite_id', 'handover_decision_type', 'trigger_rsrp', 'threshold_dbm', 'integration_timestamp']
        csv_rows.append(','.join(headers))

        for satellite_id, satellite_data in satellites_data.items():
            handover_data = satellite_data.get('handover_decision', {})
            row = [
                satellite_id,
                handover_data.get('decision_type', ''),
                str(handover_data.get('trigger_rsrp', '')),
                str(handover_data.get('threshold_dbm', '')),
                satellite_data.get('integration_timestamp', '')
            ]
            csv_rows.append(','.join(row))

        # å¯«å…¥æ–‡ä»¶
        with open(csv_filepath, 'w', encoding='utf-8') as f:
            f.write('\n'.join(csv_rows))

        return {
            'format': 'csv',
            'filename': csv_filename,
            'filepath': csv_filepath,
            'size_bytes': os.path.getsize(csv_filepath),
            'record_count': len(satellites_data)
        }

    def _generate_api_outputs(self, persistence_results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆAPIè¼¸å‡º"""
        api_results = {
            'endpoints_updated': [],
            'latest_data': {},
            'api_metadata': {}
        }

        try:
            # ç”Ÿæˆæœ€æ–°æ•¸æ“šç«¯é»
            latest_endpoint = self._create_latest_data_endpoint(persistence_results)
            api_results['endpoints_updated'].append(latest_endpoint)

            # ç”Ÿæˆçµ±è¨ˆæ•¸æ“šç«¯é»
            stats_endpoint = self._create_statistics_endpoint()
            api_results['endpoints_updated'].append(stats_endpoint)

            # æ›´æ–°APIå…ƒæ•¸æ“š
            api_results['api_metadata'] = {
                'last_update': datetime.now(timezone.utc).isoformat(),
                'data_version': '1.0.0',
                'available_endpoints': len(api_results['endpoints_updated'])
            }

            self.processing_stats['api_endpoints_updated'] = len(api_results['endpoints_updated'])

        except Exception as e:
            self.logger.error(f"APIè¼¸å‡ºç”Ÿæˆå¤±æ•—: {e}")

        return api_results

    def _create_latest_data_endpoint(self, persistence_results: Dict[str, Any]) -> Dict[str, Any]:
        """å‰µå»ºæœ€æ–°æ•¸æ“šç«¯é»"""
        endpoint_data = {
            'endpoint': '/api/v1/satellite/latest',
            'method': 'GET',
            'data_source': persistence_results.get('file_outputs', []),
            'update_time': datetime.now(timezone.utc).isoformat()
        }

        # ä¿å­˜ç«¯é»æ•¸æ“š
        endpoint_filepath = f"{self.api_output_path}/latest/satellite_latest.json"
        with open(endpoint_filepath, 'w', encoding='utf-8') as f:
            json.dump(endpoint_data, f, indent=2, ensure_ascii=False)

        return endpoint_data

    def _create_statistics_endpoint(self) -> Dict[str, Any]:
        """å‰µå»ºçµ±è¨ˆæ•¸æ“šç«¯é»"""
        endpoint_data = {
            'endpoint': '/api/v1/pipeline/statistics',
            'method': 'GET',
            'statistics': self.processing_stats,
            'update_time': datetime.now(timezone.utc).isoformat()
        }

        # ä¿å­˜ç«¯é»æ•¸æ“š
        endpoint_filepath = f"{self.api_output_path}/latest/pipeline_statistics.json"
        with open(endpoint_filepath, 'w', encoding='utf-8') as f:
            json.dump(endpoint_data, f, indent=2, ensure_ascii=False)

        return endpoint_data

    def _calculate_storage_efficiency(self) -> float:
        """è¨ˆç®—å­˜å„²æ•ˆç‡"""
        # ç°¡åŒ–çš„å­˜å„²æ•ˆç‡è¨ˆç®—
        if self.enable_compression:
            return 0.85  # å‡è¨­85%çš„å£“ç¸®ç‡
        return 1.0

    def _calculate_write_performance(self) -> float:
        """è¨ˆç®—å¯«å…¥æ€§èƒ½"""
        # ç°¡åŒ–çš„å¯«å…¥æ€§èƒ½è¨ˆç®—ï¼ˆè¨˜éŒ„/ç§’ï¼‰
        if self.processing_stats['total_records_persisted'] > 0:
            return self.processing_stats['total_records_persisted'] / max(1, self.processing_stats['storage_operations'])
        return 0.0

    def validate_input(self, input_data: Any) -> Dict[str, Any]:
        """é©—è­‰è¼¸å…¥æ•¸æ“š"""
        errors = []
        warnings = []

        if self._validate_stage5_output(input_data):
            return {'valid': True, 'errors': errors, 'warnings': warnings}
        else:
            errors.append("Stage 5è¼¸å‡ºæ•¸æ“šé©—è­‰å¤±æ•—")
            return {'valid': False, 'errors': errors, 'warnings': warnings}

    def validate_output(self, output_data: Any) -> Dict[str, Any]:
        """é©—è­‰è¼¸å‡ºæ•¸æ“š"""
        errors = []
        warnings = []

        if not isinstance(output_data, dict):
            errors.append("è¼¸å‡ºæ•¸æ“šå¿…é ˆæ˜¯å­—å…¸æ ¼å¼")
            return {'valid': False, 'errors': errors, 'warnings': warnings}

        required_fields = ['stage', 'persistence_results', 'metadata']
        for field in required_fields:
            if field not in output_data:
                errors.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

        if output_data.get('stage') != 'stage6_persistence_api':
            errors.append("éšæ®µæ¨™è­˜éŒ¯èª¤")

        return {
            'valid': len(errors) == 0,
            'errors': errors,
            'warnings': warnings
        }

    def extract_key_metrics(self) -> Dict[str, Any]:
        """æå–é—œéµæŒ‡æ¨™"""
        return {
            'stage': 'stage6_persistence_api',
            'records_persisted': self.processing_stats['total_records_persisted'],
            'files_generated': self.processing_stats['files_generated'],
            'api_endpoints_updated': self.processing_stats['api_endpoints_updated'],
            'storage_operations': self.processing_stats['storage_operations']
        }


def create_stage6_processor(config: Optional[Dict[str, Any]] = None) -> Stage6PersistenceProcessor:
    """å‰µå»ºStage 6è™•ç†å™¨å¯¦ä¾‹"""
    return Stage6PersistenceProcessor(config)