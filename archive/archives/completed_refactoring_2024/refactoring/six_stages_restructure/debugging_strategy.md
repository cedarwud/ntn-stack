# å…­éšæ®µé‡æ§‹é™¤éŒ¯ç­–ç•¥

## ğŸ¯ é™¤éŒ¯èƒ½åŠ›é©å‘½æ€§æå‡

### æ ¸å¿ƒå„ªå‹¢
é‡æ§‹å¾Œçš„æ¨¡çµ„åŒ–æ¶æ§‹å°‡é™¤éŒ¯èƒ½åŠ›å¾ã€Œ**ä¸å¯èƒ½**ã€æå‡åˆ°ã€Œ**ç²¾ç¢ºå®šä½**ã€ï¼Œé€™æ˜¯æœ¬æ¬¡é‡æ§‹æœ€é‡è¦çš„å¯¦ç”¨åƒ¹å€¼ã€‚

### ç¾ç‹€ vs é‡æ§‹å¾Œå°æ¯”
```
ç¾ç‹€å•é¡Œï¼š
âŒ Stage 5 (3,400è¡Œ) å‡ºéŒ¯ï¼Œç„¡æ³•å¿«é€Ÿå®šä½å•é¡Œ
âŒ å¿…é ˆåŸ·è¡Œå®Œæ•´å…­éšæ®µæ‰èƒ½ç™¼ç¾éŒ¯èª¤  
âŒ éŒ¯èª¤å½±éŸ¿æ•´å€‹æµç¨‹ï¼Œé›£ä»¥éš”é›¢
âŒ ç„¡æ³•æ³¨å…¥æ¸¬è©¦æ•¸æ“šé€²è¡Œå±€éƒ¨é©—è­‰

é‡æ§‹å¾Œèƒ½åŠ›ï¼š
âœ… ç²¾ç¢ºå®šä½åˆ°å…·é«”æ¨¡çµ„å’Œå‡½æ•¸
âœ… å¯å–®ç¨åŸ·è¡Œä»»ä½•éšæ®µæˆ–æ¨¡çµ„
âœ… éŒ¯èª¤å®Œå…¨éš”é›¢ï¼Œä¸å½±éŸ¿å…¶ä»–éƒ¨åˆ†
âœ… éˆæ´»çš„æ•¸æ“šæ³¨å…¥å’Œæ¸¬è©¦èƒ½åŠ›
```

## ğŸ”§ åˆ†éšæ®µé™¤éŒ¯åŠŸèƒ½è¨­è¨ˆ

### 1. å–®éšæ®µåŸ·è¡Œé™¤éŒ¯

#### æ ¸å¿ƒå·¥å…·ï¼š
```python
# netstack/src/pipeline/scripts/run_single_stage.py

import argparse
import json
import logging
from pathlib import Path
from pipeline.shared.pipeline_coordinator import PipelineCoordinator

class SingleStageDebugger:
    """å–®éšæ®µé™¤éŒ¯å·¥å…·"""
    
    def __init__(self, stage_number: int, debug_level: str = 'INFO'):
        self.stage_number = stage_number
        self.setup_logging(debug_level)
        self.coordinator = PipelineCoordinator()
    
    def execute_stage_only(self, input_file: str = None, output_dir: str = None):
        """åªåŸ·è¡ŒæŒ‡å®šéšæ®µ"""
        print(f"ğŸ” é–‹å§‹é™¤éŒ¯ Stage {self.stage_number}")
        
        # è¼‰å…¥è¼¸å…¥æ•¸æ“š
        if input_file:
            with open(input_file, 'r') as f:
                input_data = json.load(f)
            print(f"ğŸ“¥ å·²è¼‰å…¥è¼¸å…¥æ•¸æ“š: {input_file}")
        else:
            # å¾å‰ä¸€éšæ®µè¼‰å…¥æ¨™æº–è¼¸å‡º
            input_data = self.load_previous_stage_output()
        
        # åŸ·è¡Œå–®éšæ®µ
        try:
            result = self.coordinator.execute_single_stage(
                self.stage_number, 
                input_data,
                debug_mode=True
            )
            
            # ä¿å­˜çµæœ  
            if output_dir:
                output_file = Path(output_dir) / f"stage{self.stage_number}_debug_output.json"
                with open(output_file, 'w') as f:
                    json.dump(result, f, indent=2, ensure_ascii=False)
                print(f"ğŸ’¾ çµæœå·²ä¿å­˜: {output_file}")
            
            print(f"âœ… Stage {self.stage_number} é™¤éŒ¯æˆåŠŸå®Œæˆ")
            return result
            
        except Exception as e:
            print(f"âŒ Stage {self.stage_number} åŸ·è¡Œå¤±æ•—: {e}")
            self.print_debug_info(e)
            raise

# å‘½ä»¤è¡Œä½¿ç”¨
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='å–®éšæ®µé™¤éŒ¯å·¥å…·')
    parser.add_argument('--stage', type=int, required=True, help='éšæ®µç·¨è™Ÿ (1-6)')
    parser.add_argument('--input', help='è¼¸å…¥æ•¸æ“šæ–‡ä»¶è·¯å¾‘')
    parser.add_argument('--output-dir', help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--debug', default='INFO', help='é™¤éŒ¯ç­‰ç´š (DEBUG/INFO/WARNING)')
    
    args = parser.parse_args()
    
    debugger = SingleStageDebugger(args.stage, args.debug)
    debugger.execute_stage_only(args.input, args.output_dir)
```

#### ä½¿ç”¨ç¯„ä¾‹
```bash
# åªåŸ·è¡ŒStage 5ï¼Œä½¿ç”¨Stage 4çš„è¼¸å‡º
python -m pipeline.scripts.run_single_stage --stage=5 --debug=DEBUG

# ä½¿ç”¨è‡ªå®šç¾©æ¸¬è©¦æ•¸æ“š
python -m pipeline.scripts.run_single_stage --stage=5 --input=test_data.json --output-dir=/tmp/debug

# åŸ·è¡ŒStage 3åˆ°Stage 5
python -m pipeline.scripts.run_pipeline_range --start=3 --end=5 --debug=DEBUG
```

### 2. æ¨¡çµ„ç´šé™¤éŒ¯

#### æ ¸å¿ƒå·¥å…·ï¼š
```python
# netstack/src/pipeline/scripts/debug_module.py

class ModuleDebugger:
    """æ¨¡çµ„ç´šé™¤éŒ¯å·¥å…·"""
    
    def debug_stage5_data_merger(self, test_data: dict):
        """é™¤éŒ¯Stage 5çš„data_mergeræ¨¡çµ„"""
        from pipeline.stages.stage5_data_integration.data_merger import DataMerger
        
        print("ğŸ” é–‹å§‹é™¤éŒ¯ Stage5.DataMerger")
        
        # å•Ÿç”¨é™¤éŒ¯æ¨¡å¼
        merger = DataMerger(debug_mode=True)
        
        # è¨­ç½®è©³ç´°æ—¥èªŒ
        logging.getLogger('Stage5.DataMerger').setLevel(logging.DEBUG)
        
        try:
            # é€æ­¥åŸ·è¡Œä¸¦ç›£æ§
            print("ğŸ“Š è¼¸å…¥æ•¸æ“šçµ±è¨ˆ:")
            print(f"  - Starlinkè¡›æ˜Ÿ: {len(test_data.get('starlink', []))}")
            print(f"  - OneWebè¡›æ˜Ÿ: {len(test_data.get('oneweb', []))}")
            
            # åŸ·è¡Œåˆä½µ
            result = merger.merge_satellite_data(test_data)
            
            # è©³ç´°çµæœåˆ†æ
            print("ğŸ“ˆ åˆä½µçµæœçµ±è¨ˆ:")
            print(f"  - ç¸½è¨ˆè¡›æ˜Ÿ: {len(result.get('satellites', []))}")
            print(f"  - æ•¸æ“šå®Œæ•´æ€§: {result.get('completeness_score', 0):.2f}")
            print(f"  - è™•ç†æ™‚é–“: {result.get('processing_time', 0):.2f} ç§’")
            
            # æª¢æŸ¥æ½›åœ¨å•é¡Œ
            self.analyze_merge_quality(result)
            
            return result
            
        except Exception as e:
            print(f"âŒ DataMergeré™¤éŒ¯å¤±æ•—: {e}")
            self.print_merger_debug_info(merger, test_data, e)
            raise
    
    def analyze_merge_quality(self, result: dict):
        """åˆ†æåˆä½µè³ªé‡"""
        issues = []
        
        # æª¢æŸ¥æ•¸æ“šä¸€è‡´æ€§
        satellites = result.get('satellites', [])
        for sat in satellites:
            if not sat.get('tle_data'):
                issues.append(f"è¡›æ˜Ÿ {sat.get('name')} ç¼ºå°‘ TLE æ•¸æ“š")
            
            if sat.get('signal_quality', 0) < 0.5:
                issues.append(f"è¡›æ˜Ÿ {sat.get('name')} ä¿¡è™Ÿè³ªé‡åä½: {sat.get('signal_quality')}")
        
        # å ±å‘Šå•é¡Œ
        if issues:
            print("âš ï¸  ç™¼ç¾æ•¸æ“šè³ªé‡å•é¡Œ:")
            for issue in issues:
                print(f"   - {issue}")
        else:
            print("âœ… æ•¸æ“šè³ªé‡æª¢æŸ¥é€šé")

# äº’å‹•å¼ä½¿ç”¨
if __name__ == "__main__":
    debugger = ModuleDebugger()
    
    # è¼‰å…¥æ¸¬è©¦æ•¸æ“š
    with open('stage4_output.json', 'r') as f:
        test_data = json.load(f)
    
    # é™¤éŒ¯ç‰¹å®šæ¨¡çµ„
    result = debugger.debug_stage5_data_merger(test_data)
```

### 3. æ•¸æ“šæ³¨å…¥é™¤éŒ¯

#### æ ¸å¿ƒåŠŸèƒ½ï¼šæ¨™æº–åŒ–æ•¸æ“šæ¥å£
```python
# netstack/src/pipeline/shared/debug_data_injector.py

class DebugDataInjector:
    """æ•¸æ“šæ³¨å…¥é™¤éŒ¯å·¥å…·"""
    
    def create_stage_test_data(self, stage_number: int, scenario: str = 'normal'):
        """å‰µå»ºç‰¹å®šéšæ®µçš„æ¸¬è©¦æ•¸æ“š"""
        
        if stage_number == 5:
            return self.create_stage5_test_data(scenario)
        elif stage_number == 6:
            return self.create_stage6_test_data(scenario)
        # ... å…¶ä»–éšæ®µ
    
    def create_stage5_test_data(self, scenario: str):
        """å‰µå»ºStage 5æ¸¬è©¦æ•¸æ“š"""
        base_data = {
            "data": {
                "starlink": [
                    {
                        "satellite_id": "STARLINK-1234",
                        "name": "Starlink-1234",
                        "tle_data": {
                            "tle_line1": "1 44713U 19074A   21275.91667824  .00001874  00000-0  13717-3 0  9995",
                            "tle_line2": "2 44713  53.0000 123.4567   0.0013 269.3456 162.7890 15.05000000123456"
                        },
                        "orbital_positions": [...],  # 192å€‹ä½ç½®é»
                        "signal_quality": 0.85
                    }
                    # ... æ›´å¤šStarlinkè¡›æ˜Ÿ
                ],
                "oneweb": [
                    # ... OneWebè¡›æ˜Ÿæ•¸æ“š
                ]
            },
            "metadata": {
                "stage_number": 4,
                "stage_name": "signal_analysis", 
                "processing_timestamp": "2021-10-02T10:30:00Z",
                "total_records": 1500,
                "data_format_version": "unified_v1.2_phase3"
            }
        }
        
        # æ ¹æ“šæ¸¬è©¦å ´æ™¯èª¿æ•´æ•¸æ“š
        if scenario == 'error':
            # æ³¨å…¥éŒ¯èª¤æ•¸æ“šç”¨æ–¼æ¸¬è©¦éŒ¯èª¤è™•ç†
            base_data['data']['starlink'][0]['tle_data'] = None  # ç¼ºå°‘TLEæ•¸æ“š
        elif scenario == 'performance':
            # å¤§é‡æ•¸æ“šç”¨æ–¼æ€§èƒ½æ¸¬è©¦
            base_data['data']['starlink'] *= 100  # æ”¾å¤§100å€
        
        return base_data

# ä½¿ç”¨ç¯„ä¾‹
injector = DebugDataInjector()

# æ¸¬è©¦æ­£å¸¸æƒ…æ³
normal_data = injector.create_stage5_test_data('normal')
stage5_result = stage5_processor.execute(normal_data)

# æ¸¬è©¦éŒ¯èª¤æƒ…æ³  
error_data = injector.create_stage5_test_data('error')
try:
    stage5_result = stage5_processor.execute(error_data)
except Exception as e:
    print(f"éŒ¯èª¤è™•ç†æ¸¬è©¦é€šé: {e}")
```

### 4. æ€§èƒ½åˆ†æé™¤éŒ¯

#### æ ¸å¿ƒå·¥å…·ï¼š
```python
# netstack/src/pipeline/scripts/performance_profiler.py

import time
import psutil
import cProfile
import pstats
from contextlib import contextmanager

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå·¥å…·"""
    
    @contextmanager
    def profile_stage(self, stage_number: int, module_name: str = None):
        """æ€§èƒ½åˆ†æä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        
        # è¨˜éŒ„é–‹å§‹ç‹€æ…‹
        start_time = time.time()
        process = psutil.Process()
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # é–‹å§‹CPU profiling
        profiler = cProfile.Profile()
        profiler.enable()
        
        try:
            yield
        finally:
            # åœæ­¢profiling
            profiler.disable()
            
            # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
            end_time = time.time()
            end_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            duration = end_time - start_time
            memory_used = end_memory - start_memory
            
            # è¼¸å‡ºæ€§èƒ½å ±å‘Š
            print(f"\nğŸ“Š æ€§èƒ½åˆ†æå ±å‘Š - Stage {stage_number} {module_name or ''}:")
            print(f"   â±ï¸  åŸ·è¡Œæ™‚é–“: {duration:.2f} ç§’")
            print(f"   ğŸ§  è¨˜æ†¶é«”ä½¿ç”¨: {memory_used:.1f} MB")
            
            # è©³ç´°CPUåˆ†æ
            stats = pstats.Stats(profiler)
            stats.sort_stats('cumulative')
            
            print(f"\nğŸ”¥ æœ€è€—æ™‚çš„10å€‹å‡½æ•¸:")
            stats.print_stats(10)

# ä½¿ç”¨ç¯„ä¾‹
profiler = PerformanceProfiler()

# åˆ†ææ•´å€‹Stage 5
with profiler.profile_stage(5):
    result = stage5_processor.execute(test_data)

# åˆ†æç‰¹å®šæ¨¡çµ„
with profiler.profile_stage(5, 'DataMerger'):
    merger = DataMerger()
    result = merger.merge_satellite_data(test_data)
```

### 5. å¯¦æ™‚ç›£æ§é™¤éŒ¯

#### æ ¸å¿ƒå·¥å…·ï¼š
```python
# netstack/src/pipeline/scripts/debug_monitor.py

class DebugMonitor:
    """å¯¦æ™‚é™¤éŒ¯ç›£æ§"""
    
    def monitor_pipeline_execution(self, stages: list = None):
        """ç›£æ§ç®¡é“åŸ·è¡Œç‹€æ…‹"""
        stages = stages or list(range(1, 7))
        
        print("ğŸ–¥ï¸  é–‹å§‹å¯¦æ™‚ç›£æ§ç®¡é“åŸ·è¡Œ...")
        
        for stage_num in stages:
            print(f"\nğŸ“¡ ç›£æ§ Stage {stage_num}:")
            
            # ç›£æ§åŸ·è¡Œç‹€æ…‹
            self.monitor_stage_status(stage_num)
            
            # ç›£æ§è³‡æºä½¿ç”¨
            self.monitor_resource_usage(stage_num)
            
            # ç›£æ§éŒ¯èª¤æ—¥èªŒ
            self.monitor_error_logs(stage_num)
    
    def monitor_stage_status(self, stage_number: int):
        """ç›£æ§éšæ®µåŸ·è¡Œç‹€æ…‹"""
        status_file = f"/app/data/stage{stage_number}_status.json"
        
        if Path(status_file).exists():
            with open(status_file, 'r') as f:
                status = json.load(f)
            
            print(f"   âœ… ç‹€æ…‹: {status.get('status', 'unknown')}")
            print(f"   â±ï¸  é€²åº¦: {status.get('progress', 0):.1f}%")
            print(f"   ğŸ“Š è™•ç†è¨˜éŒ„: {status.get('processed_records', 0)}")
        else:
            print(f"   â¸ï¸  å°šæœªé–‹å§‹åŸ·è¡Œ")
    
    def watch_stage_logs(self, stage_number: int):
        """å¯¦æ™‚ç›£æ§éšæ®µæ—¥èªŒ"""
        import subprocess
        
        log_file = f"/app/logs/stage{stage_number}.log"
        
        if Path(log_file).exists():
            print(f"ğŸ” å¯¦æ™‚ç›£æ§ Stage {stage_number} æ—¥èªŒ:")
            
            # ä½¿ç”¨tail -fç›£æ§æ—¥èªŒ
            process = subprocess.Popen(['tail', '-f', log_file], 
                                     stdout=subprocess.PIPE, 
                                     stderr=subprocess.PIPE, 
                                     text=True)
            
            try:
                for line in process.stdout:
                    if 'ERROR' in line:
                        print(f"âŒ {line.strip()}")
                    elif 'WARNING' in line:
                        print(f"âš ï¸  {line.strip()}")
                    elif 'INFO' in line:
                        print(f"â„¹ï¸  {line.strip()}")
                        
            except KeyboardInterrupt:
                process.terminate()
                print(f"\nâ¹ï¸  åœæ­¢ç›£æ§ Stage {stage_number}")

# ä½¿ç”¨ç¯„ä¾‹
monitor = DebugMonitor()

# ç›£æ§æ‰€æœ‰éšæ®µ
monitor.monitor_pipeline_execution()

# å¯¦æ™‚ç›£æ§ç‰¹å®šéšæ®µ
monitor.watch_stage_logs(5)
```

## ğŸ¯ é™¤éŒ¯å·¥å…·å¯¦æ–½è¨ˆåŠƒ

### Phase 1: åŸºç¤é™¤éŒ¯å·¥å…· (ç¬¬8-10å¤©)

#### ç¬¬8å¤©: å–®éšæ®µåŸ·è¡Œå·¥å…·
- å¯¦æ–½ 
- å¯¦æ–½ 
- æ¸¬è©¦å–®éšæ®µåŸ·è¡ŒåŠŸèƒ½

#### ç¬¬9å¤©: æ¨¡çµ„ç´šé™¤éŒ¯å·¥å…·
- å¯¦æ–½ 
- å¯¦æ–½ 
- å‰µå»ºæ¸¬è©¦æ•¸æ“šç”Ÿæˆå™¨

#### ç¬¬10å¤©: æ€§èƒ½åˆ†æå·¥å…·
- å¯¦æ–½ 
- å¯¦æ–½ 
- æ•´åˆæ‰€æœ‰é™¤éŒ¯å·¥å…·

### Phase 2: é€²éšé™¤éŒ¯åŠŸèƒ½ (éš¨å„éšæ®µé‡æ§‹)

#### Stage 1 é‡æ§‹æ™‚
- å¯¦æ–½ Stage 1 å°ˆç”¨é™¤éŒ¯å·¥å…·
- å»ºç«‹é™¤éŒ¯æ•¸æ“šé›†
- é©—è­‰é™¤éŒ¯åŠŸèƒ½å¯ç”¨æ€§

#### å¾ŒçºŒéšæ®µ
- ç‚ºæ¯å€‹é‡æ§‹éšæ®µå»ºç«‹å°ˆç”¨é™¤éŒ¯å·¥å…·
- ç´¯ç©é™¤éŒ¯æ¡ˆä¾‹å’Œæœ€ä½³å¯¦è¸
- æŒçºŒå„ªåŒ–é™¤éŒ¯é«”é©—

## ğŸ“š é™¤éŒ¯ä½¿ç”¨æŒ‡å—

### å¸¸è¦‹é™¤éŒ¯å ´æ™¯

#### å ´æ™¯1: Stage 5è™•ç†æ™‚é–“éé•·
```bash
# 1. æ€§èƒ½åˆ†æ
python -m pipeline.scripts.performance_profiler --stage=5

# 2. æ¨¡çµ„ç´šåˆ†æ
python -m pipeline.scripts.debug_module --stage=5 --module=data_merger

# 3. æ•¸æ“šé‡åˆ†æ
python -c "
from debug_data_injector import DebugDataInjector
injector = DebugDataInjector()
small_data = injector.create_stage5_test_data('small')
# ç”¨å°æ•¸æ“šé›†æ¸¬è©¦æ˜¯å¦ä»ç„¶æ…¢
"
```

#### å ´æ™¯2: æ•¸æ“šæ ¼å¼éŒ¯èª¤
```bash
# 1. æª¢æŸ¥è¼¸å…¥æ•¸æ“šæ ¼å¼
python -m pipeline.scripts.validate_data_format --stage=5 --input=stage4_output.json

# 2. æ³¨å…¥æ¨™æº–æ¸¬è©¦æ•¸æ“š
python -c "
from debug_data_injector import DebugDataInjector
injector = DebugDataInjector()
standard_data = injector.create_stage5_test_data('normal')
# æ¸¬è©¦æ¨™æº–æ•¸æ“šæ˜¯å¦æ­£å¸¸è™•ç†
"

# 3. é€æ¨¡çµ„æª¢æŸ¥
python -m pipeline.scripts.debug_module --stage=5 --module=validator
```

#### å ´æ™¯3: é–“æ­‡æ€§éŒ¯èª¤
```bash
# 1. å¯¦æ™‚ç›£æ§åŸ·è¡Œ
python -m pipeline.scripts.debug_monitor --stage=5 --watch-logs

# 2. å¤šæ¬¡åŸ·è¡Œæ¸¬è©¦
for i in {1..10}; do
  python -m pipeline.scripts.run_single_stage --stage=5
  if [ 127 -ne 0 ]; then
    echo "ç¬¬  æ¬¡åŸ·è¡Œå¤±æ•—"
    break
  fi
done

# 3. ä¿å­˜æ¯æ¬¡åŸ·è¡Œçš„ç‹€æ…‹
python -m pipeline.scripts.run_single_stage --stage=5 --save-debug-state
```

## ğŸš€ é™¤éŒ¯æ•ˆç›Šé æœŸ

### é–‹ç™¼æ•ˆç‡æå‡
- **å•é¡Œå®šä½æ™‚é–“**: å¾æ•¸å°æ™‚ç¸®çŸ­åˆ°æ•¸åˆ†é˜
- **ä¿®å¾©é©—è­‰æ™‚é–“**: å¾å®Œæ•´æ¸¬è©¦ç¸®çŸ­åˆ°å±€éƒ¨é©—è­‰
- **å­¸ç¿’æ›²ç·š**: æ–°äººå¯å¿«é€Ÿç†è§£å„æ¨¡çµ„åŠŸèƒ½

### ç³»çµ±ç©©å®šæ€§æå‡
- **éŒ¯èª¤éš”é›¢**: å•é¡Œä¸æœƒå½±éŸ¿æ•´å€‹ç³»çµ±
- **æ¼¸é€²ä¿®å¾©**: å¯é€æ¨¡çµ„ä¿®å¾©ï¼Œä¸å½±éŸ¿å…¶ä»–åŠŸèƒ½
- **é é˜²æ€§é™¤éŒ¯**: å¯æå‰ç™¼ç¾æ½›åœ¨å•é¡Œ

### ç¶­è­·æˆæœ¬é™ä½
- **ç²¾ç¢ºä¿®å¾©**: åªéœ€è¦ä¿®æ”¹æœ‰å•é¡Œçš„æ¨¡çµ„
- **å½±éŸ¿è©•ä¼°**: æ¸…æ¥šçŸ¥é“ä¿®æ”¹çš„å½±éŸ¿ç¯„åœ
- **æ¸¬è©¦æ•ˆç‡**: å¯é‡å°æ€§æ¸¬è©¦ä¿®æ”¹çš„éƒ¨åˆ†

---

**é‡è¦**: é€™å¥—é™¤éŒ¯åŠŸèƒ½å°‡ä½¿é‡æ§‹å¾Œçš„ç³»çµ±é™¤éŒ¯èƒ½åŠ›å¾ã€Œå¹¾ä¹ä¸å¯èƒ½ã€æå‡åˆ°ã€Œç²¾ç¢ºé«˜æ•ˆã€ï¼Œæ˜¯æœ¬æ¬¡é‡æ§‹æœ€é‡è¦çš„å¯¦ç”¨åƒ¹å€¼ã€‚
