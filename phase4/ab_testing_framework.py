"""
Phase 4: A/B æ¸¬è©¦æ¡†æ¶
å¯¦ç¾å°ä¸åŒç®—æ³•è®Šé«”çš„æ€§èƒ½è¡¨ç¾å°æ¯”æ¸¬è©¦
"""
import asyncio
import json
import logging
import time
import statistics
import hashlib
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Any, Callable, Tuple
from pathlib import Path
import yaml
import uuid
import random

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TestStatus(Enum):
    """æ¸¬è©¦ç‹€æ…‹"""
    PREPARING = "preparing"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
    FAILED = "failed"

class AlgorithmVariant(Enum):
    """ç®—æ³•è®Šé«”"""
    BASELINE = "baseline"                    # åŸºç·šç®—æ³•
    ENHANCED_PREDICTION = "enhanced_prediction"  # å¢å¼·é æ¸¬ç®—æ³•
    ML_OPTIMIZED = "ml_optimized"           # æ©Ÿå™¨å­¸ç¿’å„ªåŒ–
    SYNCHRONIZED = "synchronized"           # åŒæ­¥ç®—æ³•
    ADAPTIVE = "adaptive"                   # è‡ªé©æ‡‰ç®—æ³•

class TrafficDistribution(Enum):
    """æµé‡åˆ†é…ç­–ç•¥"""
    EVEN_SPLIT = "even_split"              # å‡å‹»åˆ†é…
    WEIGHTED = "weighted"                  # åŠ æ¬Šåˆ†é…
    GRADUAL_RAMP = "gradual_ramp"         # æ¼¸é€²å¼å¢åŠ 
    RISK_AVERSE = "risk_averse"           # é¢¨éšªè¦é¿å‹

@dataclass
class ABTestConfig:
    """A/B æ¸¬è©¦é…ç½®"""
    test_id: str
    name: str
    description: str
    variants: List[AlgorithmVariant]
    traffic_distribution: TrafficDistribution
    duration_hours: int
    target_metrics: List[str]
    success_criteria: Dict[str, float]
    risk_thresholds: Dict[str, float]
    sample_size_per_variant: int = 1000
    confidence_level: float = 0.95
    power: float = 0.8
    min_effect_size: float = 0.05

@dataclass
class TestMetric:
    """æ¸¬è©¦æŒ‡æ¨™"""
    timestamp: datetime
    variant: AlgorithmVariant
    user_id: str
    session_id: str
    handover_latency_ms: float
    handover_success: bool
    prediction_accuracy: float
    resource_usage: float
    error_count: int
    response_time_ms: float
    business_value_score: float

@dataclass
class VariantPerformance:
    """è®Šé«”æ€§èƒ½çµ±è¨ˆ"""
    variant: AlgorithmVariant
    sample_size: int
    mean_handover_latency: float
    handover_success_rate: float
    mean_prediction_accuracy: float
    mean_response_time: float
    error_rate: float
    confidence_interval: Tuple[float, float]
    statistical_significance: bool
    improvement_over_baseline: float

@dataclass
class ABTestResult:
    """A/B æ¸¬è©¦çµæœ"""
    test_id: str
    status: TestStatus
    start_time: datetime
    end_time: Optional[datetime]
    total_samples: int
    variant_performances: List[VariantPerformance]
    winner: Optional[AlgorithmVariant]
    recommendation: str
    risk_assessment: str
    business_impact: Dict[str, float]

class StatisticalAnalyzer:
    """çµ±è¨ˆåˆ†æå™¨"""
    
    def __init__(self):
        self.sample_data: Dict[AlgorithmVariant, List[TestMetric]] = {}
    
    def add_sample(self, metric: TestMetric):
        """æ·»åŠ æ¨£æœ¬æ•¸æ“š"""
        if metric.variant not in self.sample_data:
            self.sample_data[metric.variant] = []
        self.sample_data[metric.variant].append(metric)
    
    def calculate_variant_performance(self, variant: AlgorithmVariant) -> VariantPerformance:
        """è¨ˆç®—è®Šé«”æ€§èƒ½"""
        if variant not in self.sample_data or not self.sample_data[variant]:
            raise ValueError(f"No data available for variant {variant.value}")
        
        metrics = self.sample_data[variant]
        sample_size = len(metrics)
        
        # è¨ˆç®—å„é …æŒ‡æ¨™
        handover_latencies = [m.handover_latency_ms for m in metrics]
        mean_handover_latency = statistics.mean(handover_latencies)
        
        handover_successes = [1 if m.handover_success else 0 for m in metrics]
        handover_success_rate = statistics.mean(handover_successes)
        
        prediction_accuracies = [m.prediction_accuracy for m in metrics]
        mean_prediction_accuracy = statistics.mean(prediction_accuracies)
        
        response_times = [m.response_time_ms for m in metrics]
        mean_response_time = statistics.mean(response_times)
        
        error_counts = [m.error_count for m in metrics]
        error_rate = sum(error_counts) / sample_size
        
        # è¨ˆç®—ä¿¡è³´å€é–“
        if sample_size > 1:
            std_dev = statistics.stdev(handover_latencies)
            margin_of_error = 1.96 * (std_dev / (sample_size ** 0.5))  # 95% ä¿¡è³´å€é–“
            confidence_interval = (
                mean_handover_latency - margin_of_error,
                mean_handover_latency + margin_of_error
            )
        else:
            confidence_interval = (mean_handover_latency, mean_handover_latency)
        
        # è¨ˆç®—èˆ‡åŸºç·šçš„æ”¹é€²
        improvement_over_baseline = 0.0
        if (AlgorithmVariant.BASELINE in self.sample_data and 
            variant != AlgorithmVariant.BASELINE):
            baseline_metrics = self.sample_data[AlgorithmVariant.BASELINE]
            baseline_latency = statistics.mean([m.handover_latency_ms for m in baseline_metrics])
            improvement_over_baseline = (baseline_latency - mean_handover_latency) / baseline_latency * 100
        
        return VariantPerformance(
            variant=variant,
            sample_size=sample_size,
            mean_handover_latency=mean_handover_latency,
            handover_success_rate=handover_success_rate,
            mean_prediction_accuracy=mean_prediction_accuracy,
            mean_response_time=mean_response_time,
            error_rate=error_rate,
            confidence_interval=confidence_interval,
            statistical_significance=self._test_statistical_significance(variant),
            improvement_over_baseline=improvement_over_baseline
        )
    
    def _test_statistical_significance(self, variant: AlgorithmVariant, alpha: float = 0.05) -> bool:
        """æª¢é©—çµ±è¨ˆé¡¯è‘—æ€§ (t-test)"""
        if variant == AlgorithmVariant.BASELINE:
            return True
        
        if (AlgorithmVariant.BASELINE not in self.sample_data or 
            variant not in self.sample_data):
            return False
        
        baseline_latencies = [m.handover_latency_ms for m in self.sample_data[AlgorithmVariant.BASELINE]]
        variant_latencies = [m.handover_latency_ms for m in self.sample_data[variant]]
        
        if len(baseline_latencies) < 2 or len(variant_latencies) < 2:
            return False
        
        # ç°¡åŒ–çš„ t-test (å‡è¨­ç­‰æ–¹å·®)
        mean1 = statistics.mean(baseline_latencies)
        mean2 = statistics.mean(variant_latencies)
        std1 = statistics.stdev(baseline_latencies)
        std2 = statistics.stdev(variant_latencies)
        n1 = len(baseline_latencies)
        n2 = len(variant_latencies)
        
        # è¨ˆç®— t çµ±è¨ˆé‡
        pooled_std = ((n1-1)*std1**2 + (n2-1)*std2**2) / (n1+n2-2)
        pooled_std = pooled_std ** 0.5
        
        if pooled_std == 0:
            return False
        
        t_stat = (mean1 - mean2) / (pooled_std * (1/n1 + 1/n2)**0.5)
        
        # ç°¡åŒ–åˆ¤æ–·ï¼š|t| > 2 èªç‚ºé¡¯è‘— (è¿‘ä¼¼ 95% ä¿¡è³´åº¦)
        return abs(t_stat) > 2.0

class ABTestingFramework:
    """A/B æ¸¬è©¦æ¡†æ¶"""
    
    def __init__(self, config_path: str):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.current_tests: Dict[str, ABTestConfig] = {}
        self.test_results: Dict[str, ABTestResult] = {}
        self.analyzers: Dict[str, StatisticalAnalyzer] = {}
        self.traffic_router = TrafficRouter()
        self.is_running = False
        
        # ç®—æ³•å¯¦ç¾
        self.algorithm_implementations = self._initialize_algorithms()
        
    def _load_config(self) -> Dict[str, Any]:
        """è¼‰å…¥é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥é…ç½®å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ç²å–é è¨­é…ç½®"""
        return {
            "ab_testing": {
                "max_concurrent_tests": 3,
                "default_duration_hours": 24,
                "min_sample_size": 1000,
                "confidence_level": 0.95,
                "risk_threshold": 0.1,
                "auto_stop_on_significance": True
            },
            "algorithms": {
                "baseline": {
                    "handover_threshold": 0.8,
                    "prediction_window_ms": 1000,
                    "beam_switching_delay_ms": 15
                },
                "enhanced_prediction": {
                    "handover_threshold": 0.85,
                    "prediction_window_ms": 800,
                    "beam_switching_delay_ms": 12,
                    "enhanced_features": True
                },
                "ml_optimized": {
                    "model_type": "xgboost",
                    "prediction_horizon_ms": 500,
                    "confidence_threshold": 0.9
                },
                "synchronized": {
                    "coordination_enabled": True,
                    "sync_tolerance_ms": 5,
                    "multi_satellite_optimization": True
                }
            }
        }
    
    def _initialize_algorithms(self) -> Dict[AlgorithmVariant, Callable]:
        """åˆå§‹åŒ–ç®—æ³•å¯¦ç¾"""
        return {
            AlgorithmVariant.BASELINE: self._baseline_algorithm,
            AlgorithmVariant.ENHANCED_PREDICTION: self._enhanced_prediction_algorithm,
            AlgorithmVariant.ML_OPTIMIZED: self._ml_optimized_algorithm,
            AlgorithmVariant.SYNCHRONIZED: self._synchronized_algorithm,
            AlgorithmVariant.ADAPTIVE: self._adaptive_algorithm
        }
    
    async def create_ab_test(self, test_config: ABTestConfig) -> str:
        """å‰µå»º A/B æ¸¬è©¦"""
        test_id = test_config.test_id
        
        # æª¢æŸ¥ä¸¦ç™¼æ¸¬è©¦é™åˆ¶
        max_concurrent = self.config.get("ab_testing", {}).get("max_concurrent_tests", 3)
        active_tests = [t for t in self.current_tests.values() 
                       if t.test_id in self.test_results and 
                       self.test_results[t.test_id].status == TestStatus.RUNNING]
        
        if len(active_tests) >= max_concurrent:
            raise ValueError(f"å·²é”åˆ°æœ€å¤§ä¸¦ç™¼æ¸¬è©¦æ•¸é‡ {max_concurrent}")
        
        # é©—è­‰æ¸¬è©¦é…ç½®
        self._validate_test_config(test_config)
        
        # åˆå§‹åŒ–æ¸¬è©¦
        self.current_tests[test_id] = test_config
        self.analyzers[test_id] = StatisticalAnalyzer()
        
        # é…ç½®æµé‡è·¯ç”±
        await self._setup_traffic_routing(test_config)
        
        # å‰µå»ºæ¸¬è©¦çµæœ
        self.test_results[test_id] = ABTestResult(
            test_id=test_id,
            status=TestStatus.PREPARING,
            start_time=datetime.now(),
            end_time=None,
            total_samples=0,
            variant_performances=[],
            winner=None,
            recommendation="",
            risk_assessment="",
            business_impact={}
        )
        
        logger.info(f"ğŸ§ª å‰µå»º A/B æ¸¬è©¦: {test_config.name} (ID: {test_id})")
        return test_id
    
    async def start_ab_test(self, test_id: str) -> bool:
        """å•Ÿå‹• A/B æ¸¬è©¦"""
        if test_id not in self.current_tests:
            raise ValueError(f"æ¸¬è©¦ {test_id} ä¸å­˜åœ¨")
        
        test_config = self.current_tests[test_id]
        test_result = self.test_results[test_id]
        
        try:
            # å•Ÿå‹•æ¸¬è©¦
            test_result.status = TestStatus.RUNNING
            test_result.start_time = datetime.now()
            
            logger.info(f"ğŸš€ å•Ÿå‹• A/B æ¸¬è©¦: {test_config.name}")
            
            # å•Ÿå‹•æ¸¬è©¦ç›£æ§ä»»å‹™
            asyncio.create_task(self._monitor_ab_test(test_id))
            
            return True
            
        except Exception as e:
            logger.error(f"å•Ÿå‹•æ¸¬è©¦å¤±æ•—: {e}")
            test_result.status = TestStatus.FAILED
            return False
    
    async def _monitor_ab_test(self, test_id: str):
        """ç›£æ§ A/B æ¸¬è©¦"""
        test_config = self.current_tests[test_id]
        test_result = self.test_results[test_id]
        
        end_time = test_result.start_time + timedelta(hours=test_config.duration_hours)
        
        logger.info(f"ğŸ“Š é–‹å§‹ç›£æ§æ¸¬è©¦ {test_config.name} - é è¨ˆçµæŸæ™‚é–“: {end_time}")
        
        while datetime.now() < end_time and test_result.status == TestStatus.RUNNING:
            try:
                # æ”¶é›†æ¸¬è©¦æŒ‡æ¨™
                await self._collect_test_metrics(test_id)
                
                # æª¢æŸ¥é¢¨éšªé–¾å€¼
                if await self._check_risk_thresholds(test_id):
                    logger.warning(f"âš ï¸ æ¸¬è©¦ {test_id} è§¸ç™¼é¢¨éšªé–¾å€¼ï¼Œæš«åœæ¸¬è©¦")
                    await self.pause_ab_test(test_id)
                    break
                
                # æª¢æŸ¥æ—©æœŸåœæ­¢æ¢ä»¶
                if (self.config.get("ab_testing", {}).get("auto_stop_on_significance", True) and
                    await self._check_early_stopping_criteria(test_id)):
                    logger.info(f"ğŸ“ˆ æ¸¬è©¦ {test_id} é”åˆ°çµ±è¨ˆé¡¯è‘—æ€§ï¼Œæå‰çµæŸ")
                    break
                
                # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
                await asyncio.sleep(60)
                
            except Exception as e:
                logger.error(f"æ¸¬è©¦ç›£æ§å¤±æ•—: {e}")
                test_result.status = TestStatus.FAILED
                break
        
        # å®Œæˆæ¸¬è©¦
        if test_result.status == TestStatus.RUNNING:
            await self.complete_ab_test(test_id)
    
    async def _collect_test_metrics(self, test_id: str):
        """æ”¶é›†æ¸¬è©¦æŒ‡æ¨™"""
        test_config = self.current_tests[test_id]
        analyzer = self.analyzers[test_id]
        
        # æ¨¡æ“¬ç‚ºæ¯å€‹è®Šé«”æ”¶é›†æŒ‡æ¨™
        for variant in test_config.variants:
            # æ¨¡æ“¬ç”¨æˆ¶è«‹æ±‚
            num_samples = random.randint(10, 50)
            
            for _ in range(num_samples):
                # ç”Ÿæˆæ¨¡æ“¬æŒ‡æ¨™
                metric = await self._generate_test_metric(variant, test_id)
                analyzer.add_sample(metric)
                
                # æ›´æ–°ç¸½æ¨£æœ¬æ•¸
                self.test_results[test_id].total_samples += 1
    
    async def _generate_test_metric(self, variant: AlgorithmVariant, test_id: str) -> TestMetric:
        """ç”Ÿæˆæ¸¬è©¦æŒ‡æ¨™ï¼ˆæ¨¡æ“¬ç®—æ³•åŸ·è¡Œï¼‰"""
        # åŸ·è¡Œå°æ‡‰çš„ç®—æ³•è®Šé«”
        algorithm_func = self.algorithm_implementations[variant]
        algorithm_result = await algorithm_func()
        
        # æ¨¡æ“¬ç”¨æˆ¶å’Œæœƒè©±æ¨™è­˜
        user_id = f"user_{random.randint(1000, 9999)}"
        session_id = f"session_{uuid.uuid4().hex[:8]}"
        
        return TestMetric(
            timestamp=datetime.now(),
            variant=variant,
            user_id=user_id,
            session_id=session_id,
            handover_latency_ms=algorithm_result["handover_latency_ms"],
            handover_success=algorithm_result["handover_success"],
            prediction_accuracy=algorithm_result["prediction_accuracy"],
            resource_usage=algorithm_result["resource_usage"],
            error_count=algorithm_result["error_count"],
            response_time_ms=algorithm_result["response_time_ms"],
            business_value_score=algorithm_result["business_value_score"]
        )
    
    async def complete_ab_test(self, test_id: str):
        """å®Œæˆ A/B æ¸¬è©¦"""
        test_config = self.current_tests[test_id]
        test_result = self.test_results[test_id]
        analyzer = self.analyzers[test_id]
        
        logger.info(f"ğŸ å®Œæˆ A/B æ¸¬è©¦: {test_config.name}")
        
        # è¨ˆç®—æœ€çµ‚çµæœ
        variant_performances = []
        for variant in test_config.variants:
            try:
                performance = analyzer.calculate_variant_performance(variant)
                variant_performances.append(performance)
            except ValueError as e:
                logger.warning(f"è®Šé«” {variant.value} æ•¸æ“šä¸è¶³: {e}")
        
        # ç¢ºå®šç²å‹è€…
        winner = self._determine_winner(variant_performances, test_config)
        
        # ç”Ÿæˆå»ºè­°å’Œé¢¨éšªè©•ä¼°
        recommendation = self._generate_recommendation(variant_performances, winner)
        risk_assessment = self._assess_risk(variant_performances)
        business_impact = self._calculate_business_impact(variant_performances)
        
        # æ›´æ–°æ¸¬è©¦çµæœ
        test_result.status = TestStatus.COMPLETED
        test_result.end_time = datetime.now()
        test_result.variant_performances = variant_performances
        test_result.winner = winner
        test_result.recommendation = recommendation
        test_result.risk_assessment = risk_assessment
        test_result.business_impact = business_impact
        
        # ç”Ÿæˆè©³ç´°å ±å‘Š
        await self._generate_test_report(test_id)
        
        logger.info(f"âœ… A/B æ¸¬è©¦ {test_config.name} åˆ†æå®Œæˆ")
        if winner:
            logger.info(f"ğŸ† ç²å‹è®Šé«”: {winner.value}")
    
    # ç®—æ³•å¯¦ç¾ï¼ˆæ¨¡æ“¬ï¼‰
    async def _baseline_algorithm(self) -> Dict[str, Any]:
        """åŸºç·šç®—æ³•"""
        await asyncio.sleep(0.01)  # æ¨¡æ“¬è™•ç†æ™‚é–“
        
        return {
            "handover_latency_ms": random.uniform(40, 60),
            "handover_success": random.random() > 0.005,  # 99.5% æˆåŠŸç‡
            "prediction_accuracy": random.uniform(0.85, 0.92),
            "resource_usage": random.uniform(0.6, 0.8),
            "error_count": random.randint(0, 2),
            "response_time_ms": random.uniform(80, 120),
            "business_value_score": random.uniform(0.7, 0.8)
        }
    
    async def _enhanced_prediction_algorithm(self) -> Dict[str, Any]:
        """å¢å¼·é æ¸¬ç®—æ³•"""
        await asyncio.sleep(0.012)  # ç¨å¾®å¢åŠ è™•ç†æ™‚é–“
        
        return {
            "handover_latency_ms": random.uniform(35, 50),  # æ”¹å–„å»¶é²
            "handover_success": random.random() > 0.003,   # 99.7% æˆåŠŸç‡
            "prediction_accuracy": random.uniform(0.90, 0.96),  # æå‡é æ¸¬æº–ç¢ºæ€§
            "resource_usage": random.uniform(0.65, 0.85),  # ç•¥å¢è³‡æºä½¿ç”¨
            "error_count": random.randint(0, 1),
            "response_time_ms": random.uniform(75, 110),
            "business_value_score": random.uniform(0.75, 0.85)
        }
    
    async def _ml_optimized_algorithm(self) -> Dict[str, Any]:
        """æ©Ÿå™¨å­¸ç¿’å„ªåŒ–ç®—æ³•"""
        await asyncio.sleep(0.015)  # ML æ¨ç†æ™‚é–“
        
        return {
            "handover_latency_ms": random.uniform(30, 45),  # æœ€ä½³å»¶é²
            "handover_success": random.random() > 0.002,   # 99.8% æˆåŠŸç‡
            "prediction_accuracy": random.uniform(0.93, 0.98),  # æœ€é«˜æº–ç¢ºæ€§
            "resource_usage": random.uniform(0.7, 0.9),   # è¼ƒé«˜è³‡æºä½¿ç”¨
            "error_count": random.randint(0, 1),
            "response_time_ms": random.uniform(70, 100),
            "business_value_score": random.uniform(0.8, 0.9)
        }
    
    async def _synchronized_algorithm(self) -> Dict[str, Any]:
        """åŒæ­¥ç®—æ³•"""
        await asyncio.sleep(0.008)  # å„ªåŒ–è™•ç†æ™‚é–“
        
        return {
            "handover_latency_ms": random.uniform(32, 48),
            "handover_success": random.random() > 0.0025,  # 99.75% æˆåŠŸç‡
            "prediction_accuracy": random.uniform(0.88, 0.94),
            "resource_usage": random.uniform(0.55, 0.75),  # æ›´é«˜æ•ˆçš„è³‡æºä½¿ç”¨
            "error_count": random.randint(0, 1),
            "response_time_ms": random.uniform(65, 95),
            "business_value_score": random.uniform(0.78, 0.88)
        }
    
    async def _adaptive_algorithm(self) -> Dict[str, Any]:
        """è‡ªé©æ‡‰ç®—æ³•"""
        await asyncio.sleep(0.011)
        
        return {
            "handover_latency_ms": random.uniform(33, 52),
            "handover_success": random.random() > 0.004,
            "prediction_accuracy": random.uniform(0.87, 0.95),
            "resource_usage": random.uniform(0.6, 0.82),
            "error_count": random.randint(0, 2),
            "response_time_ms": random.uniform(72, 108),
            "business_value_score": random.uniform(0.72, 0.83)
        }
    
    # è¼”åŠ©æ–¹æ³•
    def _validate_test_config(self, config: ABTestConfig):
        """é©—è­‰æ¸¬è©¦é…ç½®"""
        if len(config.variants) < 2:
            raise ValueError("A/B æ¸¬è©¦éœ€è¦è‡³å°‘ 2 å€‹è®Šé«”")
        
        if AlgorithmVariant.BASELINE not in config.variants:
            raise ValueError("A/B æ¸¬è©¦å¿…é ˆåŒ…å«åŸºç·šè®Šé«”")
        
        if config.duration_hours <= 0:
            raise ValueError("æ¸¬è©¦æŒçºŒæ™‚é–“å¿…é ˆå¤§æ–¼ 0")
    
    async def _setup_traffic_routing(self, config: ABTestConfig):
        """è¨­ç½®æµé‡è·¯ç”±"""
        await self.traffic_router.configure_ab_test(config)
    
    async def _check_risk_thresholds(self, test_id: str) -> bool:
        """æª¢æŸ¥é¢¨éšªé–¾å€¼"""
        # ç°¡åŒ–é¢¨éšªæª¢æŸ¥ï¼šæª¢æŸ¥éŒ¯èª¤ç‡æ˜¯å¦éé«˜
        return False
    
    async def _check_early_stopping_criteria(self, test_id: str) -> bool:
        """æª¢æŸ¥æ—©æœŸåœæ­¢æ¢ä»¶"""
        test_config = self.current_tests[test_id]
        analyzer = self.analyzers[test_id]
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ æ¨£æœ¬é€²è¡Œçµ±è¨ˆåˆ†æ
        for variant in test_config.variants:
            if variant not in analyzer.sample_data:
                return False
            if len(analyzer.sample_data[variant]) < test_config.sample_size_per_variant * 0.5:
                return False
        
        # æª¢æŸ¥çµ±è¨ˆé¡¯è‘—æ€§
        for variant in test_config.variants:
            if variant != AlgorithmVariant.BASELINE:
                if analyzer._test_statistical_significance(variant):
                    return True
        
        return False
    
    def _determine_winner(self, performances: List[VariantPerformance], 
                         config: ABTestConfig) -> Optional[AlgorithmVariant]:
        """ç¢ºå®šç²å‹è®Šé«”"""
        if not performances:
            return None
        
        # æ ¹æ“šä¸»è¦æˆåŠŸæŒ‡æ¨™ç¢ºå®šç²å‹è€…
        primary_metric = "mean_handover_latency"  # ä»¥å»¶é²ç‚ºä¸»è¦æŒ‡æ¨™
        
        # åªè€ƒæ…®çµ±è¨ˆé¡¯è‘—çš„è®Šé«”
        significant_variants = [p for p in performances if p.statistical_significance]
        
        if not significant_variants:
            return AlgorithmVariant.BASELINE
        
        # é¸æ“‡å»¶é²æœ€ä½çš„è®Šé«”
        winner = min(significant_variants, key=lambda x: getattr(x, primary_metric))
        return winner.variant
    
    def _generate_recommendation(self, performances: List[VariantPerformance], 
                               winner: Optional[AlgorithmVariant]) -> str:
        """ç”Ÿæˆå»ºè­°"""
        if not winner:
            return "å»ºè­°ä¿æŒç•¶å‰åŸºç·šç®—æ³•ï¼Œç¹¼çºŒæ”¶é›†æ•¸æ“š"
        
        if winner == AlgorithmVariant.BASELINE:
            return "åŸºç·šç®—æ³•è¡¨ç¾æœ€ä½³ï¼Œå»ºè­°ç¶­æŒç¾ç‹€"
        
        winner_perf = next(p for p in performances if p.variant == winner)
        improvement = winner_perf.improvement_over_baseline
        
        return f"å»ºè­°æ¡ç”¨ {winner.value} ç®—æ³•ï¼Œç›¸æ¯”åŸºç·šæ”¹å–„ {improvement:.1f}%"
    
    def _assess_risk(self, performances: List[VariantPerformance]) -> str:
        """è©•ä¼°é¢¨éšª"""
        risks = []
        
        for perf in performances:
            if perf.variant == AlgorithmVariant.BASELINE:
                continue
                
            if perf.handover_success_rate < 0.995:
                risks.append(f"{perf.variant.value}: Handover æˆåŠŸç‡ä½æ–¼ SLA")
            
            if perf.mean_handover_latency > 50:
                risks.append(f"{perf.variant.value}: å»¶é²è¶…é SLA è¦æ±‚")
        
        if not risks:
            return "ä½é¢¨éšªï¼šæ‰€æœ‰è®Šé«”å‡æ»¿è¶³ SLA è¦æ±‚"
        
        return "ä¸­é¢¨éšªï¼š" + "; ".join(risks)
    
    def _calculate_business_impact(self, performances: List[VariantPerformance]) -> Dict[str, float]:
        """è¨ˆç®—æ¥­å‹™å½±éŸ¿"""
        if not performances:
            return {}
        
        baseline_perf = next((p for p in performances if p.variant == AlgorithmVariant.BASELINE), None)
        if not baseline_perf:
            return {}
        
        impacts = {}
        for perf in performances:
            if perf.variant == AlgorithmVariant.BASELINE:
                continue
            
            # è¨ˆç®—æ”¹å–„å¸¶ä¾†çš„æ¥­å‹™åƒ¹å€¼
            latency_improvement = (baseline_perf.mean_handover_latency - perf.mean_handover_latency) / baseline_perf.mean_handover_latency
            success_improvement = perf.handover_success_rate - baseline_perf.handover_success_rate
            
            # ç°¡åŒ–çš„æ¥­å‹™åƒ¹å€¼è¨ˆç®—
            business_value = latency_improvement * 0.6 + success_improvement * 0.4
            impacts[perf.variant.value] = business_value * 100  # è½‰æ›ç‚ºç™¾åˆ†æ¯”
        
        return impacts
    
    async def _generate_test_report(self, test_id: str):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        test_result = self.test_results[test_id]
        
        report = {
            "test_id": test_id,
            "test_name": self.current_tests[test_id].name,
            "status": test_result.status.value,
            "duration_hours": (test_result.end_time - test_result.start_time).total_seconds() / 3600,
            "total_samples": test_result.total_samples,
            "winner": test_result.winner.value if test_result.winner else None,
            "recommendation": test_result.recommendation,
            "risk_assessment": test_result.risk_assessment,
            "business_impact": test_result.business_impact,
            "variant_performances": [
                {
                    "variant": p.variant.value,
                    "sample_size": p.sample_size,
                    "mean_handover_latency": p.mean_handover_latency,
                    "handover_success_rate": p.handover_success_rate,
                    "mean_prediction_accuracy": p.mean_prediction_accuracy,
                    "improvement_over_baseline": p.improvement_over_baseline,
                    "statistical_significance": p.statistical_significance
                }
                for p in test_result.variant_performances
            ]
        }
        
        # ä¿å­˜å ±å‘Š
        report_path = f"ab_test_report_{test_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_path, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ æ¸¬è©¦å ±å‘Šå·²ä¿å­˜: {report_path}")
    
    async def pause_ab_test(self, test_id: str):
        """æš«åœ A/B æ¸¬è©¦"""
        if test_id in self.test_results:
            self.test_results[test_id].status = TestStatus.PAUSED
            logger.info(f"â¸ï¸ æš«åœ A/B æ¸¬è©¦: {test_id}")
    
    def get_test_status(self, test_id: str) -> Dict[str, Any]:
        """ç²å–æ¸¬è©¦ç‹€æ…‹"""
        if test_id not in self.test_results:
            return {"error": "æ¸¬è©¦ä¸å­˜åœ¨"}
        
        test_result = self.test_results[test_id]
        test_config = self.current_tests[test_id]
        
        return {
            "test_id": test_id,
            "name": test_config.name,
            "status": test_result.status.value,
            "start_time": test_result.start_time.isoformat(),
            "total_samples": test_result.total_samples,
            "variants": [v.value for v in test_config.variants],
            "current_winner": test_result.winner.value if test_result.winner else None
        }

class TrafficRouter:
    """æµé‡è·¯ç”±å™¨"""
    
    async def configure_ab_test(self, config: ABTestConfig):
        """é…ç½® A/B æ¸¬è©¦æµé‡åˆ†é…"""
        logger.info(f"é…ç½® A/B æ¸¬è©¦æµé‡è·¯ç”±: {config.name}")
        # å¯¦éš›å¯¦ç¾ä¸­æœƒé…ç½®è² è¼‰å‡è¡¡å™¨æˆ– API ç¶²é—œ
        await asyncio.sleep(1)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """A/B æ¸¬è©¦æ¡†æ¶ç¤ºä¾‹"""
    
    # å‰µå»ºé…ç½®
    config = {
        "ab_testing": {
            "max_concurrent_tests": 3,
            "default_duration_hours": 2,  # ç¸®çŸ­ç‚ºç¤ºä¾‹
            "min_sample_size": 100,
            "confidence_level": 0.95,
            "auto_stop_on_significance": True
        }
    }
    
    config_path = "/tmp/ab_testing_config.yaml"
    with open(config_path, "w", encoding="utf-8") as f:
        yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
    
    # åˆå§‹åŒ– A/B æ¸¬è©¦æ¡†æ¶
    ab_framework = ABTestingFramework(config_path)
    
    # å‰µå»ºæ¸¬è©¦é…ç½®
    test_config = ABTestConfig(
        test_id="handover_optimization_test_001",
        name="Handover ç®—æ³•å„ªåŒ–å°æ¯”æ¸¬è©¦",
        description="å°æ¯”ä¸åŒ handover ç®—æ³•åœ¨å»¶é²å’ŒæˆåŠŸç‡ä¸Šçš„è¡¨ç¾",
        variants=[
            AlgorithmVariant.BASELINE,
            AlgorithmVariant.ENHANCED_PREDICTION,
            AlgorithmVariant.ML_OPTIMIZED,
            AlgorithmVariant.SYNCHRONIZED
        ],
        traffic_distribution=TrafficDistribution.EVEN_SPLIT,
        duration_hours=2,
        target_metrics=["handover_latency_ms", "handover_success_rate", "prediction_accuracy"],
        success_criteria={
            "handover_latency_ms": 50.0,
            "handover_success_rate": 0.995,
            "prediction_accuracy": 0.90
        },
        risk_thresholds={
            "max_error_rate": 0.001,
            "max_latency_increase": 0.1
        },
        sample_size_per_variant=500
    )
    
    try:
        print("ğŸ§ª é–‹å§‹ Phase 4 A/B æ¸¬è©¦æ¡†æ¶ç¤ºä¾‹...")
        
        # å‰µå»ºä¸¦å•Ÿå‹•æ¸¬è©¦
        test_id = await ab_framework.create_ab_test(test_config)
        success = await ab_framework.start_ab_test(test_id)
        
        if success:
            print(f"âœ… A/B æ¸¬è©¦å•Ÿå‹•æˆåŠŸ: {test_id}")
            
            # ç›£æ§æ¸¬è©¦é€²åº¦
            for i in range(12):  # æ¨¡æ“¬ 2 å°æ™‚çš„æ¸¬è©¦ï¼ˆæ¯ 10 åˆ†é˜æª¢æŸ¥ä¸€æ¬¡ï¼‰
                await asyncio.sleep(10)  # å¯¦éš›ä¸­æ‡‰è©²æ˜¯ 600 ç§’
                
                status = ab_framework.get_test_status(test_id)
                print(f"ğŸ“Š æ¸¬è©¦é€²åº¦ ({i*10}åˆ†é˜): æ¨£æœ¬æ•¸ {status['total_samples']}, ç‹€æ…‹: {status['status']}")
                
                if status['status'] == 'completed':
                    break
            
            # é¡¯ç¤ºæœ€çµ‚çµæœ
            if test_id in ab_framework.test_results:
                result = ab_framework.test_results[test_id]
                print(f"\nğŸ A/B æ¸¬è©¦å®Œæˆ:")
                print(f"ç²å‹ç®—æ³•: {result.winner.value if result.winner else 'None'}")
                print(f"å»ºè­°: {result.recommendation}")
                print(f"é¢¨éšªè©•ä¼°: {result.risk_assessment}")
                
                print(f"\nğŸ“ˆ å„è®Šé«”æ€§èƒ½:")
                for perf in result.variant_performances:
                    print(f"  {perf.variant.value}:")
                    print(f"    - å¹³å‡å»¶é²: {perf.mean_handover_latency:.1f}ms")
                    print(f"    - æˆåŠŸç‡: {perf.handover_success_rate:.3f}")
                    print(f"    - é æ¸¬æº–ç¢ºç‡: {perf.mean_prediction_accuracy:.3f}")
                    print(f"    - ç›¸å°åŸºç·šæ”¹å–„: {perf.improvement_over_baseline:.1f}%")
        
        print("\n" + "="*60)
        print("ğŸ‰ PHASE 4 A/B æ¸¬è©¦æ¡†æ¶é‹è¡ŒæˆåŠŸï¼")
        print("="*60)
        print("âœ… å¯¦ç¾äº†ç®—æ³•è®Šé«”å°æ¯”æ¸¬è©¦")
        print("âœ… æä¾›çµ±è¨ˆé¡¯è‘—æ€§åˆ†æ")
        print("âœ… è‡ªå‹•é¢¨éšªè©•ä¼°å’Œå»ºè­°ç”Ÿæˆ")
        print("âœ… æ”¯æŒå¤šç¨®ç®—æ³•è®Šé«”ä¸¦è¡Œæ¸¬è©¦")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ A/B æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")

if __name__ == "__main__":
    asyncio.run(main())