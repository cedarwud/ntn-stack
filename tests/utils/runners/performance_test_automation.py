#!/usr/bin/env python3
"""
æ€§èƒ½æ¸¬è©¦è‡ªå‹•åŒ–åŸ·è¡Œå’Œå ±å‘Šç³»çµ±
å¯¦ç¾éšæ®µä¸ƒè¦æ±‚çš„è‡ªå‹•åŒ–æ€§èƒ½æ¸¬è©¦åŸ·è¡Œã€èª¿åº¦å’Œç¶œåˆå ±å‘Šç”Ÿæˆ

åŠŸèƒ½ï¼š
1. è‡ªå‹•åŒ–æ¸¬è©¦èª¿åº¦å’ŒåŸ·è¡Œ
2. å¤šé¡å‹æ¸¬è©¦ç·¨æ’ï¼ˆE2Eã€è² è¼‰ã€å£“åŠ›ã€å›æ­¸ï¼‰
3. æ¸¬è©¦ç’°å¢ƒç®¡ç†
4. çµæœæ”¶é›†å’Œåˆ†æ
5. ç¶œåˆå ±å‘Šç”Ÿæˆ
6. æ¸¬è©¦åŸ·è¡Œç›£æ§
7. å¤±æ•—è‡ªå‹•é‡è©¦æ©Ÿåˆ¶
8. æ¸¬è©¦è³‡æºç®¡ç†
"""

import asyncio
import os
import time
import json
import yaml
import shutil
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Callable
from dataclasses import dataclass, field, asdict
from enum import Enum
import structlog
from pathlib import Path
import tempfile
import threading
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import psutil
import aiofiles
import schedule

logger = structlog.get_logger(__name__)


class TestType(Enum):
    """æ¸¬è©¦é¡å‹"""
    E2E = "e2e"
    LOAD = "load"
    STRESS = "stress"
    REGRESSION = "regression"
    INTEGRATION = "integration"
    SMOKE = "smoke"


class TestStatus(Enum):
    """æ¸¬è©¦ç‹€æ…‹"""
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    TIMEOUT = "timeout"
    CANCELLED = "cancelled"


class ExecutionMode(Enum):
    """åŸ·è¡Œæ¨¡å¼"""
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"
    PIPELINE = "pipeline"


@dataclass
class TestConfiguration:
    """æ¸¬è©¦é…ç½®"""
    test_id: str
    test_type: TestType
    name: str
    description: str
    script_path: str
    timeout_seconds: int = 3600
    retry_count: int = 3
    retry_delay: int = 60
    environment_requirements: Dict[str, Any] = field(default_factory=dict)
    parameters: Dict[str, Any] = field(default_factory=dict)
    dependencies: List[str] = field(default_factory=list)
    tags: List[str] = field(default_factory=list)
    enabled: bool = True
    priority: int = 1  # 1-10, 10æœ€é«˜
    resource_requirements: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TestExecutionResult:
    """æ¸¬è©¦åŸ·è¡Œçµæœ"""
    test_id: str
    test_type: TestType
    status: TestStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    duration_seconds: float = 0.0
    exit_code: int = 0
    stdout: str = ""
    stderr: str = ""
    artifacts: List[str] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    error_message: str = ""
    retry_count: int = 0
    resource_usage: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TestSuite:
    """æ¸¬è©¦å¥—ä»¶"""
    suite_id: str
    name: str
    description: str
    tests: List[TestConfiguration] = field(default_factory=list)
    execution_mode: ExecutionMode = ExecutionMode.SEQUENTIAL
    max_parallel_tests: int = 4
    suite_timeout_seconds: int = 7200
    environment_setup: Optional[str] = None
    environment_teardown: Optional[str] = None
    enabled: bool = True


@dataclass
class TestExecution:
    """æ¸¬è©¦åŸ·è¡Œå¯¦ä¾‹"""
    execution_id: str
    suite_id: str
    start_time: datetime
    end_time: Optional[datetime] = None
    status: TestStatus = TestStatus.PENDING
    total_tests: int = 0
    passed_tests: int = 0
    failed_tests: int = 0
    skipped_tests: int = 0
    test_results: List[TestExecutionResult] = field(default_factory=list)
    environment_info: Dict[str, Any] = field(default_factory=dict)
    execution_log: List[str] = field(default_factory=list)


class EnvironmentManager:
    """ç’°å¢ƒç®¡ç†å™¨"""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.environments: Dict[str, Dict[str, Any]] = {}
        self.active_environments: Dict[str, str] = {}  # execution_id -> env_id
        
    async def setup_environment(self, execution_id: str, requirements: Dict[str, Any]) -> str:
        """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
        env_id = f"env_{execution_id}_{int(time.time())}"
        env_dir = self.base_dir / "environments" / env_id
        env_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # è¤‡è£½åŸºç¤ç’°å¢ƒ
            if "base_environment" in requirements:
                base_env = requirements["base_environment"]
                if base_env in self.environments:
                    await self._copy_environment(self.environments[base_env]["path"], str(env_dir))
            
            # å®‰è£ä¾è³´
            if "dependencies" in requirements:
                await self._install_dependencies(str(env_dir), requirements["dependencies"])
            
            # è¨­ç½®ç’°å¢ƒè®Šé‡
            if "env_vars" in requirements:
                await self._setup_env_vars(str(env_dir), requirements["env_vars"])
            
            # é…ç½®æœå‹™
            if "services" in requirements:
                await self._configure_services(str(env_dir), requirements["services"])
            
            self.environments[env_id] = {
                "path": str(env_dir),
                "requirements": requirements,
                "created_at": datetime.now(),
                "execution_id": execution_id
            }
            
            self.active_environments[execution_id] = env_id
            
            logger.info(f"æ¸¬è©¦ç’°å¢ƒå·²è¨­ç½®: {env_id}", execution_id=execution_id)
            return env_id
            
        except Exception as e:
            logger.error(f"ç’°å¢ƒè¨­ç½®å¤±æ•—: {e}", execution_id=execution_id)
            await self.cleanup_environment(execution_id)
            raise
    
    async def _copy_environment(self, source: str, dest: str):
        """è¤‡è£½ç’°å¢ƒ"""
        if os.path.exists(source):
            shutil.copytree(source, dest, dirs_exist_ok=True)
    
    async def _install_dependencies(self, env_dir: str, dependencies: List[str]):
        """å®‰è£ä¾è³´"""
        if not dependencies:
            return
            
        # å‰µå»ºè™›æ“¬ç’°å¢ƒ
        venv_path = os.path.join(env_dir, "venv")
        subprocess.run([
            "python", "-m", "venv", venv_path
        ], check=True)
        
        # å®‰è£ä¾è³´
        pip_path = os.path.join(venv_path, "bin", "pip") if os.name != "nt" else os.path.join(venv_path, "Scripts", "pip.exe")
        for dep in dependencies:
            subprocess.run([
                pip_path, "install", dep
            ], check=True)
    
    async def _setup_env_vars(self, env_dir: str, env_vars: Dict[str, str]):
        """è¨­ç½®ç’°å¢ƒè®Šé‡"""
        env_file = os.path.join(env_dir, ".env")
        async with aiofiles.open(env_file, "w") as f:
            for key, value in env_vars.items():
                await f.write(f"{key}={value}\n")
    
    async def _configure_services(self, env_dir: str, services: Dict[str, Any]):
        """é…ç½®æœå‹™"""
        config_dir = os.path.join(env_dir, "config")
        os.makedirs(config_dir, exist_ok=True)
        
        for service_name, config in services.items():
            config_file = os.path.join(config_dir, f"{service_name}.yaml")
            async with aiofiles.open(config_file, "w") as f:
                await f.write(yaml.dump(config))
    
    async def cleanup_environment(self, execution_id: str):
        """æ¸…ç†ç’°å¢ƒ"""
        if execution_id in self.active_environments:
            env_id = self.active_environments[execution_id]
            if env_id in self.environments:
                env_path = self.environments[env_id]["path"]
                try:
                    shutil.rmtree(env_path)
                    logger.info(f"ç’°å¢ƒå·²æ¸…ç†: {env_id}")
                except Exception as e:
                    logger.error(f"ç’°å¢ƒæ¸…ç†å¤±æ•—: {e}")
                
                del self.environments[env_id]
            del self.active_environments[execution_id]


class ResourceMonitor:
    """è³‡æºç›£æ§å™¨"""
    
    def __init__(self):
        self.monitoring_active = False
        self.monitor_task: Optional[asyncio.Task] = None
        self.resource_data: Dict[str, List[Dict]] = {}
        
    async def start_monitoring(self, execution_id: str):
        """é–‹å§‹ç›£æ§è³‡æº"""
        if execution_id in self.resource_data:
            return
            
        self.resource_data[execution_id] = []
        if not self.monitoring_active:
            self.monitoring_active = True
            self.monitor_task = asyncio.create_task(self._monitor_loop())
        
        logger.info(f"é–‹å§‹ç›£æ§è³‡æº", execution_id=execution_id)
    
    async def stop_monitoring(self, execution_id: str):
        """åœæ­¢ç›£æ§è³‡æº"""
        if execution_id in self.resource_data:
            logger.info(f"åœæ­¢ç›£æ§è³‡æº", execution_id=execution_id, 
                       data_points=len(self.resource_data[execution_id]))
    
    async def _monitor_loop(self):
        """ç›£æ§å¾ªç’°"""
        while self.monitoring_active:
            try:
                timestamp = time.time()
                
                # æ”¶é›†ç³»çµ±è³‡æºæ•¸æ“š
                cpu_percent = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')
                network = psutil.net_io_counters()
                
                resource_snapshot = {
                    "timestamp": timestamp,
                    "cpu_percent": cpu_percent,
                    "memory_percent": memory.percent,
                    "memory_used_gb": memory.used / (1024**3),
                    "disk_percent": disk.percent,
                    "network_bytes_sent": network.bytes_sent,
                    "network_bytes_recv": network.bytes_recv
                }
                
                # æ·»åŠ åˆ°æ‰€æœ‰æ´»èºçš„åŸ·è¡Œå¯¦ä¾‹
                for execution_id in list(self.resource_data.keys()):
                    self.resource_data[execution_id].append(resource_snapshot.copy())
                    
                    # ä¿æŒæœ€è¿‘1000å€‹æ•¸æ“šé»
                    if len(self.resource_data[execution_id]) > 1000:
                        self.resource_data[execution_id] = self.resource_data[execution_id][-1000:]
                
                await asyncio.sleep(5)  # æ¯5ç§’ç›£æ§ä¸€æ¬¡
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"è³‡æºç›£æ§éŒ¯èª¤: {e}")
                await asyncio.sleep(5)
    
    def get_resource_summary(self, execution_id: str) -> Dict[str, Any]:
        """ç²å–è³‡æºä½¿ç”¨æ‘˜è¦"""
        if execution_id not in self.resource_data or not self.resource_data[execution_id]:
            return {}
        
        data = self.resource_data[execution_id]
        
        cpu_values = [d["cpu_percent"] for d in data]
        memory_values = [d["memory_percent"] for d in data]
        
        return {
            "duration_seconds": data[-1]["timestamp"] - data[0]["timestamp"] if len(data) > 1 else 0,
            "cpu_avg": sum(cpu_values) / len(cpu_values),
            "cpu_max": max(cpu_values),
            "memory_avg": sum(memory_values) / len(memory_values),
            "memory_max": max(memory_values),
            "data_points": len(data)
        }


class TestExecutor:
    """æ¸¬è©¦åŸ·è¡Œå™¨"""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.environment_manager = EnvironmentManager(str(self.base_dir))
        self.resource_monitor = ResourceMonitor()
        self.active_executions: Dict[str, TestExecution] = {}
        self.execution_history: List[TestExecution] = []
        
    async def execute_test(self, test_config: TestConfiguration, execution_id: str) -> TestExecutionResult:
        """åŸ·è¡Œå–®å€‹æ¸¬è©¦"""
        result = TestExecutionResult(
            test_id=test_config.test_id,
            test_type=test_config.test_type,
            status=TestStatus.RUNNING,
            start_time=datetime.now()
        )
        
        try:
            # æª¢æŸ¥ä¾è³´
            if not await self._check_dependencies(test_config.dependencies):
                result.status = TestStatus.SKIPPED
                result.error_message = "ä¾è³´æª¢æŸ¥å¤±æ•—"
                return result
            
            # è¨­ç½®ç’°å¢ƒ
            if test_config.environment_requirements:
                env_id = await self.environment_manager.setup_environment(
                    execution_id, test_config.environment_requirements
                )
            
            # é–‹å§‹è³‡æºç›£æ§
            await self.resource_monitor.start_monitoring(execution_id)
            
            # åŸ·è¡Œæ¸¬è©¦ï¼ˆæ”¯æŒé‡è©¦ï¼‰
            for attempt in range(test_config.retry_count + 1):
                try:
                    result.retry_count = attempt
                    
                    # åŸ·è¡Œæ¸¬è©¦è…³æœ¬
                    exit_code, stdout, stderr = await self._run_test_script(
                        test_config, execution_id
                    )
                    
                    result.exit_code = exit_code
                    result.stdout = stdout
                    result.stderr = stderr
                    
                    if exit_code == 0:
                        result.status = TestStatus.PASSED
                        break
                    else:
                        if attempt < test_config.retry_count:
                            logger.warning(f"æ¸¬è©¦å¤±æ•—ï¼Œæº–å‚™é‡è©¦ {attempt + 1}/{test_config.retry_count}",
                                         test_id=test_config.test_id)
                            await asyncio.sleep(test_config.retry_delay)
                        else:
                            result.status = TestStatus.FAILED
                            result.error_message = f"æ¸¬è©¦å¤±æ•—ï¼Œé€€å‡ºç¢¼: {exit_code}"
                
                except asyncio.TimeoutError:
                    result.status = TestStatus.TIMEOUT
                    result.error_message = f"æ¸¬è©¦è¶…æ™‚: {test_config.timeout_seconds}ç§’"
                    break
                except Exception as e:
                    if attempt < test_config.retry_count:
                        logger.warning(f"æ¸¬è©¦ç•°å¸¸ï¼Œæº–å‚™é‡è©¦: {e}", test_id=test_config.test_id)
                        await asyncio.sleep(test_config.retry_delay)
                    else:
                        result.status = TestStatus.FAILED
                        result.error_message = f"æ¸¬è©¦ç•°å¸¸: {str(e)}"
                        break
            
            # æ”¶é›†æ¸¬è©¦ç”¢ç‰©
            result.artifacts = await self._collect_artifacts(test_config, execution_id)
            
            # è§£ææ¸¬è©¦æŒ‡æ¨™
            result.metrics = await self._parse_test_metrics(test_config, result.stdout, result.stderr)
            
        except Exception as e:
            result.status = TestStatus.FAILED
            result.error_message = f"åŸ·è¡Œæ¸¬è©¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            logger.error(f"æ¸¬è©¦åŸ·è¡Œç•°å¸¸: {e}", test_id=test_config.test_id)
        
        finally:
            result.end_time = datetime.now()
            result.duration_seconds = (result.end_time - result.start_time).total_seconds()
            
            # åœæ­¢è³‡æºç›£æ§ä¸¦æ”¶é›†æ‘˜è¦
            await self.resource_monitor.stop_monitoring(execution_id)
            result.resource_usage = self.resource_monitor.get_resource_summary(execution_id)
            
            # æ¸…ç†ç’°å¢ƒ
            await self.environment_manager.cleanup_environment(execution_id)
        
        return result
    
    async def _check_dependencies(self, dependencies: List[str]) -> bool:
        """æª¢æŸ¥ä¾è³´"""
        for dep in dependencies:
            # æª¢æŸ¥æœå‹™æ˜¯å¦é‹è¡Œ
            if dep.startswith("service:"):
                service_name = dep.replace("service:", "")
                if not await self._check_service_running(service_name):
                    return False
            
            # æª¢æŸ¥ç«¯å£æ˜¯å¦å¯ç”¨
            elif dep.startswith("port:"):
                port = int(dep.replace("port:", ""))
                if not await self._check_port_available(port):
                    return False
            
            # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            elif dep.startswith("file:"):
                file_path = dep.replace("file:", "")
                if not os.path.exists(file_path):
                    return False
        
        return True
    
    async def _check_service_running(self, service_name: str) -> bool:
        """æª¢æŸ¥æœå‹™æ˜¯å¦é‹è¡Œ"""
        try:
            result = subprocess.run([
                "systemctl", "is-active", service_name
            ], capture_output=True, text=True)
            return result.returncode == 0 and "active" in result.stdout
        except:
            return True  # å¦‚æœç„¡æ³•æª¢æŸ¥ï¼Œå‡è¨­å¯ç”¨
    
    async def _check_port_available(self, port: int) -> bool:
        """æª¢æŸ¥ç«¯å£æ˜¯å¦å¯ç”¨"""
        import socket
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.settimeout(1)
                result = s.connect_ex(('localhost', port))
                return result == 0  # 0è¡¨ç¤ºé€£æ¥æˆåŠŸï¼Œç«¯å£å¯ç”¨
        except:
            return False
    
    async def _run_test_script(self, test_config: TestConfiguration, execution_id: str) -> Tuple[int, str, str]:
        """é‹è¡Œæ¸¬è©¦è…³æœ¬"""
        script_path = test_config.script_path
        if not os.path.isabs(script_path):
            script_path = str(self.base_dir / script_path)
        
        # æº–å‚™ç’°å¢ƒè®Šé‡
        env = os.environ.copy()
        env.update({
            "TEST_ID": test_config.test_id,
            "EXECUTION_ID": execution_id,
            "TEST_TYPE": test_config.test_type.value
        })
        
        # æ·»åŠ åƒæ•¸åˆ°ç’°å¢ƒè®Šé‡
        for key, value in test_config.parameters.items():
            env[f"TEST_PARAM_{key.upper()}"] = str(value)
        
        # åŸ·è¡Œè…³æœ¬
        process = await asyncio.create_subprocess_exec(
            "python", script_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            env=env,
            cwd=str(self.base_dir)
        )
        
        try:
            stdout, stderr = await asyncio.wait_for(
                process.communicate(),
                timeout=test_config.timeout_seconds
            )
            
            return process.returncode, stdout.decode(), stderr.decode()
            
        except asyncio.TimeoutError:
            process.kill()
            await process.wait()
            raise
    
    async def _collect_artifacts(self, test_config: TestConfiguration, execution_id: str) -> List[str]:
        """æ”¶é›†æ¸¬è©¦ç”¢ç‰©"""
        artifacts = []
        
        # æŸ¥æ‰¾å¸¸è¦‹çš„æ¸¬è©¦ç”¢ç‰©
        artifact_patterns = [
            f"*{test_config.test_id}*.log",
            f"*{test_config.test_id}*.json",
            f"*{test_config.test_id}*.xml",
            f"*{test_config.test_id}*.html",
            f"*{execution_id}*.log",
            "test-results.xml",
            "coverage.xml",
            "performance-report.json"
        ]
        
        # æœç´¢ç”¢ç‰©
        for pattern in artifact_patterns:
            for file_path in self.base_dir.rglob(pattern):
                if file_path.is_file():
                    artifacts.append(str(file_path))
        
        return artifacts
    
    async def _parse_test_metrics(self, test_config: TestConfiguration, stdout: str, stderr: str) -> Dict[str, Any]:
        """è§£ææ¸¬è©¦æŒ‡æ¨™"""
        metrics = {}
        
        # æ ¹æ“šæ¸¬è©¦é¡å‹è§£æä¸åŒçš„æŒ‡æ¨™
        if test_config.test_type == TestType.LOAD:
            # è§£æè² è¼‰æ¸¬è©¦æŒ‡æ¨™
            metrics.update(self._parse_load_test_metrics(stdout))
        elif test_config.test_type == TestType.STRESS:
            # è§£æå£“åŠ›æ¸¬è©¦æŒ‡æ¨™
            metrics.update(self._parse_stress_test_metrics(stdout))
        elif test_config.test_type == TestType.E2E:
            # è§£æE2Eæ¸¬è©¦æŒ‡æ¨™
            metrics.update(self._parse_e2e_test_metrics(stdout))
        
        # é€šç”¨æŒ‡æ¨™è§£æ
        metrics.update(self._parse_common_metrics(stdout, stderr))
        
        return metrics
    
    def _parse_load_test_metrics(self, output: str) -> Dict[str, Any]:
        """è§£æè² è¼‰æ¸¬è©¦æŒ‡æ¨™"""
        metrics = {}
        
        # ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼è§£æ
        import re
        
        # éŸ¿æ‡‰æ™‚é–“
        response_time_match = re.search(r"avg_response_time[:\s]+(\d+\.?\d*)", output)
        if response_time_match:
            metrics["avg_response_time_ms"] = float(response_time_match.group(1))
        
        # æˆåŠŸç‡
        success_rate_match = re.search(r"success_rate[:\s]+(\d+\.?\d*)", output)
        if success_rate_match:
            metrics["success_rate"] = float(success_rate_match.group(1))
        
        # æ¯ç§’è«‹æ±‚æ•¸
        rps_match = re.search(r"requests_per_second[:\s]+(\d+\.?\d*)", output)
        if rps_match:
            metrics["requests_per_second"] = float(rps_match.group(1))
        
        return metrics
    
    def _parse_stress_test_metrics(self, output: str) -> Dict[str, Any]:
        """è§£æå£“åŠ›æ¸¬è©¦æŒ‡æ¨™"""
        metrics = {}
        
        import re
        
        # å³°å€¼è² è¼‰
        peak_load_match = re.search(r"peak_load[:\s]+(\d+)", output)
        if peak_load_match:
            metrics["peak_load"] = int(peak_load_match.group(1))
        
        # ç³»çµ±ç©©å®šæ€§
        stability_match = re.search(r"system_stable[:\s]+(true|false)", output, re.IGNORECASE)
        if stability_match:
            metrics["system_stable"] = stability_match.group(1).lower() == "true"
        
        return metrics
    
    def _parse_e2e_test_metrics(self, output: str) -> Dict[str, Any]:
        """è§£æE2Eæ¸¬è©¦æŒ‡æ¨™"""
        metrics = {}
        
        import re
        
        # æ¸¬è©¦é€šéæ•¸
        passed_match = re.search(r"(\d+)\s+passed", output)
        if passed_match:
            metrics["tests_passed"] = int(passed_match.group(1))
        
        # æ¸¬è©¦å¤±æ•—æ•¸
        failed_match = re.search(r"(\d+)\s+failed", output)
        if failed_match:
            metrics["tests_failed"] = int(failed_match.group(1))
        
        return metrics
    
    def _parse_common_metrics(self, stdout: str, stderr: str) -> Dict[str, Any]:
        """è§£æé€šç”¨æŒ‡æ¨™"""
        metrics = {}
        
        # è¼¸å‡ºé•·åº¦ï¼ˆä½œç‚ºæ´»å‹•æŒ‡æ¨™ï¼‰
        metrics["stdout_length"] = len(stdout)
        metrics["stderr_length"] = len(stderr)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¸¸è¦‹çš„éŒ¯èª¤é—œéµå­—
        error_keywords = ["error", "exception", "failed", "timeout"]
        metrics["error_indicators"] = sum(
            1 for keyword in error_keywords 
            if keyword.lower() in stdout.lower() or keyword.lower() in stderr.lower()
        )
        
        return metrics


class TestSuiteExecutor:
    """æ¸¬è©¦å¥—ä»¶åŸ·è¡Œå™¨"""
    
    def __init__(self, base_dir: str):
        self.base_dir = Path(base_dir)
        self.test_executor = TestExecutor(str(self.base_dir))
        self.active_executions: Dict[str, TestExecution] = {}
        
    async def execute_suite(self, suite: TestSuite) -> TestExecution:
        """åŸ·è¡Œæ¸¬è©¦å¥—ä»¶"""
        execution_id = f"exec_{suite.suite_id}_{int(time.time())}"
        
        execution = TestExecution(
            execution_id=execution_id,
            suite_id=suite.suite_id,
            start_time=datetime.now(),
            total_tests=len([t for t in suite.tests if t.enabled]),
            environment_info=await self._collect_environment_info()
        )
        
        self.active_executions[execution_id] = execution
        
        try:
            execution.execution_log.append(f"é–‹å§‹åŸ·è¡Œæ¸¬è©¦å¥—ä»¶: {suite.name}")
            
            # ç’°å¢ƒè¨­ç½®
            if suite.environment_setup:
                await self._run_setup_script(suite.environment_setup, execution_id)
                execution.execution_log.append("ç’°å¢ƒè¨­ç½®å®Œæˆ")
            
            # éæ¿¾å•Ÿç”¨çš„æ¸¬è©¦
            enabled_tests = [test for test in suite.tests if test.enabled]
            
            # æ ¹æ“šåŸ·è¡Œæ¨¡å¼åŸ·è¡Œæ¸¬è©¦
            if suite.execution_mode == ExecutionMode.SEQUENTIAL:
                await self._execute_sequential(enabled_tests, execution)
            elif suite.execution_mode == ExecutionMode.PARALLEL:
                await self._execute_parallel(enabled_tests, execution, suite.max_parallel_tests)
            elif suite.execution_mode == ExecutionMode.PIPELINE:
                await self._execute_pipeline(enabled_tests, execution)
            
            # çµ±è¨ˆçµæœ
            execution.passed_tests = sum(1 for r in execution.test_results if r.status == TestStatus.PASSED)
            execution.failed_tests = sum(1 for r in execution.test_results if r.status == TestStatus.FAILED)
            execution.skipped_tests = sum(1 for r in execution.test_results if r.status == TestStatus.SKIPPED)
            
            # ç¢ºå®šæ•´é«”ç‹€æ…‹
            if execution.failed_tests == 0:
                execution.status = TestStatus.PASSED
            else:
                execution.status = TestStatus.FAILED
            
            execution.execution_log.append(
                f"æ¸¬è©¦å¥—ä»¶åŸ·è¡Œå®Œæˆ: {execution.passed_tests}é€šé, {execution.failed_tests}å¤±æ•—, {execution.skipped_tests}è·³é"
            )
            
        except Exception as e:
            execution.status = TestStatus.FAILED
            execution.execution_log.append(f"æ¸¬è©¦å¥—ä»¶åŸ·è¡Œç•°å¸¸: {str(e)}")
            logger.error(f"æ¸¬è©¦å¥—ä»¶åŸ·è¡Œç•°å¸¸: {e}", suite_id=suite.suite_id)
        
        finally:
            execution.end_time = datetime.now()
            
            # ç’°å¢ƒæ¸…ç†
            if suite.environment_teardown:
                await self._run_teardown_script(suite.environment_teardown, execution_id)
                execution.execution_log.append("ç’°å¢ƒæ¸…ç†å®Œæˆ")
            
            del self.active_executions[execution_id]
        
        return execution
    
    async def _execute_sequential(self, tests: List[TestConfiguration], execution: TestExecution):
        """é †åºåŸ·è¡Œæ¸¬è©¦"""
        for test in tests:
            execution.execution_log.append(f"é–‹å§‹åŸ·è¡Œæ¸¬è©¦: {test.name}")
            
            result = await self.test_executor.execute_test(test, execution.execution_id)
            execution.test_results.append(result)
            
            execution.execution_log.append(
                f"æ¸¬è©¦å®Œæˆ: {test.name} - {result.status.value} ({result.duration_seconds:.1f}s)"
            )
            
            # å¦‚æœæ˜¯é—œéµæ¸¬è©¦å¤±æ•—ï¼Œå¯ä»¥é¸æ“‡åœæ­¢
            if result.status == TestStatus.FAILED and "critical" in test.tags:
                execution.execution_log.append("é—œéµæ¸¬è©¦å¤±æ•—ï¼Œåœæ­¢åŸ·è¡Œ")
                break
    
    async def _execute_parallel(self, tests: List[TestConfiguration], execution: TestExecution, max_parallel: int):
        """ä¸¦è¡ŒåŸ·è¡Œæ¸¬è©¦"""
        semaphore = asyncio.Semaphore(max_parallel)
        
        async def execute_with_semaphore(test):
            async with semaphore:
                execution.execution_log.append(f"é–‹å§‹åŸ·è¡Œæ¸¬è©¦: {test.name}")
                result = await self.test_executor.execute_test(test, execution.execution_id)
                execution.test_results.append(result)
                execution.execution_log.append(
                    f"æ¸¬è©¦å®Œæˆ: {test.name} - {result.status.value} ({result.duration_seconds:.1f}s)"
                )
                return result
        
        # å‰µå»ºæ‰€æœ‰ä»»å‹™
        tasks = [execute_with_semaphore(test) for test in tests]
        
        # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _execute_pipeline(self, tests: List[TestConfiguration], execution: TestExecution):
        """æµæ°´ç·šåŸ·è¡Œæ¸¬è©¦"""
        # æŒ‰ä¾è³´é—œä¿‚æ’åºæ¸¬è©¦
        sorted_tests = self._sort_tests_by_dependencies(tests)
        
        # é †åºåŸ·è¡Œä½†å…è¨±ä¸¦è¡Œç„¡ä¾è³´çš„æ¸¬è©¦
        completed_tests = set()
        
        while len(completed_tests) < len(sorted_tests):
            # æ‰¾åˆ°å¯ä»¥åŸ·è¡Œçš„æ¸¬è©¦ï¼ˆä¾è³´å·²å®Œæˆï¼‰
            ready_tests = []
            for test in sorted_tests:
                if test.test_id not in completed_tests:
                    if all(dep in completed_tests for dep in test.dependencies):
                        ready_tests.append(test)
            
            if not ready_tests:
                break  # é˜²æ­¢æ­»é–
            
            # ä¸¦è¡ŒåŸ·è¡Œæº–å‚™å¥½çš„æ¸¬è©¦
            tasks = []
            for test in ready_tests:
                execution.execution_log.append(f"é–‹å§‹åŸ·è¡Œæ¸¬è©¦: {test.name}")
                task = asyncio.create_task(
                    self.test_executor.execute_test(test, execution.execution_id)
                )
                tasks.append((test, task))
            
            # ç­‰å¾…å®Œæˆ
            for test, task in tasks:
                result = await task
                execution.test_results.append(result)
                completed_tests.add(test.test_id)
                execution.execution_log.append(
                    f"æ¸¬è©¦å®Œæˆ: {test.name} - {result.status.value} ({result.duration_seconds:.1f}s)"
                )
    
    def _sort_tests_by_dependencies(self, tests: List[TestConfiguration]) -> List[TestConfiguration]:
        """æŒ‰ä¾è³´é—œä¿‚æ’åºæ¸¬è©¦"""
        # ç°¡å–®çš„æ‹“æ’²æ’åº
        result = []
        remaining = tests.copy()
        test_map = {t.test_id: t for t in tests}
        
        while remaining:
            # æ‰¾åˆ°æ²’æœ‰æœªæ»¿è¶³ä¾è³´çš„æ¸¬è©¦
            ready = []
            for test in remaining:
                deps_satisfied = all(
                    dep not in test_map or any(r.test_id == dep for r in result)
                    for dep in test.dependencies
                )
                if deps_satisfied:
                    ready.append(test)
            
            if not ready:
                # å¦‚æœæ²’æœ‰æº–å‚™å¥½çš„æ¸¬è©¦ï¼Œå¯èƒ½æœ‰å¾ªç’°ä¾è³´ï¼Œç›´æ¥æ·»åŠ å‰©é¤˜çš„
                result.extend(remaining)
                break
            
            # æ·»åŠ æº–å‚™å¥½çš„æ¸¬è©¦ä¸¦å¾å‰©é¤˜åˆ—è¡¨ä¸­ç§»é™¤
            result.extend(ready)
            for test in ready:
                remaining.remove(test)
        
        return result
    
    async def _collect_environment_info(self) -> Dict[str, Any]:
        """æ”¶é›†ç’°å¢ƒä¿¡æ¯"""
        import platform
        
        return {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "hostname": platform.node(),
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "disk_total_gb": psutil.disk_usage('/').total / (1024**3),
            "timestamp": datetime.now().isoformat()
        }
    
    async def _run_setup_script(self, script_path: str, execution_id: str):
        """é‹è¡Œè¨­ç½®è…³æœ¬"""
        if not os.path.isabs(script_path):
            script_path = str(self.base_dir / script_path)
        
        if os.path.exists(script_path):
            process = await asyncio.create_subprocess_exec(
                "python", script_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env={**os.environ, "EXECUTION_ID": execution_id}
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                raise Exception(f"è¨­ç½®è…³æœ¬å¤±æ•—: {stderr.decode()}")
    
    async def _run_teardown_script(self, script_path: str, execution_id: str):
        """é‹è¡Œæ¸…ç†è…³æœ¬"""
        if not os.path.isabs(script_path):
            script_path = str(self.base_dir / script_path)
        
        if os.path.exists(script_path):
            process = await asyncio.create_subprocess_exec(
                "python", script_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env={**os.environ, "EXECUTION_ID": execution_id}
            )
            
            await process.communicate()
            # æ¸…ç†è…³æœ¬å¤±æ•—ä¸æœƒé˜»æ­¢åŸ·è¡Œå®Œæˆ


class AutomationScheduler:
    """è‡ªå‹•åŒ–èª¿åº¦å™¨"""
    
    def __init__(self, suite_executor: TestSuiteExecutor):
        self.suite_executor = suite_executor
        self.scheduled_jobs: List[Dict[str, Any]] = []
        self.scheduler_active = False
        self.scheduler_task: Optional[asyncio.Task] = None
        
    def schedule_suite(self, suite: TestSuite, cron_expression: str, enabled: bool = True):
        """èª¿åº¦æ¸¬è©¦å¥—ä»¶"""
        job = {
            "suite": suite,
            "cron": cron_expression,
            "enabled": enabled,
            "last_run": None,
            "next_run": self._calculate_next_run(cron_expression)
        }
        
        self.scheduled_jobs.append(job)
        logger.info(f"å·²èª¿åº¦æ¸¬è©¦å¥—ä»¶: {suite.name}", cron=cron_expression)
    
    def _calculate_next_run(self, cron_expression: str) -> datetime:
        """è¨ˆç®—ä¸‹æ¬¡é‹è¡Œæ™‚é–“ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰"""
        # é€™è£¡å¯ä»¥ä½¿ç”¨å°ˆæ¥­çš„cronåº«å¦‚croniter
        # æš«æ™‚ä½¿ç”¨ç°¡å–®çš„å¯¦ç¾
        if cron_expression == "daily":
            return datetime.now() + timedelta(days=1)
        elif cron_expression == "hourly":
            return datetime.now() + timedelta(hours=1)
        elif cron_expression.startswith("every"):
            # ä¾‹å¦‚ "every 30 minutes"
            parts = cron_expression.split()
            if len(parts) >= 3:
                interval = int(parts[1])
                unit = parts[2]
                if unit.startswith("minute"):
                    return datetime.now() + timedelta(minutes=interval)
                elif unit.startswith("hour"):
                    return datetime.now() + timedelta(hours=interval)
        
        # é»˜èª1å°æ™‚å¾Œ
        return datetime.now() + timedelta(hours=1)
    
    async def start_scheduler(self):
        """å•Ÿå‹•èª¿åº¦å™¨"""
        if self.scheduler_active:
            return
            
        self.scheduler_active = True
        self.scheduler_task = asyncio.create_task(self._scheduler_loop())
        logger.info("æ¸¬è©¦èª¿åº¦å™¨å·²å•Ÿå‹•")
    
    async def stop_scheduler(self):
        """åœæ­¢èª¿åº¦å™¨"""
        self.scheduler_active = False
        if self.scheduler_task:
            self.scheduler_task.cancel()
            try:
                await self.scheduler_task
            except asyncio.CancelledError:
                pass
        logger.info("æ¸¬è©¦èª¿åº¦å™¨å·²åœæ­¢")
    
    async def _scheduler_loop(self):
        """èª¿åº¦å™¨ä¸»å¾ªç’°"""
        while self.scheduler_active:
            try:
                current_time = datetime.now()
                
                for job in self.scheduled_jobs:
                    if (job["enabled"] and 
                        job["next_run"] and 
                        current_time >= job["next_run"]):
                        
                        # åŸ·è¡Œæ¸¬è©¦å¥—ä»¶
                        logger.info(f"èª¿åº¦åŸ·è¡Œæ¸¬è©¦å¥—ä»¶: {job['suite'].name}")
                        
                        try:
                            execution = await self.suite_executor.execute_suite(job["suite"])
                            logger.info(f"èª¿åº¦æ¸¬è©¦å®Œæˆ: {job['suite'].name}, ç‹€æ…‹: {execution.status.value}")
                        except Exception as e:
                            logger.error(f"èª¿åº¦æ¸¬è©¦å¤±æ•—: {e}", suite=job['suite'].name)
                        
                        # æ›´æ–°èª¿åº¦ä¿¡æ¯
                        job["last_run"] = current_time
                        job["next_run"] = self._calculate_next_run(job["cron"])
                
                await asyncio.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥ä¸€æ¬¡
                
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"èª¿åº¦å™¨ç•°å¸¸: {e}")
                await asyncio.sleep(60)


class PerformanceTestAutomation:
    """æ€§èƒ½æ¸¬è©¦è‡ªå‹•åŒ–ä¸»é¡"""
    
    def __init__(self, base_dir: str = "/home/sat/ntn-stack/tests"):
        self.base_dir = Path(base_dir)
        self.config_dir = self.base_dir / "automation" / "config"
        self.results_dir = self.base_dir / "automation" / "results"
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        self.config_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.suite_executor = TestSuiteExecutor(str(self.base_dir))
        self.scheduler = AutomationScheduler(self.suite_executor)
        
        # æ¸¬è©¦å¥—ä»¶
        self.test_suites: Dict[str, TestSuite] = {}
        self.execution_history: List[TestExecution] = []
        
        # åˆå§‹åŒ–é è¨­é…ç½®
        self._initialize_default_suites()
    
    def _initialize_default_suites(self):
        """åˆå§‹åŒ–é è¨­æ¸¬è©¦å¥—ä»¶"""
        
        # E2Eæ¸¬è©¦å¥—ä»¶
        e2e_suite = TestSuite(
            suite_id="e2e_performance",
            name="E2Eæ€§èƒ½æ¸¬è©¦å¥—ä»¶",
            description="ç«¯åˆ°ç«¯æ€§èƒ½å’ŒåŠŸèƒ½æ¸¬è©¦",
            execution_mode=ExecutionMode.SEQUENTIAL,
            tests=[
                TestConfiguration(
                    test_id="e2e_basic_functionality",
                    test_type=TestType.E2E,
                    name="åŸºç¤åŠŸèƒ½æ¸¬è©¦",
                    description="é©—è­‰åŸºæœ¬ç³»çµ±åŠŸèƒ½",
                    script_path="e2e/test_e2e_quick.py",
                    timeout_seconds=1800,
                    priority=10,
                    tags=["critical", "basic"]
                ),
                TestConfiguration(
                    test_id="e2e_performance_scenarios",
                    test_type=TestType.E2E,
                    name="æ€§èƒ½å ´æ™¯æ¸¬è©¦",
                    description="è¤‡é›œæ€§èƒ½å ´æ™¯æ¸¬è©¦",
                    script_path="e2e/e2e_tests.py",
                    timeout_seconds=3600,
                    priority=8,
                    tags=["performance", "scenarios"]
                )
            ]
        )
        
        # è² è¼‰æ¸¬è©¦å¥—ä»¶
        load_suite = TestSuite(
            suite_id="load_performance",
            name="è² è¼‰æ€§èƒ½æ¸¬è©¦å¥—ä»¶",
            description="ç³»çµ±è² è¼‰èƒ½åŠ›æ¸¬è©¦",
            execution_mode=ExecutionMode.PARALLEL,
            max_parallel_tests=2,
            tests=[
                TestConfiguration(
                    test_id="load_concurrent_users",
                    test_type=TestType.LOAD,
                    name="ä¸¦ç™¼ç”¨æˆ¶è² è¼‰æ¸¬è©¦",
                    description="æ¸¬è©¦ä¸¦ç™¼ç”¨æˆ¶è² è¼‰",
                    script_path="performance/load_tests.py",
                    timeout_seconds=2400,
                    priority=8,
                    parameters={"concurrent_users": 50, "duration_seconds": 300},
                    tags=["load", "concurrent"]
                ),
                TestConfiguration(
                    test_id="load_ramp_up",
                    test_type=TestType.LOAD,
                    name="éšæ¢¯è² è¼‰æ¸¬è©¦",
                    description="æ¼¸é€²å¼è² è¼‰å¢é•·æ¸¬è©¦",
                    script_path="performance/load_tests.py",
                    timeout_seconds=3600,
                    priority=7,
                    parameters={"test_type": "ramp_up", "max_users": 100},
                    tags=["load", "ramp"]
                )
            ]
        )
        
        # å£“åŠ›æ¸¬è©¦å¥—ä»¶
        stress_suite = TestSuite(
            suite_id="stress_performance",
            name="å£“åŠ›æ€§èƒ½æ¸¬è©¦å¥—ä»¶",
            description="ç³»çµ±æ¥µé™å’Œæ¢å¾©èƒ½åŠ›æ¸¬è©¦",
            execution_mode=ExecutionMode.SEQUENTIAL,
            tests=[
                TestConfiguration(
                    test_id="stress_extreme_load",
                    test_type=TestType.STRESS,
                    name="æ¥µé™è² è¼‰å£“åŠ›æ¸¬è©¦",
                    description="æ¸¬è©¦ç³»çµ±æ¥µé™è² è¼‰èƒ½åŠ›",
                    script_path="performance/stress_tests.py",
                    timeout_seconds=3600,
                    priority=6,
                    parameters={"test_type": "extreme_load"},
                    tags=["stress", "extreme"]
                ),
                TestConfiguration(
                    test_id="stress_resource_exhaustion",
                    test_type=TestType.STRESS,
                    name="è³‡æºè€—ç›¡æ¸¬è©¦",
                    description="æ¸¬è©¦è³‡æºè€—ç›¡æƒ…æ³ä¸‹çš„ç³»çµ±è¡¨ç¾",
                    script_path="performance/stress_tests.py",
                    timeout_seconds=2400,
                    priority=5,
                    parameters={"test_type": "resource_exhaustion"},
                    tags=["stress", "resources"]
                )
            ]
        )
        
        # å›æ­¸æ¸¬è©¦å¥—ä»¶
        regression_suite = TestSuite(
            suite_id="regression_performance",
            name="æ€§èƒ½å›æ­¸æ¸¬è©¦å¥—ä»¶",
            description="æ€§èƒ½å›æ­¸æª¢æ¸¬æ¸¬è©¦",
            execution_mode=ExecutionMode.SEQUENTIAL,
            tests=[
                TestConfiguration(
                    test_id="regression_baseline",
                    test_type=TestType.REGRESSION,
                    name="æ€§èƒ½åŸºæº–å›æ­¸æ¸¬è©¦",
                    description="å»ºç«‹å’Œæ¯”è¼ƒæ€§èƒ½åŸºæº–",
                    script_path="performance/performance_regression_tester.py",
                    timeout_seconds=3600,
                    priority=7,
                    parameters={"mode": "regression_test"},
                    tags=["regression", "baseline"]
                )
            ]
        )
        
        # æ·»åŠ åˆ°å¥—ä»¶å­—å…¸
        self.test_suites = {
            "e2e_performance": e2e_suite,
            "load_performance": load_suite,
            "stress_performance": stress_suite,
            "regression_performance": regression_suite
        }
    
    async def run_suite(self, suite_id: str) -> Optional[TestExecution]:
        """é‹è¡ŒæŒ‡å®šçš„æ¸¬è©¦å¥—ä»¶"""
        if suite_id not in self.test_suites:
            logger.error(f"æ¸¬è©¦å¥—ä»¶ä¸å­˜åœ¨: {suite_id}")
            return None
        
        suite = self.test_suites[suite_id]
        
        logger.info(f"é–‹å§‹åŸ·è¡Œæ¸¬è©¦å¥—ä»¶: {suite.name}")
        execution = await self.suite_executor.execute_suite(suite)
        
        # ä¿å­˜çµæœ
        await self._save_execution_result(execution)
        self.execution_history.append(execution)
        
        logger.info(f"æ¸¬è©¦å¥—ä»¶åŸ·è¡Œå®Œæˆ: {suite.name}, ç‹€æ…‹: {execution.status.value}")
        
        return execution
    
    async def run_all_suites(self) -> List[TestExecution]:
        """é‹è¡Œæ‰€æœ‰æ¸¬è©¦å¥—ä»¶"""
        results = []
        
        # æŒ‰å„ªå…ˆç´šæ’åºå¥—ä»¶
        sorted_suites = sorted(
            self.test_suites.items(),
            key=lambda x: max((t.priority for t in x[1].tests), default=1),
            reverse=True
        )
        
        for suite_id, suite in sorted_suites:
            if suite.enabled:
                execution = await self.run_suite(suite_id)
                if execution:
                    results.append(execution)
        
        return results
    
    async def schedule_automatic_runs(self):
        """è¨­ç½®è‡ªå‹•åŒ–é‹è¡Œèª¿åº¦"""
        
        # æ¯æ—¥ E2E æ¸¬è©¦
        self.scheduler.schedule_suite(
            self.test_suites["e2e_performance"],
            "daily",
            True
        )
        
        # æ¯6å°æ™‚è² è¼‰æ¸¬è©¦
        self.scheduler.schedule_suite(
            self.test_suites["load_performance"],
            "every 6 hours",
            True
        )
        
        # æ¯é€±å£“åŠ›æ¸¬è©¦
        self.scheduler.schedule_suite(
            self.test_suites["stress_performance"],
            "weekly",
            True
        )
        
        # æ¯æ—¥æ€§èƒ½å›æ­¸æ¸¬è©¦
        self.scheduler.schedule_suite(
            self.test_suites["regression_performance"],
            "daily",
            True
        )
        
        # å•Ÿå‹•èª¿åº¦å™¨
        await self.scheduler.start_scheduler()
        
        logger.info("è‡ªå‹•åŒ–æ¸¬è©¦èª¿åº¦å·²å•Ÿå‹•")
    
    async def _save_execution_result(self, execution: TestExecution):
        """ä¿å­˜åŸ·è¡Œçµæœ"""
        timestamp = execution.start_time.strftime("%Y%m%d_%H%M%S")
        result_file = self.results_dir / f"execution_{execution.suite_id}_{timestamp}.json"
        
        # è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„å­—å…¸
        result_data = {
            "execution_id": execution.execution_id,
            "suite_id": execution.suite_id,
            "start_time": execution.start_time.isoformat(),
            "end_time": execution.end_time.isoformat() if execution.end_time else None,
            "status": execution.status.value,
            "total_tests": execution.total_tests,
            "passed_tests": execution.passed_tests,
            "failed_tests": execution.failed_tests,
            "skipped_tests": execution.skipped_tests,
            "environment_info": execution.environment_info,
            "execution_log": execution.execution_log,
            "test_results": [
                {
                    "test_id": r.test_id,
                    "test_type": r.test_type.value,
                    "status": r.status.value,
                    "start_time": r.start_time.isoformat(),
                    "end_time": r.end_time.isoformat() if r.end_time else None,
                    "duration_seconds": r.duration_seconds,
                    "exit_code": r.exit_code,
                    "stdout_length": len(r.stdout),
                    "stderr_length": len(r.stderr),
                    "artifacts": r.artifacts,
                    "metrics": r.metrics,
                    "error_message": r.error_message,
                    "retry_count": r.retry_count,
                    "resource_usage": r.resource_usage
                }
                for r in execution.test_results
            ]
        }
        
        async with aiofiles.open(result_file, "w") as f:
            await f.write(json.dumps(result_data, indent=2, ensure_ascii=False))
        
        logger.info(f"åŸ·è¡Œçµæœå·²ä¿å­˜: {result_file}")
    
    def get_execution_summary(self) -> Dict[str, Any]:
        """ç²å–åŸ·è¡Œæ‘˜è¦"""
        total_executions = len(self.execution_history)
        
        if total_executions == 0:
            return {
                "total_executions": 0,
                "summary": "å°šç„¡åŸ·è¡Œè¨˜éŒ„"
            }
        
        passed_executions = sum(1 for e in self.execution_history if e.status == TestStatus.PASSED)
        failed_executions = sum(1 for e in self.execution_history if e.status == TestStatus.FAILED)
        
        # æœ€è¿‘åŸ·è¡Œ
        recent_executions = sorted(self.execution_history, key=lambda x: x.start_time, reverse=True)[:10]
        
        # æŒ‰å¥—ä»¶çµ±è¨ˆ
        suite_stats = {}
        for execution in self.execution_history:
            suite_id = execution.suite_id
            if suite_id not in suite_stats:
                suite_stats[suite_id] = {"total": 0, "passed": 0, "failed": 0}
            
            suite_stats[suite_id]["total"] += 1
            if execution.status == TestStatus.PASSED:
                suite_stats[suite_id]["passed"] += 1
            elif execution.status == TestStatus.FAILED:
                suite_stats[suite_id]["failed"] += 1
        
        return {
            "total_executions": total_executions,
            "passed_executions": passed_executions,
            "failed_executions": failed_executions,
            "success_rate": passed_executions / total_executions if total_executions > 0 else 0,
            "suite_statistics": suite_stats,
            "recent_executions": [
                {
                    "execution_id": e.execution_id,
                    "suite_id": e.suite_id,
                    "status": e.status.value,
                    "start_time": e.start_time.isoformat(),
                    "duration": (e.end_time - e.start_time).total_seconds() if e.end_time else 0,
                    "passed_tests": e.passed_tests,
                    "failed_tests": e.failed_tests
                }
                for e in recent_executions
            ],
            "test_suites": list(self.test_suites.keys()),
            "scheduler_active": self.scheduler.scheduler_active
        }


async def main():
    """ä¸»å‡½æ•¸ - ç¤ºä¾‹ç”¨æ³•"""
    automation = PerformanceTestAutomation()
    
    print("ğŸš€ æ€§èƒ½æ¸¬è©¦è‡ªå‹•åŒ–åŸ·è¡Œå’Œå ±å‘Šç³»çµ±")
    print("=" * 60)
    
    # åŸ·è¡Œå–®å€‹å¥—ä»¶ç¤ºä¾‹
    print("ğŸ“‹ åŸ·è¡ŒE2Eæ€§èƒ½æ¸¬è©¦å¥—ä»¶...")
    e2e_execution = await automation.run_suite("e2e_performance")
    
    if e2e_execution:
        print(f"âœ… E2Eæ¸¬è©¦å®Œæˆ: {e2e_execution.status.value}")
        print(f"   é€šé: {e2e_execution.passed_tests}")
        print(f"   å¤±æ•—: {e2e_execution.failed_tests}")
        print(f"   æŒçºŒæ™‚é–“: {(e2e_execution.end_time - e2e_execution.start_time).total_seconds():.1f}ç§’")
    
    # ç²å–æ‘˜è¦
    summary = automation.get_execution_summary()
    print(f"\nğŸ“Š åŸ·è¡Œæ‘˜è¦:")
    print(f"   ç¸½åŸ·è¡Œæ¬¡æ•¸: {summary['total_executions']}")
    print(f"   æˆåŠŸç‡: {summary['success_rate']:.1%}")
    print(f"   å¯ç”¨æ¸¬è©¦å¥—ä»¶: {', '.join(summary['test_suites'])}")
    
    # è¨­ç½®è‡ªå‹•åŒ–èª¿åº¦ï¼ˆç¤ºä¾‹ï¼‰
    print(f"\nâ° è¨­ç½®è‡ªå‹•åŒ–èª¿åº¦...")
    await automation.schedule_automatic_runs()
    
    print("âœ… æ€§èƒ½æ¸¬è©¦è‡ªå‹•åŒ–ç³»çµ±å·²æº–å‚™å°±ç·’")
    
    # é‹è¡Œä¸€å°æ®µæ™‚é–“ä»¥æ¼”ç¤ºèª¿åº¦
    await asyncio.sleep(5)
    
    await automation.scheduler.stop_scheduler()
    print("ğŸ”š ç³»çµ±å·²åœæ­¢")


if __name__ == "__main__":
    asyncio.run(main())