#!/usr/bin/env python3
"""
å®Œæ•´çš„ç«¯åˆ°ç«¯æ¸¬è©¦å’Œæ€§èƒ½å„ªåŒ–åŸ·è¡Œè…³æœ¬
æ•´åˆ TODO.md ç¬¬14é …å’Œç¬¬15é …è¦æ±‚ï¼Œç¢ºä¿100%æ¸¬è©¦é€šé
"""

import asyncio
import logging
import sys
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ è·¯å¾‘
sys.path.append("tests/e2e")
sys.path.append("tests/performance")

from e2e.e2e_test_framework import E2ETestFramework
from performance.performance_optimizer import PerformanceOptimizer

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(
            f"tests/reports/complete_e2e_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        ),
    ],
)
logger = logging.getLogger(__name__)


class CompleteE2ETestRunner:
    """å®Œæ•´ç«¯åˆ°ç«¯æ¸¬è©¦åŸ·è¡Œå™¨"""

    def __init__(self):
        self.e2e_framework = E2ETestFramework()
        self.performance_optimizer = PerformanceOptimizer()
        self.max_retry_attempts = 3
        self.current_attempt = 0

    async def run_complete_test_suite(self) -> bool:
        """åŸ·è¡Œå®Œæ•´çš„æ¸¬è©¦å¥—ä»¶"""
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æ¸¬è©¦å’Œæ€§èƒ½å„ªåŒ–")
        start_time = datetime.utcnow()

        # åˆå§‹ç³»çµ±æª¢æŸ¥
        if not await self._initial_system_check():
            logger.error("âŒ åˆå§‹ç³»çµ±æª¢æŸ¥å¤±æ•—")
            return False

        # åŸ·è¡Œå„ªåŒ–å’Œæ¸¬è©¦å¾ªç’°ï¼Œç›´åˆ°100%é€šé
        success = False
        self.current_attempt = 0

        while self.current_attempt < self.max_retry_attempts and not success:
            self.current_attempt += 1
            logger.info(f"ğŸ“‹ åŸ·è¡Œç¬¬ {self.current_attempt} æ¬¡å˜—è©¦")

            try:
                # æ­¥é©Ÿ1: ç³»çµ±æ€§èƒ½å„ªåŒ–
                logger.info("âš¡ æ­¥é©Ÿ1: åŸ·è¡Œç³»çµ±æ€§èƒ½å„ªåŒ–")
                optimization_success = await self._run_performance_optimization()

                # æ­¥é©Ÿ2: ç«¯åˆ°ç«¯ç³»çµ±é›†æˆæ¸¬è©¦
                logger.info("ğŸ§ª æ­¥é©Ÿ2: åŸ·è¡Œç«¯åˆ°ç«¯ç³»çµ±é›†æˆæ¸¬è©¦")
                test_success = await self._run_e2e_tests()

                if optimization_success and test_success:
                    success = True
                    logger.info("ğŸ‰ æ‰€æœ‰æ¸¬è©¦100%é€šéï¼")
                else:
                    logger.warning(f"âš ï¸ ç¬¬ {self.current_attempt} æ¬¡å˜—è©¦æœªå®Œå…¨æˆåŠŸ")

                    if self.current_attempt < self.max_retry_attempts:
                        logger.info("ğŸ”„ åŸ·è¡Œç³»çµ±èª¿æ•´å¾Œé‡è©¦")
                        await self._adjust_system_for_retry()

            except Exception as e:
                logger.error(f"âŒ ç¬¬ {self.current_attempt} æ¬¡å˜—è©¦ç•°å¸¸: {e}")
                if self.current_attempt >= self.max_retry_attempts:
                    break

        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        await self._generate_final_report(success)

        total_duration = (datetime.utcnow() - start_time).total_seconds()
        logger.info(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {total_duration:.1f} ç§’")

        return success

    async def _initial_system_check(self) -> bool:
        """åˆå§‹ç³»çµ±æª¢æŸ¥"""
        logger.info("ğŸ” åŸ·è¡Œåˆå§‹ç³»çµ±æª¢æŸ¥")

        try:
            # æª¢æŸ¥ç³»çµ±è³‡æº
            import psutil

            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage("/")

            logger.info(f"ç³»çµ±è³‡æºç‹€æ…‹:")
            logger.info(f"  CPU ä½¿ç”¨ç‡: {cpu_percent:.1f}%")
            logger.info(f"  è¨˜æ†¶é«”ä½¿ç”¨ç‡: {memory.percent:.1f}%")
            logger.info(f"  ç£ç¢Ÿä½¿ç”¨ç‡: {disk.percent:.1f}%")

            # æª¢æŸ¥è³‡æºæ˜¯å¦å……è¶³
            if cpu_percent > 90:
                logger.warning(f"âš ï¸ CPU ä½¿ç”¨ç‡éé«˜: {cpu_percent:.1f}%")

            if memory.percent > 90:
                logger.warning(f"âš ï¸ è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜: {memory.percent:.1f}%")

            if disk.percent > 90:
                logger.warning(f"âš ï¸ ç£ç¢Ÿä½¿ç”¨ç‡éé«˜: {disk.percent:.1f}%")

            # æª¢æŸ¥å¿…è¦çš„ç›®éŒ„å’Œæ–‡ä»¶
            required_paths = [
                "tests/configs/e2e_test_config.yaml",
                "tests/configs/performance_optimization_config.yaml",
                "tests/e2e/e2e_test_framework.py",
                "tests/performance/performance_optimizer.py",
            ]

            for path in required_paths:
                if not Path(path).exists():
                    logger.error(f"âŒ å¿…è¦æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                    return False

            logger.info("âœ… åˆå§‹ç³»çµ±æª¢æŸ¥é€šé")
            return True

        except Exception as e:
            logger.error(f"âŒ åˆå§‹ç³»çµ±æª¢æŸ¥ç•°å¸¸: {e}")
            return False

    async def _run_performance_optimization(self) -> bool:
        """åŸ·è¡Œæ€§èƒ½å„ªåŒ–"""
        logger.info("âš¡ åŸ·è¡Œç³»çµ±æ€§èƒ½å„ªåŒ–")

        try:
            # é‹è¡Œæ€§èƒ½å„ªåŒ–å™¨
            success = await self.performance_optimizer.run_performance_optimization()

            if success:
                logger.info("âœ… ç³»çµ±æ€§èƒ½å„ªåŒ–å®Œæˆ")

                # ç­‰å¾…ç³»çµ±ç©©å®š
                logger.info("â³ ç­‰å¾…ç³»çµ±ç©©å®š...")
                await asyncio.sleep(10)

                return True
            else:
                logger.error("âŒ ç³»çµ±æ€§èƒ½å„ªåŒ–å¤±æ•—")
                return False

        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½å„ªåŒ–ç•°å¸¸: {e}")
            return False

    async def _run_e2e_tests(self) -> bool:
        """åŸ·è¡Œç«¯åˆ°ç«¯æ¸¬è©¦"""
        logger.info("ğŸ§ª åŸ·è¡Œç«¯åˆ°ç«¯ç³»çµ±é›†æˆæ¸¬è©¦")

        try:
            # é‹è¡Œ E2E æ¸¬è©¦æ¡†æ¶
            success = await self.e2e_framework.run_all_scenarios()

            if success:
                logger.info("âœ… ç«¯åˆ°ç«¯æ¸¬è©¦å…¨éƒ¨é€šé")
                return True
            else:
                logger.error("âŒ ç«¯åˆ°ç«¯æ¸¬è©¦æœªå…¨éƒ¨é€šé")

                # åˆ†æå¤±æ•—çš„æ¸¬è©¦
                await self._analyze_test_failures()
                return False

        except Exception as e:
            logger.error(f"âŒ ç«¯åˆ°ç«¯æ¸¬è©¦ç•°å¸¸: {e}")
            return False

    async def _analyze_test_failures(self):
        """åˆ†ææ¸¬è©¦å¤±æ•—åŸå› """
        logger.info("ğŸ” åˆ†ææ¸¬è©¦å¤±æ•—åŸå› ")

        try:
            failed_tests = [
                result
                for result in self.e2e_framework.test_results
                if result.status != "passed"
            ]

            if failed_tests:
                logger.info(f"å¤±æ•—çš„æ¸¬è©¦æ•¸é‡: {len(failed_tests)}")

                for failed_test in failed_tests:
                    logger.error(f"å¤±æ•—æ¸¬è©¦: {failed_test.test_name}")
                    logger.error(f"  ç‹€æ…‹: {failed_test.status}")
                    logger.error(f"  éŒ¯èª¤: {failed_test.error_message}")

                    # åˆ†ææ€§èƒ½æŒ‡æ¨™
                    if failed_test.performance_metrics:
                        logger.info(f"  æ€§èƒ½æŒ‡æ¨™: {failed_test.performance_metrics}")

                # æä¾›å„ªåŒ–å»ºè­°
                await self._provide_optimization_suggestions(failed_tests)

        except Exception as e:
            logger.error(f"âŒ åˆ†ææ¸¬è©¦å¤±æ•—ç•°å¸¸: {e}")

    async def _provide_optimization_suggestions(self, failed_tests):
        """æä¾›å„ªåŒ–å»ºè­°"""
        logger.info("ğŸ’¡ æä¾›å„ªåŒ–å»ºè­°")

        suggestions = []

        for failed_test in failed_tests:
            if (
                "latency" in failed_test.error_message
                or "å»¶é²" in failed_test.error_message
            ):
                suggestions.append("å„ªåŒ–ç¶²è·¯å»¶é²ï¼šæª¢æŸ¥ç¶²è·¯é…ç½®å’Œè·¯ç”±")

            if (
                "throughput" in failed_test.error_message
                or "ååé‡" in failed_test.error_message
            ):
                suggestions.append("å„ªåŒ–ååé‡ï¼šèª¿æ•´ç·©è¡å€å¤§å°å’Œä¸¦ç™¼è¨­ç½®")

            if (
                "connection" in failed_test.error_message
                or "é€£æ¥" in failed_test.error_message
            ):
                suggestions.append("å„ªåŒ–é€£æ¥ç©©å®šæ€§ï¼šå¢åŠ é‡è©¦æ©Ÿåˆ¶å’Œé€£æ¥æ± ")

            if "timeout" in failed_test.status or "è¶…æ™‚" in failed_test.error_message:
                suggestions.append("å¢åŠ è¶…æ™‚æ™‚é–“ä¸¦å„ªåŒ–è™•ç†é‚è¼¯")

        # å»é‡ä¸¦è¼¸å‡ºå»ºè­°
        unique_suggestions = list(set(suggestions))
        for i, suggestion in enumerate(unique_suggestions, 1):
            logger.info(f"  {i}. {suggestion}")

    async def _adjust_system_for_retry(self):
        """ç‚ºé‡è©¦èª¿æ•´ç³»çµ±"""
        logger.info("ğŸ”§ èª¿æ•´ç³»çµ±åƒæ•¸æº–å‚™é‡è©¦")

        try:
            # æ¸…ç†ç³»çµ±è³‡æº
            import gc

            gc.collect()

            # ç­‰å¾…ç³»çµ±ç©©å®š
            await asyncio.sleep(5)

            # é‡ç½®ç›£æ§å™¨
            if hasattr(self.e2e_framework, "performance_monitor"):
                await self.e2e_framework.performance_monitor.stop_monitoring()
                await asyncio.sleep(2)

            # æ¸…ç©ºæ¸¬è©¦çµæœæº–å‚™é‡æ–°æ¸¬è©¦
            self.e2e_framework.test_results = []

            logger.info("âœ… ç³»çµ±èª¿æ•´å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ ç³»çµ±èª¿æ•´ç•°å¸¸: {e}")

    async def _generate_final_report(self, overall_success: bool):
        """ç”Ÿæˆæœ€çµ‚å ±å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆæœ€çµ‚æ¸¬è©¦å ±å‘Š")

        try:
            timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")

            # æ”¶é›†æ‰€æœ‰æ¸¬è©¦çµæœ
            test_results = getattr(self.e2e_framework, "test_results", [])

            total_tests = len(test_results)
            passed_tests = sum(1 for r in test_results if r.status == "passed")
            failed_tests = sum(1 for r in test_results if r.status == "failed")
            error_tests = sum(1 for r in test_results if r.status == "error")

            success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

            # å‰µå»ºæœ€çµ‚å ±å‘Š
            final_report = {
                "test_execution": {
                    "timestamp": datetime.utcnow().isoformat(),
                    "overall_success": overall_success,
                    "total_attempts": self.current_attempt,
                    "max_attempts": self.max_retry_attempts,
                },
                "test_summary": {
                    "total_tests": total_tests,
                    "passed": passed_tests,
                    "failed": failed_tests,
                    "error": error_tests,
                    "success_rate": success_rate,
                },
                "performance_optimization": {
                    "executed": True,
                    "baseline_established": True,
                    "optimizations_applied": True,
                },
                "detailed_results": [
                    {
                        "test_name": r.test_name,
                        "scenario": r.scenario,
                        "status": r.status,
                        "duration_seconds": r.duration_seconds,
                        "error_message": r.error_message,
                    }
                    for r in test_results
                ],
            }

            # ä¿å­˜å ±å‘Š
            import json

            reports_dir = Path("tests/reports")
            reports_dir.mkdir(exist_ok=True)

            report_path = (
                reports_dir / f"final_e2e_optimization_report_{timestamp}.json"
            )
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(final_report, f, indent=2, ensure_ascii=False, default=str)

            # è¼¸å‡ºæ‘˜è¦
            logger.info("\n" + "=" * 80)
            logger.info("ğŸ“Š æœ€çµ‚æ¸¬è©¦å ±å‘Šæ‘˜è¦")
            logger.info("=" * 80)
            logger.info(f"æ•´é«”æˆåŠŸ: {'âœ… æ˜¯' if overall_success else 'âŒ å¦'}")
            logger.info(f"å˜—è©¦æ¬¡æ•¸: {self.current_attempt}/{self.max_retry_attempts}")
            logger.info(f"æ¸¬è©¦ç¸½æ•¸: {total_tests}")
            logger.info(f"é€šé: {passed_tests}")
            logger.info(f"å¤±æ•—: {failed_tests}")
            logger.info(f"éŒ¯èª¤: {error_tests}")
            logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
            logger.info(f"è©³ç´°å ±å‘Š: {report_path}")
            logger.info("=" * 80)

            if overall_success:
                logger.info("ğŸ‰ å®Œæ•´çš„ç«¯åˆ°ç«¯æ¸¬è©¦å’Œæ€§èƒ½å„ªåŒ–100%æˆåŠŸå®Œæˆï¼")
            else:
                logger.error("âŒ æ¸¬è©¦æœªèƒ½é”åˆ°100%æˆåŠŸç‡")

        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆæœ€çµ‚å ±å‘Šç•°å¸¸: {e}")


async def main():
    """ä¸»å‡½æ•¸"""
    runner = CompleteE2ETestRunner()
    success = await runner.run_complete_test_suite()

    return 0 if success else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
