#!/usr/bin/env python3
"""
Gymnasium RL è¨“ç·´çµ±ä¸€æ¸¬è©¦ç¨‹å¼

æ•´åˆæ‰€æœ‰ Gymnasium ç›¸é—œæ¸¬è©¦ï¼š
- ç’°å¢ƒæ¸¬è©¦
- è¨“ç·´æ¸¬è©¦  
- æ¨¡å‹é©—è­‰æ¸¬è©¦

åŸ·è¡Œæ–¹å¼:
python gymnasium_tests.py [--env=all|satellite|handover] [--quick]
"""

import sys
import os
import time
import logging
import argparse
import numpy as np
from typing import Dict, List, Any, Optional

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class GymnasiumTestFramework:
    """Gymnasium æ¸¬è©¦çµ±ä¸€æ¡†æ¶"""
    
    def __init__(self, quick_mode=False):
        self.results = []
        self.quick_mode = quick_mode
        self.start_time = None
        
    def log_result(self, test_name: str, success: bool, details: str = "", metrics: Dict = None):
        """è¨˜éŒ„æ¸¬è©¦çµæœ"""
        self.results.append({
            'name': test_name,
            'success': success,
            'details': details,
            'metrics': metrics or {},
            'timestamp': time.time()
        })
        
        status = "âœ… PASS" if success else "âŒ FAIL"
        logger.info(f"{status} {test_name} - {details}")

    def test_satellite_environment(self):
        """æ¸¬è©¦è¡›æ˜Ÿç’°å¢ƒ"""
        logger.info("ğŸ›°ï¸ æ¸¬è©¦è¡›æ˜Ÿç’°å¢ƒ...")
        
        try:
            # æ¨¡æ“¬ Gymnasium è¡›æ˜Ÿç’°å¢ƒ
            class SatelliteEnv:
                def __init__(self):
                    self.num_satellites = 5
                    self.num_ues = 10
                    self.observation_space_size = self.num_satellites * 4 + self.num_ues * 3  # ä½ç½®+ç‹€æ…‹
                    self.action_space_size = self.num_ues  # æ¯å€‹UEçš„è¡›æ˜Ÿé¸æ“‡
                    self.state = None
                    self.step_count = 0
                
                def reset(self):
                    """é‡ç½®ç’°å¢ƒ"""
                    self.state = np.random.rand(self.observation_space_size)
                    self.step_count = 0
                    return self.state.copy()
                
                def step(self, action):
                    """åŸ·è¡Œå‹•ä½œ"""
                    assert len(action) == self.action_space_size, f"å‹•ä½œç¶­åº¦éŒ¯èª¤: {len(action)} vs {self.action_space_size}"
                    
                    # æ¨¡æ“¬ç’°å¢ƒå‹•æ…‹
                    self.state += np.random.normal(0, 0.01, self.observation_space_size)
                    self.state = np.clip(self.state, 0, 1)
                    self.step_count += 1
                    
                    # è¨ˆç®—çå‹µ
                    reward = self._calculate_reward(action)
                    
                    # åˆ¤æ–·æ˜¯å¦çµæŸ
                    done = self.step_count >= 100
                    
                    return self.state.copy(), reward, done, {}
                
                def _calculate_reward(self, action):
                    """è¨ˆç®—çå‹µ"""
                    # æ¨¡æ“¬çå‹µè¨ˆç®—ï¼šåŸºæ–¼è² è¼‰å¹³è¡¡å’Œæ›æ‰‹é »ç‡
                    load_balance_penalty = np.var(np.bincount(action, minlength=self.num_satellites))
                    handover_penalty = np.sum(np.abs(np.diff(action))) * 0.1
                    
                    base_reward = 10.0
                    return base_reward - load_balance_penalty - handover_penalty
            
            # æ¸¬è©¦ç’°å¢ƒåŸºæœ¬åŠŸèƒ½
            env = SatelliteEnv()
            
            # æ¸¬è©¦é‡ç½®
            initial_state = env.reset()
            assert len(initial_state) == env.observation_space_size, "ç‹€æ…‹ç¶­åº¦éŒ¯èª¤"
            assert np.all(initial_state >= 0) and np.all(initial_state <= 1), "ç‹€æ…‹ç¯„åœéŒ¯èª¤"
            
            # æ¸¬è©¦æ­¥é©ŸåŸ·è¡Œ
            episodes = 2 if self.quick_mode else 5
            total_reward = 0
            total_steps = 0
            
            for episode in range(episodes):
                state = env.reset()
                episode_reward = 0
                step_count = 0
                
                while step_count < (20 if self.quick_mode else 50):
                    # éš¨æ©Ÿå‹•ä½œ
                    action = np.random.randint(0, env.num_satellites, env.num_ues)
                    
                    next_state, reward, done, info = env.step(action)
                    episode_reward += reward
                    step_count += 1
                    
                    assert len(next_state) == env.observation_space_size, "ä¸‹ä¸€ç‹€æ…‹ç¶­åº¦éŒ¯èª¤"
                    
                    if done:
                        break
                
                total_reward += episode_reward
                total_steps += step_count
            
            avg_reward = total_reward / episodes
            avg_steps = total_steps / episodes
            
            details = f"å¹³å‡çå‹µ: {avg_reward:.2f}, å¹³å‡æ­¥æ•¸: {avg_steps:.1f}"
            self.log_result("è¡›æ˜Ÿç’°å¢ƒæ¸¬è©¦", True, details, 
                          {'avg_reward': avg_reward, 'avg_steps': avg_steps, 'episodes': episodes})
            return True
            
        except Exception as e:
            self.log_result("è¡›æ˜Ÿç’°å¢ƒæ¸¬è©¦", False, str(e))
            return False

    def test_handover_environment(self):
        """æ¸¬è©¦æ›æ‰‹ç’°å¢ƒ"""
        logger.info("ğŸ”„ æ¸¬è©¦æ›æ‰‹ç’°å¢ƒ...")
        
        try:
            # æ¨¡æ“¬æ›æ‰‹å°ˆç”¨ç’°å¢ƒ
            class HandoverEnv:
                def __init__(self):
                    self.num_satellites = 3
                    self.ue_position = np.array([0.0, 0.0])  # 2Dä½ç½®
                    self.satellite_positions = np.random.rand(self.num_satellites, 2)
                    self.current_satellite = 0
                    self.handover_count = 0
                    self.step_count = 0
                
                def reset(self):
                    """é‡ç½®ç’°å¢ƒ"""
                    self.ue_position = np.random.rand(2)
                    self.satellite_positions = np.random.rand(self.num_satellites, 2)
                    self.current_satellite = 0
                    self.handover_count = 0
                    self.step_count = 0
                    return self._get_observation()
                
                def _get_observation(self):
                    """ç²å–è§€å¯Ÿ"""
                    obs = []
                    obs.extend(self.ue_position)
                    obs.extend(self.satellite_positions.flatten())
                    obs.append(self.current_satellite)
                    obs.append(self.handover_count)
                    return np.array(obs)
                
                def step(self, action):
                    """åŸ·è¡Œå‹•ä½œ"""
                    # action: 0=ä¿æŒ, 1=æ›æ‰‹åˆ°è¡›æ˜Ÿ1, 2=æ›æ‰‹åˆ°è¡›æ˜Ÿ2, ...
                    
                    old_satellite = self.current_satellite
                    
                    if action > 0 and action <= self.num_satellites:
                        new_satellite = action - 1
                        if new_satellite != self.current_satellite:
                            self.current_satellite = new_satellite
                            self.handover_count += 1
                    
                    # æ¨¡æ“¬UEç§»å‹•
                    self.ue_position += np.random.normal(0, 0.01, 2)
                    self.ue_position = np.clip(self.ue_position, 0, 1)
                    
                    # æ¨¡æ“¬è¡›æ˜Ÿç§»å‹•
                    self.satellite_positions += np.random.normal(0, 0.005, (self.num_satellites, 2))
                    self.satellite_positions = np.clip(self.satellite_positions, 0, 1)
                    
                    self.step_count += 1
                    
                    # è¨ˆç®—çå‹µ
                    reward = self._calculate_handover_reward(old_satellite, self.current_satellite)
                    
                    done = self.step_count >= 50
                    
                    return self._get_observation(), reward, done, {'handovers': self.handover_count}
                
                def _calculate_handover_reward(self, old_sat: int, new_sat: int):
                    """è¨ˆç®—æ›æ‰‹çå‹µ"""
                    # è·é›¢çå‹µï¼šèˆ‡ç•¶å‰è¡›æ˜Ÿçš„è·é›¢è¶Šè¿‘çå‹µè¶Šé«˜
                    current_sat_pos = self.satellite_positions[self.current_satellite]
                    distance = np.linalg.norm(self.ue_position - current_sat_pos)
                    distance_reward = 1.0 - distance
                    
                    # æ›æ‰‹æ‡²ç½°
                    handover_penalty = 0.1 if old_sat != new_sat else 0.0
                    
                    return distance_reward - handover_penalty
            
            # æ¸¬è©¦æ›æ‰‹ç’°å¢ƒ
            env = HandoverEnv()
            
            episodes = 3 if self.quick_mode else 5
            total_handovers = 0
            total_rewards = 0
            
            for episode in range(episodes):
                state = env.reset()
                episode_reward = 0
                
                for step in range(30 if self.quick_mode else 50):
                    # ç°¡å–®ç­–ç•¥ï¼šåŸºæ–¼è·é›¢é¸æ“‡è¡›æ˜Ÿ
                    ue_pos = state[:2]
                    sat_positions = state[2:2+env.num_satellites*2].reshape(env.num_satellites, 2)
                    
                    distances = [np.linalg.norm(ue_pos - sat_pos) for sat_pos in sat_positions]
                    best_satellite = np.argmin(distances)
                    
                    action = best_satellite + 1  # +1 å› ç‚º0æ˜¯ä¿æŒå‹•ä½œ
                    
                    next_state, reward, done, info = env.step(action)
                    episode_reward += reward
                    state = next_state
                    
                    if done:
                        break
                
                total_handovers += info.get('handovers', 0)
                total_rewards += episode_reward
            
            avg_handovers = total_handovers / episodes
            avg_reward = total_rewards / episodes
            
            details = f"å¹³å‡æ›æ‰‹æ¬¡æ•¸: {avg_handovers:.1f}, å¹³å‡çå‹µ: {avg_reward:.2f}"
            self.log_result("æ›æ‰‹ç’°å¢ƒæ¸¬è©¦", True, details,
                          {'avg_handovers': avg_handovers, 'avg_reward': avg_reward})
            return True
            
        except Exception as e:
            self.log_result("æ›æ‰‹ç’°å¢ƒæ¸¬è©¦", False, str(e))
            return False

    def test_rl_training_simulation(self):
        """æ¸¬è©¦RLè¨“ç·´æ¨¡æ“¬"""
        logger.info("ğŸ§  æ¸¬è©¦RLè¨“ç·´æ¨¡æ“¬...")
        
        try:
            # æ¨¡æ“¬ç°¡å–®çš„Q-learningä»£ç†
            class SimpleQLearningAgent:
                def __init__(self, state_size: int, action_size: int, learning_rate=0.1, epsilon=0.1):
                    self.state_size = state_size
                    self.action_size = action_size
                    self.learning_rate = learning_rate
                    self.epsilon = epsilon
                    self.q_table = {}  # ä½¿ç”¨å­—å…¸æ¨¡æ“¬Qè¡¨
                    self.training_rewards = []
                
                def _state_to_key(self, state):
                    """å°‡ç‹€æ…‹è½‰æ›ç‚ºå­—å…¸éµ"""
                    # ç°¡åŒ–ï¼šåªä½¿ç”¨ç‹€æ…‹çš„å‰å¹¾å€‹ç¶­åº¦
                    return tuple(np.round(state[:min(4, len(state))], 1))
                
                def choose_action(self, state, training=True):
                    """é¸æ“‡å‹•ä½œ"""
                    state_key = self._state_to_key(state)
                    
                    if training and np.random.random() < self.epsilon:
                        return np.random.randint(self.action_size)
                    
                    if state_key not in self.q_table:
                        self.q_table[state_key] = np.zeros(self.action_size)
                    
                    return np.argmax(self.q_table[state_key])
                
                def update_q_value(self, state, action, reward, next_state):
                    """æ›´æ–°Qå€¼"""
                    state_key = self._state_to_key(state)
                    next_state_key = self._state_to_key(next_state)
                    
                    if state_key not in self.q_table:
                        self.q_table[state_key] = np.zeros(self.action_size)
                    if next_state_key not in self.q_table:
                        self.q_table[next_state_key] = np.zeros(self.action_size)
                    
                    # Q-learningæ›´æ–°
                    old_q = self.q_table[state_key][action]
                    next_max_q = np.max(self.q_table[next_state_key])
                    
                    new_q = old_q + self.learning_rate * (reward + 0.9 * next_max_q - old_q)
                    self.q_table[state_key][action] = new_q
                
                def train_episode(self, env):
                    """è¨“ç·´ä¸€å€‹episode"""
                    state = env.reset()
                    total_reward = 0
                    steps = 0
                    
                    while steps < 30:  # é™åˆ¶æ­¥æ•¸
                        action = self.choose_action(state, training=True)
                        next_state, reward, done, _ = env.step([action] if hasattr(env, 'num_ues') else action)
                        
                        self.update_q_value(state, action, reward, next_state)
                        
                        total_reward += reward
                        state = next_state
                        steps += 1
                        
                        if done:
                            break
                    
                    self.training_rewards.append(total_reward)
                    return total_reward
            
            # å‰µå»ºç°¡å–®ç’°å¢ƒç”¨æ–¼è¨“ç·´æ¸¬è©¦
            class SimpleEnv:
                def __init__(self):
                    self.state_size = 4
                    self.action_size = 3
                    self.state = None
                    self.step_count = 0
                
                def reset(self):
                    self.state = np.random.rand(self.state_size)
                    self.step_count = 0
                    return self.state.copy()
                
                def step(self, action):
                    # ç°¡åŒ–ç’°å¢ƒå‹•æ…‹
                    self.state += np.random.normal(0, 0.1, self.state_size)
                    self.state = np.clip(self.state, 0, 1)
                    self.step_count += 1
                    
                    # ç°¡å–®çå‹µï¼šç‹€æ…‹å€¼è¶Šé«˜çå‹µè¶Šé«˜ï¼Œå‹•ä½œ2çå‹µæ›´é«˜
                    reward = np.sum(self.state) + (0.5 if action == 2 else 0)
                    done = self.step_count >= 20
                    
                    return self.state.copy(), reward, done, {}
            
            # åŸ·è¡Œè¨“ç·´æ¸¬è©¦
            env = SimpleEnv()
            agent = SimpleQLearningAgent(env.state_size, env.action_size)
            
            # è¨“ç·´
            training_episodes = 20 if self.quick_mode else 50
            for episode in range(training_episodes):
                agent.train_episode(env)
            
            # è©•ä¼°æ”¹å–„
            initial_rewards = agent.training_rewards[:5]
            final_rewards = agent.training_rewards[-5:]
            
            initial_avg = np.mean(initial_rewards)
            final_avg = np.mean(final_rewards)
            improvement = (final_avg - initial_avg) / abs(initial_avg) if initial_avg != 0 else 0
            
            # æ¸¬è©¦å­¸ç¿’åˆ°çš„ç­–ç•¥
            test_episodes = 5
            test_rewards = []
            
            for _ in range(test_episodes):
                state = env.reset()
                total_reward = 0
                
                for _ in range(20):
                    action = agent.choose_action(state, training=False)
                    next_state, reward, done, _ = env.step(action)
                    total_reward += reward
                    state = next_state
                    
                    if done:
                        break
                
                test_rewards.append(total_reward)
            
            avg_test_reward = np.mean(test_rewards)
            
            details = f"è¨“ç·´æ”¹å–„: {improvement:.2%}, æ¸¬è©¦çå‹µ: {avg_test_reward:.2f}, Qè¡¨å¤§å°: {len(agent.q_table)}"
            self.log_result("RLè¨“ç·´æ¨¡æ“¬", True, details,
                          {'improvement': improvement, 'test_reward': avg_test_reward, 'q_table_size': len(agent.q_table)})
            return True
            
        except Exception as e:
            self.log_result("RLè¨“ç·´æ¨¡æ“¬", False, str(e))
            return False

    def test_model_evaluation(self):
        """æ¸¬è©¦æ¨¡å‹è©•ä¼°"""
        logger.info("ğŸ“Š æ¸¬è©¦æ¨¡å‹è©•ä¼°...")
        
        try:
            # æ¨¡æ“¬æ¨¡å‹è©•ä¼°æ¡†æ¶
            class ModelEvaluator:
                def __init__(self):
                    self.evaluation_metrics = ['reward', 'handover_efficiency', 'load_balance', 'latency']
                
                def evaluate_model(self, model_type: str, episodes: int) -> dict:
                    """è©•ä¼°æ¨¡å‹æ•ˆèƒ½"""
                    np.random.seed(42)  # å›ºå®šéš¨æ©Ÿç¨®å­ç¢ºä¿å¯é‡è¤‡
                    
                    results = {
                        'model_type': model_type,
                        'episodes': episodes,
                        'metrics': {}
                    }
                    
                    # æ¨¡æ“¬ä¸åŒæ¨¡å‹çš„æ•ˆèƒ½ç‰¹æ€§
                    if model_type == 'random':
                        base_reward = 5.0
                        reward_std = 2.0
                        handover_eff = 0.3
                        load_balance = 0.4
                        latency = 100.0
                    elif model_type == 'greedy':
                        base_reward = 8.0
                        reward_std = 1.5
                        handover_eff = 0.6
                        load_balance = 0.5
                        latency = 80.0
                    elif model_type == 'q_learning':
                        base_reward = 10.0
                        reward_std = 1.0
                        handover_eff = 0.8
                        load_balance = 0.7
                        latency = 60.0
                    elif model_type == 'dqn':
                        base_reward = 12.0
                        reward_std = 0.8
                        handover_eff = 0.85
                        load_balance = 0.8
                        latency = 50.0
                    else:
                        base_reward = 6.0
                        reward_std = 2.5
                        handover_eff = 0.4
                        load_balance = 0.3
                        latency = 120.0
                    
                    # ç”Ÿæˆè©•ä¼°æ•¸æ“š
                    rewards = np.random.normal(base_reward, reward_std, episodes)
                    
                    results['metrics'] = {
                        'avg_reward': float(np.mean(rewards)),
                        'reward_std': float(np.std(rewards)),
                        'handover_efficiency': handover_eff + np.random.normal(0, 0.05),
                        'load_balance_score': load_balance + np.random.normal(0, 0.05),
                        'avg_latency_ms': latency + np.random.normal(0, 5)
                    }
                    
                    return results
                
                def compare_models(self, model_results: List[dict]) -> dict:
                    """æ¯”è¼ƒå¤šå€‹æ¨¡å‹"""
                    if not model_results:
                        return {}
                    
                    comparison = {
                        'models': len(model_results),
                        'best_model': {},
                        'rankings': {}
                    }
                    
                    # æ‰¾å‡ºå„é …æŒ‡æ¨™çš„æœ€ä½³æ¨¡å‹
                    metrics = ['avg_reward', 'handover_efficiency', 'load_balance_score']
                    
                    for metric in metrics:
                        best_value = max(result['metrics'][metric] for result in model_results)
                        best_model = next(r for r in model_results if r['metrics'][metric] == best_value)
                        comparison['best_model'][metric] = best_model['model_type']
                    
                    # å»¶é²è¶Šä½è¶Šå¥½
                    best_latency = min(result['metrics']['avg_latency_ms'] for result in model_results)
                    best_latency_model = next(r for r in model_results if r['metrics']['avg_latency_ms'] == best_latency)
                    comparison['best_model']['avg_latency_ms'] = best_latency_model['model_type']
                    
                    # è¨ˆç®—ç¶œåˆæ’å
                    for result in model_results:
                        score = (
                            result['metrics']['avg_reward'] * 0.3 +
                            result['metrics']['handover_efficiency'] * 10 * 0.3 +
                            result['metrics']['load_balance_score'] * 10 * 0.2 +
                            (200 - result['metrics']['avg_latency_ms']) / 200 * 0.2
                        )
                        comparison['rankings'][result['model_type']] = score
                    
                    # æ‰¾å‡ºç¸½é«”æœ€ä½³æ¨¡å‹
                    best_overall = max(comparison['rankings'], key=comparison['rankings'].get)
                    comparison['best_overall'] = best_overall
                    
                    return comparison
            
            # åŸ·è¡Œæ¨¡å‹è©•ä¼°æ¸¬è©¦
            evaluator = ModelEvaluator()
            
            # æ¸¬è©¦ä¸åŒæ¨¡å‹
            models_to_test = ['random', 'greedy', 'q_learning'] if self.quick_mode else ['random', 'greedy', 'q_learning', 'dqn']
            episodes_per_model = 10 if self.quick_mode else 20
            
            model_results = []
            for model_type in models_to_test:
                result = evaluator.evaluate_model(model_type, episodes_per_model)
                model_results.append(result)
                
                # é©—è­‰çµæœåˆç†æ€§
                assert result['metrics']['avg_reward'] > 0, f"çå‹µç•°å¸¸: {model_type}"
                assert 0 <= result['metrics']['handover_efficiency'] <= 1, f"æ›æ‰‹æ•ˆç‡ç•°å¸¸: {model_type}"
                assert result['metrics']['avg_latency_ms'] > 0, f"å»¶é²ç•°å¸¸: {model_type}"
            
            # æ¯”è¼ƒæ¨¡å‹
            comparison = evaluator.compare_models(model_results)
            
            # é©—è­‰æ¯”è¼ƒçµæœ
            assert len(comparison['rankings']) == len(models_to_test), "æ’åæ•¸é‡ä¸åŒ¹é…"
            assert comparison['best_overall'] in models_to_test, "æœ€ä½³æ¨¡å‹ä¸åœ¨æ¸¬è©¦åˆ—è¡¨ä¸­"
            
            # é©—è­‰é€²éšæ¨¡å‹æ•ˆèƒ½æ›´å¥½
            if 'q_learning' in comparison['rankings'] and 'random' in comparison['rankings']:
                assert comparison['rankings']['q_learning'] > comparison['rankings']['random'], \
                    "Q-learningæ‡‰è©²æ¯”éš¨æ©Ÿç­–ç•¥æ•ˆèƒ½æ›´å¥½"
            
            best_model = comparison['best_overall']
            best_score = comparison['rankings'][best_model]
            
            details = f"æœ€ä½³æ¨¡å‹: {best_model}, ç¶œåˆåˆ†æ•¸: {best_score:.2f}, æ¸¬è©¦æ¨¡å‹æ•¸: {len(models_to_test)}"
            self.log_result("æ¨¡å‹è©•ä¼°", True, details,
                          {'best_model': best_model, 'best_score': best_score, 'models_tested': len(models_to_test)})
            return True
            
        except Exception as e:
            self.log_result("æ¨¡å‹è©•ä¼°", False, str(e))
            return False

    def run_all_tests(self):
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦"""
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡ŒGymnasiumæ¸¬è©¦...")
        self.start_time = time.time()
        
        results = []
        results.append(self.test_satellite_environment())
        results.append(self.test_handover_environment())
        results.append(self.test_rl_training_simulation())
        results.append(self.test_model_evaluation())
        
        return all(results)
    
    def print_summary(self):
        """å°å‡ºæ¸¬è©¦æ‘˜è¦"""
        if not self.results:
            logger.warning("æ²’æœ‰æ¸¬è©¦çµæœ")
            return
        
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results if r['success'])
        failed_tests = total_tests - passed_tests
        total_duration = time.time() - self.start_time if self.start_time else 0
        
        print("\n" + "="*60)
        print("ğŸ® Gymnasium RL æ¸¬è©¦æ¡†æ¶ - æ¸¬è©¦å ±å‘Š")
        print("="*60)
        print(f"ğŸ“Š æ¸¬è©¦çµ±è¨ˆ:")
        print(f"   ç¸½æ¸¬è©¦æ•¸: {total_tests}")
        print(f"   é€šé: {passed_tests} âœ…")
        print(f"   å¤±æ•—: {failed_tests} âŒ") 
        print(f"   æˆåŠŸç‡: {passed_tests/total_tests:.1%}")
        print(f"   ç¸½è€—æ™‚: {total_duration:.2f}s")
        print(f"   å¿«é€Ÿæ¨¡å¼: {'æ˜¯' if self.quick_mode else 'å¦'}")
        print()
        
        if failed_tests > 0:
            print("âŒ å¤±æ•—çš„æ¸¬è©¦:")
            for result in self.results:
                if not result['success']:
                    print(f"   - {result['name']}: {result['details']}")
            print()
        
        print("âœ… Gymnasiumæ¸¬è©¦å®Œæˆ" if passed_tests == total_tests else "âš ï¸  éƒ¨åˆ†æ¸¬è©¦å¤±æ•—")
        print("="*60)

def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(description='Gymnasium RL æ¸¬è©¦æ¡†æ¶')
    parser.add_argument('--env', choices=['all', 'satellite', 'handover'], default='all', help='ç’°å¢ƒé¡å‹')
    parser.add_argument('--quick', action='store_true', help='å¿«é€Ÿæ¸¬è©¦æ¨¡å¼')
    parser.add_argument('--verbose', '-v', action='store_true', help='è©³ç´°è¼¸å‡º')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    framework = GymnasiumTestFramework(quick_mode=args.quick)
    
    try:
        if args.env == 'satellite':
            success = framework.test_satellite_environment()
        elif args.env == 'handover':
            success = framework.test_handover_environment()
        else:  # all
            success = framework.run_all_tests()
        
        framework.print_summary()
        
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        logger.info("æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(130)
    except Exception as e:
        logger.error(f"æ¸¬è©¦åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
