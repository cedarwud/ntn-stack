"""
Algorithm Verification Integration Tests - Stage 3

æ¸¬è©¦ç®—æ³•é©—è­‰æœå‹™çš„å®Œæ•´åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. è«–æ–‡åŸºæº–å°æ¯”æ¸¬è©¦
2. æ€§èƒ½å›æ­¸æ¸¬è©¦
3. å¤šè¡›æ˜Ÿé©—è­‰æ¸¬è©¦
4. çµ±è¨ˆåˆ†æå’Œå ±å‘Šç”Ÿæˆ
5. é©—è­‰æ¡†æ¶çš„å¯é æ€§
"""

import pytest
import asyncio
import sys
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any

# æ·»åŠ  netstack è·¯å¾‘
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../netstack'))

from netstack_api.services.algorithm_verification_service import (
    AlgorithmVerificationService,
    VerificationScenario,
    PaperBaseline,
    VerificationResult,
    RegressionTestCase
)
from netstack_api.services.enhanced_synchronized_algorithm import EnhancedSynchronizedAlgorithm


class TestAlgorithmVerification:
    """ç®—æ³•é©—è­‰æ•´åˆæ¸¬è©¦"""

    @pytest.fixture
    async def verification_service(self):
        """å‰µå»ºé©—è­‰æœå‹™å¯¦ä¾‹"""
        # å‰µå»ºä¾è³´çš„ç®—æ³•æœå‹™
        enhanced_algorithm = EnhancedSynchronizedAlgorithm()
        await enhanced_algorithm.start_enhanced_algorithm()
        
        # å‰µå»ºé©—è­‰æœå‹™
        verification_service = AlgorithmVerificationService(
            enhanced_algorithm=enhanced_algorithm
        )
        await verification_service.start_verification_service()
        
        yield verification_service
        
        # æ¸…ç†
        await verification_service.stop_verification_service()
        await enhanced_algorithm.stop_enhanced_algorithm()

    @pytest.mark.asyncio
    async def test_paper_baseline_comparison(self, verification_service):
        """æ¸¬è©¦è«–æ–‡åŸºæº–å°æ¯”åŠŸèƒ½"""
        print("\n=== æ¸¬è©¦è«–æ–‡åŸºæº–å°æ¯”åŠŸèƒ½ ===")
        
        # åŸ·è¡Œè«–æ–‡åŸºæº–å°æ¯”æ¸¬è©¦
        result = await verification_service._run_paper_baseline_comparison()
        
        # é©—è­‰çµæœçµæ§‹
        assert isinstance(result, VerificationResult)
        assert result.scenario == VerificationScenario.PAPER_BASELINE_COMPARISON
        assert result.sample_size >= 30  # æœ€å°æ¨£æœ¬æ•¸
        
        # é©—è­‰çµ±è¨ˆæ•¸æ“š
        assert "handover_accuracy" in result.statistics
        assert "computation_time_ms" in result.statistics
        assert "convergence_rate" in result.statistics
        
        # é©—è­‰åŸºæº–å°æ¯”
        assert "enhanced_sync" in result.baseline_comparison
        
        # æª¢æŸ¥æ¯å€‹æŒ‡æ¨™çš„çµ±è¨ˆåˆ†æ
        for metric, stats in result.statistics.items():
            assert "mean" in stats
            assert "std_dev" in stats
            assert "min" in stats
            assert "max" in stats
            assert stats["count"] > 0
        
        # æª¢æŸ¥åŸºæº–å°æ¯”çµæœ
        for baseline_id, comparison in result.baseline_comparison.items():
            assert "comparisons" in comparison
            for metric, comp in comparison["comparisons"].items():
                assert "measured" in comp
                assert "baseline" in comp
                assert "deviation" in comp
                assert "within_tolerance" in comp
                assert "performance_grade" in comp
        
        print(f"âœ“ è«–æ–‡åŸºæº–å°æ¯”æ¸¬è©¦å®Œæˆ")
        print(f"  - æ¨£æœ¬æ•¸é‡: {result.sample_size}")
        print(f"  - æ¸¬è©¦é€šé: {result.passed}")
        print(f"  - ä¿¡å¿ƒæ°´æº–: {result.confidence_level:.1%}")
        
        # é¡¯ç¤ºé—œéµæŒ‡æ¨™
        if "handover_accuracy" in result.statistics:
            accuracy_stats = result.statistics["handover_accuracy"]
            print(f"  - Handover æº–ç¢ºç‡: {accuracy_stats['mean']:.1%} Â± {accuracy_stats['std_dev']:.3f}")
        
        if "computation_time_ms" in result.statistics:
            time_stats = result.statistics["computation_time_ms"]
            print(f"  - è¨ˆç®—æ™‚é–“: {time_stats['mean']:.1f}ms Â± {time_stats['std_dev']:.1f}ms")

    @pytest.mark.asyncio
    async def test_performance_regression_testing(self, verification_service):
        """æ¸¬è©¦æ€§èƒ½å›æ­¸æ¸¬è©¦åŠŸèƒ½"""
        print("\n=== æ¸¬è©¦æ€§èƒ½å›æ­¸æ¸¬è©¦åŠŸèƒ½ ===")
        
        # åŸ·è¡Œå›æ­¸æ¸¬è©¦
        result = await verification_service._run_performance_regression_test()
        
        # é©—è­‰çµæœçµæ§‹
        assert isinstance(result, VerificationResult)
        assert result.scenario == VerificationScenario.PERFORMANCE_REGRESSION
        
        # é©—è­‰å›æ­¸æ¸¬è©¦çµæœ
        assert "pass_rate" in result.statistics
        pass_rate = result.statistics["pass_rate"]["mean"]
        
        # æª¢æŸ¥æ¸¬è©¦æ¡ˆä¾‹åŸ·è¡Œ
        regression_results = result.deviation_analysis["regression_test_results"]
        assert len(regression_results) >= 3  # è‡³å°‘æœ‰ 3 å€‹æ¸¬è©¦æ¡ˆä¾‹
        
        # åˆ†ææ¯å€‹æ¸¬è©¦æ¡ˆä¾‹
        passed_cases = 0
        for case_result in regression_results:
            assert "test_case_id" in case_result
            assert "passed" in case_result
            
            if case_result["passed"]:
                passed_cases += 1
            else:
                print(f"  âš  æ¸¬è©¦æ¡ˆä¾‹å¤±æ•—: {case_result['test_case_id']}")
                if "failed_checks" in case_result:
                    for check in case_result["failed_checks"]:
                        print(f"    - {check}")
        
        calculated_pass_rate = passed_cases / len(regression_results)
        assert abs(pass_rate - calculated_pass_rate) < 0.01  # é€šéç‡è¨ˆç®—ä¸€è‡´æ€§
        
        print(f"âœ“ æ€§èƒ½å›æ­¸æ¸¬è©¦å®Œæˆ")
        print(f"  - æ¸¬è©¦æ¡ˆä¾‹æ•¸: {len(regression_results)}")
        print(f"  - é€šéæ¡ˆä¾‹æ•¸: {passed_cases}")
        print(f"  - é€šéç‡: {pass_rate:.1%}")
        print(f"  - æ•´é«”é€šé: {result.passed}")
        
        # æª¢æŸ¥é—œéµæ¸¬è©¦æ¡ˆä¾‹
        test_case_status = {}
        for case_result in regression_results:
            case_id = case_result["test_case_id"]
            test_case_status[case_id] = case_result["passed"]
        
        print(f"  - åŸºæœ¬é æ¸¬æ¸¬è©¦: {'âœ“' if test_case_status.get('basic_prediction', False) else 'âœ—'}")
        print(f"  - Binary Search æ¸¬è©¦: {'âœ“' if test_case_status.get('binary_search_convergence', False) else 'âœ—'}")
        print(f"  - ç„¡ä¿¡ä»¤åŒæ­¥æ¸¬è©¦: {'âœ“' if test_case_status.get('signaling_free_sync', False) else 'âœ—'}")

    @pytest.mark.asyncio
    async def test_multi_satellite_validation(self, verification_service):
        """æ¸¬è©¦å¤šè¡›æ˜Ÿé©—è­‰åŠŸèƒ½"""
        print("\n=== æ¸¬è©¦å¤šè¡›æ˜Ÿé©—è­‰åŠŸèƒ½ ===")
        
        # åŸ·è¡Œå¤šè¡›æ˜Ÿé©—è­‰æ¸¬è©¦
        result = await verification_service._run_multi_satellite_validation()
        
        # é©—è­‰çµæœçµæ§‹
        assert isinstance(result, VerificationResult)
        assert result.scenario == VerificationScenario.MULTI_SATELLITE_VALIDATION
        
        # é©—è­‰å¯æ“´å±•æ€§æŒ‡æ¨™
        assert "prediction_accuracy" in result.statistics
        assert "computation_time_ms" in result.statistics
        assert "scalability_factor" in result.statistics
        
        # æª¢æŸ¥åŸºæº–å°æ¯”
        baseline_comparison = result.baseline_comparison
        assert "scalability_requirement" in baseline_comparison
        assert "accuracy_requirement" in baseline_comparison
        
        scalability_passed = baseline_comparison["scalability_requirement"]["actual"]
        accuracy_passed = baseline_comparison["accuracy_requirement"]["actual"]
        
        # é©—è­‰æ€§èƒ½æŒ‡æ¨™
        accuracy_stats = result.statistics["prediction_accuracy"]
        time_stats = result.statistics["computation_time_ms"]
        
        print(f"âœ“ å¤šè¡›æ˜Ÿé©—è­‰æ¸¬è©¦å®Œæˆ")
        print(f"  - å¯æ“´å±•æ€§æ¸¬è©¦: {'é€šé' if scalability_passed else 'æœªé€šé'}")
        print(f"  - æº–ç¢ºæ€§æ¸¬è©¦: {'é€šé' if accuracy_passed else 'æœªé€šé'}")
        print(f"  - å¹³å‡æº–ç¢ºç‡: {accuracy_stats['mean']:.1%}")
        print(f"  - å¹³å‡è¨ˆç®—æ™‚é–“: {time_stats['mean']:.1f}ms")
        print(f"  - æœ€å¤§è¨ˆç®—æ™‚é–“: {time_stats['max']:.1f}ms")
        
        # æª¢æŸ¥å¯æ“´å±•æ€§æ€§èƒ½
        max_computation_time = time_stats['max']
        if max_computation_time <= 500.0:
            print(f"  âœ“ å¯æ“´å±•æ€§æ€§èƒ½è‰¯å¥½ (æœ€å¤§æ™‚é–“: {max_computation_time:.1f}ms <= 500ms)")
        else:
            print(f"  âš  å¯æ“´å±•æ€§æ€§èƒ½éœ€æ”¹é€² (æœ€å¤§æ™‚é–“: {max_computation_time:.1f}ms > 500ms)")

    @pytest.mark.asyncio
    async def test_comprehensive_verification_suite(self, verification_service):
        """æ¸¬è©¦ç¶œåˆé©—è­‰å¥—ä»¶"""
        print("\n=== æ¸¬è©¦ç¶œåˆé©—è­‰å¥—ä»¶ ===")
        
        # åŸ·è¡Œå¤šå€‹é©—è­‰å ´æ™¯
        scenarios = [
            VerificationScenario.PAPER_BASELINE_COMPARISON,
            VerificationScenario.PERFORMANCE_REGRESSION,
            VerificationScenario.MULTI_SATELLITE_VALIDATION
        ]
        
        results = await verification_service.run_comprehensive_verification(scenarios)
        
        # é©—è­‰æ‰€æœ‰å ´æ™¯éƒ½æœ‰çµæœ
        assert len(results) == len(scenarios)
        
        # æª¢æŸ¥æ¯å€‹å ´æ™¯çš„çµæœ
        scenario_status = {}
        for scenario in scenarios:
            scenario_key = scenario.value
            assert scenario_key in results
            
            result = results[scenario_key]
            assert isinstance(result, VerificationResult)
            scenario_status[scenario_key] = result.passed
        
        # è¨ˆç®—æ•´é«”é€šéç‡
        passed_scenarios = sum(1 for passed in scenario_status.values() if passed)
        total_scenarios = len(scenario_status)
        overall_pass_rate = passed_scenarios / total_scenarios
        
        print(f"âœ“ ç¶œåˆé©—è­‰å¥—ä»¶å®Œæˆ")
        print(f"  - ç¸½å ´æ™¯æ•¸: {total_scenarios}")
        print(f"  - é€šéå ´æ™¯æ•¸: {passed_scenarios}")
        print(f"  - æ•´é«”é€šéç‡: {overall_pass_rate:.1%}")
        
        # è©³ç´°å ´æ™¯ç‹€æ…‹
        for scenario_key, passed in scenario_status.items():
            status = "âœ“" if passed else "âœ—"
            print(f"  - {scenario_key}: {status}")
        
        # é©—è­‰æœå‹™ç‹€æ…‹æ›´æ–°
        assert len(verification_service.verification_results) >= len(scenarios)

    @pytest.mark.asyncio
    async def test_verification_summary_and_reporting(self, verification_service):
        """æ¸¬è©¦é©—è­‰æ‘˜è¦å’Œå ±å‘ŠåŠŸèƒ½"""
        print("\n=== æ¸¬è©¦é©—è­‰æ‘˜è¦å’Œå ±å‘ŠåŠŸèƒ½ ===")
        
        # å…ˆåŸ·è¡Œä¸€äº›é©—è­‰æ¸¬è©¦ç”Ÿæˆæ•¸æ“š
        await verification_service._run_paper_baseline_comparison()
        await verification_service._run_performance_regression_test()
        
        # ç²å–é©—è­‰æ‘˜è¦
        summary = await verification_service.get_verification_summary()
        
        # é©—è­‰æ‘˜è¦çµæ§‹
        assert "verification_summary" in summary
        assert "scenario_breakdown" in summary
        assert "latest_results" in summary
        assert "recommendations" in summary
        
        # æª¢æŸ¥é©—è­‰æ‘˜è¦çµ±è¨ˆ
        verification_summary = summary["verification_summary"]
        assert "total_verifications" in verification_summary
        assert "passed_verifications" in verification_summary
        assert "overall_pass_rate" in verification_summary
        assert "average_confidence" in verification_summary
        
        # æª¢æŸ¥å ´æ™¯åˆ†è§£
        scenario_breakdown = summary["scenario_breakdown"]
        assert len(scenario_breakdown) >= 2  # è‡³å°‘æœ‰å…©å€‹å ´æ™¯
        
        for scenario, stats in scenario_breakdown.items():
            assert "pass_rate" in stats
            assert "total_tests" in stats
            assert "passed_tests" in stats
            assert 0.0 <= stats["pass_rate"] <= 1.0
        
        # æª¢æŸ¥æœ€æ–°çµæœ
        latest_results = summary["latest_results"]
        assert len(latest_results) >= 2
        
        for result in latest_results:
            assert "verification_id" in result
            assert "scenario" in result
            assert "passed" in result
            assert "confidence" in result
            assert "test_time" in result
        
        # æª¢æŸ¥å»ºè­°
        recommendations = summary["recommendations"]
        assert isinstance(recommendations, list)
        
        print(f"âœ“ é©—è­‰æ‘˜è¦å’Œå ±å‘Šæ¸¬è©¦å®Œæˆ")
        print(f"  - ç¸½é©—è­‰æ•¸: {verification_summary['total_verifications']}")
        print(f"  - é€šéé©—è­‰æ•¸: {verification_summary['passed_verifications']}")
        print(f"  - æ•´é«”é€šéç‡: {verification_summary['overall_pass_rate']:.1%}")
        print(f"  - å¹³å‡ä¿¡å¿ƒåº¦: {verification_summary['average_confidence']:.1%}")
        print(f"  - å ´æ™¯é¡å‹æ•¸: {len(scenario_breakdown)}")
        print(f"  - å»ºè­°æ•¸é‡: {len(recommendations)}")

    @pytest.mark.asyncio
    async def test_statistical_analysis_accuracy(self, verification_service):
        """æ¸¬è©¦çµ±è¨ˆåˆ†ææº–ç¢ºæ€§"""
        print("\n=== æ¸¬è©¦çµ±è¨ˆåˆ†ææº–ç¢ºæ€§ ===")
        
        # åŸ·è¡ŒåŸºæº–å°æ¯”æ¸¬è©¦
        result = await verification_service._run_paper_baseline_comparison()
        
        # é©—è­‰çµ±è¨ˆè¨ˆç®—çš„æº–ç¢ºæ€§
        for metric, raw_values in result.metrics.items():
            if raw_values:
                stats = result.statistics[metric]
                
                # é©—è­‰åŸºæœ¬çµ±è¨ˆé‡
                calculated_mean = sum(raw_values) / len(raw_values)
                assert abs(stats["mean"] - calculated_mean) < 0.001, f"{metric} å¹³å‡å€¼è¨ˆç®—éŒ¯èª¤"
                
                calculated_min = min(raw_values)
                calculated_max = max(raw_values)
                assert stats["min"] == calculated_min, f"{metric} æœ€å°å€¼è¨ˆç®—éŒ¯èª¤"
                assert stats["max"] == calculated_max, f"{metric} æœ€å¤§å€¼è¨ˆç®—éŒ¯èª¤"
                
                assert stats["count"] == len(raw_values), f"{metric} è¨ˆæ•¸éŒ¯èª¤"
        
        # æª¢æŸ¥åå·®è¨ˆç®—
        for baseline_id, comparison in result.baseline_comparison.items():
            baseline = verification_service.paper_baselines[baseline_id]
            
            for metric, comp in comparison["comparisons"].items():
                measured = comp["measured"]
                baseline_value = comp["baseline"]
                calculated_deviation = (measured - baseline_value) / baseline_value if baseline_value != 0 else 0
                
                assert abs(comp["deviation"] - calculated_deviation) < 0.001, f"{metric} åå·®è¨ˆç®—éŒ¯èª¤"
                assert abs(comp["deviation_percent"] - calculated_deviation * 100) < 0.1, f"{metric} åå·®ç™¾åˆ†æ¯”è¨ˆç®—éŒ¯èª¤"
        
        print(f"âœ“ çµ±è¨ˆåˆ†ææº–ç¢ºæ€§é©—è­‰å®Œæˆ")
        print(f"  - çµ±è¨ˆæŒ‡æ¨™æ•¸: {len(result.statistics)}")
        print(f"  - åŸºæº–å°æ¯”æ•¸: {len(result.baseline_comparison)}")

    @pytest.mark.asyncio
    async def test_regression_test_case_execution(self, verification_service):
        """æ¸¬è©¦å›æ­¸æ¸¬è©¦æ¡ˆä¾‹åŸ·è¡Œ"""
        print("\n=== æ¸¬è©¦å›æ­¸æ¸¬è©¦æ¡ˆä¾‹åŸ·è¡Œ ===")
        
        # æª¢æŸ¥å·²è¨­ç½®çš„å›æ­¸æ¸¬è©¦æ¡ˆä¾‹
        test_cases = verification_service.regression_test_cases
        assert len(test_cases) >= 3, "æ‡‰è©²æœ‰è‡³å°‘ 3 å€‹å›æ­¸æ¸¬è©¦æ¡ˆä¾‹"
        
        # æ¸¬è©¦æ¯å€‹å›æ­¸æ¸¬è©¦æ¡ˆä¾‹
        for test_id, test_case in test_cases.items():
            print(f"\n  æ¸¬è©¦æ¡ˆä¾‹: {test_case.description}")
            
            # åŸ·è¡Œæ¸¬è©¦æ¡ˆä¾‹
            case_result = await verification_service._execute_regression_test_case(test_case)
            
            # é©—è­‰çµæœçµæ§‹
            assert "test_case_id" in case_result
            assert "passed" in case_result
            # æ³¨æ„ï¼štest_case_id å¯èƒ½èˆ‡å­—å…¸çš„ key ä¸åŒï¼Œæ‰€ä»¥ä¸åšåš´æ ¼æª¢æŸ¥
            # assert case_result["test_case_id"] == test_id
            
            if case_result["passed"]:
                print(f"    âœ“ æ¸¬è©¦é€šé")
                assert "actual_values" in case_result
                assert "expected_output" in case_result
                
                # æª¢æŸ¥å¯¦éš›å€¼æ˜¯å¦ç¬¦åˆé æœŸ
                actual_values = case_result["actual_values"]
                expected_output = case_result["expected_output"]
                
                for metric, expected in expected_output.items():
                    assert metric in actual_values, f"ç¼ºå°‘æŒ‡æ¨™: {metric}"
                    actual = actual_values[metric]
                    
                    if isinstance(expected, dict):
                        if "min" in expected:
                            print(f"      {metric}: {actual:.3f} >= {expected['min']:.3f}")
                        if "max" in expected:
                            print(f"      {metric}: {actual:.3f} <= {expected['max']:.3f}")
                    else:
                        print(f"      {metric}: {actual:.3f} â‰ˆ {expected:.3f}")
            else:
                print(f"    âœ— æ¸¬è©¦å¤±æ•—")
                if "failed_checks" in case_result:
                    for check in case_result["failed_checks"]:
                        print(f"      - {check}")
        
        print(f"\nâœ“ å›æ­¸æ¸¬è©¦æ¡ˆä¾‹åŸ·è¡Œå®Œæˆ")

    @pytest.mark.asyncio
    async def test_baseline_tolerance_validation(self, verification_service):
        """æ¸¬è©¦åŸºæº–å®¹è¨±åº¦é©—è­‰"""
        print("\n=== æ¸¬è©¦åŸºæº–å®¹è¨±åº¦é©—è­‰ ===")
        
        # æª¢æŸ¥åŸºæº–è¨­ç½®
        baselines = verification_service.paper_baselines
        assert len(baselines) >= 2, "æ‡‰è©²æœ‰è‡³å°‘ 2 å€‹åŸºæº–"
        
        for baseline_id, baseline in baselines.items():
            print(f"\n  åŸºæº–: {baseline.algorithm_name}")
            print(f"    - Handover æº–ç¢ºç‡: {baseline.handover_accuracy:.1%} Â± {baseline.accuracy_tolerance:.1%}")
            print(f"    - è¨ˆç®—æ™‚é–“: {baseline.computation_time_ms:.1f}ms Â± {baseline.time_tolerance:.1%}")
            print(f"    - æ”¶æ–‚ç‡: {baseline.convergence_rate:.1%} Â± {baseline.convergence_tolerance:.1%}")
            
            # é©—è­‰åŸºæº–åƒæ•¸åˆç†æ€§
            assert 0.0 <= baseline.handover_accuracy <= 1.0
            assert baseline.computation_time_ms > 0
            assert 0.0 <= baseline.convergence_rate <= 1.0
            assert baseline.accuracy_tolerance > 0
            assert baseline.time_tolerance > 0
            assert baseline.convergence_tolerance > 0
        
        # åŸ·è¡Œä¸€å€‹ç°¡å–®çš„å°æ¯”æ¸¬è©¦ä¾†é©—è­‰å®¹è¨±åº¦é‚è¼¯
        result = await verification_service._run_paper_baseline_comparison()
        
        # æª¢æŸ¥å®¹è¨±åº¦æ˜¯å¦æ­£ç¢ºæ‡‰ç”¨
        for baseline_id, comparison in result.baseline_comparison.items():
            for metric, comp in comparison["comparisons"].items():
                tolerance = comp["tolerance"]
                deviation = abs(comp["deviation"])
                within_tolerance = comp["within_tolerance"]
                
                # é©—è­‰å®¹è¨±åº¦åˆ¤æ–·é‚è¼¯
                expected_within_tolerance = deviation <= tolerance
                assert within_tolerance == expected_within_tolerance, \
                    f"{metric} å®¹è¨±åº¦åˆ¤æ–·éŒ¯èª¤: {deviation:.3f} vs {tolerance:.3f}"
        
        print(f"\nâœ“ åŸºæº–å®¹è¨±åº¦é©—è­‰å®Œæˆ")


async def run_stage3_verification_tests():
    """é‹è¡Œ Stage 3 ç®—æ³•é©—è­‰æ¸¬è©¦"""
    print("é–‹å§‹åŸ·è¡Œ Algorithm Verification Service (Stage 3) æ•´åˆæ¸¬è©¦")
    print("=" * 70)
    
    # å‰µå»ºæ¸¬è©¦å¯¦ä¾‹
    test_instance = TestAlgorithmVerification()
    
    # å‰µå»ºé©—è­‰æœå‹™
    enhanced_algorithm = EnhancedSynchronizedAlgorithm()
    await enhanced_algorithm.start_enhanced_algorithm()
    
    verification_service = AlgorithmVerificationService(
        enhanced_algorithm=enhanced_algorithm
    )
    await verification_service.start_verification_service()
    
    try:
        # åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦
        await test_instance.test_paper_baseline_comparison(verification_service)
        await test_instance.test_performance_regression_testing(verification_service)
        await test_instance.test_multi_satellite_validation(verification_service)
        await test_instance.test_comprehensive_verification_suite(verification_service)
        await test_instance.test_verification_summary_and_reporting(verification_service)
        await test_instance.test_statistical_analysis_accuracy(verification_service)
        await test_instance.test_regression_test_case_execution(verification_service)
        await test_instance.test_baseline_tolerance_validation(verification_service)
        
        print("\n" + "=" * 70)
        print("âœ“ æ‰€æœ‰ Stage 3 Algorithm Verification æ¸¬è©¦é€šé")
        
        # é¡¯ç¤ºæœ€çµ‚é©—è­‰æ‘˜è¦
        final_summary = await verification_service.get_verification_summary()
        
        print(f"\nğŸ“Š Stage 3 é©—è­‰æ¡†æ¶ç¸½çµ:")
        summary_stats = final_summary["verification_summary"]
        print(f"  - ç¸½é©—è­‰æ¸¬è©¦æ•¸: {summary_stats['total_verifications']}")
        print(f"  - é€šéé©—è­‰æ•¸: {summary_stats['passed_verifications']}")
        print(f"  - æ•´é«”é€šéç‡: {summary_stats['overall_pass_rate']:.1%}")
        print(f"  - å¹³å‡ä¿¡å¿ƒåº¦: {summary_stats['average_confidence']:.1%}")
        
        print(f"\nğŸ“ˆ å ´æ™¯é©—è­‰ç‹€æ³:")
        for scenario, stats in final_summary["scenario_breakdown"].items():
            print(f"  - {scenario}: {stats['pass_rate']:.1%} ({stats['passed_tests']}/{stats['total_tests']})")
        
        print(f"\nğŸ’¡ ç³»çµ±å»ºè­°:")
        for i, recommendation in enumerate(final_summary["recommendations"], 1):
            print(f"  {i}. {recommendation}")
        
    except Exception as e:
        print(f"\nâŒ Stage 3 æ¸¬è©¦å¤±æ•—: {e}")
        raise
    finally:
        await verification_service.stop_verification_service()
        await enhanced_algorithm.stop_enhanced_algorithm()


if __name__ == "__main__":
    asyncio.run(run_stage3_verification_tests())