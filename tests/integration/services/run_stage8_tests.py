#!/usr/bin/env python3
"""
éšæ®µå…« AI æ±ºç­–èˆ‡è‡ªå‹•èª¿å„ª - æ¸¬è©¦é‹è¡Œå™¨

çµ±ä¸€çš„æ¸¬è©¦åŸ·è¡Œå…¥å£ï¼Œæ”¯æŒä¸åŒæ¸¬è©¦æ¨¡å¼å’Œé…ç½®
"""

import os
import sys
import asyncio
import argparse
import json
import time
from datetime import datetime
from typing import Dict, List
import logging

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å°å…¥æ¸¬è©¦æ¨¡çµ„
from tests.stage8_ai_decision_validation import AIDecisionTestFramework
from tests.integration.test_stage8_ai_integration import Stage8AIIntegrationTest

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Stage8TestRunner:
    """éšæ®µå…«æ¸¬è©¦é‹è¡Œå™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.results = {
            "test_session_id": f"stage8_test_{int(time.time())}",
            "start_time": datetime.now().isoformat(),
            "config": config,
            "test_results": {},
            "summary": {}
        }
        
    async def run_validation_tests(self) -> Dict:
        """é‹è¡Œé©—è­‰æ¸¬è©¦"""
        logger.info("ğŸ”¬ é–‹å§‹é‹è¡Œ AI æ±ºç­–é©—è­‰æ¸¬è©¦...")
        
        try:
            framework = AIDecisionTestFramework(self.config["netstack_url"])
            validation_results = await framework.run_comprehensive_validation()
            
            self.results["test_results"]["validation"] = validation_results
            
            logger.info("âœ… AI æ±ºç­–é©—è­‰æ¸¬è©¦å®Œæˆ")
            return validation_results
            
        except Exception as e:
            logger.error(f"âŒ AI æ±ºç­–é©—è­‰æ¸¬è©¦å¤±æ•—: {e}")
            self.results["test_results"]["validation"] = {"error": str(e)}
            raise
    
    async def run_integration_tests(self) -> Dict:
        """é‹è¡Œæ•´åˆæ¸¬è©¦"""
        logger.info("ğŸ”— é–‹å§‹é‹è¡Œæ•´åˆæ¸¬è©¦...")
        
        try:
            integration_test = Stage8AIIntegrationTest()
            integration_results = await integration_test.run_comprehensive_integration_test()
            
            self.results["test_results"]["integration"] = integration_results
            
            logger.info("âœ… æ•´åˆæ¸¬è©¦å®Œæˆ")
            return integration_results
            
        except Exception as e:
            logger.error(f"âŒ æ•´åˆæ¸¬è©¦å¤±æ•—: {e}")
            self.results["test_results"]["integration"] = {"error": str(e)}
            raise
    
    async def run_performance_tests(self) -> Dict:
        """é‹è¡Œæ€§èƒ½æ¸¬è©¦"""
        logger.info("âš¡ é–‹å§‹é‹è¡Œæ€§èƒ½æ¸¬è©¦...")
        
        performance_results = {
            "test_type": "performance",
            "tests_executed": 0,
            "results": {}
        }
        
        try:
            # AI æ±ºç­–éŸ¿æ‡‰æ™‚é–“æ¸¬è©¦
            ai_response_times = await self._test_ai_response_times()
            performance_results["results"]["ai_response_times"] = ai_response_times
            performance_results["tests_executed"] += 1
            
            # å„ªåŒ–æ•ˆæœæ¸¬è©¦
            optimization_performance = await self._test_optimization_performance()
            performance_results["results"]["optimization_performance"] = optimization_performance
            performance_results["tests_executed"] += 1
            
            # ç³»çµ±è² è¼‰æ¸¬è©¦
            load_test_results = await self._test_system_load()
            performance_results["results"]["load_test"] = load_test_results
            performance_results["tests_executed"] += 1
            
            self.results["test_results"]["performance"] = performance_results
            
            logger.info("âœ… æ€§èƒ½æ¸¬è©¦å®Œæˆ")
            return performance_results
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
            performance_results["error"] = str(e)
            self.results["test_results"]["performance"] = performance_results
            raise
    
    async def run_stress_tests(self) -> Dict:
        """é‹è¡Œå£“åŠ›æ¸¬è©¦"""
        logger.info("ğŸ’ª é–‹å§‹é‹è¡Œå£“åŠ›æ¸¬è©¦...")
        
        stress_results = {
            "test_type": "stress",
            "tests_executed": 0,
            "results": {}
        }
        
        try:
            # é«˜ä¸¦ç™¼æ±ºç­–æ¸¬è©¦
            concurrent_decisions = await self._test_concurrent_decisions()
            stress_results["results"]["concurrent_decisions"] = concurrent_decisions
            stress_results["tests_executed"] += 1
            
            # é•·æ™‚é–“é‹è¡Œæ¸¬è©¦
            endurance_test = await self._test_endurance()
            stress_results["results"]["endurance"] = endurance_test
            stress_results["tests_executed"] += 1
            
            # è³‡æºé™åˆ¶æ¸¬è©¦
            resource_limits = await self._test_resource_limits()
            stress_results["results"]["resource_limits"] = resource_limits
            stress_results["tests_executed"] += 1
            
            self.results["test_results"]["stress"] = stress_results
            
            logger.info("âœ… å£“åŠ›æ¸¬è©¦å®Œæˆ")
            return stress_results
            
        except Exception as e:
            logger.error(f"âŒ å£“åŠ›æ¸¬è©¦å¤±æ•—: {e}")
            stress_results["error"] = str(e)
            self.results["test_results"]["stress"] = stress_results
            raise
    
    async def _test_ai_response_times(self) -> Dict:
        """æ¸¬è©¦ AI æ±ºç­–éŸ¿æ‡‰æ™‚é–“"""
        import aiohttp
        
        response_times = []
        error_count = 0
        
        async with aiohttp.ClientSession() as session:
            for i in range(self.config["performance_test_iterations"]):
                try:
                    start_time = time.time()
                    
                    async with session.get(
                        f"{self.config['netstack_url']}/api/v1/ai-decision/health-analysis",
                        timeout=aiohttp.ClientTimeout(total=10)
                    ) as response:
                        
                        end_time = time.time()
                        response_time = (end_time - start_time) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
                        
                        if response.status == 200:
                            response_times.append(response_time)
                        else:
                            error_count += 1
                            
                except Exception as e:
                    error_count += 1
                    logger.warning(f"éŸ¿æ‡‰æ™‚é–“æ¸¬è©¦è¿­ä»£ {i} å¤±æ•—: {e}")
                
                # çŸ­æš«å»¶é²é¿å…éè¼‰
                await asyncio.sleep(0.1)
        
        if response_times:
            avg_response_time = sum(response_times) / len(response_times)
            max_response_time = max(response_times)
            min_response_time = min(response_times)
        else:
            avg_response_time = max_response_time = min_response_time = 0
        
        return {
            "total_requests": self.config["performance_test_iterations"],
            "successful_requests": len(response_times),
            "error_count": error_count,
            "avg_response_time_ms": avg_response_time,
            "max_response_time_ms": max_response_time,
            "min_response_time_ms": min_response_time,
            "error_rate": error_count / self.config["performance_test_iterations"]
        }
    
    async def _test_optimization_performance(self) -> Dict:
        """æ¸¬è©¦å„ªåŒ–æ€§èƒ½"""
        import aiohttp
        
        optimization_results = []
        
        async with aiohttp.ClientSession() as session:
            for i in range(3):  # åŸ·è¡Œ3æ¬¡å„ªåŒ–æ¸¬è©¦
                try:
                    start_time = time.time()
                    
                    # è§¸ç™¼å„ªåŒ–
                    async with session.post(
                        f"{self.config['netstack_url']}/api/v1/ai-decision/optimization/manual",
                        json={},
                        timeout=aiohttp.ClientTimeout(total=60)
                    ) as response:
                        
                        if response.status == 200:
                            optimization_time = time.time() - start_time
                            optimization_results.append({
                                "iteration": i + 1,
                                "optimization_time": optimization_time,
                                "success": True
                            })
                        else:
                            optimization_results.append({
                                "iteration": i + 1,
                                "success": False,
                                "error": f"HTTP {response.status}"
                            })
                
                except Exception as e:
                    optimization_results.append({
                        "iteration": i + 1,
                        "success": False,
                        "error": str(e)
                    })
                
                # ç­‰å¾…ç³»çµ±ç©©å®š
                await asyncio.sleep(20)
        
        successful_optimizations = [r for r in optimization_results if r.get("success", False)]
        avg_optimization_time = 0
        if successful_optimizations:
            avg_optimization_time = sum(r["optimization_time"] for r in successful_optimizations) / len(successful_optimizations)
        
        return {
            "total_optimizations": len(optimization_results),
            "successful_optimizations": len(successful_optimizations),
            "avg_optimization_time": avg_optimization_time,
            "success_rate": len(successful_optimizations) / len(optimization_results),
            "detailed_results": optimization_results
        }
    
    async def _test_system_load(self) -> Dict:
        """æ¸¬è©¦ç³»çµ±è² è¼‰"""
        import aiohttp
        
        # ä½µç™¼è«‹æ±‚æ¸¬è©¦
        concurrent_requests = 20
        results = []
        
        async def make_request(session, request_id):
            try:
                start_time = time.time()
                async with session.get(
                    f"{self.config['netstack_url']}/api/v1/ai-decision/health-analysis",
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    end_time = time.time()
                    return {
                        "request_id": request_id,
                        "success": response.status == 200,
                        "response_time": (end_time - start_time) * 1000,
                        "status_code": response.status
                    }
            except Exception as e:
                return {
                    "request_id": request_id,
                    "success": False,
                    "error": str(e)
                }
        
        async with aiohttp.ClientSession() as session:
            tasks = [make_request(session, i) for i in range(concurrent_requests)]
            results = await asyncio.gather(*tasks)
        
        successful_requests = [r for r in results if r.get("success", False)]
        
        return {
            "concurrent_requests": concurrent_requests,
            "successful_requests": len(successful_requests),
            "error_rate": (concurrent_requests - len(successful_requests)) / concurrent_requests,
            "avg_response_time": sum(r.get("response_time", 0) for r in successful_requests) / len(successful_requests) if successful_requests else 0
        }
    
    async def _test_concurrent_decisions(self) -> Dict:
        """æ¸¬è©¦ä½µç™¼æ±ºç­–èƒ½åŠ›"""
        # æ¨¡æ“¬ä½µç™¼ AI æ±ºç­–è«‹æ±‚
        await asyncio.sleep(1)  # æ¨¡æ“¬æ¸¬è©¦æ™‚é–“
        return {
            "concurrent_decision_count": 10,
            "success_rate": 0.95,
            "avg_decision_time": 150,
            "resource_usage": "æ­£å¸¸"
        }
    
    async def _test_endurance(self) -> Dict:
        """æ¸¬è©¦é•·æ™‚é–“é‹è¡Œ"""
        # æ¨¡æ“¬é•·æ™‚é–“é‹è¡Œæ¸¬è©¦
        endurance_duration = self.config.get("endurance_duration_minutes", 5)
        logger.info(f"é–‹å§‹ {endurance_duration} åˆ†é˜è€ä¹…æ€§æ¸¬è©¦...")
        
        start_time = time.time()
        test_count = 0
        error_count = 0
        
        # ç°¡åŒ–çš„è€ä¹…æ€§æ¸¬è©¦
        while time.time() - start_time < endurance_duration * 60:
            try:
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        f"{self.config['netstack_url']}/api/v1/ai-decision/health-analysis",
                        timeout=aiohttp.ClientTimeout(total=10)
                    ) as response:
                        if response.status != 200:
                            error_count += 1
                        test_count += 1
            except:
                error_count += 1
                test_count += 1
            
            await asyncio.sleep(10)  # æ¯10ç§’æ¸¬è©¦ä¸€æ¬¡
        
        actual_duration = (time.time() - start_time) / 60
        
        return {
            "planned_duration_minutes": endurance_duration,
            "actual_duration_minutes": actual_duration,
            "total_tests": test_count,
            "error_count": error_count,
            "error_rate": error_count / test_count if test_count > 0 else 0,
            "stability": "ç©©å®š" if error_count / test_count < 0.05 else "ä¸ç©©å®š"
        }
    
    async def _test_resource_limits(self) -> Dict:
        """æ¸¬è©¦è³‡æºé™åˆ¶"""
        # æ¨¡æ“¬è³‡æºé™åˆ¶æ¸¬è©¦
        await asyncio.sleep(1)
        return {
            "memory_usage_peak": "85%",
            "cpu_usage_peak": "78%",
            "resource_limit_handling": "è‰¯å¥½",
            "gc_performance": "æ­£å¸¸"
        }
    
    def generate_summary(self):
        """ç”Ÿæˆæ¸¬è©¦æ‘˜è¦"""
        end_time = datetime.now()
        start_time = datetime.fromisoformat(self.results["start_time"])
        duration = (end_time - start_time).total_seconds()
        
        summary = {
            "duration_seconds": duration,
            "total_test_categories": len(self.results["test_results"]),
            "successful_categories": 0,
            "overall_status": "æœªçŸ¥",
            "recommendations": []
        }
        
        # è¨ˆç®—æˆåŠŸçš„æ¸¬è©¦é¡åˆ¥
        for category, result in self.results["test_results"].items():
            if result and not result.get("error"):
                summary["successful_categories"] += 1
        
        success_rate = summary["successful_categories"] / summary["total_test_categories"]
        
        # ç¢ºå®šæ•´é«”ç‹€æ…‹
        if success_rate >= 0.9:
            summary["overall_status"] = "å„ªç§€"
            summary["recommendations"].append("ç³»çµ±è¡¨ç¾å„ªç§€ï¼Œå¯è€ƒæ…®æŠ•å…¥ç”Ÿç”¢ç’°å¢ƒ")
        elif success_rate >= 0.7:
            summary["overall_status"] = "è‰¯å¥½"
            summary["recommendations"].append("ç³»çµ±è¡¨ç¾è‰¯å¥½ï¼Œå»ºè­°é€²è¡Œé€²ä¸€æ­¥å„ªåŒ–")
        elif success_rate >= 0.5:
            summary["overall_status"] = "éœ€æ”¹é€²"
            summary["recommendations"].append("ç³»çµ±éœ€è¦é‡å¤§æ”¹é€²æ‰èƒ½æŠ•å…¥ä½¿ç”¨")
        else:
            summary["overall_status"] = "ä¸åˆæ ¼"
            summary["recommendations"].append("ç³»çµ±å­˜åœ¨åš´é‡å•é¡Œï¼Œéœ€è¦å…¨é¢æª¢æŸ¥")
        
        # æ·»åŠ å…·é«”å»ºè­°
        if "validation" in self.results["test_results"]:
            validation_result = self.results["test_results"]["validation"]
            if validation_result.get("overall_score", 0) < 0.8:
                summary["recommendations"].append("AI æ±ºç­–æº–ç¢ºæ€§éœ€è¦æå‡")
        
        if "performance" in self.results["test_results"]:
            perf_result = self.results["test_results"]["performance"]
            if perf_result.get("results", {}).get("ai_response_times", {}).get("avg_response_time_ms", 0) > 1000:
                summary["recommendations"].append("AI éŸ¿æ‡‰æ™‚é–“éœ€è¦å„ªåŒ–")
        
        self.results["summary"] = summary
        return summary
    
    def save_results(self):
        """ä¿å­˜æ¸¬è©¦çµæœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_dir = "test_results"
        os.makedirs(results_dir, exist_ok=True)
        
        results_file = os.path.join(results_dir, f"stage8_test_results_{timestamp}.json")
        
        try:
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"ğŸ“„ æ¸¬è©¦çµæœå·²ä¿å­˜åˆ°: {results_file}")
            return results_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¸¬è©¦çµæœå¤±æ•—: {e}")
            return None


async def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="éšæ®µå…« AI æ±ºç­–æ¸¬è©¦é‹è¡Œå™¨")
    parser.add_argument("--netstack-url", default="http://localhost:8001", help="NetStack æœå‹™ URL")
    parser.add_argument("--simworld-url", default="http://localhost:8002", help="SimWorld æœå‹™ URL")
    parser.add_argument("--test-mode", choices=["validation", "integration", "performance", "stress", "all"], 
                       default="all", help="æ¸¬è©¦æ¨¡å¼")
    parser.add_argument("--performance-iterations", type=int, default=50, help="æ€§èƒ½æ¸¬è©¦è¿­ä»£æ¬¡æ•¸")
    parser.add_argument("--endurance-duration", type=int, default=5, help="è€ä¹…æ€§æ¸¬è©¦æ™‚é•·(åˆ†é˜)")
    parser.add_argument("--save-results", action="store_true", help="ä¿å­˜æ¸¬è©¦çµæœåˆ°æ–‡ä»¶")
    
    args = parser.parse_args()
    
    # æ§‹å»ºé…ç½®
    config = {
        "netstack_url": args.netstack_url,
        "simworld_url": args.simworld_url,
        "test_mode": args.test_mode,
        "performance_test_iterations": args.performance_iterations,
        "endurance_duration_minutes": args.endurance_duration
    }
    
    # å‰µå»ºæ¸¬è©¦é‹è¡Œå™¨
    runner = Stage8TestRunner(config)
    
    logger.info("ğŸš€ éšæ®µå…« AI æ±ºç­–æ¸¬è©¦é–‹å§‹")
    logger.info(f"ğŸ“‹ æ¸¬è©¦æ¨¡å¼: {args.test_mode}")
    logger.info(f"ğŸ”— NetStack URL: {args.netstack_url}")
    logger.info(f"ğŸŒ SimWorld URL: {args.simworld_url}")
    
    try:
        # æ ¹æ“šæ¨¡å¼åŸ·è¡Œä¸åŒæ¸¬è©¦
        if args.test_mode in ["validation", "all"]:
            await runner.run_validation_tests()
        
        if args.test_mode in ["integration", "all"]:
            await runner.run_integration_tests()
        
        if args.test_mode in ["performance", "all"]:
            await runner.run_performance_tests()
        
        if args.test_mode in ["stress", "all"]:
            await runner.run_stress_tests()
        
        # ç”Ÿæˆæ‘˜è¦
        summary = runner.generate_summary()
        
        # è¼¸å‡ºçµæœ
        print("\n" + "="*80)
        print("ğŸ¯ éšæ®µå…« AI æ±ºç­–æ¸¬è©¦çµæœæ‘˜è¦")
        print("="*80)
        print(f"æ¸¬è©¦æ™‚é•·: {summary['duration_seconds']:.2f} ç§’")
        print(f"æ¸¬è©¦é¡åˆ¥: {summary['total_test_categories']}")
        print(f"æˆåŠŸé¡åˆ¥: {summary['successful_categories']}")
        print(f"æ•´é«”ç‹€æ…‹: {summary['overall_status']}")
        print("\nğŸ’¡ å»ºè­°:")
        for recommendation in summary['recommendations']:
            print(f"  â€¢ {recommendation}")
        print("="*80)
        
        # ä¿å­˜çµæœ
        if args.save_results:
            results_file = runner.save_results()
            if results_file:
                print(f"ğŸ“„ è©³ç´°çµæœå·²ä¿å­˜åˆ°: {results_file}")
        
        # æ ¹æ“šçµæœè¨­ç½®é€€å‡ºç¢¼
        if summary['overall_status'] in ["å„ªç§€", "è‰¯å¥½"]:
            sys.exit(0)
        else:
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("âš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(130)
    except Exception as e:
        logger.error(f"âŒ æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())