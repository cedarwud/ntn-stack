"""
DQN ç®—æ³•è¨“ç·´æ¸¬è©¦

æ¸¬è©¦é›¢æ•£å‹•ä½œç©ºé–“ç’°å¢ƒèˆ‡ DQN ç®—æ³•çš„å…¼å®¹æ€§
"""

import sys
import os
import time
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ é …ç›®è·¯å¾‘
sys.path.insert(0, "/home/sat/ntn-stack")
sys.path.insert(0, "/home/sat/ntn-stack/netstack")

import numpy as np
import gymnasium as gym
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_discrete_environment():
    """æ¸¬è©¦é›¢æ•£å‹•ä½œç’°å¢ƒåŸºæœ¬åŠŸèƒ½"""
    print("=== æ¸¬è©¦é›¢æ•£å‹•ä½œç’°å¢ƒ ===")

    try:
        # å°Žå…¥é›¢æ•£ç’°å¢ƒ
        from netstack_api.envs.discrete_handover_env import (
            DiscreteLEOSatelliteHandoverEnv,
            DQNCompatibleHandoverEnv,
        )
        from netstack_api.envs.handover_env_fixed import HandoverScenario

        print("âœ… æˆåŠŸå°Žå…¥é›¢æ•£ç’°å¢ƒæ¨¡å¡Š")

        # æ¸¬è©¦åŸºæœ¬é›¢æ•£ç’°å¢ƒ
        print("\n1. æ¸¬è©¦åŸºæœ¬é›¢æ•£ç’°å¢ƒ...")
        env = DiscreteLEOSatelliteHandoverEnv(
            scenario=HandoverScenario.SINGLE_UE,
            max_ues=3,
            max_satellites=5,
            discrete_actions=10,
        )

        print(f"  å‹•ä½œç©ºé–“: {env.action_space}")
        print(f"  è§€æ¸¬ç©ºé–“: {env.observation_space}")
        print(f"  é›¢æ•£å‹•ä½œæ•¸é‡: {env.discrete_actions}")

        # æ¸¬è©¦é‡ç½®å’Œæ­¥é€²
        obs, info = env.reset()
        print(f"  é‡ç½®æˆåŠŸï¼Œè§€æ¸¬ç¶­åº¦: {obs.shape}")

        # æ¸¬è©¦æ‰€æœ‰å‹•ä½œ
        print("\n  æ¸¬è©¦æ‰€æœ‰é›¢æ•£å‹•ä½œ:")
        for action in range(env.discrete_actions):
            action_info = env.get_action_info(action)
            print(f"    å‹•ä½œ {action}: {action_info['description']}")

        # åŸ·è¡Œå¹¾æ­¥æ¸¬è©¦
        total_reward = 0
        for step in range(10):
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            total_reward += reward

            print(
                f"  æ­¥é©Ÿ {step}: å‹•ä½œ={action}, çŽå‹µ={reward:.3f}, çµæŸ={terminated or truncated}"
            )

            if terminated or truncated:
                obs, info = env.reset()
                print(f"  ç’°å¢ƒé‡ç½®ï¼Œç¸½çŽå‹µ: {total_reward:.3f}")
                total_reward = 0

        env.close()
        print("âœ… åŸºæœ¬é›¢æ•£ç’°å¢ƒæ¸¬è©¦é€šéŽ")

        # æ¸¬è©¦DQNå„ªåŒ–ç’°å¢ƒ
        print("\n2. æ¸¬è©¦ DQN å„ªåŒ–ç’°å¢ƒ...")
        dqn_env = DQNCompatibleHandoverEnv(
            scenario=HandoverScenario.SINGLE_UE,
            max_ues=3,
            max_satellites=5,
            discrete_actions=10,
            max_episode_steps=50,
            reward_scale=1.0,
            penalty_scale=0.1,
        )

        obs, info = dqn_env.reset()
        print(f"  DQNç’°å¢ƒé‡ç½®æˆåŠŸ")
        print(f"  æœ€å¤§æ­¥æ•¸: {info.get('max_episode_steps')}")

        # æ¸¬è©¦å®Œæ•´episode
        episode_reward = 0
        step_count = 0

        while True:
            action = dqn_env.action_space.sample()
            obs, reward, terminated, truncated, info = dqn_env.step(action)
            episode_reward += reward
            step_count += 1

            if step_count % 10 == 0:
                print(f"    æ­¥é©Ÿ {step_count}: ç´¯ç©çŽå‹µ={episode_reward:.3f}")

            if terminated or truncated:
                print(f"  EpisodeçµæŸ: æ­¥æ•¸={step_count}, ç¸½çŽå‹µ={episode_reward:.3f}")
                print(f"  å‹•ä½œé »çŽ‡: {info.get('action_frequency', {})}")
                break

        dqn_env.close()
        print("âœ… DQN å„ªåŒ–ç’°å¢ƒæ¸¬è©¦é€šéŽ")

        return True

    except Exception as e:
        print(f"âŒ é›¢æ•£ç’°å¢ƒæ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_dqn_training():
    """æ¸¬è©¦ DQN ç®—æ³•è¨“ç·´"""
    print("\n=== æ¸¬è©¦ DQN ç®—æ³•è¨“ç·´ ===")

    try:
        # å˜—è©¦å°Žå…¥ stable-baselines3
        try:
            from stable_baselines3 import DQN
            from stable_baselines3.common.evaluation import evaluate_policy
            from stable_baselines3.common.env_checker import check_env

            print("âœ… æˆåŠŸå°Žå…¥ stable-baselines3")
        except ImportError:
            print("âŒ stable-baselines3 æœªå®‰è£ï¼Œè·³éŽ DQN è¨“ç·´æ¸¬è©¦")
            return True

        # å‰µå»ºç’°å¢ƒ
        from netstack_api.envs.discrete_handover_env import DQNCompatibleHandoverEnv
        from netstack_api.envs.handover_env_fixed import HandoverScenario

        env = DQNCompatibleHandoverEnv(
            scenario=HandoverScenario.SINGLE_UE,
            max_ues=2,
            max_satellites=3,
            discrete_actions=8,
            max_episode_steps=100,
        )

        print("âœ… DQN ç’°å¢ƒå‰µå»ºæˆåŠŸ")

        # æª¢æŸ¥ç’°å¢ƒå…¼å®¹æ€§
        print("\n1. æª¢æŸ¥ç’°å¢ƒå…¼å®¹æ€§...")
        try:
            check_env(env)
            print("âœ… ç’°å¢ƒå…¼å®¹æ€§æª¢æŸ¥é€šéŽ")
        except Exception as e:
            print(f"âš ï¸ ç’°å¢ƒå…¼å®¹æ€§è­¦å‘Š: {e}")

        # å‰µå»º DQN æ¨¡åž‹
        print("\n2. å‰µå»º DQN æ¨¡åž‹...")
        model = DQN(
            "MlpPolicy",  # å¤šå±¤æ„ŸçŸ¥æ©Ÿç­–ç•¥
            env,
            learning_rate=1e-3,
            buffer_size=1000,
            learning_starts=100,
            batch_size=32,
            target_update_interval=100,
            train_freq=4,
            gradient_steps=1,
            exploration_fraction=0.3,
            exploration_initial_eps=1.0,
            exploration_final_eps=0.1,
            verbose=1,
        )

        print("âœ… DQN æ¨¡åž‹å‰µå»ºæˆåŠŸ")
        print(f"  ç­–ç•¥ç¶²è·¯: {model.policy}")
        print(f"  å‹•ä½œç©ºé–“: {model.action_space}")
        print(f"  è§€æ¸¬ç©ºé–“: {model.observation_space}")

        # è¨“ç·´å‰è©•ä¼°
        print("\n3. è¨“ç·´å‰æ€§èƒ½è©•ä¼°...")
        mean_reward_before, std_reward_before = evaluate_policy(
            model, env, n_eval_episodes=5, deterministic=True
        )
        print(f"  è¨“ç·´å‰å¹³å‡çŽå‹µ: {mean_reward_before:.3f} Â± {std_reward_before:.3f}")

        # çŸ­æœŸè¨“ç·´
        print("\n4. é–‹å§‹çŸ­æœŸè¨“ç·´...")
        training_start = time.time()

        model.learn(
            total_timesteps=1000, log_interval=100, progress_bar=True  # çŸ­æœŸè¨“ç·´
        )

        training_time = time.time() - training_start
        print(f"âœ… è¨“ç·´å®Œæˆï¼Œè€—æ™‚: {training_time:.2f}ç§’")

        # è¨“ç·´å¾Œè©•ä¼°
        print("\n5. è¨“ç·´å¾Œæ€§èƒ½è©•ä¼°...")
        mean_reward_after, std_reward_after = evaluate_policy(
            model, env, n_eval_episodes=5, deterministic=True
        )
        print(f"  è¨“ç·´å¾Œå¹³å‡çŽå‹µ: {mean_reward_after:.3f} Â± {std_reward_after:.3f}")

        # æ€§èƒ½æ”¹é€²åˆ†æž
        improvement = mean_reward_after - mean_reward_before
        print(f"  æ€§èƒ½æ”¹é€²: {improvement:.3f}")

        if improvement > 0:
            print("âœ… DQN è¨“ç·´é¡¯ç¤ºæ­£å‘å­¸ç¿’æ•ˆæžœ")
        else:
            print("âš ï¸ çŸ­æœŸè¨“ç·´æœªé¡¯ç¤ºæ˜Žé¡¯æ”¹é€²ï¼ˆé€™æ˜¯æ­£å¸¸çš„ï¼‰")

        # æ¸¬è©¦è¨“ç·´å¾Œçš„æ±ºç­–
        print("\n6. æ¸¬è©¦è¨“ç·´å¾Œçš„æ±ºç­–...")
        obs, _ = env.reset()

        for i in range(10):
            action, _states = model.predict(obs, deterministic=True)
            action_info = env.get_action_info(action)
            print(
                f"  æ±ºç­– {i}: å‹•ä½œ={action}, æè¿°={action_info.get('description', 'N/A')}"
            )

            obs, reward, terminated, truncated, info = env.step(action)

            if terminated or truncated:
                obs, _ = env.reset()

        # ä¿å­˜æ¨¡åž‹
        model_dir = Path("/home/sat/ntn-stack/results/dqn_test")
        model_dir.mkdir(parents=True, exist_ok=True)

        model_path = (
            model_dir / f"dqn_test_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        model.save(str(model_path))
        print(f"âœ… æ¨¡åž‹å·²ä¿å­˜: {model_path}.zip")

        env.close()

        return True

    except Exception as e:
        print(f"âŒ DQN è¨“ç·´æ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_dqn_vs_baseline():
    """æ¸¬è©¦ DQN èˆ‡åŸºæº–ç®—æ³•å°æ¯”"""
    print("\n=== DQN èˆ‡åŸºæº–ç®—æ³•å°æ¯”æ¸¬è©¦ ===")

    try:
        # æª¢æŸ¥ä¾è³´
        try:
            from stable_baselines3 import DQN
            from stable_baselines3.common.evaluation import evaluate_policy
        except ImportError:
            print("âŒ è·³éŽå°æ¯”æ¸¬è©¦ï¼šç¼ºå°‘ stable-baselines3")
            return True

        # å°Žå…¥åŸºæº–ç®—æ³•
        sys.path.insert(0, "/home/sat/ntn-stack/netstack/scripts")
        from baseline_algorithms.simple_threshold_algorithm import (
            SimpleThresholdAlgorithm,
        )
        from baseline_algorithms.random_algorithm import RandomAlgorithm

        # å‰µå»ºç’°å¢ƒ
        from netstack_api.envs.discrete_handover_env import DQNCompatibleHandoverEnv
        from netstack_api.envs.handover_env_fixed import HandoverScenario

        env = DQNCompatibleHandoverEnv(
            scenario=HandoverScenario.SINGLE_UE,
            max_ues=2,
            max_satellites=3,
            discrete_actions=8,
            max_episode_steps=50,
        )

        print("âœ… å°æ¯”æ¸¬è©¦ç’°å¢ƒå‰µå»ºæˆåŠŸ")

        # å¿«é€Ÿè¨“ç·´ DQN
        print("\n1. å¿«é€Ÿè¨“ç·´ DQN...")
        model = DQN("MlpPolicy", env, verbose=0)
        model.learn(total_timesteps=500)

        # è©•ä¼° DQN
        print("\n2. è©•ä¼° DQN æ€§èƒ½...")
        dqn_reward, dqn_std = evaluate_policy(model, env, n_eval_episodes=5)
        print(f"  DQN å¹³å‡çŽå‹µ: {dqn_reward:.3f} Â± {dqn_std:.3f}")

        # è©•ä¼°åŸºæº–ç®—æ³•ï¼ˆç°¡åŒ–ç‰ˆï¼Œå› ç‚ºéœ€è¦é©é…é›¢æ•£å‹•ä½œï¼‰
        print("\n3. è©•ä¼°åŸºæº–ç®—æ³•æ€§èƒ½...")

        # éš¨æ©ŸåŸºæº–
        random_rewards = []
        for episode in range(5):
            obs, _ = env.reset()
            episode_reward = 0

            while True:
                action = env.action_space.sample()  # éš¨æ©Ÿå‹•ä½œ
                obs, reward, terminated, truncated, info = env.step(action)
                episode_reward += reward

                if terminated or truncated:
                    break

            random_rewards.append(episode_reward)

        random_mean = np.mean(random_rewards)
        random_std = np.std(random_rewards)
        print(f"  éš¨æ©Ÿç®—æ³•å¹³å‡çŽå‹µ: {random_mean:.3f} Â± {random_std:.3f}")

        # ä¿å®ˆç­–ç•¥ï¼ˆç¸½æ˜¯ç¶­æŒé€£æŽ¥ï¼‰
        conservative_rewards = []
        for episode in range(5):
            obs, _ = env.reset()
            episode_reward = 0

            while True:
                action = 0  # ç¶­æŒé€£æŽ¥å‹•ä½œ
                obs, reward, terminated, truncated, info = env.step(action)
                episode_reward += reward

                if terminated or truncated:
                    break

            conservative_rewards.append(episode_reward)

        conservative_mean = np.mean(conservative_rewards)
        conservative_std = np.std(conservative_rewards)
        print(f"  ä¿å®ˆç­–ç•¥å¹³å‡çŽå‹µ: {conservative_mean:.3f} Â± {conservative_std:.3f}")

        # å°æ¯”çµæžœ
        print("\n4. å°æ¯”åˆ†æž:")
        results = {
            "DQN": dqn_reward,
            "éš¨æ©Ÿç®—æ³•": random_mean,
            "ä¿å®ˆç­–ç•¥": conservative_mean,
        }

        best_algorithm = max(results, key=results.get)
        print(f"  æœ€ä½³ç®—æ³•: {best_algorithm} (çŽå‹µ: {results[best_algorithm]:.3f})")

        # æŽ’å
        ranked = sorted(results.items(), key=lambda x: x[1], reverse=True)
        print("\n  æ€§èƒ½æŽ’å:")
        for i, (algo, reward) in enumerate(ranked, 1):
            print(f"    {i}. {algo}: {reward:.3f}")

        # åˆ†æžçµæžœ
        if dqn_reward > max(random_mean, conservative_mean):
            print("âœ… DQN ç®—æ³•è¡¨ç¾å„ªæ–¼åŸºæº–ç®—æ³•")
        else:
            print("âš ï¸ DQN éœ€è¦æ›´é•·æ™‚é–“è¨“ç·´ä»¥è¶…è¶ŠåŸºæº–ç®—æ³•")

        env.close()

        return True

    except Exception as e:
        print(f"âŒ å°æ¯”æ¸¬è©¦å¤±æ•—: {e}")
        import traceback

        traceback.print_exc()
        return False


def save_test_results(results: dict):
    """ä¿å­˜æ¸¬è©¦çµæžœ"""
    output_dir = Path("/home/sat/ntn-stack/results/dqn_tests")
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_file = output_dir / f"dqn_test_results_{timestamp}.json"

    test_results = {
        "timestamp": datetime.now().isoformat(),
        "test_results": results,
        "environment_info": {
            "discrete_actions": 10,
            "max_episode_steps": 200,
            "python_version": sys.version,
        },
        "summary": {
            "total_tests": len(results),
            "passed_tests": sum(1 for r in results.values() if r),
            "success_rate": sum(1 for r in results.values() if r) / len(results) * 100,
        },
    }

    with open(result_file, "w", encoding="utf-8") as f:
        json.dump(test_results, f, ensure_ascii=False, indent=2)

    print(f"\nðŸ“Š æ¸¬è©¦çµæžœå·²ä¿å­˜: {result_file}")


def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ðŸš€ é–‹å§‹ DQN ç®—æ³•æ•´åˆæ¸¬è©¦")
    print("=" * 60)

    test_results = {}

    # æ¸¬è©¦é›¢æ•£ç’°å¢ƒ
    test_results["discrete_environment"] = test_discrete_environment()

    # æ¸¬è©¦ DQN è¨“ç·´
    test_results["dqn_training"] = test_dqn_training()

    # æ¸¬è©¦å°æ¯”è©•ä¼°
    test_results["dqn_vs_baseline"] = test_dqn_vs_baseline()

    # ç¸½çµ
    print("\n" + "=" * 60)
    print("ðŸ“Š æ¸¬è©¦çµæžœç¸½çµ:")

    for test_name, result in test_results.items():
        status = "âœ… é€šéŽ" if result else "âŒ å¤±æ•—"
        print(f"  {test_name}: {status}")

    success_rate = sum(1 for r in test_results.values() if r) / len(test_results) * 100
    print(f"\nç¸½é«”æˆåŠŸçŽ‡: {success_rate:.1f}%")

    # ä¿å­˜çµæžœ
    save_test_results(test_results)

    if success_rate >= 80:
        print("ðŸŽ‰ DQN ç®—æ³•æ•´åˆæ¸¬è©¦åŸºæœ¬æˆåŠŸï¼")
        print("ðŸ’¡ å»ºè­°: é€²è¡Œæ›´é•·æ™‚é–“çš„è¨“ç·´ä»¥ç²å¾—æ›´å¥½çš„æ€§èƒ½")
    else:
        print("âš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦é€²ä¸€æ­¥èª¿è©¦")

    return success_rate >= 80


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
