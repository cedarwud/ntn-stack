#!/usr/bin/env python3
"""
LEO è¡›æ˜Ÿåˆ‡æ›ç’°å¢ƒæ€§èƒ½åŸºæº–æ¸¬è©¦

æ¸¬è©¦ä¸åŒè¦æ¨¡å ´æ™¯ä¸‹çš„ç’°å¢ƒæ€§èƒ½ï¼ŒåŒ…æ‹¬ï¼š
- FPS (æ¯ç§’å¹€æ•¸)
- è¨˜æ†¶é«”ä½¿ç”¨é‡
- å»¶é²åˆ†æ
- å¤§è¦æ¨¡å ´æ™¯æ”¯æ´èƒ½åŠ›
"""

import sys
import os
import time
import psutil
import gc
import json
import logging
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
import threading

# ç¢ºä¿èƒ½æ‰¾åˆ° netstack_api
sys.path.append('/app')
sys.path.append('/home/sat/ntn-stack')

import gymnasium as gym
from netstack_api.envs.action_space_wrapper import CompatibleLEOHandoverEnv

def setup_logging():
    """è¨­ç½®æ—¥èªŒ"""
    log_dir = Path('/tmp/benchmark_logs')
    log_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f'benchmark_{timestamp}.log'
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

@dataclass
class BenchmarkConfig:
    """åŸºæº–æ¸¬è©¦é…ç½®"""
    ues: int
    satellites: int
    episodes: int
    episode_length: int = 100
    scenario: str = "MULTI_UE"
    description: str = ""

@dataclass
class BenchmarkResult:
    """åŸºæº–æ¸¬è©¦çµæœ"""
    config: BenchmarkConfig
    total_time: float
    fps: float
    avg_step_time: float
    avg_reset_time: float
    memory_usage_mb: float
    peak_memory_mb: float
    cpu_usage_percent: float
    total_steps: int
    total_resets: int
    observations_per_sec: float
    success: bool
    error_message: str = ""

class PerformanceMonitor:
    """æ€§èƒ½ç›£æ§å™¨"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        """é‡ç½®ç›£æ§å™¨"""
        self.start_time = None
        self.memory_samples = []
        self.cpu_samples = []
        self.step_times = []
        self.reset_times = []
        self.monitoring = False
        self.monitor_thread = None
    
    def start_monitoring(self):
        """é–‹å§‹ç›£æ§"""
        self.start_time = time.time()
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢ç›£æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
    
    def _monitor_loop(self):
        """ç›£æ§å¾ªç’°"""
        process = psutil.Process()
        while self.monitoring:
            try:
                # è¨˜éŒ„è¨˜æ†¶é«”ä½¿ç”¨
                memory_info = process.memory_info()
                memory_mb = memory_info.rss / 1024 / 1024
                self.memory_samples.append(memory_mb)
                
                # è¨˜éŒ„ CPU ä½¿ç”¨
                cpu_percent = process.cpu_percent()
                self.cpu_samples.append(cpu_percent)
                
                time.sleep(0.1)  # æ¯100msæ¡æ¨£ä¸€æ¬¡
            except:
                break
    
    def record_step_time(self, step_time: float):
        """è¨˜éŒ„æ­¥é©Ÿæ™‚é–“"""
        self.step_times.append(step_time)
    
    def record_reset_time(self, reset_time: float):
        """è¨˜éŒ„é‡ç½®æ™‚é–“"""
        self.reset_times.append(reset_time)
    
    def get_results(self) -> Dict[str, float]:
        """ç²å–ç›£æ§çµæœ"""
        total_time = time.time() - self.start_time if self.start_time else 0
        
        return {
            'total_time': total_time,
            'avg_step_time': np.mean(self.step_times) if self.step_times else 0,
            'avg_reset_time': np.mean(self.reset_times) if self.reset_times else 0,
            'avg_memory_mb': np.mean(self.memory_samples) if self.memory_samples else 0,
            'peak_memory_mb': np.max(self.memory_samples) if self.memory_samples else 0,
            'avg_cpu_percent': np.mean(self.cpu_samples) if self.cpu_samples else 0,
            'total_steps': len(self.step_times),
            'total_resets': len(self.reset_times)
        }

def benchmark_scenario(config: BenchmarkConfig, logger: logging.Logger) -> BenchmarkResult:
    """åŸ·è¡Œå–®å€‹å ´æ™¯çš„åŸºæº–æ¸¬è©¦"""
    logger.info(f"é–‹å§‹åŸºæº–æ¸¬è©¦: {config.description}")
    logger.info(f"é…ç½®: {config.ues} UEs, {config.satellites} è¡›æ˜Ÿ, {config.episodes} episodes")
    
    monitor = PerformanceMonitor()
    
    try:
        # å‰µå»ºç’°å¢ƒ
        import netstack_api.envs
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦ä¿®æ”¹ç’°å¢ƒåƒæ•¸
        base_env = gym.make('netstack/LEOSatelliteHandover-v0')
        
        # ä½¿ç”¨åŒ…è£å™¨ç¢ºä¿å…¼å®¹æ€§
        env = CompatibleLEOHandoverEnv(base_env, force_box_action=True)
        
        # é–‹å§‹ç›£æ§
        monitor.start_monitoring()
        
        total_steps = 0
        successful_episodes = 0
        
        # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
        for episode in range(config.episodes):
            reset_start = time.time()
            obs, info = env.reset()
            reset_time = time.time() - reset_start
            monitor.record_reset_time(reset_time)
            
            episode_steps = 0
            done = False
            
            while not done and episode_steps < config.episode_length:
                step_start = time.time()
                
                # åŸ·è¡Œéš¨æ©Ÿå‹•ä½œ
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                
                step_time = time.time() - step_start
                monitor.record_step_time(step_time)
                
                done = terminated or truncated
                episode_steps += 1
                total_steps += 1
            
            successful_episodes += 1
            
            # æ¯10å€‹episodeå ±å‘Šé€²åº¦
            if (episode + 1) % max(1, config.episodes // 10) == 0:
                progress = (episode + 1) / config.episodes * 100
                logger.info(f"é€²åº¦: {progress:.1f}% ({episode + 1}/{config.episodes})")
        
        # åœæ­¢ç›£æ§
        monitor.stop_monitoring()
        env.close()
        
        # è¨ˆç®—çµæœ
        results = monitor.get_results()
        
        fps = total_steps / results['total_time'] if results['total_time'] > 0 else 0
        observations_per_sec = fps * env.observation_space.shape[0] if hasattr(env.observation_space, 'shape') else fps
        
        benchmark_result = BenchmarkResult(
            config=config,
            total_time=results['total_time'],
            fps=fps,
            avg_step_time=results['avg_step_time'],
            avg_reset_time=results['avg_reset_time'],
            memory_usage_mb=results['avg_memory_mb'],
            peak_memory_mb=results['peak_memory_mb'],
            cpu_usage_percent=results['avg_cpu_percent'],
            total_steps=results['total_steps'],
            total_resets=results['total_resets'],
            observations_per_sec=observations_per_sec,
            success=True
        )
        
        logger.info(f"åŸºæº–æ¸¬è©¦å®Œæˆ: FPS={fps:.1f}, è¨˜æ†¶é«”={results['avg_memory_mb']:.1f}MB")
        return benchmark_result
        
    except Exception as e:
        monitor.stop_monitoring()
        logger.error(f"åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
        
        return BenchmarkResult(
            config=config,
            total_time=0,
            fps=0,
            avg_step_time=0,
            avg_reset_time=0,
            memory_usage_mb=0,
            peak_memory_mb=0,
            cpu_usage_percent=0,
            total_steps=0,
            total_resets=0,
            observations_per_sec=0,
            success=False,
            error_message=str(e)
        )

def run_comprehensive_benchmark(logger: logging.Logger) -> List[BenchmarkResult]:
    """åŸ·è¡Œç¶œåˆåŸºæº–æ¸¬è©¦"""
    
    # å®šç¾©æ¸¬è©¦å ´æ™¯ï¼ˆæ ¹æ“š gym_todo.md çš„è¦æ ¼ï¼‰
    scenarios = [
        BenchmarkConfig(
            ues=1, 
            satellites=5, 
            episodes=100,
            episode_length=50,
            description="å°è¦æ¨¡å ´æ™¯ - 1UE/5è¡›æ˜Ÿ"
        ),
        BenchmarkConfig(
            ues=5, 
            satellites=20, 
            episodes=50,
            episode_length=75,
            description="ä¸­è¦æ¨¡å ´æ™¯ - 5UE/20è¡›æ˜Ÿ"
        ),
        BenchmarkConfig(
            ues=10, 
            satellites=50, 
            episodes=25,
            episode_length=100,
            description="å¤§è¦æ¨¡å ´æ™¯ - 10UE/50è¡›æ˜Ÿ"
        ),
        BenchmarkConfig(
            ues=20, 
            satellites=100, 
            episodes=10,
            episode_length=50,
            description="è¶…å¤§è¦æ¨¡å ´æ™¯ - 20UE/100è¡›æ˜Ÿ"
        ),
    ]
    
    results = []
    
    logger.info("=" * 60)
    logger.info("é–‹å§‹ LEO è¡›æ˜Ÿåˆ‡æ›ç’°å¢ƒç¶œåˆæ€§èƒ½åŸºæº–æ¸¬è©¦")
    logger.info("=" * 60)
    
    for i, scenario in enumerate(scenarios, 1):
        logger.info(f"\n[{i}/{len(scenarios)}] {scenario.description}")
        
        # å¼·åˆ¶åƒåœ¾å›æ”¶
        gc.collect()
        
        result = benchmark_scenario(scenario, logger)
        results.append(result)
        
        if not result.success:
            logger.warning(f"å ´æ™¯ {scenario.description} æ¸¬è©¦å¤±æ•—ï¼Œç¹¼çºŒä¸‹ä¸€å€‹å ´æ™¯")
        
        # çŸ­æš«ä¼‘æ¯
        time.sleep(1)
    
    return results

def analyze_results(results: List[BenchmarkResult], logger: logging.Logger) -> Dict[str, Any]:
    """åˆ†æåŸºæº–æ¸¬è©¦çµæœ"""
    logger.info("\n" + "=" * 60)
    logger.info("åŸºæº–æ¸¬è©¦çµæœåˆ†æ")
    logger.info("=" * 60)
    
    successful_results = [r for r in results if r.success]
    
    if not successful_results:
        logger.error("æ²’æœ‰æˆåŠŸçš„æ¸¬è©¦çµæœå¯ä¾›åˆ†æ")
        return {}
    
    # æ€§èƒ½åˆ†æ
    analysis = {
        'total_scenarios': len(results),
        'successful_scenarios': len(successful_results),
        'performance_summary': {},
        'memory_analysis': {},
        'scalability_analysis': {},
        'recommendations': []
    }
    
    # æ€§èƒ½ç¸½çµ
    fps_values = [r.fps for r in successful_results]
    memory_values = [r.memory_usage_mb for r in successful_results]
    
    analysis['performance_summary'] = {
        'max_fps': max(fps_values),
        'min_fps': min(fps_values),
        'avg_fps': np.mean(fps_values),
        'fps_std': np.std(fps_values)
    }
    
    analysis['memory_analysis'] = {
        'max_memory_mb': max(memory_values),
        'min_memory_mb': min(memory_values),
        'avg_memory_mb': np.mean(memory_values),
        'memory_std': np.std(memory_values)
    }
    
    # å¯æ“´å±•æ€§åˆ†æ
    ue_counts = [r.config.ues for r in successful_results]
    satellite_counts = [r.config.satellites for r in successful_results]
    
    analysis['scalability_analysis'] = {
        'max_ues_tested': max(ue_counts),
        'max_satellites_tested': max(satellite_counts),
        'fps_degradation_per_ue': 0,
        'memory_increase_per_ue': 0
    }
    
    # è¨ˆç®—æ¯å€‹ UE çš„æ€§èƒ½å½±éŸ¿
    if len(successful_results) > 1:
        # ç°¡å–®ç·šæ€§å›æ­¸åˆ†æ
        ue_fps_correlation = np.corrcoef(ue_counts, fps_values)[0, 1] if len(ue_counts) > 1 else 0
        ue_memory_correlation = np.corrcoef(ue_counts, memory_values)[0, 1] if len(ue_counts) > 1 else 0
        
        analysis['scalability_analysis']['ue_fps_correlation'] = ue_fps_correlation
        analysis['scalability_analysis']['ue_memory_correlation'] = ue_memory_correlation
    
    # ç”Ÿæˆå»ºè­°
    recommendations = []
    
    if analysis['performance_summary']['min_fps'] < 1000:
        recommendations.append("å»ºè­°å„ªåŒ–ç’°å¢ƒå¯¦ç¾ä»¥æé«˜ FPS")
    
    if analysis['memory_analysis']['max_memory_mb'] > 500:
        recommendations.append("è¨˜æ†¶é«”ä½¿ç”¨è¼ƒé«˜ï¼Œå»ºè­°å„ªåŒ–ç‹€æ…‹è¡¨ç¤º")
    
    if len(successful_results) < len(results):
        recommendations.append("éƒ¨åˆ†å¤§è¦æ¨¡å ´æ™¯æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦æ”¹é€²å¯æ“´å±•æ€§")
    
    analysis['recommendations'] = recommendations
    
    # è©³ç´°å ±å‘Š
    logger.info("\næ€§èƒ½æ‘˜è¦:")
    logger.info(f"  æœ€å¤§ FPS: {analysis['performance_summary']['max_fps']:.1f}")
    logger.info(f"  æœ€å° FPS: {analysis['performance_summary']['min_fps']:.1f}")
    logger.info(f"  å¹³å‡ FPS: {analysis['performance_summary']['avg_fps']:.1f}")
    
    logger.info("\nè¨˜æ†¶é«”ä½¿ç”¨:")
    logger.info(f"  æœ€å¤§è¨˜æ†¶é«”: {analysis['memory_analysis']['max_memory_mb']:.1f} MB")
    logger.info(f"  æœ€å°è¨˜æ†¶é«”: {analysis['memory_analysis']['min_memory_mb']:.1f} MB")
    logger.info(f"  å¹³å‡è¨˜æ†¶é«”: {analysis['memory_analysis']['avg_memory_mb']:.1f} MB")
    
    logger.info("\nå¯æ“´å±•æ€§:")
    logger.info(f"  æœ€å¤§æ¸¬è©¦è¦æ¨¡: {analysis['scalability_analysis']['max_ues_tested']} UEs, "
                f"{analysis['scalability_analysis']['max_satellites_tested']} è¡›æ˜Ÿ")
    
    if recommendations:
        logger.info("\nå»ºè­°:")
        for i, rec in enumerate(recommendations, 1):
            logger.info(f"  {i}. {rec}")
    
    return analysis

def generate_performance_report(results: List[BenchmarkResult], analysis: Dict[str, Any]) -> str:
    """ç”Ÿæˆæ€§èƒ½å ±å‘Š"""
    
    # å‰µå»ºå ±å‘Šç›®éŒ„
    report_dir = Path('/tmp/benchmark_reports')
    report_dir.mkdir(exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨
    if results:
        plt.figure(figsize=(15, 10))
        
        successful_results = [r for r in results if r.success]
        
        # å­åœ–1: FPS vs UEæ•¸é‡
        plt.subplot(2, 3, 1)
        ues = [r.config.ues for r in successful_results]
        fps = [r.fps for r in successful_results]
        plt.scatter(ues, fps, alpha=0.7, s=50)
        plt.xlabel('UE æ•¸é‡')
        plt.ylabel('FPS')
        plt.title('FPS vs UE æ•¸é‡')
        plt.grid(True)
        
        # å­åœ–2: è¨˜æ†¶é«”ä½¿ç”¨ vs UEæ•¸é‡
        plt.subplot(2, 3, 2)
        memory = [r.memory_usage_mb for r in successful_results]
        plt.scatter(ues, memory, alpha=0.7, s=50, color='orange')
        plt.xlabel('UE æ•¸é‡')
        plt.ylabel('è¨˜æ†¶é«”ä½¿ç”¨ (MB)')
        plt.title('è¨˜æ†¶é«”ä½¿ç”¨ vs UE æ•¸é‡')
        plt.grid(True)
        
        # å­åœ–3: æ­¥é©Ÿæ™‚é–“åˆ†ä½ˆ
        plt.subplot(2, 3, 3)
        step_times = [r.avg_step_time * 1000 for r in successful_results]  # è½‰æ›ç‚ºæ¯«ç§’
        plt.bar(range(len(step_times)), step_times, alpha=0.7, color='green')
        plt.xlabel('å ´æ™¯')
        plt.ylabel('å¹³å‡æ­¥é©Ÿæ™‚é–“ (ms)')
        plt.title('æ­¥é©Ÿæ™‚é–“åˆ†ä½ˆ')
        plt.xticks(range(len(step_times)), [f"{r.config.ues}UE" for r in successful_results])
        plt.grid(True, axis='y')
        
        # å­åœ–4: CPU ä½¿ç”¨ç‡
        plt.subplot(2, 3, 4)
        cpu_usage = [r.cpu_usage_percent for r in successful_results]
        plt.bar(range(len(cpu_usage)), cpu_usage, alpha=0.7, color='red')
        plt.xlabel('å ´æ™¯')
        plt.ylabel('CPU ä½¿ç”¨ç‡ (%)')
        plt.title('CPU ä½¿ç”¨ç‡')
        plt.xticks(range(len(cpu_usage)), [f"{r.config.ues}UE" for r in successful_results])
        plt.grid(True, axis='y')
        
        # å­åœ–5: è§€æ¸¬è™•ç†é€Ÿç‡
        plt.subplot(2, 3, 5)
        obs_per_sec = [r.observations_per_sec for r in successful_results]
        plt.scatter(ues, obs_per_sec, alpha=0.7, s=50, color='purple')
        plt.xlabel('UE æ•¸é‡')
        plt.ylabel('è§€æ¸¬/ç§’')
        plt.title('è§€æ¸¬è™•ç†é€Ÿç‡')
        plt.grid(True)
        
        # å­åœ–6: æ€§èƒ½è©•ç´š
        plt.subplot(2, 3, 6)
        # è¨ˆç®—æ€§èƒ½è©•ç´šï¼ˆåŸºæ–¼ FPS å’Œè¨˜æ†¶é«”æ•ˆç‡ï¼‰
        performance_scores = []
        for r in successful_results:
            fps_score = min(100, r.fps / 10)  # FPS/10 ä½œç‚ºåˆ†æ•¸ï¼Œæœ€é«˜100
            memory_efficiency = max(0, 100 - r.memory_usage_mb / 5)  # è¨˜æ†¶é«”æ•ˆç‡
            overall_score = (fps_score + memory_efficiency) / 2
            performance_scores.append(overall_score)
        
        colors = ['red' if s < 50 else 'orange' if s < 75 else 'green' for s in performance_scores]
        plt.bar(range(len(performance_scores)), performance_scores, color=colors, alpha=0.7)
        plt.xlabel('å ´æ™¯')
        plt.ylabel('æ€§èƒ½è©•ç´š')
        plt.title('ç¶œåˆæ€§èƒ½è©•ç´š')
        plt.xticks(range(len(performance_scores)), [f"{r.config.ues}UE" for r in successful_results])
        plt.ylim(0, 100)
        plt.grid(True, axis='y')
        
        plt.tight_layout()
        chart_file = report_dir / f'benchmark_charts_{timestamp}.png'
        plt.savefig(chart_file, dpi=300, bbox_inches='tight')
        plt.close()
    
    # ç”Ÿæˆæ–‡å­—å ±å‘Š
    report_file = report_dir / f'benchmark_report_{timestamp}.txt'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("LEO è¡›æ˜Ÿåˆ‡æ›ç’°å¢ƒæ€§èƒ½åŸºæº–æ¸¬è©¦å ±å‘Š\n")
        f.write("=" * 80 + "\n")
        f.write(f"æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ¸¬è©¦å ´æ™¯: {len(results)} å€‹\n")
        f.write(f"æˆåŠŸå ´æ™¯: {len([r for r in results if r.success])} å€‹\n\n")
        
        # è©³ç´°çµæœ
        f.write("è©³ç´°æ¸¬è©¦çµæœ:\n")
        f.write("-" * 80 + "\n")
        for i, result in enumerate(results, 1):
            f.write(f"{i}. {result.config.description}\n")
            if result.success:
                f.write(f"   ç‹€æ…‹: âœ… æˆåŠŸ\n")
                f.write(f"   FPS: {result.fps:.1f}\n")
                f.write(f"   å¹³å‡æ­¥é©Ÿæ™‚é–“: {result.avg_step_time*1000:.2f} ms\n")
                f.write(f"   å¹³å‡é‡ç½®æ™‚é–“: {result.avg_reset_time*1000:.2f} ms\n")
                f.write(f"   è¨˜æ†¶é«”ä½¿ç”¨: {result.memory_usage_mb:.1f} MB\n")
                f.write(f"   å³°å€¼è¨˜æ†¶é«”: {result.peak_memory_mb:.1f} MB\n")
                f.write(f"   CPU ä½¿ç”¨ç‡: {result.cpu_usage_percent:.1f}%\n")
                f.write(f"   ç¸½æ­¥é©Ÿæ•¸: {result.total_steps}\n")
                f.write(f"   è§€æ¸¬è™•ç†é€Ÿç‡: {result.observations_per_sec:.1f} obs/s\n")
            else:
                f.write(f"   ç‹€æ…‹: âŒ å¤±æ•—\n")
                f.write(f"   éŒ¯èª¤: {result.error_message}\n")
            f.write("\n")
        
        # åˆ†æçµæœ
        if analysis:
            f.write("æ€§èƒ½åˆ†æ:\n")
            f.write("-" * 80 + "\n")
            
            perf = analysis.get('performance_summary', {})
            f.write(f"FPS ç¯„åœ: {perf.get('min_fps', 0):.1f} - {perf.get('max_fps', 0):.1f}\n")
            f.write(f"å¹³å‡ FPS: {perf.get('avg_fps', 0):.1f} Â± {perf.get('fps_std', 0):.1f}\n")
            
            mem = analysis.get('memory_analysis', {})
            f.write(f"è¨˜æ†¶é«”ç¯„åœ: {mem.get('min_memory_mb', 0):.1f} - {mem.get('max_memory_mb', 0):.1f} MB\n")
            f.write(f"å¹³å‡è¨˜æ†¶é«”: {mem.get('avg_memory_mb', 0):.1f} Â± {mem.get('memory_std', 0):.1f} MB\n")
            
            scale = analysis.get('scalability_analysis', {})
            f.write(f"æœ€å¤§æ¸¬è©¦è¦æ¨¡: {scale.get('max_ues_tested', 0)} UEs, {scale.get('max_satellites_tested', 0)} è¡›æ˜Ÿ\n")
            
            recommendations = analysis.get('recommendations', [])
            if recommendations:
                f.write("\nå„ªåŒ–å»ºè­°:\n")
                for i, rec in enumerate(recommendations, 1):
                    f.write(f"{i}. {rec}\n")
        
        # åŸºæº–é”æˆç‹€æ³
        f.write("\nåŸºæº–é”æˆç‹€æ³ (æ ¹æ“š gym_todo.md):\n")
        f.write("-" * 80 + "\n")
        
        benchmarks = [
            ("1UE_5SAT", 20000, 50),    # FPS > 20000, è¨˜æ†¶é«” < 50MB
            ("5UE_20SAT", 15000, 100),  # FPS > 15000, è¨˜æ†¶é«” < 100MB
            ("10UE_50SAT", 5000, 200),  # FPS > 5000, è¨˜æ†¶é«” < 200MB
            ("20UE_100SAT", 1000, 500)  # FPS > 1000, è¨˜æ†¶é«” < 500MB
        ]
        
        for i, (scenario_name, target_fps, target_memory) in enumerate(benchmarks):
            if i < len(results) and results[i].success:
                actual_fps = results[i].fps
                actual_memory = results[i].memory_usage_mb
                
                fps_status = "âœ…" if actual_fps >= target_fps else "âŒ"
                memory_status = "âœ…" if actual_memory <= target_memory else "âŒ"
                
                f.write(f"{scenario_name}:\n")
                f.write(f"  FPS: {actual_fps:.1f} / {target_fps} {fps_status}\n")
                f.write(f"  è¨˜æ†¶é«”: {actual_memory:.1f} / {target_memory} MB {memory_status}\n")
            else:
                f.write(f"{scenario_name}: âŒ æ¸¬è©¦å¤±æ•—æˆ–æœªåŸ·è¡Œ\n")
    
    return str(report_file)

def main():
    """ä¸»å‡½æ•¸"""
    logger = setup_logging()
    
    try:
        logger.info("é–‹å§‹ LEO è¡›æ˜Ÿåˆ‡æ›ç’°å¢ƒæ€§èƒ½åŸºæº–æ¸¬è©¦")
        
        # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
        results = run_comprehensive_benchmark(logger)
        
        # åˆ†æçµæœ
        analysis = analyze_results(results, logger)
        
        # ç”Ÿæˆå ±å‘Š
        report_file = generate_performance_report(results, analysis)
        
        logger.info(f"\nåŸºæº–æ¸¬è©¦å®Œæˆï¼")
        logger.info(f"å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        # è¿”å›æ˜¯å¦é”åˆ°åŸºæº–è¦æ±‚
        successful_count = len([r for r in results if r.success])
        total_count = len(results)
        
        if successful_count == total_count:
            logger.info("ğŸ‰ æ‰€æœ‰å ´æ™¯æ¸¬è©¦æˆåŠŸï¼")
            return True
        else:
            logger.warning(f"âš ï¸  éƒ¨åˆ†å ´æ™¯æ¸¬è©¦å¤±æ•— ({successful_count}/{total_count})")
            return False
            
    except Exception as e:
        logger.error(f"åŸºæº–æ¸¬è©¦éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)