"""
Phase 4: æ©Ÿå™¨å­¸ç¿’æ•´åˆå’Œé æ¸¬æ¨¡å‹ç³»çµ±
ä½¿ç”¨ç”Ÿç”¢æ•¸æ“šè¨“ç·´é æ¸¬æ¨¡å‹ï¼Œæå‡ handover æ±ºç­–æº–ç¢ºæ€§
"""
import asyncio
import json
import logging
import numpy as np
import pandas as pd
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple, Union
from pathlib import Path
import yaml
import pickle
import uuid

# é…ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelType(Enum):
    """æ¨¡å‹é¡å‹"""
    XGBOOST = "xgboost"
    NEURAL_NETWORK = "neural_network"
    RANDOM_FOREST = "random_forest"
    SVM = "svm"
    ENSEMBLE = "ensemble"

class PredictionTask(Enum):
    """é æ¸¬ä»»å‹™é¡å‹"""
    HANDOVER_TIMING = "handover_timing"           # åˆ‡æ›æ™‚æ©Ÿé æ¸¬
    SATELLITE_SELECTION = "satellite_selection"   # è¡›æ˜Ÿé¸æ“‡é æ¸¬
    LINK_QUALITY = "link_quality"                 # éˆè·¯å“è³ªé æ¸¬
    INTERFERENCE_PREDICTION = "interference_prediction"  # å¹²æ“¾é æ¸¬
    LOAD_BALANCING = "load_balancing"             # è² è¼‰å‡è¡¡é æ¸¬

class ModelStatus(Enum):
    """æ¨¡å‹ç‹€æ…‹"""
    TRAINING = "training"
    VALIDATING = "validating"
    DEPLOYED = "deployed"
    RETRAINING = "retraining"
    DEPRECATED = "deprecated"
    FAILED = "failed"

@dataclass
class FeatureConfig:
    """ç‰¹å¾µé…ç½®"""
    name: str
    type: str  # "numerical", "categorical", "timestamp"
    importance: float = 0.0
    transformation: Optional[str] = None  # "log", "normalize", "one_hot"
    window_size: Optional[int] = None     # æ™‚é–“çª—å£å¤§å°
    aggregation: Optional[str] = None     # "mean", "max", "std"

@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""
    model_id: str
    name: str
    type: ModelType
    task: PredictionTask
    features: List[FeatureConfig]
    target_variable: str
    hyperparameters: Dict[str, Any]
    training_config: Dict[str, Any]
    validation_config: Dict[str, Any]
    deployment_config: Dict[str, Any]

@dataclass
class TrainingData:
    """è¨“ç·´æ•¸æ“š"""
    timestamp: datetime
    satellite_id: str
    ue_id: str
    position_lat: float
    position_lon: float
    elevation_angle: float
    azimuth_angle: float
    signal_strength: float
    snr: float
    interference_level: float
    handover_occurred: bool
    handover_success: bool
    handover_latency_ms: float
    beam_id: str
    load_factor: float
    weather_condition: str
    time_of_day: int
    doppler_shift: float
    prediction_horizon_ms: int

@dataclass
class ModelMetrics:
    """æ¨¡å‹è©•ä¼°æŒ‡æ¨™"""
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    auc_roc: float
    mean_absolute_error: float
    root_mean_squared_error: float
    prediction_latency_ms: float
    feature_importance: Dict[str, float]

@dataclass
class PredictionResult:
    """é æ¸¬çµæœ"""
    prediction_id: str
    model_id: str
    input_features: Dict[str, Any]
    prediction: Union[float, int, str, bool]
    confidence: float
    prediction_time: datetime
    processing_time_ms: float
    model_version: str

class FeatureEngineer:
    """ç‰¹å¾µå·¥ç¨‹å™¨"""
    
    def __init__(self):
        self.feature_transformers = {}
        self.feature_statistics = {}
    
    def extract_features(self, raw_data: List[TrainingData]) -> pd.DataFrame:
        """æå–ç‰¹å¾µ"""
        logger.info(f"é–‹å§‹ç‰¹å¾µæå–ï¼ŒåŸå§‹æ•¸æ“šé‡: {len(raw_data)}")
        
        # è½‰æ›ç‚º DataFrame
        df = pd.DataFrame([vars(data) for data in raw_data])
        
        # åŸºç¤ç‰¹å¾µ
        features_df = pd.DataFrame()
        
        # æ™‚é–“ç‰¹å¾µ
        features_df['hour_of_day'] = pd.to_datetime(df['timestamp']).dt.hour
        features_df['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek
        features_df['is_peak_hour'] = features_df['hour_of_day'].apply(lambda x: 1 if x in [8, 9, 17, 18, 19] else 0)
        
        # ä½ç½®ç‰¹å¾µ
        features_df['latitude'] = df['position_lat']
        features_df['longitude'] = df['position_lon']
        features_df['elevation_angle'] = df['elevation_angle']
        features_df['azimuth_angle'] = df['azimuth_angle']
        
        # ä¿¡è™Ÿå“è³ªç‰¹å¾µ
        features_df['signal_strength'] = df['signal_strength']
        features_df['snr'] = df['snr']
        features_df['interference_level'] = df['interference_level']
        features_df['doppler_shift'] = df['doppler_shift']
        
        # è¡ç”Ÿç‰¹å¾µ
        features_df['signal_to_interference'] = df['signal_strength'] / (df['interference_level'] + 1e-6)
        features_df['elevation_sin'] = np.sin(np.radians(df['elevation_angle']))
        features_df['elevation_cos'] = np.cos(np.radians(df['elevation_angle']))
        features_df['azimuth_sin'] = np.sin(np.radians(df['azimuth_angle']))
        features_df['azimuth_cos'] = np.cos(np.radians(df['azimuth_angle']))
        
        # è² è¼‰ç‰¹å¾µ
        features_df['load_factor'] = df['load_factor']
        
        # å¤©æ°£ç‰¹å¾µ (One-hot ç·¨ç¢¼)
        weather_dummies = pd.get_dummies(df['weather_condition'], prefix='weather')
        features_df = pd.concat([features_df, weather_dummies], axis=1)
        
        # æ™‚é–“çª—å£ç‰¹å¾µ
        features_df = self._add_temporal_features(features_df, df)
        
        # ç›®æ¨™è®Šé‡
        if 'handover_occurred' in df.columns:
            features_df['target_handover'] = df['handover_occurred'].astype(int)
        if 'handover_success' in df.columns:
            features_df['target_success'] = df['handover_success'].astype(int)
        if 'handover_latency_ms' in df.columns:
            features_df['target_latency'] = df['handover_latency_ms']
        
        logger.info(f"ç‰¹å¾µæå–å®Œæˆï¼Œç‰¹å¾µæ•¸é‡: {features_df.shape[1]}")
        return features_df
    
    def _add_temporal_features(self, features_df: pd.DataFrame, original_df: pd.DataFrame) -> pd.DataFrame:
        """æ·»åŠ æ™‚é–“çª—å£ç‰¹å¾µ"""
        # æ»‘å‹•çª—å£çµ±è¨ˆç‰¹å¾µ
        window_size = 10
        
        # ä¿¡è™Ÿå¼·åº¦çš„æ»‘å‹•çµ±è¨ˆ
        features_df['signal_strength_mean_10'] = original_df['signal_strength'].rolling(window=window_size, min_periods=1).mean()
        features_df['signal_strength_std_10'] = original_df['signal_strength'].rolling(window=window_size, min_periods=1).std().fillna(0)
        features_df['signal_strength_trend'] = original_df['signal_strength'].diff().fillna(0)
        
        # SNR çš„æ»‘å‹•çµ±è¨ˆ
        features_df['snr_mean_10'] = original_df['snr'].rolling(window=window_size, min_periods=1).mean()
        features_df['snr_std_10'] = original_df['snr'].rolling(window=window_size, min_periods=1).std().fillna(0)
        
        # å¹²æ“¾æ°´æº–çš„æ»‘å‹•çµ±è¨ˆ
        features_df['interference_mean_10'] = original_df['interference_level'].rolling(window=window_size, min_periods=1).mean()
        features_df['interference_trend'] = original_df['interference_level'].diff().fillna(0)
        
        return features_df
    
    def normalize_features(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾µæ­£è¦åŒ–"""
        numerical_features = features_df.select_dtypes(include=[np.number]).columns
        
        for feature in numerical_features:
            if feature.startswith('target_'):
                continue
            
            mean_val = features_df[feature].mean()
            std_val = features_df[feature].std()
            
            if std_val > 0:
                features_df[feature] = (features_df[feature] - mean_val) / std_val
                self.feature_statistics[feature] = {'mean': mean_val, 'std': std_val}
        
        return features_df

class MLModel:
    """æ©Ÿå™¨å­¸ç¿’æ¨¡å‹åŸºé¡"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.model = None
        self.is_trained = False
        self.metrics = None
        self.feature_columns = None
        self.version = "1.0.0"
    
    def train(self, X_train: pd.DataFrame, y_train: pd.Series, 
              X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """è¨“ç·´æ¨¡å‹"""
        raise NotImplementedError
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é æ¸¬"""
        raise NotImplementedError
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """é æ¸¬æ©Ÿç‡"""
        raise NotImplementedError
    
    def evaluate(self, X_test: pd.DataFrame, y_test: pd.Series) -> ModelMetrics:
        """è©•ä¼°æ¨¡å‹"""
        raise NotImplementedError

class XGBoostModel(MLModel):
    """XGBoost æ¨¡å‹å¯¦ç¾"""
    
    def __init__(self, config: ModelConfig):
        super().__init__(config)
        try:
            import xgboost as xgb
            self.xgb = xgb
        except ImportError:
            logger.warning("XGBoost æœªå®‰è£ï¼Œä½¿ç”¨æ¨¡æ“¬å¯¦ç¾")
            self.xgb = None
    
    def train(self, X_train: pd.DataFrame, y_train: pd.Series, 
              X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None):
        """è¨“ç·´ XGBoost æ¨¡å‹"""
        logger.info(f"é–‹å§‹è¨“ç·´ XGBoost æ¨¡å‹: {self.config.name}")
        
        self.feature_columns = X_train.columns.tolist()
        
        if self.xgb:
            # çœŸå¯¦ XGBoost å¯¦ç¾
            hyperparams = self.config.hyperparameters
            
            if self.config.task in [PredictionTask.HANDOVER_TIMING, PredictionTask.SATELLITE_SELECTION]:
                # åˆ†é¡ä»»å‹™
                self.model = self.xgb.XGBClassifier(
                    n_estimators=hyperparams.get('n_estimators', 100),
                    max_depth=hyperparams.get('max_depth', 6),
                    learning_rate=hyperparams.get('learning_rate', 0.1),
                    subsample=hyperparams.get('subsample', 0.8),
                    colsample_bytree=hyperparams.get('colsample_bytree', 0.8),
                    random_state=42
                )
            else:
                # å›æ­¸ä»»å‹™
                self.model = self.xgb.XGBRegressor(
                    n_estimators=hyperparams.get('n_estimators', 100),
                    max_depth=hyperparams.get('max_depth', 6),
                    learning_rate=hyperparams.get('learning_rate', 0.1),
                    subsample=hyperparams.get('subsample', 0.8),
                    colsample_bytree=hyperparams.get('colsample_bytree', 0.8),
                    random_state=42
                )
            
            self.model.fit(X_train, y_train)
        else:
            # æ¨¡æ“¬è¨“ç·´
            logger.info("ä½¿ç”¨æ¨¡æ“¬ XGBoost æ¨¡å‹")
            self.model = MockXGBoostModel(X_train.shape[1])
            await asyncio.sleep(2)  # æ¨¡æ“¬è¨“ç·´æ™‚é–“
        
        self.is_trained = True
        logger.info(f"XGBoost æ¨¡å‹è¨“ç·´å®Œæˆ")
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """é æ¸¬"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´")
        
        if self.xgb and hasattr(self.model, 'predict'):
            return self.model.predict(X[self.feature_columns])
        else:
            # æ¨¡æ“¬é æ¸¬
            return self.model.predict(X[self.feature_columns].values)
    
    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """é æ¸¬æ©Ÿç‡"""
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´")
        
        if self.xgb and hasattr(self.model, 'predict_proba'):
            return self.model.predict_proba(X[self.feature_columns])
        else:
            # æ¨¡æ“¬æ©Ÿç‡é æ¸¬
            return self.model.predict_proba(X[self.feature_columns].values)
    
    def evaluate(self, X_test: pd.DataFrame, y_test: pd.Series) -> ModelMetrics:
        """è©•ä¼°æ¨¡å‹"""
        predictions = self.predict(X_test)
        
        if self.config.task in [PredictionTask.HANDOVER_TIMING, PredictionTask.SATELLITE_SELECTION]:
            # åˆ†é¡æŒ‡æ¨™
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
            
            accuracy = accuracy_score(y_test, predictions)
            precision = precision_score(y_test, predictions, average='weighted', zero_division=0)
            recall = recall_score(y_test, predictions, average='weighted', zero_division=0)
            f1 = f1_score(y_test, predictions, average='weighted', zero_division=0)
            
            try:
                proba_predictions = self.predict_proba(X_test)
                if proba_predictions.shape[1] == 2:
                    auc_roc = roc_auc_score(y_test, proba_predictions[:, 1])
                else:
                    auc_roc = roc_auc_score(y_test, proba_predictions, multi_class='ovr')
            except:
                auc_roc = 0.0
            
            mae = 0.0
            rmse = 0.0
        else:
            # å›æ­¸æŒ‡æ¨™
            from sklearn.metrics import mean_absolute_error, mean_squared_error
            
            mae = mean_absolute_error(y_test, predictions)
            rmse = np.sqrt(mean_squared_error(y_test, predictions))
            
            # å›æ­¸ä»»å‹™çš„åˆ†é¡æŒ‡æ¨™è¨­ç‚º0
            accuracy = 0.0
            precision = 0.0
            recall = 0.0
            f1 = 0.0
            auc_roc = 0.0
        
        # ç‰¹å¾µé‡è¦æ€§
        feature_importance = {}
        if self.xgb and hasattr(self.model, 'feature_importances_'):
            for i, feature in enumerate(self.feature_columns):
                feature_importance[feature] = float(self.model.feature_importances_[i])
        else:
            # æ¨¡æ“¬ç‰¹å¾µé‡è¦æ€§
            import random
            for feature in self.feature_columns:
                feature_importance[feature] = random.uniform(0, 1)
        
        return ModelMetrics(
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1_score=f1,
            auc_roc=auc_roc,
            mean_absolute_error=mae,
            root_mean_squared_error=rmse,
            prediction_latency_ms=5.0,  # æ¨¡æ“¬é æ¸¬å»¶é²
            feature_importance=feature_importance
        )

class MockXGBoostModel:
    """æ¨¡æ“¬ XGBoost æ¨¡å‹ï¼ˆç”¨æ–¼æ²’æœ‰å®‰è£ XGBoost çš„ç’°å¢ƒï¼‰"""
    
    def __init__(self, n_features: int):
        self.n_features = n_features
        self.weights = np.random.randn(n_features) * 0.1
        self.bias = np.random.randn() * 0.1
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """æ¨¡æ“¬é æ¸¬"""
        predictions = np.dot(X, self.weights) + self.bias
        return (predictions > 0).astype(int)  # äºŒåˆ†é¡
    
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """æ¨¡æ“¬æ©Ÿç‡é æ¸¬"""
        scores = np.dot(X, self.weights) + self.bias
        proba = 1 / (1 + np.exp(-scores))  # sigmoid
        return np.column_stack([1 - proba, proba])

class MLIntegrationSystem:
    """æ©Ÿå™¨å­¸ç¿’æ•´åˆç³»çµ±"""
    
    def __init__(self, config_path: str):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.models: Dict[str, MLModel] = {}
        self.feature_engineer = FeatureEngineer()
        self.data_storage = DataStorage()
        self.model_registry = ModelRegistry()
        self.is_running = False
    
    def _load_config(self) -> Dict[str, Any]:
        """è¼‰å…¥é…ç½®"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥é…ç½®å¤±æ•—: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """ç²å–é è¨­é…ç½®"""
        return {
            "ml_system": {
                "training_schedule": "daily",
                "model_validation_split": 0.2,
                "feature_selection_threshold": 0.01,
                "model_performance_threshold": 0.85,
                "prediction_cache_ttl": 3600,
                "max_models_per_task": 3
            },
            "data_collection": {
                "batch_size": 10000,
                "collection_interval_minutes": 60,
                "data_retention_days": 90,
                "quality_check_enabled": True
            },
            "model_deployment": {
                "auto_deployment_enabled": True,
                "performance_improvement_threshold": 0.05,
                "canary_traffic_percentage": 10,
                "rollback_on_performance_drop": True
            }
        }
    
    async def start_ml_system(self):
        """å•Ÿå‹•æ©Ÿå™¨å­¸ç¿’ç³»çµ±"""
        if self.is_running:
            logger.warning("ML ç³»çµ±å·²åœ¨é‹è¡Œ")
            return
        
        self.is_running = True
        logger.info("ğŸ¤– å•Ÿå‹•æ©Ÿå™¨å­¸ç¿’æ•´åˆç³»çµ±")
        
        # å•Ÿå‹•ç³»çµ±ä»»å‹™
        tasks = [
            asyncio.create_task(self._data_collection_loop()),
            asyncio.create_task(self._training_loop()),
            asyncio.create_task(self._model_monitoring_loop()),
            asyncio.create_task(self._prediction_service_loop())
        ]
        
        try:
            await asyncio.gather(*tasks)
        except asyncio.CancelledError:
            logger.info("ML ç³»çµ±å·²åœæ­¢")
        finally:
            self.is_running = False
    
    async def stop_ml_system(self):
        """åœæ­¢æ©Ÿå™¨å­¸ç¿’ç³»çµ±"""
        logger.info("ğŸ›‘ åœæ­¢æ©Ÿå™¨å­¸ç¿’ç³»çµ±")
        self.is_running = False
    
    async def _data_collection_loop(self):
        """æ•¸æ“šæ”¶é›†å¾ªç’°"""
        collection_config = self.config.get("data_collection", {})
        interval_minutes = collection_config.get("collection_interval_minutes", 60)
        
        while self.is_running:
            try:
                # æ”¶é›†ç”Ÿç”¢æ•¸æ“š
                await self._collect_production_data()
                
                # æ¯å°æ™‚æ”¶é›†ä¸€æ¬¡
                await asyncio.sleep(interval_minutes * 60)
                
            except Exception as e:
                logger.error(f"æ•¸æ“šæ”¶é›†å¤±æ•—: {e}")
                await asyncio.sleep(interval_minutes * 60)
    
    async def _training_loop(self):
        """è¨“ç·´å¾ªç’°"""
        while self.is_running:
            try:
                # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è¨“ç·´
                if await self._should_retrain_models():
                    await self._retrain_all_models()
                
                # æ¯æ—¥æª¢æŸ¥ä¸€æ¬¡
                await asyncio.sleep(86400)
                
            except Exception as e:
                logger.error(f"æ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
                await asyncio.sleep(86400)
    
    async def _model_monitoring_loop(self):
        """æ¨¡å‹ç›£æ§å¾ªç’°"""
        while self.is_running:
            try:
                # ç›£æ§æ¨¡å‹æ€§èƒ½
                await self._monitor_model_performance()
                
                # æ¯å°æ™‚ç›£æ§ä¸€æ¬¡
                await asyncio.sleep(3600)
                
            except Exception as e:
                logger.error(f"æ¨¡å‹ç›£æ§å¤±æ•—: {e}")
                await asyncio.sleep(3600)
    
    async def _prediction_service_loop(self):
        """é æ¸¬æœå‹™å¾ªç’°"""
        while self.is_running:
            try:
                # è™•ç†é æ¸¬è«‹æ±‚
                await self._process_prediction_requests()
                
                # æ¯ç§’è™•ç†ä¸€æ¬¡
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"é æ¸¬æœå‹™å¤±æ•—: {e}")
                await asyncio.sleep(1)
    
    async def create_model(self, model_config: ModelConfig) -> str:
        """å‰µå»ºæ–°æ¨¡å‹"""
        logger.info(f"å‰µå»ºæ¨¡å‹: {model_config.name}")
        
        if model_config.type == ModelType.XGBOOST:
            model = XGBoostModel(model_config)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹é¡å‹: {model_config.type}")
        
        self.models[model_config.model_id] = model
        
        # è¨»å†Šæ¨¡å‹
        await self.model_registry.register_model(model_config)
        
        return model_config.model_id
    
    async def train_model(self, model_id: str, training_data: List[TrainingData]) -> bool:
        """è¨“ç·´æ¨¡å‹"""
        if model_id not in self.models:
            raise ValueError(f"æ¨¡å‹ {model_id} ä¸å­˜åœ¨")
        
        model = self.models[model_id]
        logger.info(f"é–‹å§‹è¨“ç·´æ¨¡å‹: {model.config.name}")
        
        try:
            # ç‰¹å¾µå·¥ç¨‹
            features_df = self.feature_engineer.extract_features(training_data)
            features_df = self.feature_engineer.normalize_features(features_df)
            
            # åˆ†é›¢ç‰¹å¾µå’Œç›®æ¨™è®Šé‡
            target_column = model.config.target_variable
            if target_column not in features_df.columns:
                raise ValueError(f"ç›®æ¨™è®Šé‡ {target_column} ä¸å­˜åœ¨")
            
            X = features_df.drop([col for col in features_df.columns if col.startswith('target_')], axis=1)
            y = features_df[target_column]
            
            # åˆ†å‰²è¨“ç·´å’Œé©—è­‰é›†
            split_ratio = self.config.get("ml_system", {}).get("model_validation_split", 0.2)
            split_index = int(len(X) * (1 - split_ratio))
            
            X_train, X_val = X[:split_index], X[split_index:]
            y_train, y_val = y[:split_index], y[split_index:]
            
            # è¨“ç·´æ¨¡å‹
            await model.train(X_train, y_train, X_val, y_val)
            
            # è©•ä¼°æ¨¡å‹
            model.metrics = model.evaluate(X_val, y_val)
            
            logger.info(f"æ¨¡å‹è¨“ç·´å®Œæˆ: {model.config.name}")
            logger.info(f"é©—è­‰é›†æ€§èƒ½: æº–ç¢ºç‡={model.metrics.accuracy:.3f}, F1={model.metrics.f1_score:.3f}")
            
            return True
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è¨“ç·´å¤±æ•—: {e}")
            return False
    
    async def predict(self, model_id: str, input_features: Dict[str, Any]) -> PredictionResult:
        """åŸ·è¡Œé æ¸¬"""
        if model_id not in self.models:
            raise ValueError(f"æ¨¡å‹ {model_id} ä¸å­˜åœ¨")
        
        model = self.models[model_id]
        if not model.is_trained:
            raise ValueError(f"æ¨¡å‹ {model_id} å°šæœªè¨“ç·´")
        
        start_time = datetime.now()
        
        # æº–å‚™è¼¸å…¥ç‰¹å¾µ
        input_df = pd.DataFrame([input_features])
        
        # ç‰¹å¾µé è™•ç†
        input_df = self._preprocess_features(input_df, model)
        
        # åŸ·è¡Œé æ¸¬
        prediction = model.predict(input_df)[0]
        
        # è¨ˆç®—ç½®ä¿¡åº¦
        try:
            proba = model.predict_proba(input_df)[0]
            confidence = float(np.max(proba))
        except:
            confidence = 0.8  # é»˜èªç½®ä¿¡åº¦
        
        processing_time = (datetime.now() - start_time).total_seconds() * 1000
        
        result = PredictionResult(
            prediction_id=str(uuid.uuid4()),
            model_id=model_id,
            input_features=input_features,
            prediction=prediction,
            confidence=confidence,
            prediction_time=datetime.now(),
            processing_time_ms=processing_time,
            model_version=model.version
        )
        
        return result
    
    async def _collect_production_data(self):
        """æ”¶é›†ç”Ÿç”¢æ•¸æ“š"""
        logger.info("æ”¶é›†ç”Ÿç”¢æ•¸æ“š")
        
        # æ¨¡æ“¬å¾ç”Ÿç”¢ç³»çµ±æ”¶é›†æ•¸æ“š
        batch_size = self.config.get("data_collection", {}).get("batch_size", 10000)
        
        simulated_data = []
        for _ in range(100):  # æ¨¡æ“¬100æ¢è¨˜éŒ„
            data = TrainingData(
                timestamp=datetime.now(),
                satellite_id=f"sat_{np.random.randint(1, 10)}",
                ue_id=f"ue_{np.random.randint(1000, 9999)}",
                position_lat=np.random.uniform(-90, 90),
                position_lon=np.random.uniform(-180, 180),
                elevation_angle=np.random.uniform(10, 90),
                azimuth_angle=np.random.uniform(0, 360),
                signal_strength=np.random.uniform(-120, -70),
                snr=np.random.uniform(0, 30),
                interference_level=np.random.uniform(0, 20),
                handover_occurred=np.random.random() > 0.8,
                handover_success=np.random.random() > 0.05,
                handover_latency_ms=np.random.uniform(20, 80),
                beam_id=f"beam_{np.random.randint(1, 64)}",
                load_factor=np.random.uniform(0, 1),
                weather_condition=np.random.choice(['clear', 'cloudy', 'rainy', 'snowy']),
                time_of_day=np.random.randint(0, 24),
                doppler_shift=np.random.uniform(-1000, 1000),
                prediction_horizon_ms=np.random.randint(100, 2000)
            )
            simulated_data.append(data)
        
        # å­˜å„²æ•¸æ“š
        await self.data_storage.store_training_data(simulated_data)
        
        logger.info(f"æ”¶é›†äº† {len(simulated_data)} æ¢ç”Ÿç”¢æ•¸æ“š")
    
    async def _should_retrain_models(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è¨“ç·´æ¨¡å‹"""
        # æª¢æŸ¥æ•¸æ“šé‡æ˜¯å¦è¶³å¤ 
        data_count = await self.data_storage.get_data_count()
        
        # æª¢æŸ¥æ¨¡å‹æ€§èƒ½æ˜¯å¦ä¸‹é™
        performance_threshold = self.config.get("ml_system", {}).get("model_performance_threshold", 0.85)
        
        for model_id, model in self.models.items():
            if model.metrics and model.metrics.accuracy < performance_threshold:
                logger.info(f"æ¨¡å‹ {model_id} æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦é‡æ–°è¨“ç·´")
                return True
        
        return data_count > 10000  # æœ‰è¶³å¤ æ–°æ•¸æ“šæ™‚é‡æ–°è¨“ç·´
    
    async def _retrain_all_models(self):
        """é‡æ–°è¨“ç·´æ‰€æœ‰æ¨¡å‹"""
        logger.info("é–‹å§‹é‡æ–°è¨“ç·´æ‰€æœ‰æ¨¡å‹")
        
        # ç²å–æœ€æ–°è¨“ç·´æ•¸æ“š
        training_data = await self.data_storage.get_recent_training_data(days=30)
        
        for model_id in self.models:
            try:
                await self.train_model(model_id, training_data)
            except Exception as e:
                logger.error(f"é‡æ–°è¨“ç·´æ¨¡å‹ {model_id} å¤±æ•—: {e}")
    
    async def _monitor_model_performance(self):
        """ç›£æ§æ¨¡å‹æ€§èƒ½"""
        for model_id, model in self.models.items():
            if model.is_trained and model.metrics:
                logger.info(f"æ¨¡å‹ {model_id} æ€§èƒ½: æº–ç¢ºç‡={model.metrics.accuracy:.3f}")
    
    async def _process_prediction_requests(self):
        """è™•ç†é æ¸¬è«‹æ±‚"""
        # æ¨¡æ“¬è™•ç†é æ¸¬è«‹æ±‚
        await asyncio.sleep(0.1)
    
    def _preprocess_features(self, input_df: pd.DataFrame, model: MLModel) -> pd.DataFrame:
        """é è™•ç†ç‰¹å¾µ"""
        # ç¢ºä¿æ‰€æœ‰å¿…éœ€çš„ç‰¹å¾µéƒ½å­˜åœ¨
        for feature in model.feature_columns:
            if feature not in input_df.columns:
                input_df[feature] = 0.0  # é»˜èªå€¼
        
        # é¸æ“‡æ¨¡å‹éœ€è¦çš„ç‰¹å¾µ
        return input_df[model.feature_columns]
    
    def get_system_status(self) -> Dict[str, Any]:
        """ç²å–ç³»çµ±ç‹€æ…‹"""
        return {
            "is_running": self.is_running,
            "models_count": len(self.models),
            "trained_models": len([m for m in self.models.values() if m.is_trained]),
            "system_metrics": {
                "total_predictions": 0,  # å¯¦éš›ä¸­æœƒå¾ç›£æ§ç³»çµ±ç²å–
                "average_prediction_latency": 5.0,
                "model_accuracy_avg": sum([m.metrics.accuracy for m in self.models.values() if m.metrics]) / max(len(self.models), 1)
            }
        }

class DataStorage:
    """æ•¸æ“šå­˜å„²"""
    
    def __init__(self):
        self.training_data: List[TrainingData] = []
    
    async def store_training_data(self, data: List[TrainingData]):
        """å­˜å„²è¨“ç·´æ•¸æ“š"""
        self.training_data.extend(data)
        # ä¿ç•™æœ€è¿‘30å¤©çš„æ•¸æ“š
        cutoff_time = datetime.now() - timedelta(days=30)
        self.training_data = [d for d in self.training_data if d.timestamp > cutoff_time]
    
    async def get_recent_training_data(self, days: int = 7) -> List[TrainingData]:
        """ç²å–æœ€è¿‘çš„è¨“ç·´æ•¸æ“š"""
        cutoff_time = datetime.now() - timedelta(days=days)
        return [d for d in self.training_data if d.timestamp > cutoff_time]
    
    async def get_data_count(self) -> int:
        """ç²å–æ•¸æ“šç¸½é‡"""
        return len(self.training_data)

class ModelRegistry:
    """æ¨¡å‹è¨»å†Šè¡¨"""
    
    def __init__(self):
        self.registered_models: Dict[str, ModelConfig] = {}
    
    async def register_model(self, config: ModelConfig):
        """è¨»å†Šæ¨¡å‹"""
        self.registered_models[config.model_id] = config
        logger.info(f"æ¨¡å‹å·²è¨»å†Š: {config.name}")
    
    async def get_model_config(self, model_id: str) -> Optional[ModelConfig]:
        """ç²å–æ¨¡å‹é…ç½®"""
        return self.registered_models.get(model_id)

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """æ©Ÿå™¨å­¸ç¿’æ•´åˆç³»çµ±ç¤ºä¾‹"""
    
    # å‰µå»ºé…ç½®
    config = {
        "ml_system": {
            "training_schedule": "daily",
            "model_validation_split": 0.2,
            "model_performance_threshold": 0.85
        },
        "data_collection": {
            "batch_size": 1000,
            "collection_interval_minutes": 60
        }
    }
    
    config_path = "/tmp/ml_integration_config.yaml"
    with open(config_path, "w", encoding="utf-8") as f:
        yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
    
    # åˆå§‹åŒ– ML ç³»çµ±
    ml_system = MLIntegrationSystem(config_path)
    
    try:
        print("ğŸ¤– é–‹å§‹ Phase 4 æ©Ÿå™¨å­¸ç¿’æ•´åˆç³»çµ±ç¤ºä¾‹...")
        
        # å‰µå»º Handover æ™‚æ©Ÿé æ¸¬æ¨¡å‹
        handover_config = ModelConfig(
            model_id="handover_timing_v1",
            name="Handover Timing Predictor",
            type=ModelType.XGBOOST,
            task=PredictionTask.HANDOVER_TIMING,
            features=[
                FeatureConfig(name="signal_strength", type="numerical"),
                FeatureConfig(name="snr", type="numerical"),
                FeatureConfig(name="elevation_angle", type="numerical"),
                FeatureConfig(name="interference_level", type="numerical"),
                FeatureConfig(name="load_factor", type="numerical")
            ],
            target_variable="target_handover",
            hyperparameters={
                "n_estimators": 100,
                "max_depth": 6,
                "learning_rate": 0.1
            },
            training_config={},
            validation_config={},
            deployment_config={}
        )
        
        model_id = await ml_system.create_model(handover_config)
        print(f"âœ… å‰µå»ºæ¨¡å‹: {model_id}")
        
        # ç”Ÿæˆæ¨¡æ“¬è¨“ç·´æ•¸æ“š
        training_data = []
        for i in range(5000):
            data = TrainingData(
                timestamp=datetime.now() - timedelta(minutes=i),
                satellite_id=f"sat_{np.random.randint(1, 10)}",
                ue_id=f"ue_{np.random.randint(1000, 9999)}",
                position_lat=np.random.uniform(-90, 90),
                position_lon=np.random.uniform(-180, 180),
                elevation_angle=np.random.uniform(10, 90),
                azimuth_angle=np.random.uniform(0, 360),
                signal_strength=np.random.uniform(-120, -70),
                snr=np.random.uniform(0, 30),
                interference_level=np.random.uniform(0, 20),
                handover_occurred=np.random.random() > 0.8,
                handover_success=np.random.random() > 0.05,
                handover_latency_ms=np.random.uniform(20, 80),
                beam_id=f"beam_{np.random.randint(1, 64)}",
                load_factor=np.random.uniform(0, 1),
                weather_condition=np.random.choice(['clear', 'cloudy', 'rainy', 'snowy']),
                time_of_day=np.random.randint(0, 24),
                doppler_shift=np.random.uniform(-1000, 1000),
                prediction_horizon_ms=np.random.randint(100, 2000)
            )
            training_data.append(data)
        
        # è¨“ç·´æ¨¡å‹
        print("ğŸš€ é–‹å§‹è¨“ç·´æ¨¡å‹...")
        success = await ml_system.train_model(model_id, training_data)
        
        if success:
            print("âœ… æ¨¡å‹è¨“ç·´æˆåŠŸ")
            
            # æ¸¬è©¦é æ¸¬
            test_features = {
                "signal_strength": -85.5,
                "snr": 15.2,
                "elevation_angle": 45.0,
                "interference_level": 5.3,
                "load_factor": 0.7,
                "hour_of_day": 14,
                "weather_clear": 1,
                "weather_cloudy": 0,
                "weather_rainy": 0,
                "weather_snowy": 0
            }
            
            result = await ml_system.predict(model_id, test_features)
            print(f"ğŸ“Š é æ¸¬çµæœ:")
            print(f"  é æ¸¬å€¼: {result.prediction}")
            print(f"  ç½®ä¿¡åº¦: {result.confidence:.3f}")
            print(f"  è™•ç†æ™‚é–“: {result.processing_time_ms:.1f}ms")
            
            # é¡¯ç¤ºç³»çµ±ç‹€æ…‹
            status = ml_system.get_system_status()
            print(f"\nğŸ” ç³»çµ±ç‹€æ…‹:")
            print(f"  é‹è¡Œä¸­: {status['is_running']}")
            print(f"  æ¨¡å‹ç¸½æ•¸: {status['models_count']}")
            print(f"  å·²è¨“ç·´æ¨¡å‹: {status['trained_models']}")
            print(f"  å¹³å‡æº–ç¢ºç‡: {status['system_metrics']['model_accuracy_avg']:.3f}")
        
        print("\n" + "="*60)
        print("ğŸ‰ PHASE 4 æ©Ÿå™¨å­¸ç¿’æ•´åˆç³»çµ±é‹è¡ŒæˆåŠŸï¼")
        print("="*60)
        print("âœ… å¯¦ç¾äº†åŸºæ–¼ç”Ÿç”¢æ•¸æ“šçš„æ¨¡å‹è¨“ç·´")
        print("âœ… æä¾›é«˜æ€§èƒ½çš„å¯¦æ™‚é æ¸¬æœå‹™")
        print("âœ… æ”¯æŒå¤šç¨® ML ç®—æ³•å’Œé æ¸¬ä»»å‹™")
        print("âœ… è‡ªå‹•åŒ–çš„æ¨¡å‹ç›£æ§å’Œé‡æ–°è¨“ç·´")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ ML ç³»çµ±åŸ·è¡Œå¤±æ•—: {e}")

if __name__ == "__main__":
    asyncio.run(main())