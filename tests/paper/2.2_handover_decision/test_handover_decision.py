#!/usr/bin/env python3
"""
éšæ®µäºŒ 2.2 æ›æ‰‹æ±ºç­–æœå‹™æ¸¬è©¦ç¨‹å¼

æ¸¬è©¦æ›æ‰‹æ±ºç­–æœå‹™çš„æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ›æ‰‹è§¸ç™¼æ¢ä»¶åˆ¤æ–·
2. å¤šè¡›æ˜Ÿæ›æ‰‹ç­–ç•¥
3. æ›æ‰‹æˆæœ¬ä¼°ç®—

åŸ·è¡Œæ–¹å¼:
cd /home/sat/ntn-stack
source venv/bin/activate
python paper/2.2_handover_decision/test_handover_decision.py
"""

import sys
import asyncio
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any
import statistics

# æ·»åŠ  SimWorld è·¯å¾‘
sys.path.insert(0, "/home/sat/ntn-stack/simworld/backend")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HandoverDecisionTester:
    """æ›æ‰‹æ±ºç­–æœå‹™æ¸¬è©¦å™¨"""

    def __init__(self):
        self.test_results = []
        self.performance_metrics = {}
        self.service = None

    async def setup_test_environment(self) -> bool:
        """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
        try:
            from app.domains.handover.services.handover_decision_service import (
                HandoverDecisionService,
                HandoverTrigger,
                HandoverStrategy,
                HandoverDecision,
                HandoverCandidate,
            )
            from app.domains.coordinates.models.coordinate_model import GeoCoordinate

            # åˆå§‹åŒ–æœå‹™
            self.service = HandoverDecisionService()

            # ä¿å­˜é¡åˆ¥å¼•ç”¨ä¾›å¾ŒçºŒä½¿ç”¨
            self.HandoverTrigger = HandoverTrigger
            self.HandoverStrategy = HandoverStrategy
            self.HandoverDecision = HandoverDecision
            self.HandoverCandidate = HandoverCandidate
            self.GeoCoordinate = GeoCoordinate

            print("âœ… æ›æ‰‹æ±ºç­–æœå‹™åˆå§‹åŒ–æˆåŠŸ")
            return True

        except Exception as e:
            print(f"âŒ æ¸¬è©¦ç’°å¢ƒè¨­ç½®å¤±æ•—: {e}")
            return False

    async def test_handover_trigger_evaluation(self) -> bool:
        """æ¸¬è©¦æ›æ‰‹è§¸ç™¼æ¢ä»¶åˆ¤æ–·"""
        print("\nğŸ”¬ æ¸¬è©¦æ›æ‰‹è§¸ç™¼æ¢ä»¶åˆ¤æ–·")
        print("-" * 50)

        try:
            # æ¸¬è©¦ç”¨ UE ä½ç½®å’Œè¡›æ˜Ÿ
            ue_position = self.GeoCoordinate(
                latitude=25.0330, longitude=121.5654, altitude_m=100.0
            )

            test_scenarios = [
                # å ´æ™¯1ï¼šæ­£å¸¸ä¿¡è™Ÿï¼Œæ‡‰è©²ç„¡è§¸ç™¼
                ("æ­£å¸¸ä¿¡è™Ÿå ´æ™¯", "sat_12345", datetime.utcnow()),
                # å ´æ™¯2ï¼šä¿¡è™Ÿè¼ƒå¼±ï¼Œå¯èƒ½è§¸ç™¼ä¿¡è™ŸåŠ£åŒ–
                ("å¼±ä¿¡è™Ÿå ´æ™¯", "sat_67890", datetime.utcnow()),
                # å ´æ™¯3ï¼šè² è¼‰æ¸¬è©¦
                ("è² è¼‰æ¸¬è©¦å ´æ™¯", "sat_11111", datetime.utcnow()),
            ]

            trigger_results = []
            start_time = time.time()

            for scenario_name, satellite_id, test_time in test_scenarios:
                try:
                    triggers = await self.service.evaluate_handover_triggers(
                        ue_id=f"test_ue_{scenario_name.replace(' ', '_')}",
                        current_satellite_id=satellite_id,
                        ue_position=ue_position,
                        current_time=test_time,
                    )

                    triggered_count = len([t for t in triggers if t.triggered])
                    trigger_results.append(
                        (scenario_name, triggered_count, len(triggers))
                    )

                    print(
                        f"  ğŸ“‹ {scenario_name}: {triggered_count}/{len(triggers)} è§¸ç™¼æ¢ä»¶"
                    )

                except Exception as e:
                    print(f"  âŒ {scenario_name} å¤±æ•—: {e}")
                    trigger_results.append((scenario_name, 0, 0))

            total_duration = (time.time() - start_time) * 1000

            # é©—è­‰çµæœ
            tests_passed = 0
            total_tests = 4

            # æ¸¬è©¦ 1: æ‰€æœ‰å ´æ™¯éƒ½æœ‰çµæœ
            if len(trigger_results) == len(test_scenarios):
                tests_passed += 1
                print("  âœ… è§¸ç™¼æ¢ä»¶è©•ä¼°å®Œæ•´æ€§")

            # æ¸¬è©¦ 2: å¹³å‡è©•ä¼°æ™‚é–“åˆç†
            avg_duration = total_duration / len(test_scenarios)
            if avg_duration < 1000:  # æ¯æ¬¡è©•ä¼° < 1ç§’
                tests_passed += 1
                print(f"  âœ… è©•ä¼°æ•ˆç‡è‰¯å¥½: {avg_duration:.1f}ms/å ´æ™¯")
            else:
                print(f"  âŒ è©•ä¼°æ•ˆç‡éä½: {avg_duration:.1f}ms/å ´æ™¯")

            # æ¸¬è©¦ 3: è§¸ç™¼æ¢ä»¶å¤šæ¨£æ€§
            unique_trigger_counts = set(result[1] for result in trigger_results)
            if len(unique_trigger_counts) >= 1:
                tests_passed += 1
                print(f"  âœ… è§¸ç™¼æ¢ä»¶å¤šæ¨£æ€§: {len(unique_trigger_counts)} ç¨®çµæœ")

            # æ¸¬è©¦ 4: æœå‹™å¯ç”¨æ€§
            service_status = await self.service.get_service_status()
            if service_status.get("status") == "active":
                tests_passed += 1
                print("  âœ… æœå‹™ç‹€æ…‹æ­£å¸¸")

            self.performance_metrics["trigger_evaluation"] = {
                "total_scenarios": len(test_scenarios),
                "avg_duration_ms": avg_duration,
                "trigger_results": trigger_results,
            }

            success_rate = tests_passed / total_tests
            print(
                f"\nğŸ“Š è§¸ç™¼æ¢ä»¶åˆ¤æ–·æ¸¬è©¦çµæœ: {tests_passed}/{total_tests} ({success_rate:.1%})"
            )

            self.test_results.append(("æ›æ‰‹è§¸ç™¼æ¢ä»¶åˆ¤æ–·", success_rate >= 0.75))
            return success_rate >= 0.75

        except Exception as e:
            print(f"âŒ è§¸ç™¼æ¢ä»¶åˆ¤æ–·æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results.append(("æ›æ‰‹è§¸ç™¼æ¢ä»¶åˆ¤æ–·", False))
            return False

    async def test_handover_decision_making(self) -> bool:
        """æ¸¬è©¦æ›æ‰‹æ±ºç­–åˆ¶å®š"""
        print("\nğŸ”¬ æ¸¬è©¦æ›æ‰‹æ±ºç­–åˆ¶å®š")
        print("-" * 50)

        try:
            # æ¸¬è©¦ç”¨ UE ä½ç½®
            ue_position = self.GeoCoordinate(
                latitude=24.1477, longitude=120.6736, altitude_m=200.0
            )

            decision_scenarios = [
                ("æ±ºç­–å ´æ™¯A", "sat_source_01", datetime.utcnow()),
                (
                    "æ±ºç­–å ´æ™¯B",
                    "sat_source_02",
                    datetime.utcnow() + timedelta(seconds=10),
                ),
                (
                    "æ±ºç­–å ´æ™¯C",
                    "sat_source_03",
                    datetime.utcnow() + timedelta(seconds=20),
                ),
            ]

            decision_results = []
            start_time = time.time()

            for scenario_name, source_satellite, test_time in decision_scenarios:
                try:
                    decision = await self.service.make_handover_decision(
                        ue_id=f"test_ue_{scenario_name.replace(' ', '_')}",
                        current_satellite_id=source_satellite,
                        ue_position=ue_position,
                        current_time=test_time,
                    )

                    if decision:
                        decision_results.append(
                            (scenario_name, True, decision.confidence_score)
                        )
                        print(
                            f"  ğŸ“‹ {scenario_name}: æ±ºç­–å®Œæˆï¼Œä¿¡å¿ƒåº¦ {decision.confidence_score:.2f}"
                        )
                    else:
                        decision_results.append((scenario_name, False, 0.0))
                        print(f"  ğŸ“‹ {scenario_name}: ç„¡éœ€æ›æ‰‹")

                except Exception as e:
                    print(f"  âŒ {scenario_name} æ±ºç­–å¤±æ•—: {e}")
                    decision_results.append((scenario_name, False, 0.0))

            total_duration = (time.time() - start_time) * 1000

            # é©—è­‰çµæœ
            tests_passed = 0
            total_tests = 4

            # æ¸¬è©¦ 1: æ±ºç­–æµç¨‹å®Œæ•´æ€§
            if len(decision_results) == len(decision_scenarios):
                tests_passed += 1
                print("  âœ… æ±ºç­–æµç¨‹å®Œæ•´æ€§")

            # æ¸¬è©¦ 2: æ±ºç­–æ•ˆç‡
            avg_decision_time = total_duration / len(decision_scenarios)
            if avg_decision_time < 2000:  # æ¯æ¬¡æ±ºç­– < 2ç§’
                tests_passed += 1
                print(f"  âœ… æ±ºç­–æ•ˆç‡è‰¯å¥½: {avg_decision_time:.1f}ms/æ±ºç­–")
            else:
                print(f"  âŒ æ±ºç­–æ•ˆç‡éä½: {avg_decision_time:.1f}ms/æ±ºç­–")

            # æ¸¬è©¦ 3: æ±ºç­–è³ªé‡
            decisions_made = [r for r in decision_results if r[1]]
            if len(decisions_made) >= 1:
                avg_confidence = statistics.mean([r[2] for r in decisions_made])
                if avg_confidence > 0.6:
                    tests_passed += 1
                    print(f"  âœ… æ±ºç­–ä¿¡å¿ƒåº¦è‰¯å¥½: {avg_confidence:.2f}")
                else:
                    print(f"  âŒ æ±ºç­–ä¿¡å¿ƒåº¦éä½: {avg_confidence:.2f}")
            else:
                tests_passed += 1  # ç„¡æ±ºç­–ä¹Ÿæ˜¯æ­£å¸¸æƒ…æ³
                print("  âœ… æ±ºç­–ä¿å®ˆç­–ç•¥æ­£å¸¸")

            # æ¸¬è©¦ 4: çµ±è¨ˆè³‡è¨Šæœ‰æ•ˆæ€§
            stats = self.service.decision_stats
            if stats["total_decisions"] >= 0:
                tests_passed += 1
                print(f"  âœ… çµ±è¨ˆè³‡è¨Šæ­£å¸¸: {stats['total_decisions']} æ¬¡æ±ºç­–")

            self.performance_metrics["decision_making"] = {
                "total_scenarios": len(decision_scenarios),
                "avg_decision_time_ms": avg_decision_time,
                "decisions_made": len(decisions_made),
                "decision_results": decision_results,
            }

            success_rate = tests_passed / total_tests
            print(
                f"\nğŸ“Š æ›æ‰‹æ±ºç­–åˆ¶å®šæ¸¬è©¦çµæœ: {tests_passed}/{total_tests} ({success_rate:.1%})"
            )

            self.test_results.append(("æ›æ‰‹æ±ºç­–åˆ¶å®š", success_rate >= 0.75))
            return success_rate >= 0.75

        except Exception as e:
            print(f"âŒ æ›æ‰‹æ±ºç­–åˆ¶å®šæ¸¬è©¦å¤±æ•—: {e}")
            self.test_results.append(("æ›æ‰‹æ±ºç­–åˆ¶å®š", False))
            return False

    async def test_multi_satellite_handover(self) -> bool:
        """æ¸¬è©¦å¤šè¡›æ˜Ÿæ›æ‰‹ç­–ç•¥"""
        print("\nğŸ”¬ æ¸¬è©¦å¤šè¡›æ˜Ÿæ›æ‰‹ç­–ç•¥")
        print("-" * 50)

        try:
            # æ¸¬è©¦ç”¨ UE ä½ç½®
            ue_position = self.GeoCoordinate(
                latitude=22.6273, longitude=120.3014, altitude_m=50.0
            )

            multi_handover_scenarios = [
                (
                    "æˆæœ¬å„ªåŒ–",
                    "sat_current_01",
                    ["sat_target_01", "sat_target_02"],
                    "minimize_cost",
                ),
                (
                    "å»¶é²å„ªåŒ–",
                    "sat_current_02",
                    ["sat_target_03", "sat_target_04"],
                    "minimize_latency",
                ),
                (
                    "å¹³è¡¡å„ªåŒ–",
                    "sat_current_03",
                    ["sat_target_05", "sat_target_06"],
                    "balanced",
                ),
            ]

            optimization_results = []
            start_time = time.time()

            for (
                scenario_name,
                current_sat,
                target_sats,
                strategy,
            ) in multi_handover_scenarios:
                try:
                    multi_handover = (
                        await self.service.execute_multi_satellite_handover(
                            ue_id=f"test_ue_{scenario_name.replace(' ', '_')}",
                            current_satellite_id=current_sat,
                            target_satellites=target_sats,
                            ue_position=ue_position,
                            optimization_strategy=strategy,
                        )
                    )

                    sequence_length = len(multi_handover.handover_sequence)
                    total_cost = multi_handover.total_handover_cost

                    optimization_results.append(
                        (scenario_name, True, sequence_length, total_cost)
                    )
                    print(
                        f"  ğŸ“‹ {scenario_name}: {sequence_length} æ­¥é©Ÿï¼Œç¸½æˆæœ¬ {total_cost:.1f}"
                    )

                except Exception as e:
                    print(f"  âŒ {scenario_name} å„ªåŒ–å¤±æ•—: {e}")
                    optimization_results.append((scenario_name, False, 0, 0.0))

            total_duration = (time.time() - start_time) * 1000

            # é©—è­‰çµæœ
            tests_passed = 0
            total_tests = 4

            # æ¸¬è©¦ 1: å¤šè¡›æ˜Ÿå„ªåŒ–å®Œæ•´æ€§
            successful_optimizations = [r for r in optimization_results if r[1]]
            if len(successful_optimizations) >= 2:
                tests_passed += 1
                print(f"  âœ… å¤šè¡›æ˜Ÿå„ªåŒ–æˆåŠŸ: {len(successful_optimizations)}/3")
            else:
                print(f"  âŒ å¤šè¡›æ˜Ÿå„ªåŒ–æˆåŠŸç‡ä½: {len(successful_optimizations)}/3")

            # æ¸¬è©¦ 2: å„ªåŒ–æ•ˆç‡
            avg_optimization_time = total_duration / len(multi_handover_scenarios)
            if avg_optimization_time < 3000:  # æ¯æ¬¡å„ªåŒ– < 3ç§’
                tests_passed += 1
                print(f"  âœ… å„ªåŒ–æ•ˆç‡è‰¯å¥½: {avg_optimization_time:.1f}ms/å„ªåŒ–")
            else:
                print(f"  âŒ å„ªåŒ–æ•ˆç‡éä½: {avg_optimization_time:.1f}ms/å„ªåŒ–")

            # æ¸¬è©¦ 3: æ›æ‰‹åºåˆ—åˆç†æ€§
            sequence_lengths = [r[2] for r in successful_optimizations if r[2] > 0]
            if sequence_lengths and max(sequence_lengths) <= 5:
                tests_passed += 1
                print(f"  âœ… æ›æ‰‹åºåˆ—é•·åº¦åˆç†: æœ€å¤§ {max(sequence_lengths)} æ­¥é©Ÿ")
            else:
                print(
                    f"  âŒ æ›æ‰‹åºåˆ—éé•·: æœ€å¤§ {max(sequence_lengths) if sequence_lengths else 0} æ­¥é©Ÿ"
                )

            # æ¸¬è©¦ 4: æˆæœ¬ä¼°ç®—æœ‰æ•ˆæ€§
            total_costs = [r[3] for r in successful_optimizations if r[3] > 0]
            if total_costs and all(0 < cost < 1000 for cost in total_costs):
                tests_passed += 1
                print(
                    f"  âœ… æˆæœ¬ä¼°ç®—åˆç†: ç¯„åœ {min(total_costs):.1f}-{max(total_costs):.1f}"
                )
            else:
                print(f"  âŒ æˆæœ¬ä¼°ç®—ç•°å¸¸")

            self.performance_metrics["multi_satellite_handover"] = {
                "total_scenarios": len(multi_handover_scenarios),
                "avg_optimization_time_ms": avg_optimization_time,
                "successful_optimizations": len(successful_optimizations),
                "optimization_results": optimization_results,
            }

            success_rate = tests_passed / total_tests
            print(
                f"\nğŸ“Š å¤šè¡›æ˜Ÿæ›æ‰‹ç­–ç•¥æ¸¬è©¦çµæœ: {tests_passed}/{total_tests} ({success_rate:.1%})"
            )

            self.test_results.append(("å¤šè¡›æ˜Ÿæ›æ‰‹ç­–ç•¥", success_rate >= 0.75))
            return success_rate >= 0.75

        except Exception as e:
            print(f"âŒ å¤šè¡›æ˜Ÿæ›æ‰‹ç­–ç•¥æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results.append(("å¤šè¡›æ˜Ÿæ›æ‰‹ç­–ç•¥", False))
            return False

    async def test_handover_cost_estimation(self) -> bool:
        """æ¸¬è©¦æ›æ‰‹æˆæœ¬ä¼°ç®—"""
        print("\nğŸ”¬ æ¸¬è©¦æ›æ‰‹æˆæœ¬ä¼°ç®—")
        print("-" * 50)

        try:
            # æ¸¬è©¦ç”¨æ›æ‰‹å ´æ™¯
            cost_scenarios = [
                ("æ­£å¸¸æ›æ‰‹", "sat_source_01", "sat_target_01", []),
                ("ç·Šæ€¥æ›æ‰‹", "sat_source_02", "sat_target_02", ["emergency"]),
                ("é æ¸¬æ›æ‰‹", "sat_source_03", "sat_target_03", ["predicted"]),
                ("è² è¼‰å‡è¡¡", "sat_source_04", "sat_target_04", ["load_balance"]),
            ]

            cost_results = []
            start_time = time.time()

            for scenario_name, source_sat, target_sat, trigger_types in cost_scenarios:
                try:
                    # æ¨¡æ“¬è§¸ç™¼æ¢ä»¶
                    triggers = []
                    for trigger_type in trigger_types:
                        if trigger_type == "emergency":
                            trigger = type(
                                "MockTrigger",
                                (),
                                {
                                    "trigger_type": self.HandoverTrigger.EMERGENCY_HANDOVER,
                                    "triggered": True,
                                },
                            )()
                        elif trigger_type == "predicted":
                            trigger = type(
                                "MockTrigger",
                                (),
                                {
                                    "trigger_type": self.HandoverTrigger.PREDICTED_OUTAGE,
                                    "triggered": True,
                                },
                            )()
                        elif trigger_type == "load_balance":
                            trigger = type(
                                "MockTrigger",
                                (),
                                {
                                    "trigger_type": self.HandoverTrigger.LOAD_BALANCING,
                                    "triggered": True,
                                },
                            )()
                        triggers.append(trigger)

                    # ä¼°ç®—æ›æ‰‹æˆæœ¬
                    cost = await self.service._estimate_handover_cost(
                        source_satellite_id=source_sat,
                        target_satellite_id=target_sat,
                        triggers=triggers,
                    )

                    cost_results.append((scenario_name, True, cost))
                    print(f"  ğŸ“‹ {scenario_name}: æˆæœ¬ {cost:.1f}")

                except Exception as e:
                    print(f"  âŒ {scenario_name} æˆæœ¬ä¼°ç®—å¤±æ•—: {e}")
                    cost_results.append((scenario_name, False, 0.0))

            total_duration = (time.time() - start_time) * 1000

            # é©—è­‰çµæœ
            tests_passed = 0
            total_tests = 4

            # æ¸¬è©¦ 1: æˆæœ¬ä¼°ç®—å®Œæ•´æ€§
            successful_estimations = [r for r in cost_results if r[1]]
            if len(successful_estimations) >= 3:
                tests_passed += 1
                print(f"  âœ… æˆæœ¬ä¼°ç®—å®Œæ•´æ€§: {len(successful_estimations)}/4")

            # æ¸¬è©¦ 2: ä¼°ç®—æ•ˆç‡
            avg_estimation_time = total_duration / len(cost_scenarios)
            if avg_estimation_time < 500:  # æ¯æ¬¡ä¼°ç®— < 500ms
                tests_passed += 1
                print(f"  âœ… ä¼°ç®—æ•ˆç‡å„ªç§€: {avg_estimation_time:.1f}ms/ä¼°ç®—")
            else:
                print(f"  âŒ ä¼°ç®—æ•ˆç‡éä½: {avg_estimation_time:.1f}ms/ä¼°ç®—")

            # æ¸¬è©¦ 3: æˆæœ¬å€¼åˆç†æ€§
            costs = [r[2] for r in successful_estimations if r[2] > 0]
            if costs and all(10 <= cost <= 200 for cost in costs):
                tests_passed += 1
                print(f"  âœ… æˆæœ¬å€¼åˆç†: ç¯„åœ {min(costs):.1f}-{max(costs):.1f}")
            else:
                print(f"  âŒ æˆæœ¬å€¼ç•°å¸¸")

            # æ¸¬è©¦ 4: æˆæœ¬å·®ç•°åŒ–
            if costs and (max(costs) - min(costs)) > 5:
                tests_passed += 1
                print(f"  âœ… æˆæœ¬å·®ç•°åŒ–æ­£å¸¸: å·®ç•° {max(costs) - min(costs):.1f}")
            else:
                print(f"  âŒ æˆæœ¬å·®ç•°åŒ–ä¸è¶³")

            self.performance_metrics["cost_estimation"] = {
                "total_scenarios": len(cost_scenarios),
                "avg_estimation_time_ms": avg_estimation_time,
                "successful_estimations": len(successful_estimations),
                "cost_range": [min(costs), max(costs)] if costs else [0, 0],
            }

            success_rate = tests_passed / total_tests
            print(
                f"\nğŸ“Š æ›æ‰‹æˆæœ¬ä¼°ç®—æ¸¬è©¦çµæœ: {tests_passed}/{total_tests} ({success_rate:.1%})"
            )

            self.test_results.append(("æ›æ‰‹æˆæœ¬ä¼°ç®—", success_rate >= 0.75))
            return success_rate >= 0.75

        except Exception as e:
            print(f"âŒ æ›æ‰‹æˆæœ¬ä¼°ç®—æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results.append(("æ›æ‰‹æˆæœ¬ä¼°ç®—", False))
            return False

    def generate_test_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“Š éšæ®µäºŒ 2.2 æ›æ‰‹æ±ºç­–æœå‹™æ¸¬è©¦å ±å‘Š")
        print("=" * 70)

        # ç¸½é«”çµæœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for _, result in self.test_results if result)
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        print(f"\nğŸ“‹ æ¸¬è©¦çµæœæ¦‚è¦½:")
        for test_name, result in self.test_results:
            status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
            print(f"   {status} {test_name}")

        print(f"\nğŸ“Š ç¸½é«”çµ±è¨ˆ:")
        print(f"   ç¸½æ¸¬è©¦æ•¸: {total_tests}")
        print(f"   é€šéæ¸¬è©¦: {passed_tests}")
        print(f"   å¤±æ•—æ¸¬è©¦: {total_tests - passed_tests}")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}%")

        # æ€§èƒ½æŒ‡æ¨™ç¸½çµ
        if self.performance_metrics:
            print(f"\nâš¡ æ€§èƒ½æŒ‡æ¨™ç¸½çµ:")

            if "trigger_evaluation" in self.performance_metrics:
                trigger_metrics = self.performance_metrics["trigger_evaluation"]
                print(f"   è§¸ç™¼æ¢ä»¶åˆ¤æ–·:")
                print(
                    f"     - å¹³å‡è©•ä¼°æ™‚é–“: {trigger_metrics['avg_duration_ms']:.1f}ms"
                )
                print(f"     - æ¸¬è©¦å ´æ™¯æ•¸: {trigger_metrics['total_scenarios']}")

            if "decision_making" in self.performance_metrics:
                decision_metrics = self.performance_metrics["decision_making"]
                print(f"   æ›æ‰‹æ±ºç­–åˆ¶å®š:")
                print(
                    f"     - å¹³å‡æ±ºç­–æ™‚é–“: {decision_metrics['avg_decision_time_ms']:.1f}ms"
                )
                print(f"     - æˆåŠŸæ±ºç­–æ•¸: {decision_metrics['decisions_made']}")

            if "multi_satellite_handover" in self.performance_metrics:
                multi_metrics = self.performance_metrics["multi_satellite_handover"]
                print(f"   å¤šè¡›æ˜Ÿæ›æ‰‹:")
                print(
                    f"     - å¹³å‡å„ªåŒ–æ™‚é–“: {multi_metrics['avg_optimization_time_ms']:.1f}ms"
                )
                print(f"     - æˆåŠŸå„ªåŒ–æ•¸: {multi_metrics['successful_optimizations']}")

            if "cost_estimation" in self.performance_metrics:
                cost_metrics = self.performance_metrics["cost_estimation"]
                print(f"   æˆæœ¬ä¼°ç®—:")
                print(
                    f"     - å¹³å‡ä¼°ç®—æ™‚é–“: {cost_metrics['avg_estimation_time_ms']:.1f}ms"
                )
                print(
                    f"     - æˆæœ¬ç¯„åœ: {cost_metrics['cost_range'][0]:.1f}-{cost_metrics['cost_range'][1]:.1f}"
                )

        # éšæ®µäºŒ 2.2 å®Œæˆåº¦è©•ä¼°
        print(f"\nğŸ¯ éšæ®µäºŒ 2.2 å®Œæˆåº¦è©•ä¼°:")

        feature_completion = {
            "æ›æ‰‹è§¸ç™¼æ¢ä»¶åˆ¤æ–·": any(
                name == "æ›æ‰‹è§¸ç™¼æ¢ä»¶åˆ¤æ–·" and result
                for name, result in self.test_results
            ),
            "æ›æ‰‹æ±ºç­–åˆ¶å®š": any(
                name == "æ›æ‰‹æ±ºç­–åˆ¶å®š" and result for name, result in self.test_results
            ),
            "å¤šè¡›æ˜Ÿæ›æ‰‹ç­–ç•¥": any(
                name == "å¤šè¡›æ˜Ÿæ›æ‰‹ç­–ç•¥" and result
                for name, result in self.test_results
            ),
            "æ›æ‰‹æˆæœ¬ä¼°ç®—": any(
                name == "æ›æ‰‹æˆæœ¬ä¼°ç®—" and result for name, result in self.test_results
            ),
        }

        completed_features = sum(feature_completion.values())
        total_features = len(feature_completion)

        for feature, completed in feature_completion.items():
            status = "âœ… å®Œæˆ" if completed else "âŒ æœªå®Œæˆ"
            print(f"   {status} {feature}")

        completion_rate = (
            (completed_features / total_features * 100) if total_features > 0 else 0
        )
        print(
            f"\n   éšæ®µå®Œæˆåº¦: {completed_features}/{total_features} ({completion_rate:.1f}%)"
        )

        if success_rate >= 90.0:
            print(f"\nğŸ‰ éšæ®µäºŒ 2.2 æ›æ‰‹æ±ºç­–æœå‹™å¯¦ä½œæˆåŠŸï¼")
            print(f"âœ¨ æ™ºèƒ½æ›æ‰‹æ±ºç­–åŠŸèƒ½å·²å®Œæˆ")
        elif success_rate >= 75.0:
            print(f"\nâš ï¸  éšæ®µäºŒ 2.2 åŸºæœ¬å®Œæˆï¼Œå»ºè­°å„ªåŒ–å¤±æ•—é …ç›®")
        else:
            print(f"\nâŒ éšæ®µäºŒ 2.2 å¯¦ä½œéœ€è¦æ”¹é€²")

        return success_rate >= 75.0


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹åŸ·è¡Œéšæ®µäºŒ 2.2 æ›æ‰‹æ±ºç­–æœå‹™æ¸¬è©¦")

    tester = HandoverDecisionTester()

    # è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
    if not await tester.setup_test_environment():
        print("âŒ æ¸¬è©¦ç’°å¢ƒè¨­ç½®å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ")
        return False

    # åŸ·è¡Œæ¸¬è©¦
    test_functions = [
        tester.test_handover_trigger_evaluation,
        tester.test_handover_decision_making,
        tester.test_multi_satellite_handover,
        tester.test_handover_cost_estimation,
    ]

    for test_func in test_functions:
        try:
            await test_func()
            await asyncio.sleep(0.5)  # çŸ­æš«ä¼‘æ¯
        except Exception as e:
            print(f"âŒ æ¸¬è©¦åŸ·è¡Œç•°å¸¸: {e}")

    # ç”Ÿæˆå ±å‘Š
    success = tester.generate_test_report()

    return success


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        exit_code = 0 if success else 1
        print(f"\næ¸¬è©¦å®Œæˆï¼Œé€€å‡ºç¢¼: {exit_code}")
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        exit_code = 130
    except Exception as e:
        print(f"\nğŸ’¥ æ¸¬è©¦åŸ·è¡ŒéŒ¯èª¤: {e}")
        exit_code = 1

    sys.exit(exit_code)
