#!/usr/bin/env python3
"""
æ€§èƒ½æ¸¬è©¦æ¨¡çµ„
è² è²¬æ¸¬è©¦ç³»çµ±çš„é—œéµæ€§èƒ½æŒ‡æ¨™ï¼Œç‰¹åˆ¥æ˜¯å¯¦é©—å®¤é©—æ¸¬è¦æ±‚çš„é‡åŒ–æŒ‡æ¨™

é—œéµæ¸¬è©¦æŒ‡æ¨™ï¼š
- ç«¯åˆ°ç«¯å»¶é² < 50ms é©—è­‰
- é€£æ¥ä¸­æ–·å¾Œ 2s å…§é‡å»ºé€£ç·šé©—è­‰
- API éŸ¿æ‡‰æ™‚é–“
- ååé‡æ¸¬è©¦
- è³‡æºåˆ©ç”¨ç‡
"""

import asyncio
import logging
import statistics
import time
from datetime import datetime
from typing import Dict, List, Tuple, Any
import aiohttp
import psutil
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""

    metric_name: str
    value: float
    unit: str
    target_value: float
    threshold_type: str  # "less_than", "greater_than", "equal"
    passed: bool
    timestamp: str
    additional_data: Dict[str, Any] = None


@dataclass
class LatencyTestResult:
    """å»¶é²æ¸¬è©¦çµæœ"""

    test_name: str
    min_latency_ms: float
    max_latency_ms: float
    avg_latency_ms: float
    median_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    success_rate: float
    sample_count: int
    target_met: bool


class PerformanceTester:
    """æ€§èƒ½æ¸¬è©¦å™¨"""

    def __init__(self, config: Dict):
        self.config = config
        self.environment = config["environment"]
        self.services = self.environment["services"]
        self.benchmarks = config["performance_benchmarks"]
        self.results: List[PerformanceMetric] = []
        self.latency_results: List[LatencyTestResult] = []

    async def run_performance_tests(self) -> Tuple[bool, Dict]:
        """åŸ·è¡Œæ€§èƒ½æ¸¬è©¦å¥—ä»¶"""
        logger.info("âš¡ é–‹å§‹åŸ·è¡Œæ€§èƒ½æ¸¬è©¦")

        test_methods = [
            self._test_api_response_times,
            self._test_end_to_end_latency,
            self._test_connection_recovery_time,
            self._test_throughput_capabilities,
            self._test_concurrent_request_handling,
            self._test_resource_utilization,
        ]

        all_passed = True
        details = {
            "tests_executed": len(test_methods),
            "tests_passed": 0,
            "tests_failed": 0,
            "performance_metrics": [],
            "latency_analysis": [],
            "summary": {},
        }

        for test_method in test_methods:
            try:
                test_passed = await test_method()
                if test_passed:
                    details["tests_passed"] += 1
                else:
                    details["tests_failed"] += 1
                    all_passed = False

            except Exception as e:
                logger.error(f"æ€§èƒ½æ¸¬è©¦ç•°å¸¸: {test_method.__name__} - {e}")
                details["tests_failed"] += 1
                all_passed = False

        # æ•´ç†çµæœ
        details["performance_metrics"] = [
            {
                "metric": m.metric_name,
                "value": m.value,
                "unit": m.unit,
                "target": m.target_value,
                "passed": m.passed,
                "timestamp": m.timestamp,
            }
            for m in self.results
        ]

        details["latency_analysis"] = [
            {
                "test": l.test_name,
                "avg_ms": l.avg_latency_ms,
                "p95_ms": l.p95_latency_ms,
                "p99_ms": l.p99_latency_ms,
                "success_rate": l.success_rate,
                "target_met": l.target_met,
            }
            for l in self.latency_results
        ]

        # è¨ˆç®—é—œéµæŒ‡æ¨™é”æˆæƒ…æ³
        critical_metrics = ["e2e_latency", "recovery_time", "api_response_time"]
        critical_passed = all(
            any(m.metric_name == metric and m.passed for m in self.results)
            for metric in critical_metrics
        )

        details["summary"] = {
            "overall_success": all_passed and critical_passed,
            "success_rate": details["tests_passed"] / details["tests_executed"],
            "critical_metrics_passed": critical_passed,
            "total_metrics_collected": len(self.results),
            "passed_metrics": sum(1 for m in self.results if m.passed),
        }

        logger.info(
            f"âš¡ æ€§èƒ½æ¸¬è©¦å®Œæˆï¼ŒæˆåŠŸç‡: {details['summary']['success_rate']:.1%}"
        )
        return all_passed and critical_passed, details

    async def _test_api_response_times(self) -> bool:
        """æ¸¬è©¦ API éŸ¿æ‡‰æ™‚é–“"""
        logger.info("ğŸ• æ¸¬è©¦ API éŸ¿æ‡‰æ™‚é–“")

        test_endpoints = [
            {"service": "netstack", "endpoint": "/health"},
            {"service": "netstack", "endpoint": "/api/v1/uav"},
            {"service": "simworld", "endpoint": "/api/v1/wireless/health"},
            {"service": "simworld", "endpoint": "/api/v1/uav/positions"},
        ]

        target_ms = self.benchmarks["latency"]["api_response_target_ms"]
        warning_ms = self.benchmarks["latency"]["api_response_warning_ms"]

        all_passed = True

        async with aiohttp.ClientSession() as session:
            for endpoint_config in test_endpoints:
                service_name = endpoint_config["service"]
                endpoint = endpoint_config["endpoint"]
                base_url = self.services[service_name]["url"]
                url = f"{base_url}{endpoint}"

                # åŸ·è¡Œå¤šæ¬¡æ¸¬è©¦ä»¥ç²å¾—çµ±è¨ˆæ•¸æ“š
                latencies = []
                success_count = 0

                for i in range(10):  # æ¸¬è©¦ 10 æ¬¡
                    start_time = time.time()
                    try:
                        async with session.get(url, timeout=10) as response:
                            latency = (time.time() - start_time) * 1000
                            latencies.append(latency)

                            if response.status < 500:
                                success_count += 1

                    except Exception as e:
                        logger.warning(f"API è«‹æ±‚å¤±æ•—: {url} - {e}")
                        latencies.append(float("inf"))

                if latencies:
                    # éæ¿¾ç„¡æ•ˆå€¼
                    valid_latencies = [l for l in latencies if l != float("inf")]

                    if valid_latencies:
                        avg_latency = statistics.mean(valid_latencies)
                        p95_latency = (
                            statistics.quantiles(valid_latencies, n=20)[18]
                            if len(valid_latencies) >= 5
                            else max(valid_latencies)
                        )

                        # è¨˜éŒ„è©³ç´°çš„å»¶é²æ¸¬è©¦çµæœ
                        test_name = (
                            f"api_response_{service_name}_{endpoint.replace('/', '_')}"
                        )
                        self.latency_results.append(
                            LatencyTestResult(
                                test_name=test_name,
                                min_latency_ms=min(valid_latencies),
                                max_latency_ms=max(valid_latencies),
                                avg_latency_ms=avg_latency,
                                median_latency_ms=statistics.median(valid_latencies),
                                p95_latency_ms=p95_latency,
                                p99_latency_ms=max(
                                    valid_latencies
                                ),  # æ¨£æœ¬é‡å°ï¼Œç”¨æœ€å¤§å€¼ä»£æ›¿
                                success_rate=success_count / len(latencies),
                                sample_count=len(valid_latencies),
                                target_met=avg_latency <= target_ms,
                            )
                        )

                        # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
                        metric_passed = avg_latency <= target_ms
                        if not metric_passed:
                            all_passed = False

                        self.results.append(
                            PerformanceMetric(
                                metric_name=f"api_response_time_{service_name}",
                                value=avg_latency,
                                unit="ms",
                                target_value=target_ms,
                                threshold_type="less_than",
                                passed=metric_passed,
                                timestamp=datetime.utcnow().isoformat(),
                                additional_data={
                                    "endpoint": endpoint,
                                    "p95_latency": p95_latency,
                                    "success_rate": success_count / len(latencies),
                                },
                            )
                        )

                        if metric_passed:
                            logger.info(
                                f"âœ… {service_name}{endpoint} éŸ¿æ‡‰æ™‚é–“: {avg_latency:.1f}ms (ç›®æ¨™: {target_ms}ms)"
                            )
                        else:
                            logger.error(
                                f"âŒ {service_name}{endpoint} éŸ¿æ‡‰æ™‚é–“: {avg_latency:.1f}ms è¶…éç›®æ¨™ {target_ms}ms"
                            )

        return all_passed

    async def _test_end_to_end_latency(self) -> bool:
        """æ¸¬è©¦ç«¯åˆ°ç«¯å»¶é² - å¯¦é©—å®¤é©—æ¸¬çš„é—œéµæŒ‡æ¨™"""
        logger.info("ğŸ¯ æ¸¬è©¦ç«¯åˆ°ç«¯å»¶é² (< 50ms ç›®æ¨™)")

        target_ms = self.benchmarks["latency"]["e2e_target_ms"]  # 50ms
        warning_ms = self.benchmarks["latency"]["e2e_warning_ms"]  # 40ms

        # æ¨¡æ“¬ç«¯åˆ°ç«¯é€šä¿¡å ´æ™¯ï¼šSimWorld -> NetStack -> éŸ¿æ‡‰
        e2e_latencies = []
        success_count = 0

        async with aiohttp.ClientSession() as session:
            for i in range(20):  # é€²è¡Œ 20 æ¬¡ç«¯åˆ°ç«¯æ¸¬è©¦
                start_time = time.time()

                try:
                    # éšæ®µ 1: å¾ SimWorld ç²å– UAV ä½ç½®
                    simworld_url = (
                        f"{self.services['simworld']['url']}/api/v1/uav/positions"
                    )
                    async with session.get(simworld_url, timeout=5) as sim_response:
                        if sim_response.status == 200:
                            uav_data = await sim_response.json()

                            # éšæ®µ 2: å°‡æ•¸æ“šç™¼é€åˆ° NetStack é€²è¡Œè™•ç†
                            netstack_url = (
                                f"{self.services['netstack']['url']}/api/v1/uav"
                            )
                            async with session.get(
                                netstack_url, timeout=5
                            ) as net_response:
                                if net_response.status == 200:
                                    e2e_latency = (time.time() - start_time) * 1000
                                    e2e_latencies.append(e2e_latency)
                                    success_count += 1

                                    logger.debug(
                                        f"E2E å»¶é²æ¸¬è©¦ {i+1}: {e2e_latency:.1f}ms"
                                    )

                except Exception as e:
                    logger.warning(f"E2E å»¶é²æ¸¬è©¦ {i+1} å¤±æ•—: {e}")

                # æ¸¬è©¦é–“éš”
                await asyncio.sleep(0.1)

        if e2e_latencies:
            avg_latency = statistics.mean(e2e_latencies)
            p95_latency = (
                statistics.quantiles(e2e_latencies, n=20)[18]
                if len(e2e_latencies) >= 5
                else max(e2e_latencies)
            )
            p99_latency = (
                statistics.quantiles(e2e_latencies, n=100)[98]
                if len(e2e_latencies) >= 10
                else max(e2e_latencies)
            )

            # è¨˜éŒ„è©³ç´°çµæœ
            self.latency_results.append(
                LatencyTestResult(
                    test_name="end_to_end_latency",
                    min_latency_ms=min(e2e_latencies),
                    max_latency_ms=max(e2e_latencies),
                    avg_latency_ms=avg_latency,
                    median_latency_ms=statistics.median(e2e_latencies),
                    p95_latency_ms=p95_latency,
                    p99_latency_ms=p99_latency,
                    success_rate=success_count / 20,
                    sample_count=len(e2e_latencies),
                    target_met=avg_latency <= target_ms,
                )
            )

            # è¨˜éŒ„é—œéµæ€§èƒ½æŒ‡æ¨™
            target_met = avg_latency <= target_ms
            self.results.append(
                PerformanceMetric(
                    metric_name="e2e_latency",
                    value=avg_latency,
                    unit="ms",
                    target_value=target_ms,
                    threshold_type="less_than",
                    passed=target_met,
                    timestamp=datetime.utcnow().isoformat(),
                    additional_data={
                        "p95_latency": p95_latency,
                        "p99_latency": p99_latency,
                        "success_rate": success_count / 20,
                        "sample_count": len(e2e_latencies),
                    },
                )
            )

            if target_met:
                logger.info(
                    f"âœ… ç«¯åˆ°ç«¯å»¶é²: {avg_latency:.1f}ms (ç›®æ¨™: < {target_ms}ms) âœ¨ ç¬¦åˆå¯¦é©—å®¤é©—æ¸¬è¦æ±‚"
                )
            else:
                logger.error(
                    f"âŒ ç«¯åˆ°ç«¯å»¶é²: {avg_latency:.1f}ms è¶…éç›®æ¨™ {target_ms}ms"
                )

            return target_met
        else:
            logger.error("âŒ ç„¡æ³•å®Œæˆç«¯åˆ°ç«¯å»¶é²æ¸¬è©¦")
            return False

    async def _test_connection_recovery_time(self) -> bool:
        """æ¸¬è©¦é€£æ¥æ¢å¾©æ™‚é–“ - å¯¦é©—å®¤é©—æ¸¬çš„é—œéµæŒ‡æ¨™"""
        logger.info("ğŸ”„ æ¸¬è©¦é€£æ¥æ¢å¾©æ™‚é–“ (< 2s ç›®æ¨™)")

        target_s = self.benchmarks["reliability"][
            "connection_recovery_target_s"
        ]  # 2.0s
        warning_s = self.benchmarks["reliability"][
            "connection_recovery_warning_s"
        ]  # 1.5s

        recovery_times = []

        async with aiohttp.ClientSession() as session:
            for i in range(5):  # æ¸¬è©¦ 5 æ¬¡é€£æ¥æ¢å¾©
                # 1. ç¢ºèªæœå‹™æ­£å¸¸
                try:
                    netstack_url = f"{self.services['netstack']['url']}/health"
                    async with session.get(netstack_url, timeout=5) as response:
                        if response.status != 200:
                            logger.warning(f"æœå‹™åˆå§‹ç‹€æ…‹ç•°å¸¸: {response.status}")
                            continue

                    # 2. æ¨¡æ“¬çŸ­æš«ä¸­æ–·ï¼ˆé€éç­‰å¾…æ¨¡æ“¬ç¶²è·¯ä¸­æ–·æ¢å¾©ï¼‰
                    logger.debug(f"æ¨¡æ“¬é€£æ¥ä¸­æ–·æ¢å¾©æ¸¬è©¦ {i+1}")
                    await asyncio.sleep(0.5)  # æ¨¡æ“¬çŸ­æš«ä¸­æ–·

                    # 3. æ¸¬é‡æ¢å¾©æ™‚é–“
                    recovery_start = time.time()

                    # æŒçºŒå˜—è©¦é€£æ¥ç›´åˆ°æˆåŠŸ
                    max_attempts = 50  # æœ€å¤šå˜—è©¦ 5 ç§’ (50 * 0.1s)
                    recovered = False

                    for attempt in range(max_attempts):
                        try:
                            async with session.get(netstack_url, timeout=2) as response:
                                if response.status == 200:
                                    recovery_time = time.time() - recovery_start
                                    recovery_times.append(recovery_time)
                                    recovered = True
                                    logger.debug(
                                        f"é€£æ¥æ¢å¾©æ¸¬è©¦ {i+1}: {recovery_time:.2f}s"
                                    )
                                    break
                        except:
                            pass

                        await asyncio.sleep(0.1)

                    if not recovered:
                        logger.warning(f"é€£æ¥æ¢å¾©æ¸¬è©¦ {i+1} è¶…æ™‚")

                except Exception as e:
                    logger.warning(f"é€£æ¥æ¢å¾©æ¸¬è©¦ {i+1} ç•°å¸¸: {e}")

        if recovery_times:
            avg_recovery = statistics.mean(recovery_times)
            max_recovery = max(recovery_times)

            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            target_met = avg_recovery <= target_s
            self.results.append(
                PerformanceMetric(
                    metric_name="recovery_time",
                    value=avg_recovery,
                    unit="s",
                    target_value=target_s,
                    threshold_type="less_than",
                    passed=target_met,
                    timestamp=datetime.utcnow().isoformat(),
                    additional_data={
                        "max_recovery_time": max_recovery,
                        "min_recovery_time": min(recovery_times),
                        "sample_count": len(recovery_times),
                    },
                )
            )

            if target_met:
                logger.info(
                    f"âœ… é€£æ¥æ¢å¾©æ™‚é–“: {avg_recovery:.2f}s (ç›®æ¨™: < {target_s}s) âœ¨ ç¬¦åˆå¯¦é©—å®¤é©—æ¸¬è¦æ±‚"
                )
            else:
                logger.error(
                    f"âŒ é€£æ¥æ¢å¾©æ™‚é–“: {avg_recovery:.2f}s è¶…éç›®æ¨™ {target_s}s"
                )

            return target_met
        else:
            logger.error("âŒ ç„¡æ³•å®Œæˆé€£æ¥æ¢å¾©æ™‚é–“æ¸¬è©¦")
            return False

    async def _test_throughput_capabilities(self) -> bool:
        """æ¸¬è©¦ååé‡èƒ½åŠ›"""
        logger.info("ğŸ“Š æ¸¬è©¦ç³»çµ±ååé‡")

        min_mbps = self.benchmarks["throughput"]["min_mbps"]
        target_mbps = self.benchmarks["throughput"]["target_mbps"]

        # æ¸¬è©¦ API ååé‡ï¼ˆæ¯ç§’è«‹æ±‚æ•¸ï¼‰
        async with aiohttp.ClientSession() as session:
            test_duration = 10  # æ¸¬è©¦ 10 ç§’
            start_time = time.time()
            request_count = 0
            success_count = 0

            # ä¸¦ç™¼ç™¼é€è«‹æ±‚
            async def make_request():
                nonlocal request_count, success_count
                try:
                    netstack_url = f"{self.services['netstack']['url']}/health"
                    async with session.get(netstack_url, timeout=5) as response:
                        request_count += 1
                        if response.status == 200:
                            success_count += 1
                except:
                    request_count += 1

            # æŒçºŒç™¼é€è«‹æ±‚
            while time.time() - start_time < test_duration:
                tasks = [make_request() for _ in range(10)]  # æ¯æ‰¹ 10 å€‹ä¸¦ç™¼è«‹æ±‚
                await asyncio.gather(*tasks, return_exceptions=True)
                await asyncio.sleep(0.1)  # çŸ­æš«é–“éš”

            actual_duration = time.time() - start_time
            requests_per_second = request_count / actual_duration
            success_rate = success_count / request_count if request_count > 0 else 0

            # å°‡è«‹æ±‚é€Ÿç‡è½‰æ›ç‚ºæ¦‚å¿µæ€§çš„ååé‡æŒ‡æ¨™
            # é€™è£¡æˆ‘å€‘ä½¿ç”¨ RPS (Requests Per Second) ä½œç‚ºååé‡æŒ‡æ¨™
            throughput_metric = requests_per_second / 100 * target_mbps  # æ¦‚å¿µæ€§è½‰æ›

            target_met = throughput_metric >= min_mbps

            self.results.append(
                PerformanceMetric(
                    metric_name="api_throughput",
                    value=throughput_metric,
                    unit="conceptual_mbps",
                    target_value=min_mbps,
                    threshold_type="greater_than",
                    passed=target_met,
                    timestamp=datetime.utcnow().isoformat(),
                    additional_data={
                        "requests_per_second": requests_per_second,
                        "success_rate": success_rate,
                        "total_requests": request_count,
                        "test_duration": actual_duration,
                    },
                )
            )

            if target_met:
                logger.info(
                    f"âœ… API ååé‡: {requests_per_second:.1f} RPS (æˆåŠŸç‡: {success_rate:.1%})"
                )
            else:
                logger.error(f"âŒ API ååé‡ä¸è¶³: {requests_per_second:.1f} RPS")

            return target_met

    async def _test_concurrent_request_handling(self) -> bool:
        """æ¸¬è©¦ä¸¦ç™¼è«‹æ±‚è™•ç†èƒ½åŠ›"""
        logger.info("ğŸ”€ æ¸¬è©¦ä¸¦ç™¼è«‹æ±‚è™•ç†")

        concurrent_levels = [5, 10, 20, 50]  # ä¸åŒä¸¦ç™¼ç´šåˆ¥
        all_passed = True

        async with aiohttp.ClientSession() as session:
            for concurrent_count in concurrent_levels:
                logger.debug(f"æ¸¬è©¦ {concurrent_count} ä¸¦ç™¼è«‹æ±‚")

                start_time = time.time()

                # å‰µå»ºä¸¦ç™¼è«‹æ±‚
                async def single_request():
                    try:
                        netstack_url = f"{self.services['netstack']['url']}/health"
                        async with session.get(netstack_url, timeout=10) as response:
                            return response.status == 200
                    except:
                        return False

                # åŒæ™‚ç™¼é€å¤šå€‹è«‹æ±‚
                tasks = [single_request() for _ in range(concurrent_count)]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                duration = time.time() - start_time
                success_count = sum(1 for r in results if r is True)
                success_rate = success_count / concurrent_count

                # è¨˜éŒ„ä¸¦ç™¼æ¸¬è©¦çµæœ
                concurrent_passed = success_rate >= 0.9  # 90% æˆåŠŸç‡è¦æ±‚
                if not concurrent_passed:
                    all_passed = False

                self.results.append(
                    PerformanceMetric(
                        metric_name=f"concurrent_requests_{concurrent_count}",
                        value=success_rate,
                        unit="success_rate",
                        target_value=0.9,
                        threshold_type="greater_than",
                        passed=concurrent_passed,
                        timestamp=datetime.utcnow().isoformat(),
                        additional_data={
                            "concurrent_count": concurrent_count,
                            "duration": duration,
                            "success_count": success_count,
                        },
                    )
                )

                if concurrent_passed:
                    logger.info(
                        f"âœ… {concurrent_count} ä¸¦ç™¼è«‹æ±‚: {success_rate:.1%} æˆåŠŸç‡"
                    )
                else:
                    logger.error(
                        f"âŒ {concurrent_count} ä¸¦ç™¼è«‹æ±‚: {success_rate:.1%} æˆåŠŸç‡éä½"
                    )

        return all_passed

    async def _test_resource_utilization(self) -> bool:
        """æ¸¬è©¦è³‡æºåˆ©ç”¨ç‡"""
        logger.info("ğŸ’» æ¸¬è©¦ç³»çµ±è³‡æºåˆ©ç”¨ç‡")

        # ç›£æ§æœŸé–“
        monitoring_duration = 30  # 30 ç§’
        sampling_interval = 1  # æ¯ç§’æ¡æ¨£

        cpu_samples = []
        memory_samples = []

        start_time = time.time()

        # åœ¨ç›£æ§æœŸé–“æŒçºŒæ¡æ¨£
        while time.time() - start_time < monitoring_duration:
            try:
                # CPU ä½¿ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=0.1)
                cpu_samples.append(cpu_percent)

                # è¨˜æ†¶é«”ä½¿ç”¨ç‡
                memory = psutil.virtual_memory()
                memory_samples.append(memory.percent)

                await asyncio.sleep(sampling_interval)

            except Exception as e:
                logger.warning(f"è³‡æºç›£æ§æ¡æ¨£å¤±æ•—: {e}")

        if cpu_samples and memory_samples:
            avg_cpu = statistics.mean(cpu_samples)
            max_cpu = max(cpu_samples)
            avg_memory = statistics.mean(memory_samples)
            max_memory = max(memory_samples)

            # æª¢æŸ¥è³‡æºä½¿ç”¨ç‡æ˜¯å¦åœ¨åˆç†ç¯„åœå…§
            cpu_threshold = 90.0  # CPU ä½¿ç”¨ç‡ä¸æ‡‰è¶…é 90%
            memory_threshold = 90.0  # è¨˜æ†¶é«”ä½¿ç”¨ç‡ä¸æ‡‰è¶…é 90%

            cpu_passed = max_cpu <= cpu_threshold
            memory_passed = max_memory <= memory_threshold

            # è¨˜éŒ„ CPU æŒ‡æ¨™
            self.results.append(
                PerformanceMetric(
                    metric_name="cpu_utilization",
                    value=avg_cpu,
                    unit="percent",
                    target_value=cpu_threshold,
                    threshold_type="less_than",
                    passed=cpu_passed,
                    timestamp=datetime.utcnow().isoformat(),
                    additional_data={"max_cpu": max_cpu, "samples": len(cpu_samples)},
                )
            )

            # è¨˜éŒ„è¨˜æ†¶é«”æŒ‡æ¨™
            self.results.append(
                PerformanceMetric(
                    metric_name="memory_utilization",
                    value=avg_memory,
                    unit="percent",
                    target_value=memory_threshold,
                    threshold_type="less_than",
                    passed=memory_passed,
                    timestamp=datetime.utcnow().isoformat(),
                    additional_data={
                        "max_memory": max_memory,
                        "samples": len(memory_samples),
                    },
                )
            )

            if cpu_passed and memory_passed:
                logger.info(
                    f"âœ… è³‡æºåˆ©ç”¨ç‡æ­£å¸¸ - CPU: {avg_cpu:.1f}% (å³°å€¼: {max_cpu:.1f}%), è¨˜æ†¶é«”: {avg_memory:.1f}% (å³°å€¼: {max_memory:.1f}%)"
                )
            else:
                logger.error(
                    f"âŒ è³‡æºåˆ©ç”¨ç‡éé«˜ - CPU: {avg_cpu:.1f}% (å³°å€¼: {max_cpu:.1f}%), è¨˜æ†¶é«”: {avg_memory:.1f}% (å³°å€¼: {max_memory:.1f}%)"
                )

            return cpu_passed and memory_passed
        else:
            logger.error("âŒ ç„¡æ³•æ”¶é›†è³‡æºåˆ©ç”¨ç‡æ•¸æ“š")
            return False
