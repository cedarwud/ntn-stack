#!/usr/bin/env python3
"""
ç«¯åˆ°ç«¯ç³»çµ±é›†æˆæ¸¬è©¦æ¡†æ¶
æ ¹æ“š TODO.md ç¬¬14é …è¦æ±‚è¨­è¨ˆçš„å®Œæ•´æ¸¬è©¦æ¡†æ¶

å¯¦ç¾åŠŸèƒ½ï¼š
1. æ¸¬è©¦å ´æ™¯è‡ªå‹•åŒ–åŸ·è¡Œ
2. æ€§èƒ½æŒ‡æ¨™å¯¦æ™‚ç›£æ§
3. æ•…éšœæª¢æ¸¬å’Œè‡ªå‹•æ¢å¾©
4. è©³ç´°æ¸¬è©¦å ±å‘Šç”Ÿæˆ
5. Docker å®¹å™¨é–“é€šä¿¡é©—è­‰
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import yaml
import aiohttp
import docker
import psutil
import numpy as np
from dataclasses import dataclass, asdict
from contextlib import asynccontextmanager
import subprocess
import sys

sys.path.append("tests/e2e/scenarios")
sys.path.append("tests/performance")

from uav_satellite_connection_test import UAVSatelliteConnectionTest
from interference_avoidance_test import InterferenceAvoidanceTest
from satellite_mesh_failover_test import SatelliteMeshFailoverTest
from performance_optimizer import PerformanceOptimizer

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class TestResult:
    """æ¸¬è©¦çµæœæ•¸æ“šçµæ§‹"""

    test_name: str
    scenario: str
    status: str  # "passed", "failed", "error", "timeout"
    start_time: datetime
    end_time: Optional[datetime] = None
    duration_seconds: float = 0.0
    performance_metrics: Dict[str, float] = None
    error_message: str = ""
    detailed_logs: List[str] = None

    def __post_init__(self):
        if self.performance_metrics is None:
            self.performance_metrics = {}
        if self.detailed_logs is None:
            self.detailed_logs = []


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""

    timestamp: datetime
    latency_ms: float
    throughput_mbps: float
    cpu_usage_percent: float
    memory_usage_mb: float
    connection_count: int
    error_rate: float

    def to_dict(self) -> Dict:
        data = asdict(self)
        data["timestamp"] = self.timestamp.isoformat()
        return data


class DockerMonitor:
    """Docker å®¹å™¨ç›£æ§å™¨"""

    def __init__(self):
        try:
            self.client = docker.from_env()
        except Exception as e:
            logger.warning(f"ç„¡æ³•é€£æ¥åˆ° Docker: {e}")
            self.client = None

    async def get_container_stats(self, container_name: str) -> Dict[str, Any]:
        """ç²å–å®¹å™¨çµ±è¨ˆä¿¡æ¯"""
        if not self.client:
            return {}

        try:
            container = self.client.containers.get(container_name)
            stats = container.stats(stream=False)

            # è¨ˆç®— CPU ä½¿ç”¨ç‡
            cpu_usage = 0.0
            if "cpu_stats" in stats and "precpu_stats" in stats:
                cpu_delta = (
                    stats["cpu_stats"]["cpu_usage"]["total_usage"]
                    - stats["precpu_stats"]["cpu_usage"]["total_usage"]
                )
                system_delta = (
                    stats["cpu_stats"]["system_cpu_usage"]
                    - stats["precpu_stats"]["system_cpu_usage"]
                )
                if system_delta > 0:
                    cpu_usage = (cpu_delta / system_delta) * 100.0

            # è¨ˆç®—è¨˜æ†¶é«”ä½¿ç”¨é‡
            memory_usage = 0
            if "memory_stats" in stats:
                memory_usage = stats["memory_stats"].get("usage", 0) / 1024 / 1024  # MB

            return {
                "status": container.status,
                "cpu_usage_percent": cpu_usage,
                "memory_usage_mb": memory_usage,
                "network_stats": stats.get("networks", {}),
                "container_id": container.id[:12],
            }
        except Exception as e:
            logger.error(f"ç²å–å®¹å™¨ {container_name} çµ±è¨ˆä¿¡æ¯å¤±æ•—: {e}")
            return {}

    async def check_container_health(self, container_name: str) -> bool:
        """æª¢æŸ¥å®¹å™¨å¥åº·ç‹€æ…‹"""
        if not self.client:
            return False

        try:
            container = self.client.containers.get(container_name)
            return container.status == "running"
        except Exception as e:
            logger.error(f"æª¢æŸ¥å®¹å™¨ {container_name} å¥åº·ç‹€æ…‹å¤±æ•—: {e}")
            return False

    async def get_network_info(self, network_name: str) -> Dict[str, Any]:
        """ç²å–ç¶²è·¯ä¿¡æ¯"""
        if not self.client:
            return {}

        try:
            network = self.client.networks.get(network_name)
            return {
                "name": network.name,
                "driver": network.attrs.get("Driver", ""),
                "containers": list(network.attrs.get("Containers", {}).keys()),
                "ipam": network.attrs.get("IPAM", {}),
            }
        except Exception as e:
            logger.error(f"ç²å–ç¶²è·¯ {network_name} ä¿¡æ¯å¤±æ•—: {e}")
            return {}


class PerformanceMonitor:
    """æ€§èƒ½ç›£æ§å™¨"""

    def __init__(self, docker_monitor: DockerMonitor):
        self.docker_monitor = docker_monitor
        self.metrics_history = []
        self.monitoring_active = False
        self.monitor_task = None

    async def start_monitoring(self, interval_seconds: float = 1.0):
        """é–‹å§‹æ€§èƒ½ç›£æ§"""
        self.monitoring_active = True
        self.monitor_task = asyncio.create_task(self._monitor_loop(interval_seconds))
        logger.info("æ€§èƒ½ç›£æ§å·²å•Ÿå‹•")

    async def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›£æ§"""
        self.monitoring_active = False
        if self.monitor_task:
            self.monitor_task.cancel()
            try:
                await self.monitor_task
            except asyncio.CancelledError:
                pass
        logger.info("æ€§èƒ½ç›£æ§å·²åœæ­¢")

    async def _monitor_loop(self, interval_seconds: float):
        """ç›£æ§å¾ªç’°"""
        while self.monitoring_active:
            try:
                metrics = await self._collect_metrics()
                self.metrics_history.append(metrics)

                # ä¿æŒæœ€è¿‘ 1000 å€‹æ•¸æ“šé»
                if len(self.metrics_history) > 1000:
                    self.metrics_history = self.metrics_history[-1000:]

                await asyncio.sleep(interval_seconds)
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"ç›£æ§å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(interval_seconds)

    async def _collect_metrics(self) -> PerformanceMetrics:
        """æ”¶é›†æ€§èƒ½æŒ‡æ¨™"""
        timestamp = datetime.utcnow()

        # ç³»çµ±æŒ‡æ¨™
        cpu_usage = psutil.cpu_percent(interval=None)
        memory = psutil.virtual_memory()
        memory_usage_mb = memory.used / 1024 / 1024

        # æ¸¬è©¦å»¶é² (æ¨¡æ“¬)
        latency_ms = await self._measure_latency()

        # æ¸¬è©¦ååé‡ (æ¨¡æ“¬)
        throughput_mbps = await self._measure_throughput()

        # é€£æ¥æ•¸ (æ¨¡æ“¬)
        connection_count = await self._count_connections()

        # éŒ¯èª¤ç‡ (æ¨¡æ“¬)
        error_rate = await self._calculate_error_rate()

        return PerformanceMetrics(
            timestamp=timestamp,
            latency_ms=latency_ms,
            throughput_mbps=throughput_mbps,
            cpu_usage_percent=cpu_usage,
            memory_usage_mb=memory_usage_mb,
            connection_count=connection_count,
            error_rate=error_rate,
        )

    async def _measure_latency(self) -> float:
        """æ¸¬é‡å»¶é²"""
        try:
            start = time.time()
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    "http://localhost:8080/health", timeout=5
                ) as response:
                    if response.status == 200:
                        return (time.time() - start) * 1000  # è½‰æ›ç‚ºæ¯«ç§’
            return 999.0  # é€£æ¥å¤±æ•—
        except Exception:
            return 999.0

    async def _measure_throughput(self) -> float:
        """æ¸¬é‡ååé‡ (æ¨¡æ“¬)"""
        # é€™è£¡æ‡‰è©²å¯¦ç¾çœŸå¯¦çš„ååé‡æ¸¬è©¦
        # ç¾åœ¨è¿”å›æ¨¡æ“¬å€¼
        return np.random.normal(25.0, 5.0)

    async def _count_connections(self) -> int:
        """çµ±è¨ˆé€£æ¥æ•¸ (æ¨¡æ“¬)"""
        # é€™è£¡æ‡‰è©²å¯¦ç¾çœŸå¯¦çš„é€£æ¥çµ±è¨ˆ
        # ç¾åœ¨è¿”å›æ¨¡æ“¬å€¼
        return np.random.randint(1, 10)

    async def _calculate_error_rate(self) -> float:
        """è¨ˆç®—éŒ¯èª¤ç‡ (æ¨¡æ“¬)"""
        # é€™è£¡æ‡‰è©²å¯¦ç¾çœŸå¯¦çš„éŒ¯èª¤ç‡è¨ˆç®—
        # ç¾åœ¨è¿”å›æ¨¡æ“¬å€¼
        return np.random.uniform(0.0, 0.1)

    def get_average_metrics(self, duration_minutes: int = 5) -> Dict[str, float]:
        """ç²å–æŒ‡å®šæ™‚é–“å…§çš„å¹³å‡æŒ‡æ¨™"""
        if not self.metrics_history:
            return {}

        cutoff_time = datetime.utcnow() - timedelta(minutes=duration_minutes)
        recent_metrics = [m for m in self.metrics_history if m.timestamp >= cutoff_time]

        if not recent_metrics:
            return {}

        return {
            "avg_latency_ms": np.mean([m.latency_ms for m in recent_metrics]),
            "avg_throughput_mbps": np.mean([m.throughput_mbps for m in recent_metrics]),
            "avg_cpu_usage_percent": np.mean(
                [m.cpu_usage_percent for m in recent_metrics]
            ),
            "avg_memory_usage_mb": np.mean([m.memory_usage_mb for m in recent_metrics]),
            "avg_connection_count": np.mean(
                [m.connection_count for m in recent_metrics]
            ),
            "avg_error_rate": np.mean([m.error_rate for m in recent_metrics]),
        }


class E2ETestFramework:
    """ç«¯åˆ°ç«¯æ¸¬è©¦æ¡†æ¶ä¸»é¡"""

    def __init__(self, config_path: str = "tests/configs/e2e_test_config.yaml"):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.docker_monitor = DockerMonitor()
        self.performance_monitor = PerformanceMonitor(self.docker_monitor)
        self.test_results = []
        self.start_time = None
        self.session = None

        # å‰µå»ºå ±å‘Šç›®éŒ„
        self.reports_dir = Path("tests/reports/e2e")
        self.reports_dir.mkdir(parents=True, exist_ok=True)

        # æ–°å¢æ€§èƒ½å„ªåŒ–å™¨
        self.performance_optimizer = PerformanceOptimizer()

        # æ¸¬è©¦å ´æ™¯æ˜ å°„
        self.scenario_classes = {
            "normal_uav_satellite_connection": UAVSatelliteConnectionTest,
            "interference_avoidance": InterferenceAvoidanceTest,
            "satellite_loss_mesh_failover": SatelliteMeshFailoverTest,
        }

    def _load_config(self) -> Dict:
        """è¼‰å…¥æ¸¬è©¦é…ç½®"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥é…ç½®æ–‡ä»¶å¤±æ•—: {e}")
            raise

    @asynccontextmanager
    async def test_session(self):
        """æ¸¬è©¦æœƒè©±ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        self.session = aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30))
        try:
            yield self.session
        finally:
            await self.session.close()
            self.session = None

    async def run_all_scenarios(self) -> bool:
        """åŸ·è¡Œæ‰€æœ‰æ¸¬è©¦å ´æ™¯"""
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œç«¯åˆ°ç«¯ç³»çµ±é›†æˆæ¸¬è©¦")
        self.start_time = datetime.utcnow()

        # æ­¥é©Ÿ1: åŸ·è¡Œæ€§èƒ½å„ªåŒ–
        logger.info("âš¡ åŸ·è¡Œç³»çµ±æ€§èƒ½å„ªåŒ–")
        optimization_success = (
            await self.performance_optimizer.run_performance_optimization()
        )

        if not optimization_success:
            logger.warning("âš ï¸ æ€§èƒ½å„ªåŒ–æœªå®Œå…¨æˆåŠŸï¼Œä½†ç¹¼çºŒé€²è¡Œæ¸¬è©¦")

        # æª¢æŸ¥ç’°å¢ƒæº–å‚™
        if not await self._verify_environment():
            logger.error("âŒ ç’°å¢ƒæª¢æŸ¥å¤±æ•—ï¼Œç„¡æ³•åŸ·è¡Œæ¸¬è©¦")
            return False

        # å•Ÿå‹•æ€§èƒ½ç›£æ§
        await self.performance_monitor.start_monitoring()

        try:
            async with self.test_session():
                # ç²å–æ¸¬è©¦å ´æ™¯
                scenarios = self.config["test_scenarios"]

                # æŒ‰å„ªå…ˆç´šæ’åº
                sorted_scenarios = sorted(
                    scenarios.items(), key=lambda x: x[1].get("priority", 999)
                )

                all_passed = True

                for scenario_name, scenario_config in sorted_scenarios:
                    logger.info(f"ğŸ“‹ åŸ·è¡Œæ¸¬è©¦å ´æ™¯: {scenario_config['name']}")

                    try:
                        # ä½¿ç”¨å°æ‡‰çš„æ¸¬è©¦å ´æ™¯é¡
                        if scenario_name in self.scenario_classes:
                            scenario_class = self.scenario_classes[scenario_name]
                            env_config = self.config["environments"]["development"]

                            scenario_instance = scenario_class(
                                env_config["netstack"]["url"],
                                env_config["simworld"]["url"],
                            )

                            scenario_result = await scenario_instance.run_scenario(
                                self.session
                            )

                            # è½‰æ›ç‚º TestResult æ ¼å¼
                            result = TestResult(
                                test_name=scenario_name,
                                scenario=scenario_config["name"],
                                status=(
                                    "passed"
                                    if scenario_result.get("success", False)
                                    else "failed"
                                ),
                                start_time=datetime.fromisoformat(
                                    scenario_result["start_time"]
                                ),
                                end_time=datetime.fromisoformat(
                                    scenario_result["end_time"]
                                ),
                                duration_seconds=scenario_result["duration_seconds"],
                                performance_metrics=scenario_result.get(
                                    "performance_metrics", {}
                                ),
                                error_message=scenario_result.get("error", ""),
                                detailed_logs=[
                                    step.get("details", "")
                                    for step in scenario_result.get("steps", [])
                                ],
                            )
                        else:
                            # ä½¿ç”¨åŸä¾†çš„é€šç”¨æ–¹æ³•
                            result = await self._run_scenario(
                                scenario_name, scenario_config
                            )

                        self.test_results.append(result)

                        if result.status != "passed":
                            all_passed = False
                            logger.error(f"âŒ å ´æ™¯å¤±æ•—: {scenario_name}")
                        else:
                            logger.info(f"âœ… å ´æ™¯é€šé: {scenario_name}")

                    except Exception as e:
                        logger.error(f"âŒ å ´æ™¯åŸ·è¡Œç•°å¸¸: {scenario_name} - {e}")
                        error_result = TestResult(
                            test_name=scenario_name,
                            scenario=scenario_config["name"],
                            status="error",
                            start_time=datetime.utcnow(),
                            error_message=str(e),
                        )
                        error_result.end_time = datetime.utcnow()
                        self.test_results.append(error_result)
                        all_passed = False

        finally:
            # åœæ­¢æ€§èƒ½ç›£æ§
            await self.performance_monitor.stop_monitoring()

        # ç”Ÿæˆå ±å‘Š
        await self._generate_comprehensive_report()

        return all_passed

    async def _verify_environment(self) -> bool:
        """é©—è­‰æ¸¬è©¦ç’°å¢ƒ"""
        logger.info("ğŸ” é©—è­‰æ¸¬è©¦ç’°å¢ƒ...")

        env_config = self.config["environments"]["development"]

        # æª¢æŸ¥æœå‹™å¥åº·ç‹€æ…‹
        health_checks = []

        # NetStack å¥åº·æª¢æŸ¥
        netstack_url = env_config["netstack"]["url"]
        health_checks.append(("NetStack", f"{netstack_url}/health"))

        # SimWorld å¥åº·æª¢æŸ¥
        simworld_url = env_config["simworld"]["url"]
        health_checks.append(("SimWorld", f"{simworld_url}/api/v1/wireless/health"))

        async with aiohttp.ClientSession() as session:
            for service_name, health_url in health_checks:
                try:
                    async with session.get(health_url, timeout=10) as response:
                        if response.status == 200:
                            logger.info(f"âœ… {service_name} å¥åº·æª¢æŸ¥é€šé")
                        else:
                            logger.error(
                                f"âŒ {service_name} å¥åº·æª¢æŸ¥å¤±æ•—: HTTP {response.status}"
                            )
                            return False
                except Exception as e:
                    logger.error(f"âŒ {service_name} å¥åº·æª¢æŸ¥ç•°å¸¸: {e}")
                    return False

        # æª¢æŸ¥ Docker å®¹å™¨ç‹€æ…‹
        containers = env_config["docker"]["containers"]
        for container_name in containers.values():
            if not await self.docker_monitor.check_container_health(container_name):
                logger.error(f"âŒ å®¹å™¨ {container_name} æœªé‹è¡Œ")
                return False
            logger.info(f"âœ… å®¹å™¨ {container_name} é‹è¡Œæ­£å¸¸")

        # æª¢æŸ¥ç¶²è·¯é€£æ¥
        network_name = env_config["docker"]["network_name"]
        network_info = await self.docker_monitor.get_network_info(network_name)
        if not network_info:
            logger.error(f"âŒ ç¶²è·¯ {network_name} ä¸å­˜åœ¨")
            return False
        logger.info(f"âœ… ç¶²è·¯ {network_name} æ­£å¸¸")

        logger.info("âœ… ç’°å¢ƒé©—è­‰å®Œæˆ")
        return True

    async def _run_scenario(
        self, scenario_name: str, scenario_config: Dict
    ) -> TestResult:
        """åŸ·è¡Œå–®å€‹æ¸¬è©¦å ´æ™¯"""
        start_time = datetime.utcnow()
        result = TestResult(
            test_name=scenario_name,
            scenario=scenario_config["name"],
            status="running",
            start_time=start_time,
        )

        try:
            # è¨­ç½®è¶…æ™‚
            timeout = scenario_config.get("timeout_seconds", 300)

            # åŸ·è¡Œæ¸¬è©¦æ­¥é©Ÿ
            async with asyncio.timeout(timeout):
                success = await self._execute_test_steps(
                    scenario_config["test_steps"],
                    scenario_config["performance_targets"],
                    result,
                )

            result.status = "passed" if success else "failed"

        except asyncio.TimeoutError:
            result.status = "timeout"
            result.error_message = f"æ¸¬è©¦è¶…æ™‚ ({timeout} ç§’)"
            logger.error(f"æ¸¬è©¦å ´æ™¯ {scenario_name} è¶…æ™‚")

        except Exception as e:
            result.status = "error"
            result.error_message = str(e)
            logger.error(f"æ¸¬è©¦å ´æ™¯ {scenario_name} åŸ·è¡Œç•°å¸¸: {e}")

        finally:
            result.end_time = datetime.utcnow()
            result.duration_seconds = (
                result.end_time - result.start_time
            ).total_seconds()

            # æ”¶é›†æ€§èƒ½æŒ‡æ¨™
            result.performance_metrics = self.performance_monitor.get_average_metrics(5)

        return result

    async def _execute_test_steps(
        self, test_steps: List[Dict], performance_targets: Dict, result: TestResult
    ) -> bool:
        """åŸ·è¡Œæ¸¬è©¦æ­¥é©Ÿ"""
        env_config = self.config["environments"]["development"]

        for step in test_steps:
            step_name = step["name"]
            result.detailed_logs.append(f"åŸ·è¡Œæ­¥é©Ÿ: {step_name}")

            try:
                # ç¢ºå®šæœå‹™ URL
                if step["endpoint"].startswith("/api/v1/uav") or step[
                    "endpoint"
                ].startswith("/api/v1/satellite"):
                    base_url = env_config["netstack"]["url"]
                else:
                    base_url = env_config["simworld"]["url"]

                url = f"{base_url}{step['endpoint']}"
                method = step["method"].upper()

                # åŸ·è¡Œ HTTP è«‹æ±‚
                start_time = time.time()

                async with self.session.request(method, url) as response:
                    duration_ms = (time.time() - start_time) * 1000

                    result.detailed_logs.append(
                        f"  {method} {url} -> {response.status} ({duration_ms:.1f}ms)"
                    )

                    # æª¢æŸ¥å›æ‡‰ç‹€æ…‹
                    if response.status not in [200, 201, 202]:
                        result.detailed_logs.append(f"  éŒ¯èª¤: HTTP {response.status}")
                        return False

                    # æª¢æŸ¥æ€§èƒ½ç›®æ¨™
                    if not await self._check_performance_targets(
                        performance_targets, duration_ms, result
                    ):
                        return False

            except Exception as e:
                result.detailed_logs.append(f"  æ­¥é©ŸåŸ·è¡Œå¤±æ•—: {e}")
                return False

        return True

    async def _check_performance_targets(
        self, targets: Dict, step_duration_ms: float, result: TestResult
    ) -> bool:
        """æª¢æŸ¥æ€§èƒ½ç›®æ¨™"""
        # æª¢æŸ¥ç«¯åˆ°ç«¯å»¶é²
        if "max_e2e_latency_ms" in targets:
            max_latency = targets["max_e2e_latency_ms"]
            if step_duration_ms > max_latency:
                result.detailed_logs.append(
                    f"  æ€§èƒ½å¤±æ•—: å»¶é² {step_duration_ms:.1f}ms > ç›®æ¨™ {max_latency}ms"
                )
                return False

        # æª¢æŸ¥é‡é€£æ™‚é–“
        if "max_reconnection_time_ms" in targets:
            max_reconnection = targets["max_reconnection_time_ms"]
            if step_duration_ms > max_reconnection:
                result.detailed_logs.append(
                    f"  æ€§èƒ½å¤±æ•—: é‡é€£æ™‚é–“ {step_duration_ms:.1f}ms > ç›®æ¨™ {max_reconnection}ms"
                )
                return False

        return True

    async def _generate_comprehensive_report(self):
        """ç”Ÿæˆç¶œåˆæ¸¬è©¦å ±å‘Š"""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")

        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r.status == "passed")
        failed_tests = sum(1 for r in self.test_results if r.status == "failed")
        error_tests = sum(1 for r in self.test_results if r.status == "error")
        timeout_tests = sum(1 for r in self.test_results if r.status == "timeout")

        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0

        total_duration = (datetime.utcnow() - self.start_time).total_seconds()

        # å‰µå»ºå ±å‘Šæ•¸æ“š
        report_data = {
            "test_execution": {
                "start_time": self.start_time.isoformat(),
                "end_time": datetime.utcnow().isoformat(),
                "total_duration_seconds": total_duration,
                "environment": "development",
            },
            "summary": {
                "total_scenarios": total_tests,
                "passed": passed_tests,
                "failed": failed_tests,
                "error": error_tests,
                "timeout": timeout_tests,
                "success_rate": success_rate,
            },
            "detailed_results": [asdict(result) for result in self.test_results],
            "performance_analysis": self.performance_monitor.get_average_metrics(10),
            "performance_history": [
                m.to_dict() for m in self.performance_monitor.metrics_history
            ],
        }

        # ä¿å­˜ JSON å ±å‘Š
        json_report_path = self.reports_dir / f"e2e_test_report_{timestamp}.json"
        with open(json_report_path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)

        # ç”Ÿæˆ HTML å ±å‘Š
        html_report_path = self.reports_dir / f"e2e_test_report_{timestamp}.html"
        await self._generate_html_report(report_data, html_report_path)

        # è¼¸å‡ºæ‘˜è¦
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š ç«¯åˆ°ç«¯ç³»çµ±é›†æˆæ¸¬è©¦å ±å‘Š")
        logger.info("=" * 80)
        logger.info(f"æ¸¬è©¦å ´æ™¯ç¸½æ•¸: {total_tests}")
        logger.info(f"é€šé: {passed_tests}")
        logger.info(f"å¤±æ•—: {failed_tests}")
        logger.info(f"éŒ¯èª¤: {error_tests}")
        logger.info(f"è¶…æ™‚: {timeout_tests}")
        logger.info(f"æˆåŠŸç‡: {success_rate:.1f}%")
        logger.info(f"ç¸½åŸ·è¡Œæ™‚é–“: {total_duration:.1f} ç§’")
        logger.info(f"è©³ç´°å ±å‘Š: {json_report_path}")
        logger.info(f"HTML å ±å‘Š: {html_report_path}")
        logger.info("=" * 80)

    async def _generate_html_report(self, report_data: Dict, output_path: Path):
        """ç”Ÿæˆ HTML å ±å‘Š"""
        html_template = """
<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NTN Stack E2E æ¸¬è©¦å ±å‘Š</title>
    <style>
        body { font-family: 'Microsoft YaHei', Arial, sans-serif; margin: 20px; }
        .header { background: #2c3e50; color: white; padding: 20px; border-radius: 5px; }
        .summary { background: #ecf0f1; padding: 15px; margin: 20px 0; border-radius: 5px; }
        .passed { color: #27ae60; font-weight: bold; }
        .failed { color: #e74c3c; font-weight: bold; }
        .error { color: #e67e22; font-weight: bold; }
        .timeout { color: #9b59b6; font-weight: bold; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background-color: #f2f2f2; }
        .metrics { display: flex; flex-wrap: wrap; gap: 20px; }
        .metric-card { background: #fff; border: 1px solid #ddd; padding: 15px; border-radius: 5px; flex: 1; min-width: 200px; }
    </style>
</head>
<body>
    <div class="header">
        <h1>ğŸš€ NTN Stack ç«¯åˆ°ç«¯ç³»çµ±é›†æˆæ¸¬è©¦å ±å‘Š</h1>
        <p>ç”Ÿæˆæ™‚é–“: {generation_time}</p>
        <p>æ¸¬è©¦ç’°å¢ƒ: {environment}</p>
    </div>
    
    <div class="summary">
        <h2>ğŸ“Š æ¸¬è©¦æ‘˜è¦</h2>
        <div class="metrics">
            <div class="metric-card">
                <h3>ç¸½æ¸¬è©¦å ´æ™¯</h3>
                <h2>{total_scenarios}</h2>
            </div>
            <div class="metric-card">
                <h3>é€šé</h3>
                <h2 class="passed">{passed}</h2>
            </div>
            <div class="metric-card">
                <h3>å¤±æ•—</h3>
                <h2 class="failed">{failed}</h2>
            </div>
            <div class="metric-card">
                <h3>æˆåŠŸç‡</h3>
                <h2>{success_rate:.1f}%</h2>
            </div>
            <div class="metric-card">
                <h3>åŸ·è¡Œæ™‚é–“</h3>
                <h2>{total_duration:.1f}s</h2>
            </div>
        </div>
    </div>
    
    <div class="details">
        <h2>ğŸ“‹ è©³ç´°çµæœ</h2>
        <table>
            <thead>
                <tr>
                    <th>æ¸¬è©¦å ´æ™¯</th>
                    <th>ç‹€æ…‹</th>
                    <th>åŸ·è¡Œæ™‚é–“</th>
                    <th>å¹³å‡å»¶é²</th>
                    <th>éŒ¯èª¤ä¿¡æ¯</th>
                </tr>
            </thead>
            <tbody>
                {test_results_rows}
            </tbody>
        </table>
    </div>
    
    <div class="performance">
        <h2>âš¡ æ€§èƒ½åˆ†æ</h2>
        <div class="metrics">
            <div class="metric-card">
                <h3>å¹³å‡å»¶é²</h3>
                <h2>{avg_latency:.1f} ms</h2>
            </div>
            <div class="metric-card">
                <h3>å¹³å‡ååé‡</h3>
                <h2>{avg_throughput:.1f} Mbps</h2>
            </div>
            <div class="metric-card">
                <h3>å¹³å‡ CPU ä½¿ç”¨ç‡</h3>
                <h2>{avg_cpu:.1f}%</h2>
            </div>
            <div class="metric-card">
                <h3>å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨</h3>
                <h2>{avg_memory:.1f} MB</h2>
            </div>
        </div>
    </div>
</body>
</html>
        """

        # ç”Ÿæˆæ¸¬è©¦çµæœè¡Œ
        test_results_rows = ""
        for result in report_data["detailed_results"]:
            status_class = result["status"]
            avg_latency = result.get("performance_metrics", {}).get("avg_latency_ms", 0)
            test_results_rows += f"""
                <tr>
                    <td>{result["scenario"]}</td>
                    <td class="{status_class}">{result["status"].upper()}</td>
                    <td>{result["duration_seconds"]:.2f}s</td>
                    <td>{avg_latency:.1f}ms</td>
                    <td>{result.get("error_message", "")}</td>
                </tr>
            """

        # æ ¼å¼åŒ– HTML
        perf_analysis = report_data["performance_analysis"]
        html_content = html_template.format(
            generation_time=datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S"),
            environment=report_data["test_execution"]["environment"],
            total_scenarios=report_data["summary"]["total_scenarios"],
            passed=report_data["summary"]["passed"],
            failed=report_data["summary"]["failed"],
            success_rate=report_data["summary"]["success_rate"],
            total_duration=report_data["test_execution"]["total_duration_seconds"],
            test_results_rows=test_results_rows,
            avg_latency=perf_analysis.get("avg_latency_ms", 0),
            avg_throughput=perf_analysis.get("avg_throughput_mbps", 0),
            avg_cpu=perf_analysis.get("avg_cpu_usage_percent", 0),
            avg_memory=perf_analysis.get("avg_memory_usage_mb", 0),
        )

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(html_content)


async def main():
    """ä¸»å‡½æ•¸"""
    framework = E2ETestFramework()
    success = await framework.run_all_scenarios()

    if success:
        logger.info("ğŸ‰ æ‰€æœ‰ç«¯åˆ°ç«¯æ¸¬è©¦é€šéï¼")
        return 0
    else:
        logger.error("âŒ éƒ¨åˆ†ç«¯åˆ°ç«¯æ¸¬è©¦å¤±æ•—ï¼")
        return 1


if __name__ == "__main__":
    import sys

    sys.exit(asyncio.run(main()))
