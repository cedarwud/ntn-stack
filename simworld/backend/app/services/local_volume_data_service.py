"""
æœ¬åœ° Docker Volume æ•¸æ“šæœå‹™
æŒ‰ç…§è¡›æ˜Ÿæ•¸æ“šæ¶æ§‹æ–‡æª”ï¼ŒSimWorld æ‡‰è©²ä½¿ç”¨ Docker Volume æœ¬åœ°æ•¸æ“šè€Œéç›´æ¥ API èª¿ç”¨
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


class LocalVolumeDataService:
    """æœ¬åœ° Docker Volume æ•¸æ“šæœå‹™ - éµå¾ªè¡›æ˜Ÿæ•¸æ“šæ¶æ§‹"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœ¬åœ°æ•¸æ“šæœå‹™"""
        # Docker Volume æ›è¼‰è·¯å¾‘
        self.netstack_data_path = Path("/app/data")  # ä¸»è¦é è¨ˆç®—æ•¸æ“š
        self.netstack_tle_data_path = Path("/app/netstack/tle_data")  # TLE åŸå§‹æ•¸æ“š
        
        # çµ±ä¸€æ™‚é–“åºåˆ—é…ç½®
        self.time_span_minutes = 120
        self.time_interval_seconds = 10
        self.total_time_points = 720
        
        # æª¢æŸ¥è·¯å¾‘æ˜¯å¦å­˜åœ¨
        self._check_volume_paths()
    
    def _check_volume_paths(self):
        """æª¢æŸ¥ Docker Volume è·¯å¾‘æ˜¯å¦æ­£ç¢ºæ›è¼‰"""
        paths = [
            (self.netstack_data_path, "é è¨ˆç®—è»Œé“æ•¸æ“š"),
            (self.netstack_tle_data_path, "TLE åŸå§‹æ•¸æ“š")
        ]
        
        for path, description in paths:
            if path.exists():
                logger.info(f"âœ… {description} è·¯å¾‘å¯ç”¨: {path}")
            else:
                logger.warning(f"âš ï¸  {description} è·¯å¾‘ä¸å­˜åœ¨: {path}")
    
    async def get_precomputed_orbit_data(
        self,
        location: str = "ntpu",
        constellation: str = "starlink"
    ) -> Optional[Dict[str, Any]]:
        """
        å¾æœ¬åœ° Docker Volume ç²å–é è¨ˆç®—è»Œé“æ•¸æ“š
        å„ªå…ˆç´š: phase0 æ•¸æ“š > layered æ•¸æ“š > ç„¡æ•¸æ“š
        """
        try:
            # ä¸»è¦é è¨ˆç®—æ•¸æ“šæ–‡ä»¶
            main_data_file = self.netstack_data_path / "phase0_precomputed_orbits.json"
            
            if main_data_file.exists():
                logger.info(f"ğŸ“Š å¾æœ¬åœ° Volume è¼‰å…¥é è¨ˆç®—è»Œé“æ•¸æ“š: {main_data_file}")
                
                with open(main_data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # æª¢æŸ¥æ•¸æ“šå®Œæ•´æ€§
                if self._validate_precomputed_data(data):
                    logger.info(f"âœ… æˆåŠŸè¼‰å…¥ {len(data.get('orbit_points', []))} å€‹è»Œé“é»")
                    return data
                else:
                    logger.warning("âš ï¸  é è¨ˆç®—æ•¸æ“šæ ¼å¼é©—è­‰å¤±æ•—")
            else:
                logger.warning(f"ğŸ“Š ä¸»è¦é è¨ˆç®—æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {main_data_file}")
            
            # å˜—è©¦åˆ†å±¤æ•¸æ“šä½œç‚ºå‚™ç”¨
            layered_data_dir = self.netstack_data_path / "layered_phase0"
            if layered_data_dir.exists():
                return await self._load_layered_data(layered_data_dir, location, constellation)
            
            logger.error("âŒ ç„¡å¯ç”¨çš„æœ¬åœ°é è¨ˆç®—æ•¸æ“š")
            return None
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æœ¬åœ°é è¨ˆç®—æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    async def _load_layered_data(
        self,
        layered_dir: Path,
        location: str,
        constellation: str
    ) -> Optional[Dict[str, Any]]:
        """è¼‰å…¥åˆ†å±¤é–€æª»æ•¸æ“š"""
        try:
            # å°‹æ‰¾å°æ‡‰çš„åˆ†å±¤æ•¸æ“šæ–‡ä»¶
            pattern = f"{location}_{constellation}_*.json"
            data_files = list(layered_dir.glob(pattern))
            
            if data_files:
                # é¸æ“‡æœ€æ–°çš„æ–‡ä»¶
                latest_file = max(data_files, key=lambda p: p.stat().st_mtime)
                logger.info(f"ğŸ“Š å¾åˆ†å±¤æ•¸æ“šè¼‰å…¥: {latest_file}")
                
                with open(latest_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                if self._validate_precomputed_data(data):
                    return data
            
            logger.warning(f"âš ï¸  æœªæ‰¾åˆ° {location}_{constellation} çš„åˆ†å±¤æ•¸æ“š")
            return None
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥åˆ†å±¤æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    def _validate_precomputed_data(self, data: Dict[str, Any]) -> bool:
        """é©—è­‰é è¨ˆç®—æ•¸æ“šæ ¼å¼"""
        try:
            required_fields = ["orbit_points", "metadata"]
            for field in required_fields:
                if field not in data:
                    logger.warning(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                    return False
            
            orbit_points = data.get("orbit_points", [])
            if not orbit_points:
                logger.warning("orbit_points ç‚ºç©º")
                return False
            
            # æª¢æŸ¥ç¬¬ä¸€å€‹è»Œé“é»çš„æ ¼å¼
            first_point = orbit_points[0]
            point_fields = ["timestamp", "latitude", "longitude", "altitude_km"]
            for field in point_fields:
                if field not in first_point:
                    logger.warning(f"è»Œé“é»ç¼ºå°‘å­—æ®µ: {field}")
                    return False
            
            return True
            
        except Exception as e:
            logger.error(f"æ•¸æ“šé©—è­‰å¤±æ•—: {e}")
            return False
    
    async def get_local_tle_data(self, constellation: str = "starlink") -> List[Dict[str, Any]]:
        """
        å¾æœ¬åœ° Docker Volume ç²å– TLE æ•¸æ“š
        æ›¿ä»£ç›´æ¥ API èª¿ç”¨
        """
        try:
            # å°‹æ‰¾å°æ‡‰æ˜Ÿåº§çš„æœ€æ–° TLE æ•¸æ“š
            constellation_dir = self.netstack_tle_data_path / constellation
            
            if not constellation_dir.exists():
                logger.warning(f"æ˜Ÿåº§ç›®éŒ„ä¸å­˜åœ¨: {constellation_dir}")
                return []
            
            # æª¢æŸ¥ JSON æ ¼å¼æ•¸æ“š
            json_dir = constellation_dir / "json"
            if json_dir.exists():
                json_files = sorted(json_dir.glob(f"{constellation}_*.json"), reverse=True)
                if json_files:
                    latest_file = json_files[0]
                    logger.info(f"ğŸ“¡ å¾æœ¬åœ° Volume è¼‰å…¥ TLE æ•¸æ“š: {latest_file}")
                    
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        tle_data = json.load(f)
                    
                    logger.info(f"âœ… æˆåŠŸè¼‰å…¥ {len(tle_data)} é¡† {constellation} è¡›æ˜Ÿæ•¸æ“š")
                    return tle_data
            
            # æª¢æŸ¥ TLE æ ¼å¼æ•¸æ“š
            tle_dir = constellation_dir / "tle"
            if tle_dir.exists():
                tle_files = sorted(tle_dir.glob(f"{constellation}_*.tle"), reverse=True)
                if tle_files:
                    latest_file = tle_files[0]
                    logger.info(f"ğŸ“¡ å¾æœ¬åœ° Volume è§£æ TLE æ–‡ä»¶: {latest_file}")
                    
                    tle_data = await self._parse_tle_file(latest_file, constellation)
                    logger.info(f"âœ… è§£æå¾—åˆ° {len(tle_data)} é¡† {constellation} è¡›æ˜Ÿæ•¸æ“š")
                    return tle_data
            
            logger.warning(f"âŒ æœªæ‰¾åˆ° {constellation} çš„æœ¬åœ° TLE æ•¸æ“š")
            return []
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥æœ¬åœ° TLE æ•¸æ“šå¤±æ•—: {e}")
            return []
    
    async def _parse_tle_file(self, tle_file: Path, constellation: str) -> List[Dict[str, Any]]:
        """è§£æ TLE æ–‡ä»¶æ ¼å¼æ•¸æ“š"""
        try:
            tle_data = []
            
            with open(tle_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æŒ‰ç…§ TLE æ ¼å¼è§£æ (3è¡Œä¸€çµ„)
            for i in range(0, len(lines), 3):
                if i + 2 < len(lines):
                    name = lines[i].strip()
                    line1 = lines[i + 1].strip()
                    line2 = lines[i + 2].strip()
                    
                    if line1.startswith("1 ") and line2.startswith("2 "):
                        try:
                            norad_id = int(line1[2:7])
                            satellite_data = {
                                "name": name,
                                "norad_id": norad_id,
                                "line1": line1,
                                "line2": line2,
                                "constellation": constellation,
                                "source": "local_volume_tle_file",
                                "file_path": str(tle_file)
                            }
                            tle_data.append(satellite_data)
                        except (ValueError, IndexError) as e:
                            logger.warning(f"è§£æ TLE è¡Œå¤±æ•—: {e}")
                            continue
            
            return tle_data
            
        except Exception as e:
            logger.error(f"è§£æ TLE æ–‡ä»¶å¤±æ•—: {e}")
            return []
    
    async def check_data_freshness(self) -> Dict[str, Any]:
        """æª¢æŸ¥æœ¬åœ°æ•¸æ“šçš„æ–°é®®åº¦"""
        try:
            freshness_info = {
                "precomputed_data": None,
                "tle_data": {},
                "data_ready": False
            }
            
            # æª¢æŸ¥æ•¸æ“šå®Œæˆæ¨™è¨˜
            data_ready_file = self.netstack_data_path / ".data_ready"
            if data_ready_file.exists():
                freshness_info["data_ready"] = True
                
                # è®€å–æ•¸æ“šç”Ÿæˆæ™‚é–“
                try:
                    with open(data_ready_file, 'r') as f:
                        ready_info = f.read().strip()
                    freshness_info["data_ready_info"] = ready_info
                except:
                    pass
            
            # æª¢æŸ¥ä¸»è¦é è¨ˆç®—æ•¸æ“š
            main_data_file = self.netstack_data_path / "phase0_precomputed_orbits.json"
            if main_data_file.exists():
                stat = main_data_file.stat()
                freshness_info["precomputed_data"] = {
                    "file": str(main_data_file),
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "age_hours": (datetime.now().timestamp() - stat.st_mtime) / 3600
                }
            
            # æª¢æŸ¥ TLE æ•¸æ“š
            for constellation in ["starlink", "oneweb"]:
                constellation_dir = self.netstack_tle_data_path / constellation
                if constellation_dir.exists():
                    freshness_info["tle_data"][constellation] = {
                        "directory": str(constellation_dir),
                        "json_files": len(list((constellation_dir / "json").glob("*.json"))) if (constellation_dir / "json").exists() else 0,
                        "tle_files": len(list((constellation_dir / "tle").glob("*.tle"))) if (constellation_dir / "tle").exists() else 0
                    }
            
            return freshness_info
            
        except Exception as e:
            logger.error(f"æª¢æŸ¥æ•¸æ“šæ–°é®®åº¦å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def is_data_available(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„æœ¬åœ°æ•¸æ“š"""
        try:
            # æª¢æŸ¥ä¸»è¦æ•¸æ“šæ–‡ä»¶
            main_data_file = self.netstack_data_path / "phase0_precomputed_orbits.json"
            if main_data_file.exists() and main_data_file.stat().st_size > 0:
                return True
            
            # æª¢æŸ¥åˆ†å±¤æ•¸æ“š
            layered_data_dir = self.netstack_data_path / "layered_phase0"
            if layered_data_dir.exists():
                json_files = list(layered_data_dir.glob("*.json"))
                if json_files:
                    return True
            
            # æª¢æŸ¥ TLE æ•¸æ“š
            for constellation in ["starlink", "oneweb"]:
                constellation_dir = self.netstack_tle_data_path / constellation
                if constellation_dir.exists():
                    if (constellation_dir / "json").exists() or (constellation_dir / "tle").exists():
                        return True
            
            return False
            
        except Exception as e:
            logger.error(f"æª¢æŸ¥æ•¸æ“šå¯ç”¨æ€§å¤±æ•—: {e}")
            return False

    async def generate_120min_timeseries(
        self,
        constellation: str = "starlink",
        reference_location: Optional[Dict[str, float]] = None
    ) -> Optional[Dict[str, Any]]:
        """
        ç”Ÿæˆ 120 åˆ†é˜çµ±ä¸€æ™‚é–“åºåˆ—æ•¸æ“š
        ç›´æ¥å¾ Docker Volume è™•ç†ï¼Œç„¡éœ€ NetStack API
        """
        try:
            if reference_location is None:
                reference_location = {
                    "latitude": 24.9441,   # å°åŒ—ç§‘æŠ€å¤§å­¸
                    "longitude": 121.3714,
                    "altitude": 0.0
                }
            
            # è¼‰å…¥æœ¬åœ° TLE æ•¸æ“š
            tle_data = await self.get_local_tle_data(constellation)
            if not tle_data:
                logger.error(f"âŒ ç„¡å¯ç”¨çš„ {constellation} TLE æ•¸æ“š")
                return None
            
            logger.info(f"ğŸ›°ï¸ é–‹å§‹ç”Ÿæˆ {constellation} 120åˆ†é˜æ™‚é–“åºåˆ—æ•¸æ“š")
            logger.info(f"ğŸ“ åƒè€ƒä½ç½®: {reference_location['latitude']:.4f}Â°N, {reference_location['longitude']:.4f}Â°E")
            
            # ç•¶å‰æ™‚é–“ä½œç‚ºèµ·å§‹é»
            from datetime import datetime, timezone, timedelta
            start_time = datetime.now(timezone.utc)
            
            satellites_timeseries = []
            
            # åªè™•ç†å‰10é¡†è¡›æ˜Ÿä»¥æé«˜æ€§èƒ½ (å¯æ ¹æ“šéœ€è¦èª¿æ•´)
            selected_satellites = tle_data[:10]
            logger.info(f"ğŸ“Š è™•ç† {len(selected_satellites)} é¡†è¡›æ˜Ÿçš„è»Œé“æ•¸æ“š")
            
            for i, sat_data in enumerate(selected_satellites):
                logger.info(f"ğŸ”„ è™•ç†è¡›æ˜Ÿ {i+1}/{len(selected_satellites)}: {sat_data.get('name', 'Unknown')}")
                
                satellite_timeseries = await self._calculate_satellite_120min_timeseries(
                    sat_data, start_time, reference_location
                )
                
                if satellite_timeseries:
                    satellites_timeseries.append({
                        "norad_id": sat_data.get("norad_id", 0),
                        "name": sat_data.get("name", "Unknown"),
                        "constellation": constellation,
                        "time_series": satellite_timeseries
                    })
            
            # ç”Ÿæˆ UE è»Œè·¡ (éœæ…‹ UE)
            ue_trajectory = []
            for i in range(self.total_time_points):
                current_time = start_time + timedelta(seconds=i * self.time_interval_seconds)
                ue_trajectory.append({
                    "time_offset_seconds": i * self.time_interval_seconds,
                    "position": reference_location.copy(),
                    "serving_satellite": satellites_timeseries[0]["name"] if satellites_timeseries else "None",
                    "handover_state": "stable"
                })
            
            # æ§‹å»ºçµ±ä¸€æ™‚é–“åºåˆ—æ•¸æ“š
            unified_data = {
                "metadata": {
                    "computation_time": start_time.isoformat(),
                    "constellation": constellation,
                    "time_span_minutes": self.time_span_minutes,
                    "time_interval_seconds": self.time_interval_seconds,
                    "total_time_points": self.total_time_points,
                    "data_source": "local_docker_volume_direct",
                    "network_dependency": False,
                    "reference_location": reference_location,
                    "satellites_processed": len(satellites_timeseries)
                },
                "satellites": satellites_timeseries,
                "ue_trajectory": ue_trajectory,
                "handover_events": []  # æš«æ™‚ç‚ºç©ºï¼Œå¾ŒçºŒå¯æ“´å±•
            }
            
            logger.info(f"âœ… æˆåŠŸç”Ÿæˆ 120åˆ†é˜æ™‚é–“åºåˆ—æ•¸æ“š: {len(satellites_timeseries)} é¡†è¡›æ˜Ÿ, {self.total_time_points} æ™‚é–“é»")
            return unified_data
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆ 120åˆ†é˜æ™‚é–“åºåˆ—æ•¸æ“šå¤±æ•—: {e}")
            return None
    
    async def _calculate_satellite_120min_timeseries(
        self,
        sat_data: Dict[str, Any],
        start_time: datetime,
        reference_location: Dict[str, float]
    ) -> List[Dict[str, Any]]:
        """è¨ˆç®—å–®é¡†è¡›æ˜Ÿçš„ 120 åˆ†é˜æ™‚é–“åºåˆ—"""
        try:
            from datetime import timedelta
            import math
            
            time_series = []
            current_time = start_time
            
            # æå– TLE æ•¸æ“š
            line1 = sat_data.get("line1", "")
            line2 = sat_data.get("line2", "")
            
            # ç°¡åŒ–çš„è»Œé“è¨ˆç®— (åŸºæ–¼åœ“è»Œé“è¿‘ä¼¼)
            # åœ¨å¯¦éš›å¯¦æ–½ä¸­ï¼Œæ‡‰ä½¿ç”¨ SGP4 é€²è¡Œç²¾ç¢ºè¨ˆç®—
            try:
                # å¾ TLE line2 æå–å¹³å‡é‹å‹• (æ¯æ—¥ç¹è¡Œæ¬¡æ•¸)
                mean_motion = float(line2[52:63]) if len(line2) > 63 else 15.5
                orbital_period_minutes = 1440 / mean_motion  # è»Œé“é€±æœŸ(åˆ†é˜)
                
                # è»Œé“å‚¾è§’
                inclination = float(line2[8:16]) if len(line2) > 16 else 53.0
                
                # è»Œé“é«˜åº¦ä¼°ç®— (åŸºæ–¼å¹³å‡é‹å‹•)
                altitude_km = 550.0  # Starlink å…¸å‹é«˜åº¦
                
            except (ValueError, IndexError):
                # Fallback å€¼
                mean_motion = 15.5
                orbital_period_minutes = 96.0
                inclination = 53.0
                altitude_km = 550.0
            
            for i in range(self.total_time_points):
                # æ™‚é–“é€²åº¦
                time_offset = i * self.time_interval_seconds
                progress = (time_offset / 60) / orbital_period_minutes  # è»Œé“é€²åº¦æ¯”ä¾‹
                
                # ç°¡åŒ–çš„ä½ç½®è¨ˆç®— (åœ“è»Œé“è¿‘ä¼¼)
                orbital_angle = (progress * 360) % 360  # è»Œé“è§’åº¦
                
                # ç·¯åº¦è®ŠåŒ– (åŸºæ–¼è»Œé“å‚¾è§’)
                latitude = inclination * math.sin(math.radians(orbital_angle))
                longitude = (orbital_angle - 180) % 360 - 180  # -180 åˆ° 180
                
                # è¨ˆç®—èˆ‡åƒè€ƒä½ç½®çš„è·é›¢å’Œè§’åº¦
                lat_diff = latitude - reference_location["latitude"]
                lon_diff = longitude - reference_location["longitude"]
                
                # åœ°é¢è·é›¢ä¼°ç®— (çƒé¢è·é›¢å…¬å¼ç°¡åŒ–ç‰ˆ)
                ground_distance_km = math.sqrt(lat_diff**2 + lon_diff**2) * 111.32  # 1åº¦â‰ˆ111.32km
                
                # 3D è·é›¢ (åŒ…å«é«˜åº¦)
                satellite_distance_km = math.sqrt(ground_distance_km**2 + altitude_km**2)
                
                # ä»°è§’è¨ˆç®—
                elevation_deg = max(0, 90 - math.degrees(math.atan2(ground_distance_km, altitude_km)))
                
                # æ–¹ä½è§’ç°¡åŒ–è¨ˆç®—
                azimuth_deg = (math.degrees(math.atan2(lon_diff, lat_diff)) + 360) % 360
                
                # å¯è¦‹æ€§åˆ¤æ–· (ä»°è§’ > 10åº¦)
                is_visible = elevation_deg > 10.0
                
                # RSRP ä¼°ç®— (åŸºæ–¼è·é›¢çš„ç°¡åŒ–æ¨¡å‹)
                rsrp_dbm = -70 - 20 * math.log10(satellite_distance_km / 500) if satellite_distance_km > 0 else -70
                rsrp_dbm = max(-120, min(-50, rsrp_dbm))  # é™åˆ¶åœ¨åˆç†ç¯„åœ
                
                time_point = {
                    "time_offset_seconds": time_offset,
                    "timestamp": (current_time + timedelta(seconds=time_offset)).isoformat(),
                    "position": {
                        "latitude": latitude,
                        "longitude": longitude,
                        "altitude": altitude_km * 1000,  # è½‰æ›ç‚ºç±³
                        "velocity": {"x": 7.8, "y": 0.0, "z": 0.0}  # ç°¡åŒ–é€Ÿåº¦
                    },
                    "observation": {
                        "elevation_deg": elevation_deg,
                        "azimuth_deg": azimuth_deg,
                        "range_km": satellite_distance_km,
                        "is_visible": is_visible,
                        "rsrp_dbm": rsrp_dbm,
                        "rsrq_db": -12.0,  # å›ºå®šå€¼
                        "sinr_db": 18.0    # å›ºå®šå€¼
                    },
                    "handover_metrics": {
                        "signal_strength": max(0, (rsrp_dbm + 120) / 70),  # æ­¸ä¸€åŒ–
                        "handover_score": 0.8 if is_visible else 0.1,
                        "is_handover_candidate": is_visible and elevation_deg > 15,
                        "predicted_service_time_seconds": 300 if is_visible else 0
                    },
                    "measurement_events": {
                        "d1_distance_m": ground_distance_km * 1000,
                        "d2_satellite_distance_m": satellite_distance_km * 1000,
                        "d2_ground_distance_m": ground_distance_km * 1000,
                        "a4_trigger_condition": rsrp_dbm > -90,
                        "t1_time_condition": True
                    }
                }
                
                time_series.append(time_point)
            
            return time_series
            
        except Exception as e:
            logger.error(f"âŒ è¨ˆç®—è¡›æ˜Ÿæ™‚é–“åºåˆ—å¤±æ•—: {e}")
            return []


# å…¨å±€å¯¦ä¾‹
_local_volume_service = None


def get_local_volume_service() -> LocalVolumeDataService:
    """ç²å–æœ¬åœ° Volume æ•¸æ“šæœå‹™å¯¦ä¾‹"""
    global _local_volume_service
    if _local_volume_service is None:
        _local_volume_service = LocalVolumeDataService()
    return _local_volume_service