# 03 - Phase 2: æ•¸æ“šé è¨ˆç®—å¼•æ“é–‹ç™¼

> **ä¸Šä¸€éšæ®µ**ï¼š[Phase 1 - æ•¸æ“šåº«è¨­ç½®](./02-phase1-database-setup.md)  < /dev/null |  **ä¸‹ä¸€éšæ®µ**ï¼š[Phase 3 - API ç«¯é»](./04-phase3-api-endpoints.md)

## ğŸ¯ Phase 2 ç›®æ¨™
**ç›®æ¨™**ï¼šé–‹ç™¼æ­·å²æ•¸æ“šé è¨ˆç®—å™¨ï¼Œå¯¦ç¾çœŸå¯¦ TLE æ•¸æ“šçš„ SGP4 è»Œé“è¨ˆç®—å’Œæ‰¹æ¬¡å­˜å„²
**é ä¼°æ™‚é–“**: 2-3 å¤©

## ğŸ“‹ é–‹ç™¼ä»»å‹™

### 2.1 æ­·å²æ•¸æ“šé è¨ˆç®—å™¨

#### **å®Œæ•´çš„é è¨ˆç®—å™¨å¯¦ç¾**
```python
# precompute_satellite_history.py
import math
from datetime import datetime, timedelta
from skyfield.api import load, wgs84, EarthSatellite
from typing import List, Dict, Tuple

class SatelliteHistoryPrecomputer:
    def __init__(self, tle_file_path, observer_coords, time_range):
        self.tle_data = self.load_tle_data(tle_file_path)
        self.observer = wgs84.latlon(*observer_coords)
        self.time_range = time_range
        
    def compute_history(self, time_interval_seconds=30):
        """é è¨ˆç®—æŒ‡å®šæ™‚é–“ç¯„åœå…§çš„æ‰€æœ‰è¡›æ˜Ÿä½ç½®"""
        results = []
        
        start_time = self.time_range[0] 
        end_time = self.time_range[1]
        
        current_time = start_time
        while current_time <= end_time:
            # è¨ˆç®—æ‰€æœ‰è¡›æ˜Ÿåœ¨ç•¶å‰æ™‚é–“çš„ä½ç½®
            visible_satellites = self.compute_visible_satellites(current_time)
            results.extend(visible_satellites)
            
            current_time += timedelta(seconds=time_interval_seconds)
            
        return results
        
    def compute_visible_satellites(self, timestamp):
        """è¨ˆç®—æŒ‡å®šæ™‚é–“æ‰€æœ‰å¯è¦‹è¡›æ˜Ÿä½ç½®"""
        visible = []
        ts = load.timescale()
        t = ts.from_datetime(timestamp)
        
        for satellite_data in self.tle_data:
            satellite = EarthSatellite(satellite_data.line1, satellite_data.line2, satellite_data.name, ts)
            
            # SGP4 è»Œé“è¨ˆç®—
            difference = satellite - self.observer
            topocentric = difference.at(t)
            alt, az, distance = topocentric.altaz()
            
            if alt.degrees >= -10:  # å¯è¦‹æ€§é–¾å€¼
                geocentric = satellite.at(t)
                subpoint = wgs84.subpoint(geocentric)
                
                position_data = {
                    "timestamp": timestamp,
                    "satellite_id": satellite_data.norad_id,
                    "satellite_name": satellite_data.name,
                    "observer_position": {
                        "latitude": self.observer.latitude.degrees,
                        "longitude": self.observer.longitude.degrees,
                        "altitude": self.observer.elevation.km * 1000
                    },
                    "satellite_position": {
                        "latitude": subpoint.latitude.degrees,
                        "longitude": subpoint.longitude.degrees,
                        "altitude": subpoint.elevation.km,
                        "elevation": alt.degrees,
                        "azimuth": az.degrees,
                        "range": distance.km,
                        "velocity": 7.5  # LEO å¹³å‡é€Ÿåº¦
                    },
                    "signal_quality": {
                        "path_loss_db": 120 + 20 * math.log10(distance.km),
                        "signal_strength": max(30, 80 - distance.km / 25)
                    }
                }
                
                visible.append(position_data)
                
        return visible
        
    def load_tle_data(self, tle_file_path):
        """è¼‰å…¥ TLE æ•¸æ“šæ–‡ä»¶"""
        with open(tle_file_path, 'r') as f:
            lines = f.readlines()
            
        tle_data = []
        for i in range(0, len(lines), 3):
            if i + 2 < len(lines):
                name = lines[i].strip()
                line1 = lines[i + 1].strip()
                line2 = lines[i + 2].strip()
                
                # æå– NORAD ID
                norad_id = int(line1[2:7])
                
                tle_data.append({
                    'name': name,
                    'line1': line1,
                    'line2': line2,
                    'norad_id': norad_id
                })
                
        return tle_data
```

#### **TLE æ•¸æ“šä¸‹è¼‰å™¨**
```python
# app/services/tle_downloader.py
import aiohttp
import asyncio
import logging
from datetime import datetime
from typing import Dict, List

class TLEDataDownloader:
    """çœŸå¯¦ TLE æ•¸æ“šä¸‹è¼‰å™¨"""
    
    def __init__(self):
        self.tle_sources = {
            "starlink": "https://celestrak.org/NORAD/elements/gp.php?GROUP=starlink&FORMAT=tle",
            "oneweb": "https://celestrak.org/NORAD/elements/gp.php?GROUP=oneweb&FORMAT=tle"
        }
        
    async def download_constellation_data(self, constellation: str) -> Dict:
        """ä¸‹è¼‰æŒ‡å®šæ˜Ÿåº§çš„ TLE æ•¸æ“š"""
        if constellation not in self.tle_sources:
            raise ValueError(f"ä¸æ”¯æ´çš„æ˜Ÿåº§: {constellation}")
            
        url = self.tle_sources[constellation]
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=30) as response:
                    if response.status == 200:
                        tle_content = await response.text()
                        parsed_data = self._parse_tle_content(tle_content, constellation)
                        
                        logging.info(f"âœ… æˆåŠŸä¸‹è¼‰ {constellation} TLE æ•¸æ“šï¼š{len(parsed_data)} é¡†è¡›æ˜Ÿ")
                        
                        return {
                            "constellation": constellation,
                            "download_time": datetime.utcnow(),
                            "satellite_count": len(parsed_data),
                            "tle_data": parsed_data
                        }
                    else:
                        raise Exception(f"HTTP {response.status}: {await response.text()}")
                        
        except Exception as e:
            logging.error(f"âŒ ä¸‹è¼‰ {constellation} TLE æ•¸æ“šå¤±æ•—: {e}")
            raise
            
    def _parse_tle_content(self, content: str, constellation: str) -> List[Dict]:
        """è§£æ TLE æ ¼å¼æ•¸æ“š"""
        lines = content.strip().split('\n')
        satellites = []
        
        for i in range(0, len(lines), 3):
            if i + 2 < len(lines):
                name = lines[i].strip()
                line1 = lines[i + 1].strip()
                line2 = lines[i + 2].strip()
                
                # é©—è­‰ TLE æ ¼å¼
                if len(line1) == 69 and len(line2) == 69 and line1[0] == '1' and line2[0] == '2':
                    norad_id = int(line1[2:7])
                    epoch = self._parse_epoch(line1[18:32])
                    
                    satellites.append({
                        "name": name,
                        "norad_id": norad_id,
                        "constellation": constellation,
                        "line1": line1,
                        "line2": line2,
                        "epoch": epoch,
                        "is_active": True
                    })
                    
        return satellites
        
    def _parse_epoch(self, epoch_str: str) -> datetime:
        """è§£æ TLE epoch æ™‚é–“"""
        year = int(epoch_str[:2])
        if year < 57:
            year += 2000
        else:
            year += 1900
            
        day_of_year = float(epoch_str[2:])
        
        # è½‰æ›ç‚º datetime
        base_date = datetime(year, 1, 1)
        epoch_date = base_date + timedelta(days=day_of_year - 1)
        
        return epoch_date
    
    async def download_multiple_constellations(self, constellations: List[str]) -> Dict:
        """ä¸¦è¡Œä¸‹è¼‰å¤šå€‹æ˜Ÿåº§æ•¸æ“š"""
        tasks = [self.download_constellation_data(const) for const in constellations]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        successful_downloads = {}
        failed_downloads = {}
        
        for i, result in enumerate(results):
            constellation = constellations[i]
            if isinstance(result, Exception):
                failed_downloads[constellation] = str(result)
            else:
                successful_downloads[constellation] = result
                
        return {
            "successful": successful_downloads,
            "failed": failed_downloads,
            "total_satellites": sum(data["satellite_count"] for data in successful_downloads.values())
        }
```

### 2.2 æ‰¹æ¬¡è™•ç†å’Œå­˜å„²

#### **å®Œæ•´çš„æ‰¹æ¬¡è™•ç†å™¨å¯¦ç¾**
```python
# batch_processor.py
import asyncpg
import logging
from typing import List, Dict
from datetime import datetime

class HistoryBatchProcessor:
    def __init__(self, postgres_url: str):
        self.postgres_url = postgres_url
        
    async def process_and_store(self, precomputed_data: List[Dict]):
        """æ‰¹æ¬¡å­˜å„²é è¨ˆç®—æ•¸æ“šåˆ° PostgreSQL"""
        batch_size = 1000
        
        conn = await asyncpg.connect(self.postgres_url)
        try:
            for i in range(0, len(precomputed_data), batch_size):
                batch = precomputed_data[i:i + batch_size]
                
                # æ‰¹æ¬¡æ’å…¥åˆ° satellite_orbital_cache
                await conn.executemany("""
                    INSERT INTO satellite_orbital_cache (
                        satellite_id, norad_id, constellation, timestamp,
                        position_x, position_y, position_z,
                        latitude, longitude, altitude,
                        elevation_angle, azimuth_angle, range_rate,
                        observer_latitude, observer_longitude, observer_altitude,
                        signal_strength, path_loss_db,
                        calculation_method, data_quality
                    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20)
                    ON CONFLICT (satellite_id, timestamp) DO UPDATE SET
                        elevation_angle = EXCLUDED.elevation_angle,
                        azimuth_angle = EXCLUDED.azimuth_angle,
                        signal_strength = EXCLUDED.signal_strength,
                        path_loss_db = EXCLUDED.path_loss_db
                """, [self._prepare_record(record) for record in batch])
                
                logging.info(f"å·²å­˜å„² {i + len(batch)}/{len(precomputed_data)} æ¢è¨˜éŒ„åˆ° PostgreSQL")
                
        finally:
            await conn.close()
            
    def _prepare_record(self, record: Dict) -> tuple:
        """æº–å‚™ PostgreSQL æ’å…¥è¨˜éŒ„"""
        return (
            record["satellite_id"], record["norad_id"], record["constellation"],
            record["timestamp"], record["position_x"], record["position_y"], record["position_z"],
            record["satellite_position"]["latitude"], record["satellite_position"]["longitude"], 
            record["satellite_position"]["altitude"], record["satellite_position"]["elevation"],
            record["satellite_position"]["azimuth"], record["satellite_position"]["range_rate"],
            record["observer_position"]["latitude"], record["observer_position"]["longitude"], 
            record["observer_position"]["altitude"], record["signal_quality"]["signal_strength"],
            record["signal_quality"]["path_loss_db"], "SGP4", 1.0
        )
        
    async def incremental_update(self, new_data: List[Dict]):
        """å¢é‡æ›´æ–°æ•¸æ“šï¼ˆç”¨æ–¼èƒŒæ™¯æ›´æ–°ï¼‰"""
        if not new_data:
            logging.info("â­ï¸ æ²’æœ‰æ–°æ•¸æ“šéœ€è¦æ›´æ–°")
            return
            
        conn = await asyncpg.connect(self.postgres_url)
        try:
            # ç²å–ç¾æœ‰æ•¸æ“šçš„æ™‚é–“ç¯„åœ
            latest_time = await conn.fetchval("""
                SELECT MAX(timestamp) FROM satellite_orbital_cache 
                WHERE observer_latitude = $1 AND observer_longitude = $2
            """, new_data[0]["observer_position"]["latitude"], 
                 new_data[0]["observer_position"]["longitude"])
            
            if latest_time:
                # åªæ’å…¥æ¯”ç¾æœ‰æ•¸æ“šæ›´æ–°çš„è¨˜éŒ„
                new_records = [
                    record for record in new_data 
                    if record["timestamp"] > latest_time.replace(tzinfo=None)
                ]
                
                if new_records:
                    await self.process_and_store(new_records)
                    logging.info(f"âœ… å¢é‡æ›´æ–° {len(new_records)} æ¢æ–°è¨˜éŒ„")
                else:
                    logging.info("â­ï¸ æ•¸æ“šå·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€æ›´æ–°")
            else:
                # æ²’æœ‰ç¾æœ‰æ•¸æ“šï¼Œå…¨éƒ¨æ’å…¥
                await self.process_and_store(new_data)
                logging.info(f"âœ… åˆæ¬¡è¼‰å…¥ {len(new_data)} æ¢è¨˜éŒ„")
                
        finally:
            await conn.close()
            
    async def cleanup_old_data(self, cutoff_date: datetime, observer_coords: tuple):
        """æ¸…ç†éæœŸæ•¸æ“šï¼ˆä¿ç•™æŒ‡å®šæ—¥æœŸä¹‹å¾Œçš„æ•¸æ“šï¼‰"""
        conn = await asyncpg.connect(self.postgres_url)
        try:
            deleted_count = await conn.fetchval("""
                DELETE FROM satellite_orbital_cache 
                WHERE timestamp < $1 
                  AND observer_latitude = $2 
                  AND observer_longitude = $3
                RETURNING COUNT(*)
            """, cutoff_date, observer_coords[0], observer_coords[1])
            
            logging.info(f"ğŸ—‘ï¸ æ¸…ç†äº† {deleted_count or 0} æ¢éæœŸæ•¸æ“šï¼ˆ{cutoff_date} ä¹‹å‰ï¼‰")
            return deleted_count or 0
            
        finally:
            await conn.close()
            
    async def get_data_statistics(self, observer_coords: tuple) -> Dict:
        """ç²å–æ•¸æ“šçµ±è¨ˆä¿¡æ¯"""
        conn = await asyncpg.connect(self.postgres_url)
        try:
            stats = await conn.fetchrow("""
                SELECT 
                    COUNT(*) as total_records,
                    COUNT(DISTINCT satellite_id) as unique_satellites,
                    COUNT(DISTINCT constellation) as constellations,
                    MIN(timestamp) as earliest_time,
                    MAX(timestamp) as latest_time,
                    AVG(signal_strength) as avg_signal_strength,
                    AVG(elevation_angle) as avg_elevation
                FROM satellite_orbital_cache 
                WHERE observer_latitude = $1 AND observer_longitude = $2
            """, observer_coords[0], observer_coords[1])
            
            if stats and stats['total_records'] > 0:
                duration_hours = (
                    stats['latest_time'] - stats['earliest_time']
                ).total_seconds() / 3600 if stats['latest_time'] and stats['earliest_time'] else 0
                
                return {
                    "total_records": stats['total_records'],
                    "unique_satellites": stats['unique_satellites'],
                    "constellations": stats['constellations'],
                    "time_range": {
                        "start": stats['earliest_time'].isoformat() if stats['earliest_time'] else None,
                        "end": stats['latest_time'].isoformat() if stats['latest_time'] else None,
                        "duration_hours": round(duration_hours, 2)
                    },
                    "signal_quality": {
                        "avg_signal_strength": round(stats['avg_signal_strength'], 2) if stats['avg_signal_strength'] else 0,
                        "avg_elevation": round(stats['avg_elevation'], 2) if stats['avg_elevation'] else 0
                    }
                }
            else:
                return {
                    "total_records": 0,
                    "message": "ç„¡æ­·å²æ•¸æ“š"
                }
                
        finally:
            await conn.close()
```

#### **é è¨ˆç®—ä½œæ¥­ç®¡ç†å™¨**
```python
# app/services/precompute_job_manager.py
import asyncio
import logging
import uuid
from datetime import datetime, timedelta
from typing import Dict, Optional
from enum import Enum

class JobStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

class PrecomputeJobManager:
    """é è¨ˆç®—ä½œæ¥­ç®¡ç†å™¨"""
    
    def __init__(self):
        self.active_jobs: Dict[str, Dict] = {}
        
    async def create_precompute_job(
        self, 
        constellation: str,
        start_time: datetime,
        end_time: datetime,
        observer_coords: tuple,
        time_step_seconds: int = 30
    ) -> str:
        """å‰µå»ºé è¨ˆç®—ä½œæ¥­"""
        
        job_id = f"precompute_{constellation}_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        # ä¼°ç®—ç¸½è¨ˆç®—é‡
        duration_seconds = (end_time - start_time).total_seconds()
        total_timepoints = int(duration_seconds / time_step_seconds)
        estimated_duration_minutes = max(1, total_timepoints / 1000)  # ä¼°ç®—ï¼š1000 å€‹æ™‚é–“é»/åˆ†é˜
        
        job_info = {
            "job_id": job_id,
            "constellation": constellation,
            "start_time": start_time,
            "end_time": end_time,
            "observer_coords": observer_coords,
            "time_step_seconds": time_step_seconds,
            "total_calculations": total_timepoints,
            "estimated_duration_minutes": estimated_duration_minutes,
            "status": JobStatus.PENDING,
            "created_at": datetime.utcnow(),
            "progress": 0.0,
            "error_message": None
        }
        
        self.active_jobs[job_id] = job_info
        
        # ç•°æ­¥å•Ÿå‹•ä½œæ¥­
        asyncio.create_task(self._execute_precompute_job(job_id))
        
        logging.info(f"ğŸ“‹ å‰µå»ºé è¨ˆç®—ä½œæ¥­ {job_id}ï¼š{constellation} æ˜Ÿåº§ï¼Œ{total_timepoints} å€‹è¨ˆç®—é»")
        
        return job_id
        
    async def get_job_status(self, job_id: str) -> Optional[Dict]:
        """ç²å–ä½œæ¥­ç‹€æ…‹"""
        if job_id in self.active_jobs:
            job = self.active_jobs[job_id].copy()
            job['status'] = job['status'].value
            
            # ç§»é™¤å…§éƒ¨ä½¿ç”¨çš„å°è±¡
            if 'start_time' in job:
                job['start_time'] = job['start_time'].isoformat()
            if 'end_time' in job:
                job['end_time'] = job['end_time'].isoformat()
            if 'created_at' in job:
                job['created_at'] = job['created_at'].isoformat()
                
            return job
        return None
        
    async def cancel_job(self, job_id: str) -> bool:
        """å–æ¶ˆä½œæ¥­"""
        if job_id in self.active_jobs:
            job = self.active_jobs[job_id]
            if job['status'] in [JobStatus.PENDING, JobStatus.RUNNING]:
                job['status'] = JobStatus.CANCELLED
                logging.info(f"âŒ å–æ¶ˆé è¨ˆç®—ä½œæ¥­ {job_id}")
                return True
        return False
        
    async def _execute_precompute_job(self, job_id: str):
        """åŸ·è¡Œé è¨ˆç®—ä½œæ¥­"""
        job = self.active_jobs[job_id]
        
        try:
            job['status'] = JobStatus.RUNNING
            job['started_at'] = datetime.utcnow()
            
            logging.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œé è¨ˆç®—ä½œæ¥­ {job_id}")
            
            # ä¸‹è¼‰ TLE æ•¸æ“š
            from .tle_downloader import TLEDataDownloader
            downloader = TLEDataDownloader()
            tle_data = await downloader.download_constellation_data(job['constellation'])
            
            # åŸ·è¡Œé è¨ˆç®—
            from ..precompute_satellite_history import SatelliteHistoryPrecomputer
            precomputer = SatelliteHistoryPrecomputer(
                tle_data=tle_data['tle_data'],
                observer_coords=job['observer_coords'],
                time_range=(job['start_time'], job['end_time'])
            )
            
            # åˆ†æ‰¹è¨ˆç®—ä»¥ä¾¿æ›´æ–°é€²åº¦
            batch_duration = timedelta(hours=1)  # æ¯æ¬¡è¨ˆç®— 1 å°æ™‚çš„æ•¸æ“š
            current_time = job['start_time']
            all_results = []
            
            while current_time < job['end_time']:
                if job['status'] == JobStatus.CANCELLED:
                    logging.info(f"â¹ï¸ ä½œæ¥­ {job_id} å·²è¢«å–æ¶ˆ")
                    return
                    
                batch_end = min(current_time + batch_duration, job['end_time'])
                
                # è¨ˆç®—ç•¶å‰æ‰¹æ¬¡
                batch_precomputer = SatelliteHistoryPrecomputer(
                    tle_data=tle_data['tle_data'],
                    observer_coords=job['observer_coords'],
                    time_range=(current_time, batch_end)
                )
                
                batch_results = batch_precomputer.compute_history(job['time_step_seconds'])
                all_results.extend(batch_results)
                
                # æ›´æ–°é€²åº¦
                progress = (current_time - job['start_time']).total_seconds() / (job['end_time'] - job['start_time']).total_seconds()
                job['progress'] = min(progress * 100, 100.0)
                
                logging.info(f"ğŸ“Š ä½œæ¥­ {job_id} é€²åº¦: {job['progress']:.1f}%")
                
                current_time = batch_end
                
            # å­˜å„²çµæœ
            from .batch_processor import HistoryBatchProcessor
            processor = HistoryBatchProcessor(os.getenv('RL_DATABASE_URL'))
            await processor.process_and_store(all_results)
            
            # ä½œæ¥­å®Œæˆ
            job['status'] = JobStatus.COMPLETED
            job['completed_at'] = datetime.utcnow()
            job['progress'] = 100.0
            job['total_records'] = len(all_results)
            
            logging.info(f"âœ… é è¨ˆç®—ä½œæ¥­ {job_id} å®Œæˆï¼šè™•ç†äº† {len(all_results)} æ¢è¨˜éŒ„")
            
        except Exception as e:
            job['status'] = JobStatus.FAILED
            job['error_message'] = str(e)
            job['failed_at'] = datetime.utcnow()
            
            logging.error(f"âŒ é è¨ˆç®—ä½œæ¥­ {job_id} å¤±æ•—: {e}")
            
    def cleanup_completed_jobs(self, max_age_hours: int = 24):
        """æ¸…ç†å®Œæˆçš„ä½œæ¥­è¨˜éŒ„"""
        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)
        
        jobs_to_remove = []
        for job_id, job in self.active_jobs.items():
            if job['status'] in [JobStatus.COMPLETED, JobStatus.FAILED, JobStatus.CANCELLED]:
                job_time = job.get('completed_at') or job.get('failed_at') or job['created_at']
                if job_time < cutoff_time:
                    jobs_to_remove.append(job_id)
                    
        for job_id in jobs_to_remove:
            del self.active_jobs[job_id]
            
        if jobs_to_remove:
            logging.info(f"ğŸ—‘ï¸ æ¸…ç†äº† {len(jobs_to_remove)} å€‹éæœŸä½œæ¥­è¨˜éŒ„")

# å…¨åŸŸä½œæ¥­ç®¡ç†å™¨å¯¦ä¾‹
job_manager = PrecomputeJobManager()
```

## ğŸ“‹ å¯¦æ–½æª¢æŸ¥æ¸…å–®

### **æ ¸å¿ƒçµ„ä»¶å¯¦ç¾æª¢æŸ¥**
- [ ] å¯¦ç¾å®Œæ•´çš„ SatelliteHistoryPrecomputer é¡
- [ ] å¯¦ç¾å®Œæ•´çš„ HistoryBatchProcessor é¡  
- [ ] å¯¦ç¾ TLEDataDownloader æ•¸æ“šä¸‹è¼‰å™¨
- [ ] å¯¦ç¾ PrecomputeJobManager ä½œæ¥­ç®¡ç†å™¨
- [ ] SGP4 è»Œé“è¨ˆç®—é›†æˆå’Œä¿¡è™Ÿå¼·åº¦è¨ˆç®—
- [ ] æ‰¹æ¬¡å­˜å„²å„ªåŒ–å’Œè¡çªè™•ç†

### **åŠŸèƒ½ç‰¹æ€§æª¢æŸ¥**
- [ ] TLE æ•¸æ“šä¸‹è¼‰å’Œæ ¼å¼é©—è­‰
- [ ] å¤šæ˜Ÿåº§ä¸¦è¡Œä¸‹è¼‰æ”¯æ´
- [ ] é è¨ˆç®—ä½œæ¥­é€²åº¦è¿½è¹¤
- [ ] å¢é‡æ•¸æ“šæ›´æ–°æ©Ÿåˆ¶
- [ ] éæœŸæ•¸æ“šè‡ªå‹•æ¸…ç†
- [ ] æ•¸æ“šçµ±è¨ˆå’Œç›£æ§åŠŸèƒ½

### **éŒ¯èª¤è™•ç†æª¢æŸ¥**
- [ ] TLE ä¸‹è¼‰å¤±æ•—è™•ç†
- [ ] é è¨ˆç®—ä½œæ¥­å–æ¶ˆæ©Ÿåˆ¶
- [ ] æ•¸æ“šåº«é€£æ¥éŒ¯èª¤è™•ç†
- [ ] è¨˜æ†¶é«”ä¸è¶³ä¿è­·æªæ–½

## ğŸ§ª é©—è­‰æ­¥é©Ÿ

### **2.1 TLE æ•¸æ“šä¸‹è¼‰é©—è­‰**
```bash
# 1. æ¸¬è©¦å–®ä¸€æ˜Ÿåº§ TLE æ•¸æ“šç²å–
curl -X POST "http://localhost:8080/api/v1/satellites/tle/download" \
  -H "Content-Type: application/json" \
  -d '{
    "constellations": ["starlink"],
    "force_update": false
  }' | jq

# é æœŸéŸ¿æ‡‰ï¼š
# {
#   "success": true,
#   "downloaded": {
#     "starlink": 6
#   },
#   "total_satellites": 6,
#   "download_time_ms": 1500
# }

# 2. æ¸¬è©¦å¤šæ˜Ÿåº§ä¸¦è¡Œä¸‹è¼‰
curl -X POST "http://localhost:8080/api/v1/satellites/tle/download" \
  -H "Content-Type: application/json" \
  -d '{
    "constellations": ["starlink", "oneweb"],
    "force_update": true
  }' | jq
```

### **2.2 é è¨ˆç®—ä½œæ¥­é©—è­‰**
```bash
# 1. å•Ÿå‹•è»Œé“é è¨ˆç®—ä½œæ¥­
curl -X POST "http://localhost:8080/api/v1/satellites/precompute" \
  -H "Content-Type: application/json" \
  -d '{
    "constellation": "starlink",
    "start_time": "2025-01-23T00:00:00Z",
    "end_time": "2025-01-23T06:00:00Z",
    "time_step_seconds": 30,
    "observer_location": {
      "latitude": 24.94417,
      "longitude": 121.37139,
      "altitude": 100
    }
  }' | jq

# é æœŸéŸ¿æ‡‰ï¼š
# {
#   "job_id": "precompute_starlink_20250123",
#   "status": "running",
#   "estimated_duration_minutes": 5,
#   "total_calculations": 2880
# }

# 2. æª¢æŸ¥é è¨ˆç®—é€²åº¦
JOB_ID="precompute_starlink_20250123_abcd1234"
curl -X GET "http://localhost:8080/api/v1/satellites/precompute/$JOB_ID/status" | jq

# 3. å–æ¶ˆä½œæ¥­ï¼ˆå¦‚éœ€è¦ï¼‰
curl -X DELETE "http://localhost:8080/api/v1/satellites/precompute/$JOB_ID" | jq
```

### **2.3 æ•¸æ“šå­˜å„²é©—è­‰**
```bash
# 1. é©—è­‰é è¨ˆç®—çµæœå­˜å„²
docker exec -it netstack-rl-postgres psql -U rl_user -d rl_research -c "
SELECT constellation, 
       COUNT(*) as total_records,
       COUNT(DISTINCT satellite_id) as unique_satellites,
       MIN(timestamp) as earliest_time,
       MAX(timestamp) as latest_time
FROM satellite_orbital_cache 
WHERE constellation = 'starlink'
GROUP BY constellation;"

# 2. æª¢æŸ¥æ•¸æ“šå“è³ª
docker exec -it netstack-rl-postgres psql -U rl_user -d rl_research -c "
SELECT constellation,
       AVG(signal_strength) as avg_signal,
       AVG(elevation_angle) as avg_elevation,
       COUNT(*) FILTER (WHERE elevation_angle >= 10) as visible_count
FROM satellite_orbital_cache 
WHERE observer_latitude = 24.94417 AND observer_longitude = 121.37139
GROUP BY constellation;"

# 3. é©—è­‰ç´¢å¼•æ•ˆèƒ½
docker exec -it netstack-rl-postgres psql -U rl_user -d rl_research -c "
EXPLAIN ANALYZE SELECT * FROM satellite_orbital_cache 
WHERE observer_latitude = 24.94417 
  AND observer_longitude = 121.37139 
  AND timestamp = '2025-01-23T12:00:00Z'
  AND elevation_angle >= 10
ORDER BY elevation_angle DESC LIMIT 10;"
```

### **2.4 æ•ˆèƒ½åŸºæº–æ¸¬è©¦**
```bash
# 1. æ‰¹é‡æ’å…¥æ•ˆèƒ½æ¸¬è©¦
curl -X POST "http://localhost:8080/api/v1/satellites/benchmark/batch_insert" \
  -H "Content-Type: application/json" \
  -d '{
    "record_count": 10000,
    "constellation": "starlink"
  }' | jq

# é æœŸï¼šthroughput > 1000 records/second

# 2. æŸ¥è©¢æ•ˆèƒ½æ¸¬è©¦
curl -X POST "http://localhost:8080/api/v1/satellites/benchmark/query_performance" \
  -H "Content-Type: application/json" \
  -d '{
    "query_count": 100,
    "timestamp": "2025-01-23T12:00:00Z"
  }' | jq

# é æœŸï¼šå¹³å‡æŸ¥è©¢æ™‚é–“ < 50ms
```

### **2.5 æ•¸æ“šçµ±è¨ˆé©—è­‰**
```bash
# ç²å–æ•¸æ“šçµ±è¨ˆå ±å‘Š
curl -X GET "http://localhost:8080/api/v1/satellites/statistics" \
  -G \
  -d "observer_lat=24.94417" \
  -d "observer_lon=121.37139" | jq

# é æœŸéŸ¿æ‡‰ï¼š
# {
#   "total_records": 15840,
#   "unique_satellites": 6,
#   "constellations": 1,
#   "time_range": {
#     "start": "2025-01-23T00:00:00Z",
#     "end": "2025-01-23T06:00:00Z",
#     "duration_hours": 6.0
#   },
#   "signal_quality": {
#     "avg_signal_strength": 65.2,
#     "avg_elevation": 25.8
#   }
# }
```

## âš ï¸ æ³¨æ„äº‹é …

### **æ€§èƒ½è€ƒé‡**
1. **è¨˜æ†¶é«”ç®¡ç†**ï¼šå¤§å‹é è¨ˆç®—ä½œæ¥­æ‡‰ä½¿ç”¨åˆ†æ‰¹è™•ç†ï¼Œé¿å…è¨˜æ†¶é«”æº¢å‡º
2. **ç¶²è·¯è¶…æ™‚**ï¼šTLE ä¸‹è¼‰è¨­ç½®åˆé©çš„è¶…æ™‚æ™‚é–“ï¼ˆå»ºè­° 30 ç§’ï¼‰
3. **æ•¸æ“šåº«é€£æ¥**ï¼šä½¿ç”¨é€£æ¥æ± ï¼Œé¿å…é »ç¹å»ºç«‹/é—œé–‰é€£æ¥
4. **ä¸¦ç™¼æ§åˆ¶**ï¼šåŒæ™‚é‹è¡Œçš„é è¨ˆç®—ä½œæ¥­ä¸è¶…é 2 å€‹

### **æ•¸æ“šå“è³ªä¿è­‰**
1. **TLE é©—è­‰**ï¼šç¢ºä¿ TLE æ ¼å¼æ­£ç¢ºï¼Œepoch æ™‚é–“åˆç†
2. **è»Œé“è¨ˆç®—é©—è­‰**ï¼šèˆ‡ Skyfield åŸºæº–æ¯”è¼ƒï¼Œä½ç½®èª¤å·® < 1km
3. **ä¿¡è™Ÿå¼·åº¦åˆç†æ€§**ï¼šç¢ºä¿ä¿¡è™Ÿå¼·åº¦åœ¨åˆç†ç¯„åœå…§
4. **æ•¸æ“šå®Œæ•´æ€§**ï¼šæª¢æŸ¥æ™‚é–“åºåˆ—æ˜¯å¦é€£çºŒï¼Œç„¡éºæ¼é»

### **å®¹éŒ¯æ©Ÿåˆ¶**
1. **åˆ†æ‰¹æ¢å¾©**ï¼šä½œæ¥­å¤±æ•—æ™‚èƒ½å¾æœ€å¾ŒæˆåŠŸçš„æ‰¹æ¬¡æ¢å¾©
2. **é™ç´šç­–ç•¥**ï¼šTLE ä¸‹è¼‰å¤±æ•—æ™‚ä½¿ç”¨ç·©å­˜æ•¸æ“š
3. **ç›£æ§å‘Šè­¦**ï¼šé—œéµéŒ¯èª¤æ‡‰è§¸ç™¼ç›£æ§å‘Šè­¦
4. **æ•¸æ“šå‚™ä»½**ï¼šé‡è¦æ•¸æ“šå®šæœŸå‚™ä»½åˆ°å¤–éƒ¨å­˜å„²

---

**ğŸ¯ å®Œæˆæ¨™æº–**ï¼š
- æˆåŠŸé è¨ˆç®— 6 å°æ™‚æ­·å²æ•¸æ“šï¼ŒåŒ…å« 6-8 é¡†å¯è¦‹è¡›æ˜Ÿ
- æ‰¹æ¬¡æ’å…¥æ•ˆèƒ½ > 1000 æ¢/ç§’
- æŸ¥è©¢éŸ¿æ‡‰æ™‚é–“ < 50ms
- ä½ç½®è¨ˆç®—ç²¾åº¦èª¤å·® < 1kmï¼ˆèˆ‡ Skyfield åŸºæº–æ¯”è¼ƒï¼‰
- æ‰€æœ‰éŒ¯èª¤æƒ…æ³éƒ½æœ‰é©ç•¶çš„è™•ç†å’Œæ—¥èªŒè¨˜éŒ„

