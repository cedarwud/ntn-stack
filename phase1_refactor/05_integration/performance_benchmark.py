#!/usr/bin/env python3
"""
Phase 1 æ€§èƒ½åŸºæº–é©—è­‰

åŠŸèƒ½:
1. é©—è­‰ Phase 1 ç³»çµ±æ€§èƒ½æŒ‡æ¨™é”æ¨™
2. å°æ¯”æ€§èƒ½æ”¹å–„å’Œå›æ­¸æª¢æ¸¬
3. ç”Ÿæˆè©³ç´°çš„æ€§èƒ½åŸºæº–å ±å‘Š
4. ç¢ºä¿å­¸è¡“ç ”ç©¶ç´šåˆ¥çš„æ€§èƒ½æ¨™æº–

ç¬¦åˆ CLAUDE.md åŸå‰‡:
- æ¸¬è©¦çœŸå¯¦ SGP4 ç®—æ³•çš„æ€§èƒ½è¡¨ç¾
- é©—è­‰ 8,715 é¡†è¡›æ˜Ÿçš„è™•ç†èƒ½åŠ›
- ç¢ºä¿ç³»çµ±ç©©å®šæ€§å’Œå¯é æ€§
"""

import logging
import time
import psutil
import threading
import json
import statistics
import sys
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np

# æ·»åŠ  Phase 1 æ¨¡çµ„è·¯å¾‘
PHASE1_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PHASE1_ROOT / "01_data_source"))
sys.path.insert(0, str(PHASE1_ROOT / "02_orbit_calculation"))
sys.path.insert(0, str(PHASE1_ROOT / "03_processing_pipeline"))
sys.path.insert(0, str(PHASE1_ROOT / "04_output_interface"))

logger = logging.getLogger(__name__)

@dataclass
class BenchmarkRequirement:
    """æ€§èƒ½åŸºæº–è¦æ±‚"""
    metric_name: str
    target_value: float
    max_value: Optional[float] = None
    min_value: Optional[float] = None
    unit: str = ""
    description: str = ""
    critical: bool = True

@dataclass
class BenchmarkResult:
    """æ€§èƒ½åŸºæº–æ¸¬è©¦çµæœ"""
    metric_name: str
    measured_value: float
    target_value: float
    passed: bool
    improvement_factor: Optional[float] = None
    unit: str = ""
    details: Optional[Dict[str, Any]] = None
    timestamp: datetime = None

@dataclass
class SystemMetrics:
    """ç³»çµ±æ€§èƒ½æŒ‡æ¨™"""
    timestamp: datetime
    cpu_usage_percent: float
    memory_usage_percent: float
    memory_available_mb: float
    disk_io_read_mb_s: float
    disk_io_write_mb_s: float
    network_sent_mb_s: float = 0.0
    network_recv_mb_s: float = 0.0

class PerformanceBenchmark:
    """æ€§èƒ½åŸºæº–æ¸¬è©¦å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ€§èƒ½åŸºæº–æ¸¬è©¦å™¨"""
        self.benchmark_requirements = self._define_benchmark_requirements()
        self.test_results = []
        self.system_baseline = None
        
        # æ¸¬è©¦çµ„ä»¶
        self.tle_loader = None
        self.sgp4_engine = None
        self.data_provider = None
        self.standard_interface = None
        self.performance_monitor = None
        
        logger.info("âœ… æ€§èƒ½åŸºæº–æ¸¬è©¦å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _define_benchmark_requirements(self) -> List[BenchmarkRequirement]:
        """å®šç¾©æ€§èƒ½åŸºæº–è¦æ±‚"""
        requirements = [
            # TLE æ•¸æ“šè¼‰å…¥æ€§èƒ½
            BenchmarkRequirement(
                metric_name="tle_load_time",
                target_value=10.0,
                max_value=30.0,
                unit="seconds",
                description="TLE æ•¸æ“šè¼‰å…¥æ™‚é–“",
                critical=True
            ),
            
            # SGP4 è¡›æ˜Ÿå°è±¡å‰µå»ºæ€§èƒ½
            BenchmarkRequirement(
                metric_name="satellite_creation_rate",
                target_value=100.0,
                min_value=50.0,
                unit="satellites/second",
                description="SGP4 è¡›æ˜Ÿå°è±¡å‰µå»ºé€Ÿç‡",
                critical=True
            ),
            
            # SGP4 è»Œé“è¨ˆç®—æ€§èƒ½
            BenchmarkRequirement(
                metric_name="sgp4_calculation_rate",
                target_value=1000.0,
                min_value=500.0,
                unit="calculations/second",
                description="SGP4 è»Œé“è¨ˆç®—é€Ÿç‡",
                critical=True
            ),
            
            # API éŸ¿æ‡‰æ™‚é–“
            BenchmarkRequirement(
                metric_name="api_response_time_p95",
                target_value=0.1,
                max_value=0.5,
                unit="seconds",
                description="API éŸ¿æ‡‰æ™‚é–“ï¼ˆ95ç™¾åˆ†ä½ï¼‰",
                critical=True
            ),
            
            # ä½µç™¼è™•ç†èƒ½åŠ›
            BenchmarkRequirement(
                metric_name="concurrent_requests_handled",
                target_value=50.0,
                min_value=20.0,
                unit="requests",
                description="ä½µç™¼è«‹æ±‚è™•ç†èƒ½åŠ›",
                critical=True
            ),
            
            # å…§å­˜ä½¿ç”¨æ•ˆç‡
            BenchmarkRequirement(
                metric_name="memory_efficiency",
                target_value=2000.0,
                max_value=4000.0,
                unit="MB",
                description="æœ€å¤§å…§å­˜ä½¿ç”¨é‡",
                critical=False
            ),
            
            # æ•¸æ“šæº–ç¢ºæ€§
            BenchmarkRequirement(
                metric_name="calculation_accuracy",
                target_value=0.99,
                min_value=0.95,
                unit="ratio",
                description="è»Œé“è¨ˆç®—æº–ç¢ºæ€§",
                critical=True
            ),
            
            # ç³»çµ±ç©©å®šæ€§
            BenchmarkRequirement(
                metric_name="system_uptime_ratio",
                target_value=0.999,
                min_value=0.99,
                unit="ratio",
                description="ç³»çµ±ç©©å®šé‹è¡Œæ¯”ä¾‹",
                critical=True
            ),
            
            # ç·©å­˜æ•ˆç‡
            BenchmarkRequirement(
                metric_name="cache_hit_rate",
                target_value=0.8,
                min_value=0.6,
                unit="ratio",
                description="SGP4 ç·©å­˜å‘½ä¸­ç‡",
                critical=False
            ),
            
            # å¤§è¦æ¨¡è™•ç†æ€§èƒ½
            BenchmarkRequirement(
                metric_name="large_scale_query_time",
                target_value=5.0,
                max_value=10.0,
                unit="seconds",
                description="å¤§è¦æ¨¡æŸ¥è©¢è™•ç†æ™‚é–“",
                critical=True
            )
        ]
        
        logger.info(f"å®šç¾©äº† {len(requirements)} å€‹æ€§èƒ½åŸºæº–è¦æ±‚")
        return requirements
    
    async def run_performance_benchmark(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„æ€§èƒ½åŸºæº–æ¸¬è©¦"""
        logger.info("ğŸš€ é–‹å§‹æ€§èƒ½åŸºæº–é©—è­‰...")
        
        benchmark_start = datetime.now(timezone.utc)
        
        benchmark_report = {
            "benchmark_start_time": benchmark_start.isoformat(),
            "benchmark_requirements": [asdict(req) for req in self.benchmark_requirements],
            "test_results": {},
            "overall_performance": {},
            "system_comparison": {},
            "recommendations": []
        }
        
        try:
            # åˆå§‹åŒ–æ¸¬è©¦ç’°å¢ƒ
            await self._initialize_benchmark_environment()
            
            # æ”¶é›†ç³»çµ±åŸºæº–ç·š
            self.system_baseline = self._collect_system_baseline()
            
            # åŸ·è¡Œå„é …æ€§èƒ½æ¸¬è©¦
            await self._test_tle_loading_performance()
            await self._test_sgp4_calculation_performance()
            await self._test_api_response_performance()
            await self._test_concurrent_processing_performance()
            await self._test_memory_efficiency()
            await self._test_large_scale_processing()
            await self._test_system_stability()
            
            # ç”Ÿæˆç¶œåˆå ±å‘Š
            benchmark_end = datetime.now(timezone.utc)
            benchmark_report["benchmark_end_time"] = benchmark_end.isoformat()
            benchmark_report["total_duration"] = (benchmark_end - benchmark_start).total_seconds()
            
            # åˆ†ææ¸¬è©¦çµæœ
            benchmark_report["test_results"] = self._analyze_test_results()
            benchmark_report["overall_performance"] = self._calculate_overall_performance()
            benchmark_report["system_comparison"] = self._compare_with_baseline()
            benchmark_report["recommendations"] = self._generate_recommendations()
            
            logger.info("ğŸ¯ æ€§èƒ½åŸºæº–é©—è­‰å®Œæˆ")
            return benchmark_report
            
        except Exception as e:
            logger.error(f"æ€§èƒ½åŸºæº–æ¸¬è©¦å¤±æ•—: {e}")
            benchmark_report["error"] = str(e)
            return benchmark_report
    
    async def _initialize_benchmark_environment(self):
        """åˆå§‹åŒ–åŸºæº–æ¸¬è©¦ç’°å¢ƒ"""
        try:
            # åˆå§‹åŒ– TLE è¼‰å…¥å™¨
            from tle_loader import create_tle_loader
            self.tle_loader = create_tle_loader()  # ä½¿ç”¨çµ±ä¸€é…ç½®
            
            # åˆå§‹åŒ– SGP4 å¼•æ“
            from sgp4_engine import create_sgp4_engine, validate_sgp4_availability
            
            if not validate_sgp4_availability():
                raise RuntimeError("SGP4 åº«ä¸å¯ç”¨")
            
            self.sgp4_engine = create_sgp4_engine()
            
            # åˆå§‹åŒ–æ•¸æ“šæä¾›è€…å’Œæ¨™æº–æ¥å£
            from phase1_data_provider import create_sgp4_data_provider
            from phase2_interface import create_standard_interface
            
            self.data_provider = create_sgp4_data_provider()
            self.standard_interface = create_standard_interface(self.data_provider)
            
            # åˆå§‹åŒ–æ€§èƒ½ç›£æ§
            from performance_monitor import create_performance_monitor
            self.performance_monitor = create_performance_monitor(0.1)  # é«˜é »ç›£æ§
            self.performance_monitor.set_sgp4_engine(self.sgp4_engine)
            self.performance_monitor.start_monitoring()
            
            logger.info("âœ… åŸºæº–æ¸¬è©¦ç’°å¢ƒåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"åŸºæº–æ¸¬è©¦ç’°å¢ƒåˆå§‹åŒ–å¤±æ•—: {e}")
            raise
    
    def _collect_system_baseline(self) -> SystemMetrics:
        """æ”¶é›†ç³»çµ±åŸºæº–ç·šæ€§èƒ½"""
        try:
            # CPU ä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1.0)
            
            # å…§å­˜ä¿¡æ¯
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            memory_available_mb = memory.available / (1024 * 1024)
            
            # ç£ç›¤ I/O
            disk_io_before = psutil.disk_io_counters()
            time.sleep(1.0)
            disk_io_after = psutil.disk_io_counters()
            
            disk_read_mb_s = (disk_io_after.read_bytes - disk_io_before.read_bytes) / (1024 * 1024)
            disk_write_mb_s = (disk_io_after.write_bytes - disk_io_before.write_bytes) / (1024 * 1024)
            
            baseline = SystemMetrics(
                timestamp=datetime.now(timezone.utc),
                cpu_usage_percent=cpu_percent,
                memory_usage_percent=memory_percent,
                memory_available_mb=memory_available_mb,
                disk_io_read_mb_s=disk_read_mb_s,
                disk_io_write_mb_s=disk_write_mb_s
            )
            
            logger.info(f"ç³»çµ±åŸºæº–ç·š: CPU {cpu_percent:.1f}%, è¨˜æ†¶é«” {memory_percent:.1f}%")
            return baseline
            
        except Exception as e:
            logger.error(f"æ”¶é›†ç³»çµ±åŸºæº–ç·šå¤±æ•—: {e}")
            return None
    
    async def _test_tle_loading_performance(self):
        """æ¸¬è©¦ TLE è¼‰å…¥æ€§èƒ½"""
        logger.info("æ¸¬è©¦ TLE è¼‰å…¥æ€§èƒ½...")
        
        try:
            # æ¸…é™¤å¯èƒ½çš„ç·©å­˜
            if hasattr(self.tle_loader, 'clear_cache'):
                self.tle_loader.clear_cache()
            
            # æ¸¬è©¦ TLE è¼‰å…¥æ™‚é–“
            load_start = time.time()
            tle_result = self.tle_loader.load_all_tle_data()
            load_time = time.time() - load_start
            
            # è¨˜éŒ„çµæœ
            result = BenchmarkResult(
                metric_name="tle_load_time",
                measured_value=load_time,
                target_value=self._get_requirement("tle_load_time").target_value,
                passed=load_time <= self._get_requirement("tle_load_time").target_value,
                unit="seconds",
                details={
                    "total_records": tle_result.total_records,
                    "load_rate": tle_result.total_records / load_time if load_time > 0 else 0
                },
                timestamp=datetime.now(timezone.utc)
            )
            
            self.test_results.append(result)
            logger.info(f"TLE è¼‰å…¥æ€§èƒ½: {load_time:.2f}s, {tle_result.total_records} è¨˜éŒ„")
            
        except Exception as e:
            logger.error(f"TLE è¼‰å…¥æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results.append(BenchmarkResult(
                metric_name="tle_load_time",
                measured_value=float('inf'),
                target_value=self._get_requirement("tle_load_time").target_value,
                passed=False,
                unit="seconds",
                details={"error": str(e)},
                timestamp=datetime.now(timezone.utc)
            ))
    
    async def _test_sgp4_calculation_performance(self):
        """æ¸¬è©¦ SGP4 è¨ˆç®—æ€§èƒ½"""
        logger.info("æ¸¬è©¦ SGP4 è¨ˆç®—æ€§èƒ½...")
        
        try:
            # æº–å‚™æ¸¬è©¦æ•¸æ“š
            tle_result = self.tle_loader.load_all_tle_data()
            test_satellites = tle_result.records[:100]  # æ¸¬è©¦å‰100é¡†è¡›æ˜Ÿ
            
            # æ¸¬è©¦è¡›æ˜Ÿå°è±¡å‰µå»ºæ€§èƒ½
            creation_start = time.time()
            created_count = 0
            
            for record in test_satellites:
                if self.sgp4_engine.create_satellite(record.satellite_id, record.line1, record.line2):
                    created_count += 1
            
            creation_time = time.time() - creation_start
            creation_rate = created_count / creation_time if creation_time > 0 else 0
            
            # è¨˜éŒ„è¡›æ˜Ÿå‰µå»ºæ€§èƒ½
            creation_result = BenchmarkResult(
                metric_name="satellite_creation_rate",
                measured_value=creation_rate,
                target_value=self._get_requirement("satellite_creation_rate").target_value,
                passed=creation_rate >= self._get_requirement("satellite_creation_rate").target_value,
                unit="satellites/second",
                details={
                    "satellites_created": created_count,
                    "total_attempted": len(test_satellites),
                    "creation_time": creation_time
                },
                timestamp=datetime.now(timezone.utc)
            )
            
            self.test_results.append(creation_result)
            
            # æ¸¬è©¦è»Œé“è¨ˆç®—æ€§èƒ½
            if created_count > 0:
                calc_start = time.time()
                successful_calculations = 0
                test_time = datetime.now(timezone.utc)
                
                # é€²è¡Œå¤šæ¬¡è¨ˆç®—æ¸¬è©¦
                for _ in range(10):  # æ¯é¡†è¡›æ˜Ÿè¨ˆç®—10æ¬¡
                    for record in test_satellites[:min(created_count, 50)]:
                        result = self.sgp4_engine.calculate_position(record.satellite_id, test_time)
                        if result and result.success:
                            successful_calculations += 1
                
                calc_time = time.time() - calc_start
                calc_rate = successful_calculations / calc_time if calc_time > 0 else 0
                
                # è¨˜éŒ„è¨ˆç®—æ€§èƒ½
                calc_result = BenchmarkResult(
                    metric_name="sgp4_calculation_rate",
                    measured_value=calc_rate,
                    target_value=self._get_requirement("sgp4_calculation_rate").target_value,
                    passed=calc_rate >= self._get_requirement("sgp4_calculation_rate").target_value,
                    unit="calculations/second",
                    details={
                        "successful_calculations": successful_calculations,
                        "calculation_time": calc_time,
                        "accuracy_rate": successful_calculations / (50 * 10) if created_count >= 50 else 0
                    },
                    timestamp=datetime.now(timezone.utc)
                )
                
                self.test_results.append(calc_result)
                logger.info(f"SGP4 è¨ˆç®—æ€§èƒ½: {calc_rate:.1f} calc/s")
            
            logger.info(f"SGP4 è¡›æ˜Ÿå‰µå»º: {creation_rate:.1f} sat/s")
            
        except Exception as e:
            logger.error(f"SGP4 è¨ˆç®—æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_api_response_performance(self):
        """æ¸¬è©¦ API éŸ¿æ‡‰æ€§èƒ½"""
        logger.info("æ¸¬è©¦ API éŸ¿æ‡‰æ€§èƒ½...")
        
        try:
            import requests
            
            # æ¸¬è©¦ä¸åŒçš„ API ç«¯é»
            api_endpoints = [
                ("health", "http://localhost:8001/health"),
                ("satellites", "http://localhost:8001/satellites?limit=10"),
                ("capabilities", "http://localhost:8001/capabilities")
            ]
            
            all_response_times = []
            endpoint_results = {}
            
            for endpoint_name, url in api_endpoints:
                response_times = []
                
                # å¤šæ¬¡æ¸¬è©¦æ¯å€‹ç«¯é»
                for _ in range(10):
                    try:
                        start_time = time.time()
                        response = requests.get(url, timeout=10)
                        response_time = time.time() - start_time
                        
                        if response.status_code == 200:
                            response_times.append(response_time)
                            all_response_times.append(response_time)
                        
                    except Exception as e:
                        logger.warning(f"API æ¸¬è©¦è«‹æ±‚å¤±æ•— {endpoint_name}: {e}")
                
                if response_times:
                    endpoint_results[endpoint_name] = {
                        "avg_response_time": statistics.mean(response_times),
                        "p95_response_time": np.percentile(response_times, 95),
                        "min_response_time": min(response_times),
                        "max_response_time": max(response_times)
                    }
            
            # è¨ˆç®—æ•´é«” P95 éŸ¿æ‡‰æ™‚é–“
            if all_response_times:
                p95_response_time = np.percentile(all_response_times, 95)
                avg_response_time = statistics.mean(all_response_times)
                
                # è¨˜éŒ„çµæœ
                result = BenchmarkResult(
                    metric_name="api_response_time_p95",
                    measured_value=p95_response_time,
                    target_value=self._get_requirement("api_response_time_p95").target_value,
                    passed=p95_response_time <= self._get_requirement("api_response_time_p95").target_value,
                    unit="seconds",
                    details={
                        "p95_response_time": p95_response_time,
                        "avg_response_time": avg_response_time,
                        "total_requests": len(all_response_times),
                        "endpoint_results": endpoint_results
                    },
                    timestamp=datetime.now(timezone.utc)
                )
                
                self.test_results.append(result)
                logger.info(f"API éŸ¿æ‡‰æ€§èƒ½: P95 {p95_response_time:.3f}s, å¹³å‡ {avg_response_time:.3f}s")
            
        except Exception as e:
            logger.error(f"API éŸ¿æ‡‰æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_concurrent_processing_performance(self):
        """æ¸¬è©¦ä½µç™¼è™•ç†æ€§èƒ½"""
        logger.info("æ¸¬è©¦ä½µç™¼è™•ç†æ€§èƒ½...")
        
        try:
            import requests
            
            def make_concurrent_request(request_id: int) -> Dict:
                try:
                    start_time = time.time()
                    response = requests.get(
                        "http://localhost:8001/satellites?constellation=starlink&limit=5",
                        timeout=30
                    )
                    response_time = time.time() - start_time
                    
                    return {
                        "request_id": request_id,
                        "success": response.status_code == 200,
                        "response_time": response_time
                    }
                except Exception as e:
                    return {
                        "request_id": request_id,
                        "success": False,
                        "error": str(e)
                    }
            
            # æ¸¬è©¦ä½µç™¼è™•ç†
            concurrent_levels = [10, 20, 50]  # ä¸åŒçš„ä½µç™¼ç´šåˆ¥
            
            for concurrent_level in concurrent_levels:
                logger.info(f"æ¸¬è©¦ {concurrent_level} ä½µç™¼è«‹æ±‚...")
                
                concurrent_start = time.time()
                
                with ThreadPoolExecutor(max_workers=concurrent_level) as executor:
                    futures = [executor.submit(make_concurrent_request, i) for i in range(concurrent_level)]
                    results = [future.result() for future in as_completed(futures)]
                
                concurrent_time = time.time() - concurrent_start
                
                # åˆ†æçµæœ
                successful_requests = sum(1 for r in results if r["success"])
                success_rate = successful_requests / len(results)
                
                if successful_requests > 0:
                    response_times = [r["response_time"] for r in results if r["success"]]
                    avg_response_time = statistics.mean(response_times)
                    
                    # è¨˜éŒ„çµæœï¼ˆä½¿ç”¨æœ€é«˜æˆåŠŸçš„ä½µç™¼ç´šåˆ¥ï¼‰
                    if success_rate >= 0.8:  # 80% æˆåŠŸç‡é–€æª»
                        result = BenchmarkResult(
                            metric_name="concurrent_requests_handled",
                            measured_value=float(successful_requests),
                            target_value=self._get_requirement("concurrent_requests_handled").target_value,
                            passed=successful_requests >= self._get_requirement("concurrent_requests_handled").target_value,
                            unit="requests",
                            details={
                                "concurrent_level": concurrent_level,
                                "successful_requests": successful_requests,
                                "total_requests": len(results),
                                "success_rate": success_rate,
                                "total_time": concurrent_time,
                                "avg_response_time": avg_response_time
                            },
                            timestamp=datetime.now(timezone.utc)
                        )
                        
                        self.test_results.append(result)
                        logger.info(f"ä½µç™¼è™•ç†: {successful_requests}/{concurrent_level} æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"ä½µç™¼è™•ç†æ€§èƒ½æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_memory_efficiency(self):
        """æ¸¬è©¦å…§å­˜ä½¿ç”¨æ•ˆç‡"""
        logger.info("æ¸¬è©¦å…§å­˜ä½¿ç”¨æ•ˆç‡...")
        
        try:
            # è¨˜éŒ„åˆå§‹å…§å­˜
            initial_memory = psutil.virtual_memory().used / (1024 * 1024)
            
            # åŸ·è¡Œä¸€ç³»åˆ—æ“ä½œä¾†æ¸¬è©¦å…§å­˜ä½¿ç”¨
            # 1. å¤§é‡è¡›æ˜Ÿå°è±¡å‰µå»º
            tle_result = self.tle_loader.load_all_tle_data()
            test_satellites = tle_result.records[:500]  # æ¸¬è©¦500é¡†è¡›æ˜Ÿ
            
            for record in test_satellites:
                self.sgp4_engine.create_satellite(record.satellite_id, record.line1, record.line2)
            
            # 2. å¤§é‡è»Œé“è¨ˆç®—
            test_time = datetime.now(timezone.utc)
            for record in test_satellites[:100]:
                for _ in range(10):  # æ¯é¡†è¡›æ˜Ÿè¨ˆç®—10æ¬¡
                    self.sgp4_engine.calculate_position(record.satellite_id, test_time)
            
            # è¨˜éŒ„å³°å€¼å…§å­˜
            peak_memory = psutil.virtual_memory().used / (1024 * 1024)
            memory_usage = peak_memory - initial_memory
            
            # è¨˜éŒ„çµæœ
            result = BenchmarkResult(
                metric_name="memory_efficiency",
                measured_value=memory_usage,
                target_value=self._get_requirement("memory_efficiency").target_value,
                passed=memory_usage <= self._get_requirement("memory_efficiency").target_value,
                unit="MB",
                details={
                    "initial_memory_mb": initial_memory,
                    "peak_memory_mb": peak_memory,
                    "memory_increase_mb": memory_usage,
                    "satellites_processed": len(test_satellites),
                    "calculations_performed": len(test_satellites[:100]) * 10
                },
                timestamp=datetime.now(timezone.utc)
            )
            
            self.test_results.append(result)
            logger.info(f"å…§å­˜ä½¿ç”¨æ•ˆç‡: {memory_usage:.1f}MB å¢åŠ ")
            
        except Exception as e:
            logger.error(f"å…§å­˜æ•ˆç‡æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_large_scale_processing(self):
        """æ¸¬è©¦å¤§è¦æ¨¡è™•ç†æ€§èƒ½"""
        logger.info("æ¸¬è©¦å¤§è¦æ¨¡è™•ç†æ€§èƒ½...")
        
        try:
            # æ¸¬è©¦å¤§è¦æ¨¡æŸ¥è©¢
            large_query_start = time.time()
            
            query_request = self.standard_interface.create_query_request(
                max_records=1000
            )
            
            query_response = self.standard_interface.execute_query(query_request)
            
            large_query_time = time.time() - large_query_start
            
            # è¨˜éŒ„çµæœ
            result = BenchmarkResult(
                metric_name="large_scale_query_time",
                measured_value=large_query_time,
                target_value=self._get_requirement("large_scale_query_time").target_value,
                passed=large_query_time <= self._get_requirement("large_scale_query_time").target_value,
                unit="seconds",
                details={
                    "query_time": large_query_time,
                    "records_returned": query_response.returned_records,
                    "total_matches": query_response.total_matches,
                    "records_per_second": query_response.returned_records / large_query_time if large_query_time > 0 else 0
                },
                timestamp=datetime.now(timezone.utc)
            )
            
            self.test_results.append(result)
            logger.info(f"å¤§è¦æ¨¡è™•ç†: {large_query_time:.2f}s, {query_response.returned_records} è¨˜éŒ„")
            
        except Exception as e:
            logger.error(f"å¤§è¦æ¨¡è™•ç†æ¸¬è©¦å¤±æ•—: {e}")
    
    async def _test_system_stability(self):
        """æ¸¬è©¦ç³»çµ±ç©©å®šæ€§"""
        logger.info("æ¸¬è©¦ç³»çµ±ç©©å®šæ€§...")
        
        try:
            stability_start = time.time()
            
            # æŒçºŒé‹è¡Œæ¸¬è©¦ï¼ˆæ¨¡æ“¬å¯¦éš›ä½¿ç”¨ï¼‰
            total_operations = 0
            successful_operations = 0
            
            test_duration = 30  # 30ç§’ç©©å®šæ€§æ¸¬è©¦
            end_time = time.time() + test_duration
            
            while time.time() < end_time:
                try:
                    # åŸ·è¡Œå„ç¨®æ“ä½œ
                    test_time = datetime.now(timezone.utc)
                    
                    # SGP4 è¨ˆç®—
                    if self.sgp4_engine and len(self.sgp4_engine.satellite_cache) > 0:
                        satellite_id = list(self.sgp4_engine.satellite_cache.keys())[0]
                        result = self.sgp4_engine.calculate_position(satellite_id, test_time)
                        total_operations += 1
                        if result and result.success:
                            successful_operations += 1
                    
                    # API æŸ¥è©¢
                    try:
                        import requests
                        response = requests.get("http://localhost:8001/health", timeout=5)
                        total_operations += 1
                        if response.status_code == 200:
                            successful_operations += 1
                    except:
                        total_operations += 1
                    
                    time.sleep(0.1)  # çŸ­æš«ä¼‘æ¯
                    
                except Exception:
                    total_operations += 1
            
            stability_time = time.time() - stability_start
            uptime_ratio = successful_operations / max(total_operations, 1)
            
            # è¨˜éŒ„çµæœ
            result = BenchmarkResult(
                metric_name="system_uptime_ratio",
                measured_value=uptime_ratio,
                target_value=self._get_requirement("system_uptime_ratio").target_value,
                passed=uptime_ratio >= self._get_requirement("system_uptime_ratio").target_value,
                unit="ratio",
                details={
                    "test_duration": stability_time,
                    "total_operations": total_operations,
                    "successful_operations": successful_operations,
                    "uptime_ratio": uptime_ratio,
                    "operations_per_second": total_operations / stability_time
                },
                timestamp=datetime.now(timezone.utc)
            )
            
            self.test_results.append(result)
            logger.info(f"ç³»çµ±ç©©å®šæ€§: {uptime_ratio:.3f} ç©©å®šæ¯”ä¾‹")
            
        except Exception as e:
            logger.error(f"ç³»çµ±ç©©å®šæ€§æ¸¬è©¦å¤±æ•—: {e}")
    
    def _get_requirement(self, metric_name: str) -> BenchmarkRequirement:
        """ç²å–æŒ‡å®šæŒ‡æ¨™çš„è¦æ±‚"""
        for req in self.benchmark_requirements:
            if req.metric_name == metric_name:
                return req
        raise ValueError(f"æœªæ‰¾åˆ°æŒ‡æ¨™è¦æ±‚: {metric_name}")
    
    def _analyze_test_results(self) -> Dict[str, Any]:
        """åˆ†ææ¸¬è©¦çµæœ"""
        try:
            results_by_metric = {}
            
            for result in self.test_results:
                results_by_metric[result.metric_name] = asdict(result)
            
            # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
            passed_tests = sum(1 for r in self.test_results if r.passed)
            total_tests = len(self.test_results)
            pass_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
            
            critical_tests = [r for r in self.test_results 
                            if self._get_requirement(r.metric_name).critical]
            critical_passed = sum(1 for r in critical_tests if r.passed)
            critical_pass_rate = (critical_passed / len(critical_tests)) * 100 if critical_tests else 0
            
            return {
                "results_by_metric": results_by_metric,
                "summary_statistics": {
                    "total_tests": total_tests,
                    "passed_tests": passed_tests,
                    "failed_tests": total_tests - passed_tests,
                    "overall_pass_rate": pass_rate,
                    "critical_tests": len(critical_tests),
                    "critical_passed": critical_passed,
                    "critical_pass_rate": critical_pass_rate
                }
            }
            
        except Exception as e:
            logger.error(f"åˆ†ææ¸¬è©¦çµæœå¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _calculate_overall_performance(self) -> Dict[str, Any]:
        """è¨ˆç®—æ•´é«”æ€§èƒ½è©•ç´š"""
        try:
            # æ€§èƒ½è©•ç´šç®—æ³•
            performance_scores = []
            
            for result in self.test_results:
                req = self._get_requirement(result.metric_name)
                
                if result.passed:
                    # æ ¹æ“šè¶…è¶Šç›®æ¨™çš„ç¨‹åº¦çµ¦åˆ†
                    if req.max_value and result.measured_value <= req.target_value:
                        score = 100  # å„ªç§€
                    elif req.min_value and result.measured_value >= req.target_value:
                        score = 100  # å„ªç§€
                    else:
                        score = 90   # è‰¯å¥½
                else:
                    score = 0        # ä¸åŠæ ¼
                
                # é—œéµæŒ‡æ¨™åŠ æ¬Š
                weight = 2.0 if req.critical else 1.0
                performance_scores.append(score * weight)
            
            # è¨ˆç®—åŠ æ¬Šå¹³å‡åˆ†
            total_weight = sum(2.0 if self._get_requirement(r.metric_name).critical else 1.0 
                             for r in self.test_results)
            
            overall_score = sum(performance_scores) / total_weight if total_weight > 0 else 0
            
            # æ€§èƒ½ç­‰ç´š
            if overall_score >= 90:
                performance_grade = "å„ªç§€"
            elif overall_score >= 80:
                performance_grade = "è‰¯å¥½"
            elif overall_score >= 70:
                performance_grade = "åˆæ ¼"
            else:
                performance_grade = "éœ€æ”¹é€²"
            
            return {
                "overall_score": overall_score,
                "performance_grade": performance_grade,
                "score_breakdown": {
                    f"{r.metric_name}": {
                        "score": 100 if r.passed else 0,
                        "weight": 2.0 if self._get_requirement(r.metric_name).critical else 1.0,
                        "critical": self._get_requirement(r.metric_name).critical
                    }
                    for r in self.test_results
                }
            }
            
        except Exception as e:
            logger.error(f"è¨ˆç®—æ•´é«”æ€§èƒ½å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _compare_with_baseline(self) -> Dict[str, Any]:
        """èˆ‡åŸºæº–ç·šå°æ¯”"""
        try:
            if not self.system_baseline:
                return {"error": "æ²’æœ‰ç³»çµ±åŸºæº–ç·šæ•¸æ“š"}
            
            current_memory = psutil.virtual_memory().used / (1024 * 1024)
            current_cpu = psutil.cpu_percent(interval=1.0)
            
            comparison = {
                "baseline_timestamp": self.system_baseline.timestamp.isoformat(),
                "current_timestamp": datetime.now(timezone.utc).isoformat(),
                "cpu_comparison": {
                    "baseline_cpu": self.system_baseline.cpu_usage_percent,
                    "current_cpu": current_cpu,
                    "cpu_change": current_cpu - self.system_baseline.cpu_usage_percent
                },
                "memory_comparison": {
                    "baseline_memory_mb": (psutil.virtual_memory().total - self.system_baseline.memory_available_mb * 1024 * 1024) / (1024 * 1024),
                    "current_memory_mb": current_memory,
                    "memory_change_mb": current_memory - ((psutil.virtual_memory().total - self.system_baseline.memory_available_mb * 1024 * 1024) / (1024 * 1024))
                }
            }
            
            return comparison
            
        except Exception as e:
            logger.error(f"åŸºæº–ç·šå°æ¯”å¤±æ•—: {e}")
            return {"error": str(e)}
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ€§èƒ½å„ªåŒ–å»ºè­°"""
        recommendations = []
        
        try:
            for result in self.test_results:
                if not result.passed:
                    req = self._get_requirement(result.metric_name)
                    
                    if result.metric_name == "tle_load_time":
                        recommendations.append("è€ƒæ…®å¯¦ç¾ TLE æ•¸æ“šç·©å­˜æˆ–ç•°æ­¥è¼‰å…¥æ©Ÿåˆ¶")
                    elif result.metric_name == "sgp4_calculation_rate":
                        recommendations.append("è€ƒæ…®ä¸¦è¡ŒåŒ– SGP4 è¨ˆç®—æˆ–å„ªåŒ–è¨ˆç®—ç®—æ³•")
                    elif result.metric_name == "api_response_time_p95":
                        recommendations.append("è€ƒæ…®æ·»åŠ  API éŸ¿æ‡‰ç·©å­˜æˆ–å„ªåŒ–æ•¸æ“šåº«æŸ¥è©¢")
                    elif result.metric_name == "memory_efficiency":
                        recommendations.append("è€ƒæ…®å„ªåŒ–å…§å­˜ä½¿ç”¨ï¼Œå¯¦ç¾å°è±¡æ± æˆ–æ›´æœ‰æ•ˆçš„ç·©å­˜ç­–ç•¥")
                    elif result.metric_name == "concurrent_requests_handled":
                        recommendations.append("è€ƒæ…®å¢åŠ å·¥ä½œç·šç¨‹æ•¸æˆ–å¯¦ç¾è«‹æ±‚éšŠåˆ—æ©Ÿåˆ¶")
                    else:
                        recommendations.append(f"å„ªåŒ– {req.description} çš„å¯¦ç¾")
            
            if not recommendations:
                recommendations.append("æ‰€æœ‰æ€§èƒ½æŒ‡æ¨™éƒ½é”åˆ°è¦æ±‚ï¼Œç³»çµ±æ€§èƒ½å„ªç§€")
            
            return recommendations
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå»ºè­°å¤±æ•—: {e}")
            return ["æ€§èƒ½åˆ†æå¤±æ•—ï¼Œç„¡æ³•ç”Ÿæˆå»ºè­°"]
    
    def export_benchmark_report(self, output_path: str, report_data: Dict[str, Any]):
        """å°å‡ºæ€§èƒ½åŸºæº–å ±å‘Š"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"æ€§èƒ½åŸºæº–å ±å‘Šå·²å°å‡º: {output_path}")
            
        except Exception as e:
            logger.error(f"å°å‡ºæ€§èƒ½åŸºæº–å ±å‘Šå¤±æ•—: {e}")

# ä¾¿åˆ©å‡½æ•¸
def create_performance_benchmark() -> PerformanceBenchmark:
    """å‰µå»ºæ€§èƒ½åŸºæº–æ¸¬è©¦å™¨"""
    return PerformanceBenchmark()

async def main():
    """ä¸»å‡½æ•¸"""
    logging.basicConfig(level=logging.INFO)
    
    try:
        print("ğŸš€ Phase 1 æ€§èƒ½åŸºæº–é©—è­‰")
        print("=" * 50)
        
        # å‰µå»ºæ€§èƒ½åŸºæº–æ¸¬è©¦å™¨
        benchmark = create_performance_benchmark()
        
        # åŸ·è¡Œæ€§èƒ½åŸºæº–æ¸¬è©¦
        report = await benchmark.run_performance_benchmark()
        
        # é¡¯ç¤ºçµæœæ‘˜è¦
        if "test_results" in report and "summary_statistics" in report["test_results"]:
            stats = report["test_results"]["summary_statistics"]
            print(f"\nğŸ“Š æ€§èƒ½åŸºæº–æ¸¬è©¦çµæœ:")
            print(f"ç¸½æ¸¬è©¦é …ç›®: {stats['total_tests']}")
            print(f"é€šéæ¸¬è©¦: {stats['passed_tests']}")
            print(f"æ•´é«”é€šéç‡: {stats['overall_pass_rate']:.1f}%")
            print(f"é—œéµæŒ‡æ¨™é€šéç‡: {stats['critical_pass_rate']:.1f}%")
        
        if "overall_performance" in report:
            perf = report["overall_performance"]
            print(f"æ€§èƒ½è©•ç´š: {perf.get('performance_grade', 'æœªçŸ¥')}")
            print(f"æ•´é«”å¾—åˆ†: {perf.get('overall_score', 0):.1f}")
        
        # å°å‡ºå ±å‘Š
        output_path = PHASE1_ROOT / "05_integration" / "performance_benchmark_report.json"
        benchmark.export_benchmark_report(str(output_path), report)
        print(f"\nè©³ç´°å ±å‘Šå·²ä¿å­˜: {output_path}")
        
        # æ¸…ç†è³‡æº
        if benchmark.performance_monitor:
            benchmark.performance_monitor.stop_monitoring()
        
        # è¿”å›æˆåŠŸç‹€æ…‹
        success = (report.get("test_results", {}).get("summary_statistics", {}).get("critical_pass_rate", 0) >= 80)
        return 0 if success else 1
        
    except Exception as e:
        logger.error(f"æ€§èƒ½åŸºæº–é©—è­‰å¤±æ•—: {e}")
        return 1

if __name__ == "__main__":
    import sys
    exit_code = asyncio.run(main())
    sys.exit(exit_code)