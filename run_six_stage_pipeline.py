#!/usr/bin/env python3
"""
å…­éšæ®µLEOè¡›æ˜Ÿç³»çµ±å®Œæ•´ç®¡ç·šåŸ·è¡Œå™¨
æ¢å¾©åŸå§‹å…­éšæ®µæ¶æ§‹ï¼Œå¯¦ç¾8,735â†’563è¡›æ˜Ÿçš„93.6%ç¯©é¸æ•ˆç‡

æ¶æ§‹æµç¨‹ï¼š
Stage 1: TLEè¼‰å…¥èˆ‡SGP4è»Œé“è¨ˆç®— â†’ 8,735é¡†è¡›æ˜Ÿè»Œé“æ•¸æ“š
Stage 2: æ™ºèƒ½ç¯©é¸ç³»çµ± â†’ 93.6%ç¯©é¸æ•ˆç‡ â†’ ~563é¡†å€™é¸è¡›æ˜Ÿ
Stage 3: ä¿¡è™Ÿåˆ†æèˆ‡3GPPäº‹ä»¶æª¢æ¸¬ â†’ A4/A5/D2åˆ‡æ›äº‹ä»¶
Stage 4: æ™‚é–“åºåˆ—æ•¸æ“šç”Ÿæˆ â†’ å‰ç«¯ç«‹é«”åœ–å‹•ç•«æ•¸æ“š
Stage 5: æ•¸æ“šæ•´åˆ â†’ PostgreSQL+Volumeæ··åˆå­˜å„²
Stage 6: å‹•æ…‹æ± è¦åŠƒ â†’ å¼·åŒ–å­¸ç¿’è¨“ç·´æ•¸æ“š
"""

import os
import sys
import json
import logging
import traceback
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional

# è¨­ç½®ç³»çµ±è·¯å¾‘
sys.path.insert(0, '/app')
sys.path.insert(0, '/app/netstack')
sys.path.insert(0, '/app/netstack/src')

# å¼•ç”¨å…­éšæ®µè™•ç†å™¨
from src.stages.stage1_tle_processor import Stage1TLEProcessor
from src.stages.stage2_filter_processor import Stage2FilterProcessor
from src.stages.stage3_signal_processor import Stage3SignalProcessor
from src.stages.stage4_timeseries_processor import Stage4TimeseriesProcessor
from src.stages.stage5_integration_processor import Stage5IntegrationProcessor
from src.stages.stage6_dynamic_pool_planner import Stage6DynamicPoolPlanner

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SixStagePipelineOrchestrator:
    """å…­éšæ®µLEOè¡›æ˜Ÿç³»çµ±å®Œæ•´ç®¡ç·šå”èª¿å™¨"""
    
    def __init__(self, data_dir: str = "/app/data", observer_lat: float = 24.9441667, 
                 observer_lon: float = 121.3713889, sample_mode: bool = False):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.observer_lat = observer_lat
        self.observer_lon = observer_lon
        self.sample_mode = sample_mode
        
        # åŸ·è¡Œçµæœçµ±è¨ˆ
        self.execution_stats = {
            "start_time": datetime.now(timezone.utc),
            "stages_completed": [],
            "stages_failed": [],
            "total_satellites": 0,
            "filtered_satellites": 0,
            "filtering_efficiency": 0.0
        }
        
        logger.info("ğŸš€ å…­éšæ®µLEOè¡›æ˜Ÿç³»çµ±åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"ğŸ“ è§€æ¸¬é»åº§æ¨™: ({observer_lat}, {observer_lon})")
        logger.info(f"ğŸ“‚ æ•¸æ“šç›®éŒ„: {data_dir}")
        logger.info(f"ğŸ›ï¸ é‹è¡Œæ¨¡å¼: {'å–æ¨£æ¨¡å¼' if sample_mode else 'å…¨é‡æ¨¡å¼'}")
    
    def execute_stage1_tle_loading(self) -> bool:
        """åŸ·è¡Œéšæ®µä¸€ï¼šTLEæ•¸æ“šè¼‰å…¥èˆ‡SGP4è»Œé“è¨ˆç®—"""
        try:
            logger.info("ğŸ“¡ Stage 1: é–‹å§‹TLEæ•¸æ“šè¼‰å…¥èˆ‡è»Œé“è¨ˆç®—...")
            
            processor = Stage1TLEProcessor(
                tle_data_dir="/app/tle_data",
                output_dir=str(self.data_dir),
                sample_mode=self.sample_mode
            )
            
            result = processor.process()
            
            if result and 'output_file' in result:
                self.execution_stats["total_satellites"] = result.get("total_satellites", 0)
                self.execution_stats["stages_completed"].append("stage1")
                logger.info(f"âœ… Stage 1 å®Œæˆ: è™•ç† {self.execution_stats['total_satellites']} é¡†è¡›æ˜Ÿ")
                return True
            else:
                logger.error("âŒ Stage 1 å¤±æ•—ï¼šè™•ç†å™¨è¿”å›ç©ºçµæœ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Stage 1 ç•°å¸¸: {e}")
            logger.error(traceback.format_exc())
            self.execution_stats["stages_failed"].append("stage1")
            return False
    
    def execute_stage2_intelligent_filtering(self) -> bool:
        """åŸ·è¡Œéšæ®µäºŒï¼šæ™ºèƒ½è¡›æ˜Ÿç¯©é¸ï¼ˆ93.6%ç¯©é¸æ•ˆç‡ï¼‰"""
        try:
            logger.info("ğŸ” Stage 2: é–‹å§‹æ™ºèƒ½è¡›æ˜Ÿç¯©é¸...")
            
            processor = Stage2FilterProcessor(
                observer_lat=self.observer_lat,
                observer_lon=self.observer_lon,
                input_dir=str(self.data_dir),
                output_dir=str(self.data_dir)
            )
            
            result = processor.process()
            
            if result and 'output_file' in result:
                self.execution_stats["filtered_satellites"] = result.get("filtered_count", 0)
                if self.execution_stats["total_satellites"] > 0:
                    self.execution_stats["filtering_efficiency"] = (
                        1.0 - self.execution_stats["filtered_satellites"] / self.execution_stats["total_satellites"]
                    ) * 100
                
                self.execution_stats["stages_completed"].append("stage2")
                logger.info(f"âœ… Stage 2 å®Œæˆ: ç¯©é¸å‡º {self.execution_stats['filtered_satellites']} é¡†è¡›æ˜Ÿ")
                logger.info(f"ğŸ“Š ç¯©é¸æ•ˆç‡: {self.execution_stats['filtering_efficiency']:.1f}%")
                return True
            else:
                logger.error("âŒ Stage 2 å¤±æ•—ï¼šç¯©é¸è™•ç†å™¨è¿”å›ç©ºçµæœ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Stage 2 ç•°å¸¸: {e}")
            logger.error(traceback.format_exc())
            self.execution_stats["stages_failed"].append("stage2")
            return False
    
    def execute_stage3_signal_analysis(self) -> bool:
        """åŸ·è¡Œéšæ®µä¸‰ï¼šä¿¡è™Ÿåˆ†æèˆ‡3GPPäº‹ä»¶æª¢æ¸¬"""
        try:
            logger.info("ğŸ“¶ Stage 3: é–‹å§‹ä¿¡è™Ÿåˆ†æèˆ‡äº‹ä»¶æª¢æ¸¬...")
            
            processor = Stage3SignalProcessor(
                input_dir=str(self.data_dir),
                output_dir=str(self.data_dir)
            )
            
            result = processor.process()
            
            if result and 'output_file' in result:
                self.execution_stats["stages_completed"].append("stage3")
                logger.info("âœ… Stage 3 å®Œæˆ: 3GPP A4/A5/D2äº‹ä»¶æª¢æ¸¬å®Œæˆ")
                return True
            else:
                logger.error("âŒ Stage 3 å¤±æ•—ï¼šä¿¡è™Ÿåˆ†æè™•ç†å™¨è¿”å›ç©ºçµæœ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Stage 3 ç•°å¸¸: {e}")
            logger.error(traceback.format_exc())
            self.execution_stats["stages_failed"].append("stage3")
            return False
    
    def execute_stage4_timeseries_generation(self) -> bool:
        """åŸ·è¡Œéšæ®µå››ï¼šæ™‚é–“åºåˆ—æ•¸æ“šç”Ÿæˆï¼ˆå‰ç«¯ç«‹é«”åœ–ï¼‰"""
        try:
            logger.info("ğŸ“ˆ Stage 4: é–‹å§‹æ™‚é–“åºåˆ—æ•¸æ“šç”Ÿæˆ...")
            
            processor = Stage4TimeseriesProcessor(
                input_dir=str(self.data_dir),
                output_dir=str(self.data_dir)
            )
            
            result = processor.process()
            
            if result and 'output_file' in result:
                self.execution_stats["stages_completed"].append("stage4")
                logger.info("âœ… Stage 4 å®Œæˆ: å‰ç«¯å‹•ç•«æ™‚é–“åºåˆ—æ•¸æ“šç”Ÿæˆå®Œæˆ")
                return True
            else:
                logger.error("âŒ Stage 4 å¤±æ•—ï¼šæ™‚é–“åºåˆ—è™•ç†å™¨è¿”å›ç©ºçµæœ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Stage 4 ç•°å¸¸: {e}")
            logger.error(traceback.format_exc())
            self.execution_stats["stages_failed"].append("stage4")
            return False
    
    def execute_stage5_data_integration(self) -> bool:
        """åŸ·è¡Œéšæ®µäº”ï¼šæ•¸æ“šæ•´åˆï¼ˆPostgreSQL+Volumeï¼‰"""
        try:
            logger.info("ğŸ—„ï¸ Stage 5: é–‹å§‹æ•¸æ“šæ•´åˆ...")
            
            processor = Stage5IntegrationProcessor(
                input_dir=str(self.data_dir),
                output_dir=str(self.data_dir)
            )
            
            result = processor.process()
            
            if result and 'output_file' in result:
                self.execution_stats["stages_completed"].append("stage5")
                logger.info("âœ… Stage 5 å®Œæˆ: PostgreSQL+Volumeæ•¸æ“šæ•´åˆå®Œæˆ")
                return True
            else:
                logger.error("âŒ Stage 5 å¤±æ•—ï¼šæ•¸æ“šæ•´åˆè™•ç†å™¨è¿”å›ç©ºçµæœ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Stage 5 ç•°å¸¸: {e}")
            logger.error(traceback.format_exc())
            self.execution_stats["stages_failed"].append("stage5")
            return False
    
    def execute_stage6_dynamic_pool_planning(self) -> bool:
        """åŸ·è¡Œéšæ®µå…­ï¼šå‹•æ…‹æ± è¦åŠƒï¼ˆå¼·åŒ–å­¸ç¿’æ•¸æ“šï¼‰"""
        try:
            logger.info("ğŸ§  Stage 6: é–‹å§‹å‹•æ…‹æ± è¦åŠƒ...")
            
            processor = Stage6DynamicPoolPlanner(
                input_dir=str(self.data_dir),
                output_dir=str(self.data_dir)
            )
            
            result = processor.process()
            
            if result and 'output_file' in result:
                self.execution_stats["stages_completed"].append("stage6")
                logger.info("âœ… Stage 6 å®Œæˆ: å‹•æ…‹æ± è¦åŠƒèˆ‡RLæ•¸æ“šæº–å‚™å®Œæˆ")
                return True
            else:
                logger.error("âŒ Stage 6 å¤±æ•—ï¼šå‹•æ…‹æ± è¦åŠƒè™•ç†å™¨è¿”å›ç©ºçµæœ")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Stage 6 ç•°å¸¸: {e}")
            logger.error(traceback.format_exc())
            self.execution_stats["stages_failed"].append("stage6")
            return False
    
    def execute_complete_pipeline(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„å…­éšæ®µç®¡ç·š"""
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå…­éšæ®µLEOè¡›æ˜Ÿç³»çµ±å®Œæ•´ç®¡ç·š")
        logger.info("=" * 60)
        
        # åŸ·è¡Œæ‰€æœ‰éšæ®µ
        stages = [
            ("Stage 1: TLEè¼‰å…¥", self.execute_stage1_tle_loading),
            ("Stage 2: æ™ºèƒ½ç¯©é¸", self.execute_stage2_intelligent_filtering),
            ("Stage 3: ä¿¡è™Ÿåˆ†æ", self.execute_stage3_signal_analysis),
            ("Stage 4: æ™‚é–“åºåˆ—", self.execute_stage4_timeseries_generation),
            ("Stage 5: æ•¸æ“šæ•´åˆ", self.execute_stage5_data_integration),
            ("Stage 6: å‹•æ…‹æ± è¦åŠƒ", self.execute_stage6_dynamic_pool_planning)
        ]
        
        for stage_name, stage_func in stages:
            logger.info(f"ğŸ”„ åŸ·è¡Œ {stage_name}...")
            success = stage_func()
            
            if not success:
                logger.error(f"âŒ {stage_name} å¤±æ•—ï¼Œåœæ­¢ç®¡ç·šåŸ·è¡Œ")
                break
            
            logger.info(f"âœ… {stage_name} æˆåŠŸå®Œæˆ")
            logger.info("-" * 40)
        
        # ç”ŸæˆåŸ·è¡Œå ±å‘Š
        self.execution_stats["end_time"] = datetime.now(timezone.utc)
        self.execution_stats["total_duration"] = (
            self.execution_stats["end_time"] - self.execution_stats["start_time"]
        ).total_seconds()
        
        success = len(self.execution_stats["stages_failed"]) == 0
        
        logger.info("=" * 60)
        logger.info("ğŸ“Š å…­éšæ®µç®¡ç·šåŸ·è¡Œå ±å‘Š")
        logger.info(f"ğŸ¯ åŸ·è¡Œç‹€æ…‹: {'âœ… å®Œå…¨æˆåŠŸ' if success else 'âŒ éƒ¨åˆ†å¤±æ•—'}")
        logger.info(f"â±ï¸ ç¸½åŸ·è¡Œæ™‚é–“: {self.execution_stats['total_duration']:.1f} ç§’")
        logger.info(f"ğŸ“¡ è™•ç†è¡›æ˜Ÿç¸½æ•¸: {self.execution_stats['total_satellites']}")
        logger.info(f"ğŸ” ç¯©é¸å¾Œè¡›æ˜Ÿæ•¸: {self.execution_stats['filtered_satellites']}")
        logger.info(f"ğŸ“ˆ ç¯©é¸æ•ˆç‡: {self.execution_stats['filtering_efficiency']:.1f}%")
        logger.info(f"âœ… æˆåŠŸéšæ®µ: {', '.join(self.execution_stats['stages_completed'])}")
        
        if self.execution_stats["stages_failed"]:
            logger.info(f"âŒ å¤±æ•—éšæ®µ: {', '.join(self.execution_stats['stages_failed'])}")
        
        # ä¿å­˜åŸ·è¡Œå ±å‘Š
        report_file = self.data_dir / "six_stage_execution_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            # è½‰æ›datetimeå°è±¡ç‚ºå­—ç¬¦ä¸²
            stats_copy = self.execution_stats.copy()
            stats_copy["start_time"] = stats_copy["start_time"].isoformat()
            stats_copy["end_time"] = stats_copy["end_time"].isoformat()
            json.dump(stats_copy, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ åŸ·è¡Œå ±å‘Šå·²ä¿å­˜: {report_file}")
        
        return {
            "success": success,
            "stats": self.execution_stats,
            "report_file": str(report_file)
        }

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="å…­éšæ®µLEOè¡›æ˜Ÿç³»çµ±å®Œæ•´ç®¡ç·šåŸ·è¡Œå™¨")
    parser.add_argument("--data-dir", default="/app/data", help="æ•¸æ“šç›®éŒ„è·¯å¾‘")
    parser.add_argument("--observer-lat", type=float, default=24.9441667, help="è§€æ¸¬é»ç·¯åº¦")
    parser.add_argument("--observer-lon", type=float, default=121.3713889, help="è§€æ¸¬é»ç¶“åº¦")
    parser.add_argument("--sample-mode", action="store_true", help="å•Ÿç”¨å–æ¨£æ¨¡å¼")
    
    args = parser.parse_args()
    
    try:
        orchestrator = SixStagePipelineOrchestrator(
            data_dir=args.data_dir,
            observer_lat=args.observer_lat,
            observer_lon=args.observer_lon,
            sample_mode=args.sample_mode
        )
        
        result = orchestrator.execute_complete_pipeline()
        
        if result["success"]:
            logger.info("ğŸ‰ å…­éšæ®µç®¡ç·šåŸ·è¡Œå®Œå…¨æˆåŠŸï¼")
            sys.exit(0)
        else:
            logger.error("ğŸ’¥ å…­éšæ®µç®¡ç·šåŸ·è¡Œå¤±æ•—ï¼")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"ğŸ’¥ ç®¡ç·šåŸ·è¡Œå™¨ç•°å¸¸: {e}")
        logger.error(traceback.format_exc())
        sys.exit(1)

if __name__ == "__main__":
    main()