#!/usr/bin/env python3
"""
éšæ®µäºŒ 2.4 å¤šæ–¹æ¡ˆæ¸¬è©¦æ”¯æ´æ¸¬è©¦ç¨‹å¼

æ¸¬è©¦å››ç¨®åˆ‡æ›æ–¹æ¡ˆçš„æ”¯æ´åŠŸèƒ½ï¼š
1. Baseline (NTN) - 3GPP æ¨™æº–éåœ°é¢ç¶²è·¯æ›æ‰‹
2. NTN-GS - åœ°é¢ç«™å”åŠ©æ–¹æ¡ˆ  
3. NTN-SMN - è¡›æ˜Ÿç¶²è·¯å…§æ›æ‰‹æ–¹æ¡ˆ
4. Proposed - æœ¬è«–æ–‡æ–¹æ¡ˆ

åŸ·è¡Œæ–¹å¼:
cd /home/sat/ntn-stack
source venv/bin/activate  
python paper/2.4_multi_scheme_support/test_multi_scheme_support.py
"""

import sys
import asyncio
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any
import statistics
import os
from pathlib import Path

# æ·»åŠ  NetStack è·¯å¾‘
sys.path.insert(0, '/home/sat/ntn-stack/netstack')

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MultiSchemeSupportTester:
    """å¤šæ–¹æ¡ˆæ¸¬è©¦æ”¯æ´æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.test_results = []
        self.scheme_metrics = {}
        self.measurement_service = None
    
    async def setup_test_environment(self) -> bool:
        """è¨­ç½®æ¸¬è©¦ç’°å¢ƒ"""
        try:
            from netstack_api.services.handover_measurement_service import (
                HandoverMeasurementService,
                HandoverMeasurement,
                HandoverScheme,
                HandoverResult,
                HandoverEvent
            )
            
            # åˆå§‹åŒ–æ¸¬è©¦æœå‹™
            self.measurement_service = HandoverMeasurementService()
            self.handover_measurement = self.measurement_service.measurement  # ç›´æ¥å¼•ç”¨åº•å±¤æ¸¬é‡å°è±¡
            
            # ä¿å­˜é¡åˆ¥å¼•ç”¨
            self.HandoverScheme = HandoverScheme
            self.HandoverResult = HandoverResult
            self.HandoverEvent = HandoverEvent
            
            print("âœ… å¤šæ–¹æ¡ˆæ¸¬è©¦æ”¯æ´ç’°å¢ƒåˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âŒ æ¸¬è©¦ç’°å¢ƒè¨­ç½®å¤±æ•—: {e}")
            return False
    
    async def test_scheme_initialization(self) -> bool:
        """æ¸¬è©¦æ–¹æ¡ˆåˆå§‹åŒ–åŠŸèƒ½"""
        print("\nğŸ”¬ æ¸¬è©¦æ–¹æ¡ˆåˆå§‹åŒ–åŠŸèƒ½")
        print("-" * 50)
        
        try:
            # æ¸¬è©¦æ‰€æœ‰æ–¹æ¡ˆé¡å‹æ˜¯å¦å¯ç”¨
            expected_schemes = [
                self.HandoverScheme.NTN_BASELINE,
                self.HandoverScheme.NTN_GS, 
                self.HandoverScheme.NTN_SMN,
                self.HandoverScheme.PROPOSED
            ]
            
            tests_passed = 0
            total_tests = 4
            
            # æ¸¬è©¦ 1: æ–¹æ¡ˆæšèˆ‰å®Œæ•´æ€§
            available_schemes = list(self.HandoverScheme)
            if len(available_schemes) >= 4:
                tests_passed += 1
                print(f"  âœ… æ–¹æ¡ˆæšèˆ‰å®Œæ•´æ€§: {len(available_schemes)} ç¨®æ–¹æ¡ˆå¯ç”¨")
            else:
                print(f"  âŒ æ–¹æ¡ˆæšèˆ‰ä¸å®Œæ•´: åƒ… {len(available_schemes)} ç¨®æ–¹æ¡ˆ")
            
            # æ¸¬è©¦ 2: æ–¹æ¡ˆå€¼æ­£ç¢ºæ€§
            scheme_values = {scheme.value for scheme in expected_schemes}
            expected_values = {"NTN", "NTN-GS", "NTN-SMN", "Proposed"}
            if scheme_values == expected_values:
                tests_passed += 1
                print(f"  âœ… æ–¹æ¡ˆå€¼æ­£ç¢ºæ€§: {scheme_values}")
            else:
                print(f"  âŒ æ–¹æ¡ˆå€¼éŒ¯èª¤: æœŸå¾… {expected_values}ï¼Œå¯¦éš› {scheme_values}")
            
            # æ¸¬è©¦ 3: æ¸¬é‡æœå‹™æ–¹æ¡ˆæ”¯æ´
            measurement_schemes = []
            for scheme in expected_schemes:
                try:
                    # æ¸¬è©¦è¨˜éŒ„ä¸€å€‹ç°¡å–®äº‹ä»¶
                    start_time = time.time()
                    event_id = self.handover_measurement.record_handover(
                        ue_id=f"test_init_{scheme.value}",
                        source_gnb="test_source",
                        target_gnb="test_target",
                        start_time=start_time,
                        end_time=start_time + 0.025,  # 25ms å»¶é²
                        handover_scheme=scheme,
                        result=self.HandoverResult.SUCCESS
                    )
                    measurement_schemes.append(scheme)
                except Exception as e:
                    print(f"    âš ï¸  æ–¹æ¡ˆ {scheme.value} è¨˜éŒ„å¤±æ•—: {e}")
            
            if len(measurement_schemes) == len(expected_schemes):
                tests_passed += 1
                print(f"  âœ… æ¸¬é‡æœå‹™æ–¹æ¡ˆæ”¯æ´: {len(measurement_schemes)}/{len(expected_schemes)}")
            else:
                print(f"  âŒ æ¸¬é‡æœå‹™æ–¹æ¡ˆæ”¯æ´ä¸å®Œæ•´: {len(measurement_schemes)}/{len(expected_schemes)}")
            
            # æ¸¬è©¦ 4: æ–¹æ¡ˆç‰¹æ€§é©—è­‰
            scheme_characteristics = {
                self.HandoverScheme.NTN_BASELINE: {"expected_latency_range": (200, 300)},
                self.HandoverScheme.NTN_GS: {"expected_latency_range": (130, 180)},
                self.HandoverScheme.NTN_SMN: {"expected_latency_range": (140, 180)},
                self.HandoverScheme.PROPOSED: {"expected_latency_range": (15, 40)}
            }
            
            characteristics_valid = True
            for scheme, chars in scheme_characteristics.items():
                latency_range = chars["expected_latency_range"]
                if latency_range[0] > 0 and latency_range[1] > latency_range[0]:
                    continue
                else:
                    characteristics_valid = False
                    break
            
            if characteristics_valid:
                tests_passed += 1
                print(f"  âœ… æ–¹æ¡ˆç‰¹æ€§é©—è­‰: å»¶é²ç¯„åœå®šç¾©æ­£ç¢º")
            else:
                print(f"  âŒ æ–¹æ¡ˆç‰¹æ€§é©—è­‰å¤±æ•—")
            
            success_rate = tests_passed / total_tests
            print(f"\nğŸ“Š æ–¹æ¡ˆåˆå§‹åŒ–æ¸¬è©¦çµæœ: {tests_passed}/{total_tests} ({success_rate:.1%})")
            
            self.test_results.append(("æ–¹æ¡ˆåˆå§‹åŒ–", success_rate >= 0.8))
            return success_rate >= 0.8
            
        except Exception as e:
            print(f"âŒ æ–¹æ¡ˆåˆå§‹åŒ–æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results.append(("æ–¹æ¡ˆåˆå§‹åŒ–", False))
            return False
    
    async def test_scheme_differentiation(self) -> bool:
        """æ¸¬è©¦æ–¹æ¡ˆå·®ç•°åŒ–åŠŸèƒ½"""
        print("\nğŸ”¬ æ¸¬è©¦æ–¹æ¡ˆå·®ç•°åŒ–åŠŸèƒ½")
        print("-" * 50)
        
        try:
            # ç‚ºæ¯ç¨®æ–¹æ¡ˆç”Ÿæˆæ¸¬è©¦æ•¸æ“š
            schemes_config = {
                self.HandoverScheme.NTN_BASELINE: {"target_latency": 250.0, "variance": 25.0},
                self.HandoverScheme.NTN_GS: {"target_latency": 153.0, "variance": 15.0},
                self.HandoverScheme.NTN_SMN: {"target_latency": 158.5, "variance": 16.0},
                self.HandoverScheme.PROPOSED: {"target_latency": 25.0, "variance": 5.0}
            }
            
            scheme_events = {}
            start_time = time.time()
            
            # ç‚ºæ¯ç¨®æ–¹æ¡ˆç”Ÿæˆ 30 å€‹äº‹ä»¶
            for scheme, config in schemes_config.items():
                events = []
                target_latency = config["target_latency"]
                variance = config["variance"]
                
                for i in range(30):
                    # æ¨¡æ“¬å»¶é²å€¼ï¼ˆä½¿ç”¨ç›®æ¨™å»¶é² Â± è®ŠåŒ–é‡ï¼‰
                    import random
                    latency_variation = random.uniform(-variance, variance)
                    simulated_latency = max(5.0, target_latency + latency_variation)
                    
                    event_start = time.time()
                    event_end = event_start + (simulated_latency / 1000.0)
                    
                    event_id = self.handover_measurement.record_handover(
                        ue_id=f"test_diff_{scheme.value}_{i}",
                        source_gnb=f"src_{i}",
                        target_gnb=f"tgt_{i}",
                        start_time=event_start,
                        end_time=event_end,
                        handover_scheme=scheme,
                        result=self.HandoverResult.SUCCESS
                    )
                    
                    events.append({
                        "event_id": event_id,
                        "latency_ms": simulated_latency,
                        "scheme": scheme
                    })
                
                scheme_events[scheme] = events
            
            total_duration = (time.time() - start_time) * 1000
            
            # é©—è­‰çµæœ
            tests_passed = 0
            total_tests = 5
            
            # æ¸¬è©¦ 1: äº‹ä»¶ç”Ÿæˆå®Œæ•´æ€§
            total_events = sum(len(events) for events in scheme_events.values())
            expected_events = len(schemes_config) * 30
            if total_events == expected_events:
                tests_passed += 1
                print(f"  âœ… äº‹ä»¶ç”Ÿæˆå®Œæ•´æ€§: {total_events}/{expected_events}")
            else:
                print(f"  âŒ äº‹ä»¶ç”Ÿæˆä¸å®Œæ•´: {total_events}/{expected_events}")
            
            # æ¸¬è©¦ 2: æ–¹æ¡ˆåˆ†ä½ˆå‡å‹»æ€§
            scheme_counts = {scheme: len(events) for scheme, events in scheme_events.items()}
            if all(count == 30 for count in scheme_counts.values()):
                tests_passed += 1
                print(f"  âœ… æ–¹æ¡ˆåˆ†ä½ˆå‡å‹»: æ¯æ–¹æ¡ˆ 30 å€‹äº‹ä»¶")
            else:
                print(f"  âŒ æ–¹æ¡ˆåˆ†ä½ˆä¸å‡: {scheme_counts}")
            
            # æ¸¬è©¦ 3: å»¶é²å·®ç•°åŒ–é©—è­‰
            scheme_latencies = {}
            for scheme, events in scheme_events.items():
                latencies = [event["latency_ms"] for event in events]
                avg_latency = statistics.mean(latencies)
                scheme_latencies[scheme] = avg_latency
            
            # é©—è­‰ Proposed æ–¹æ¡ˆå»¶é²æœ€ä½
            proposed_latency = scheme_latencies[self.HandoverScheme.PROPOSED]
            other_latencies = [lat for scheme, lat in scheme_latencies.items() 
                             if scheme != self.HandoverScheme.PROPOSED]
            
            if proposed_latency < min(other_latencies):
                tests_passed += 1
                print(f"  âœ… Proposed æ–¹æ¡ˆå»¶é²æœ€ä½: {proposed_latency:.1f}ms")
            else:
                print(f"  âŒ Proposed æ–¹æ¡ˆå»¶é²æœªé”æœ€å„ª: {proposed_latency:.1f}ms")
            
            # æ¸¬è©¦ 4: æ–¹æ¡ˆå»¶é²ç¯„åœåˆç†æ€§
            latency_ranges_valid = True
            for scheme, config in schemes_config.items():
                avg_latency = scheme_latencies[scheme]
                target_latency = config["target_latency"]
                acceptable_deviation = config["variance"] * 2  # å…è¨± 2 å€è®ŠåŒ–é‡åå·®
                
                if abs(avg_latency - target_latency) <= acceptable_deviation:
                    continue
                else:
                    latency_ranges_valid = False
                    print(f"    âš ï¸  {scheme.value} å»¶é²åå·®éå¤§: {avg_latency:.1f}ms vs ç›®æ¨™ {target_latency:.1f}ms")
            
            if latency_ranges_valid:
                tests_passed += 1
                print(f"  âœ… æ–¹æ¡ˆå»¶é²ç¯„åœåˆç†æ€§é€šé")
            else:
                print(f"  âŒ æ–¹æ¡ˆå»¶é²ç¯„åœåˆç†æ€§å¤±æ•—")
            
            # æ¸¬è©¦ 5: å·®ç•°åŒ–ç”Ÿæˆæ•ˆç‡
            avg_generation_time = total_duration / total_events
            if avg_generation_time < 100:  # æ¯äº‹ä»¶ < 100ms
                tests_passed += 1
                print(f"  âœ… å·®ç•°åŒ–ç”Ÿæˆæ•ˆç‡è‰¯å¥½: {avg_generation_time:.1f}ms/äº‹ä»¶")
            else:
                print(f"  âŒ å·®ç•°åŒ–ç”Ÿæˆæ•ˆç‡éä½: {avg_generation_time:.1f}ms/äº‹ä»¶")
            
            self.scheme_metrics["differentiation"] = {
                "total_events": total_events,
                "scheme_latencies": {str(k): v for k, v in scheme_latencies.items()},
                "generation_time_ms": total_duration
            }
            
            success_rate = tests_passed / total_tests
            print(f"\nğŸ“Š æ–¹æ¡ˆå·®ç•°åŒ–æ¸¬è©¦çµæœ: {tests_passed}/{total_tests} ({success_rate:.1%})")
            
            self.test_results.append(("æ–¹æ¡ˆå·®ç•°åŒ–", success_rate >= 0.8))
            return success_rate >= 0.8
            
        except Exception as e:
            print(f"âŒ æ–¹æ¡ˆå·®ç•°åŒ–æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results.append(("æ–¹æ¡ˆå·®ç•°åŒ–", False))
            return False
    
    async def test_scheme_switching(self) -> bool:
        """æ¸¬è©¦æ–¹æ¡ˆåˆ‡æ›åŠŸèƒ½"""
        print("\nğŸ”¬ æ¸¬è©¦æ–¹æ¡ˆåˆ‡æ›åŠŸèƒ½")
        print("-" * 50)
        
        try:
            # æ¨¡æ“¬é‹è¡Œæ™‚æ–¹æ¡ˆåˆ‡æ›
            switch_sequence = [
                self.HandoverScheme.NTN_BASELINE,
                self.HandoverScheme.PROPOSED,
                self.HandoverScheme.NTN_GS,
                self.HandoverScheme.NTN_SMN,
                self.HandoverScheme.PROPOSED
            ]
            
            switch_results = []
            start_time = time.time()
            
            for i, scheme in enumerate(switch_sequence):
                # æ¨¡æ“¬æ–¹æ¡ˆåˆ‡æ›çš„åˆ‡æ›äº‹ä»¶
                ue_id = f"switching_ue_{i}"
                event_start = time.time()
                
                # æ ¹æ“šæ–¹æ¡ˆèª¿æ•´å»¶é²
                if scheme == self.HandoverScheme.PROPOSED:
                    latency_ms = 25.0
                elif scheme == self.HandoverScheme.NTN_GS:
                    latency_ms = 153.0
                elif scheme == self.HandoverScheme.NTN_SMN:
                    latency_ms = 158.5
                else:  # NTN_BASELINE
                    latency_ms = 250.0
                
                event_end = event_start + (latency_ms / 1000.0)
                
                event_id = self.handover_measurement.record_handover(
                    ue_id=ue_id,
                    source_gnb=f"switch_src_{i}",
                    target_gnb=f"switch_tgt_{i}",
                    start_time=event_start,
                    end_time=event_end,
                    handover_scheme=scheme,
                    result=self.HandoverResult.SUCCESS
                )
                
                switch_results.append({
                    "step": i,
                    "scheme": scheme,
                    "event_id": event_id,
                    "latency_ms": latency_ms
                })
                
                # çŸ­æš«ç­‰å¾…æ¨¡æ“¬å¯¦éš›åˆ‡æ›é–“éš”
                await asyncio.sleep(0.1)
            
            total_duration = (time.time() - start_time) * 1000
            
            # é©—è­‰çµæœ
            tests_passed = 0
            total_tests = 4
            
            # æ¸¬è©¦ 1: åˆ‡æ›åºåˆ—å®Œæ•´æ€§
            if len(switch_results) == len(switch_sequence):
                tests_passed += 1
                print(f"  âœ… åˆ‡æ›åºåˆ—å®Œæ•´æ€§: {len(switch_results)}/{len(switch_sequence)}")
            else:
                print(f"  âŒ åˆ‡æ›åºåˆ—ä¸å®Œæ•´: {len(switch_results)}/{len(switch_sequence)}")
            
            # æ¸¬è©¦ 2: æ–¹æ¡ˆåˆ‡æ›æ­£ç¢ºæ€§
            correct_switches = 0
            for i, result in enumerate(switch_results):
                if result["scheme"] == switch_sequence[i]:
                    correct_switches += 1
            
            if correct_switches == len(switch_sequence):
                tests_passed += 1
                print(f"  âœ… æ–¹æ¡ˆåˆ‡æ›æ­£ç¢ºæ€§: {correct_switches}/{len(switch_sequence)}")
            else:
                print(f"  âŒ æ–¹æ¡ˆåˆ‡æ›éŒ¯èª¤: {correct_switches}/{len(switch_sequence)}")
            
            # æ¸¬è©¦ 3: åˆ‡æ›å»¶é²ä¸€è‡´æ€§
            latency_consistent = True
            for result in switch_results:
                scheme = result["scheme"]
                actual_latency = result["latency_ms"]
                
                # æª¢æŸ¥å»¶é²æ˜¯å¦ç¬¦åˆæ–¹æ¡ˆç‰¹æ€§
                if scheme == self.HandoverScheme.PROPOSED and 20 <= actual_latency <= 35:
                    continue
                elif scheme == self.HandoverScheme.NTN_GS and 140 <= actual_latency <= 170:
                    continue
                elif scheme == self.HandoverScheme.NTN_SMN and 140 <= actual_latency <= 180:
                    continue
                elif scheme == self.HandoverScheme.NTN_BASELINE and 200 <= actual_latency <= 300:
                    continue
                else:
                    latency_consistent = False
                    print(f"    âš ï¸  {scheme.value} å»¶é²ç•°å¸¸: {actual_latency:.1f}ms")
            
            if latency_consistent:
                tests_passed += 1
                print(f"  âœ… åˆ‡æ›å»¶é²ä¸€è‡´æ€§é€šé")
            else:
                print(f"  âŒ åˆ‡æ›å»¶é²ä¸€è‡´æ€§å¤±æ•—")
            
            # æ¸¬è©¦ 4: åˆ‡æ›æ•ˆç‡
            avg_switch_time = total_duration / len(switch_results)
            if avg_switch_time < 200:  # æ¯æ¬¡åˆ‡æ› < 200ms
                tests_passed += 1
                print(f"  âœ… åˆ‡æ›æ•ˆç‡è‰¯å¥½: {avg_switch_time:.1f}ms/åˆ‡æ›")
            else:
                print(f"  âŒ åˆ‡æ›æ•ˆç‡éä½: {avg_switch_time:.1f}ms/åˆ‡æ›")
            
            self.scheme_metrics["switching"] = {
                "switch_count": len(switch_results),
                "total_duration_ms": total_duration,
                "avg_switch_time_ms": avg_switch_time
            }
            
            success_rate = tests_passed / total_tests
            print(f"\nğŸ“Š æ–¹æ¡ˆåˆ‡æ›æ¸¬è©¦çµæœ: {tests_passed}/{total_tests} ({success_rate:.1%})")
            
            self.test_results.append(("æ–¹æ¡ˆåˆ‡æ›", success_rate >= 0.8))
            return success_rate >= 0.8
            
        except Exception as e:
            print(f"âŒ æ–¹æ¡ˆåˆ‡æ›æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results.append(("æ–¹æ¡ˆåˆ‡æ›", False))
            return False
    
    async def test_scheme_performance_isolation(self) -> bool:
        """æ¸¬è©¦æ–¹æ¡ˆæ•ˆèƒ½éš”é›¢åŠŸèƒ½"""
        print("\nğŸ”¬ æ¸¬è©¦æ–¹æ¡ˆæ•ˆèƒ½éš”é›¢åŠŸèƒ½")
        print("-" * 50)
        
        try:
            # æ¨¡æ“¬åŒæ™‚åŸ·è¡Œå¤šç¨®æ–¹æ¡ˆçš„å ´æ™¯
            concurrent_schemes = [
                self.HandoverScheme.NTN_BASELINE,
                self.HandoverScheme.NTN_GS,
                self.HandoverScheme.NTN_SMN,
                self.HandoverScheme.PROPOSED
            ]
            
            # ç‚ºæ¯ç¨®æ–¹æ¡ˆä¸¦è¡Œç”Ÿæˆäº‹ä»¶
            tasks = []
            start_time = time.time()
            
            async def generate_scheme_events(scheme: self.HandoverScheme, count: int = 15):
                events = []
                base_latency = {
                    self.HandoverScheme.NTN_BASELINE: 250.0,
                    self.HandoverScheme.NTN_GS: 153.0,
                    self.HandoverScheme.NTN_SMN: 158.5,
                    self.HandoverScheme.PROPOSED: 25.0
                }[scheme]
                
                for i in range(count):
                    import random
                    # æ·»åŠ ä¸€äº›éš¨æ©Ÿè®ŠåŒ–
                    latency_ms = base_latency + random.uniform(-5, 5)
                    
                    event_start = time.time()
                    event_end = event_start + (latency_ms / 1000.0)
                    
                    event_id = self.handover_measurement.record_handover(
                        ue_id=f"isolation_{scheme.value}_{i}",
                        source_gnb=f"iso_src_{i}",
                        target_gnb=f"iso_tgt_{i}",
                        start_time=event_start,
                        end_time=event_end,
                        handover_scheme=scheme,
                        result=self.HandoverResult.SUCCESS
                    )
                    
                    events.append({
                        "scheme": scheme,
                        "event_id": event_id,
                        "latency_ms": latency_ms
                    })
                    
                    # çŸ­æš«ç­‰å¾…æ¨¡æ“¬çœŸå¯¦é–“éš”
                    await asyncio.sleep(0.01)
                
                return events
            
            # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰æ–¹æ¡ˆ
            for scheme in concurrent_schemes:
                task = asyncio.create_task(generate_scheme_events(scheme))
                tasks.append(task)
            
            # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
            results = await asyncio.gather(*tasks)
            total_duration = (time.time() - start_time) * 1000
            
            # æ•´ç†çµæœ
            all_events = []
            scheme_event_counts = {}
            for events in results:
                all_events.extend(events)
                for event in events:
                    scheme = event["scheme"]
                    scheme_event_counts[scheme] = scheme_event_counts.get(scheme, 0) + 1
            
            # é©—è­‰çµæœ
            tests_passed = 0
            total_tests = 4
            
            # æ¸¬è©¦ 1: ä¸¦è¡ŒåŸ·è¡Œå®Œæ•´æ€§
            expected_total = len(concurrent_schemes) * 15
            if len(all_events) == expected_total:
                tests_passed += 1
                print(f"  âœ… ä¸¦è¡ŒåŸ·è¡Œå®Œæ•´æ€§: {len(all_events)}/{expected_total}")
            else:
                print(f"  âŒ ä¸¦è¡ŒåŸ·è¡Œä¸å®Œæ•´: {len(all_events)}/{expected_total}")
            
            # æ¸¬è©¦ 2: æ–¹æ¡ˆéš”é›¢æ€§é©—è­‰
            isolation_valid = True
            for scheme in concurrent_schemes:
                if scheme_event_counts.get(scheme, 0) == 15:
                    continue
                else:
                    isolation_valid = False
                    print(f"    âš ï¸  {scheme.value} äº‹ä»¶æ•¸ç•°å¸¸: {scheme_event_counts.get(scheme, 0)}")
            
            if isolation_valid:
                tests_passed += 1
                print(f"  âœ… æ–¹æ¡ˆéš”é›¢æ€§é©—è­‰é€šé")
            else:
                print(f"  âŒ æ–¹æ¡ˆéš”é›¢æ€§é©—è­‰å¤±æ•—")
            
            # æ¸¬è©¦ 3: æ•ˆèƒ½éš”é›¢é©—è­‰
            scheme_latencies = {}
            for event in all_events:
                scheme = event["scheme"]
                if scheme not in scheme_latencies:
                    scheme_latencies[scheme] = []
                scheme_latencies[scheme].append(event["latency_ms"])
            
            # è¨ˆç®—å¹³å‡å»¶é²ä¸¦é©—è­‰å·®ç•°åŒ–
            avg_latencies = {}
            for scheme, latencies in scheme_latencies.items():
                avg_latencies[scheme] = statistics.mean(latencies)
            
            performance_isolated = True
            # ç¢ºèª Proposed å»¶é²ä»ç„¶æœ€ä½
            proposed_avg = avg_latencies[self.HandoverScheme.PROPOSED]
            for scheme, avg_lat in avg_latencies.items():
                if scheme != self.HandoverScheme.PROPOSED and avg_lat <= proposed_avg:
                    performance_isolated = False
                    break
            
            if performance_isolated:
                tests_passed += 1
                print(f"  âœ… æ•ˆèƒ½éš”é›¢é©—è­‰: Proposed å»¶é²æœ€ä½ ({proposed_avg:.1f}ms)")
            else:
                print(f"  âŒ æ•ˆèƒ½éš”é›¢é©—è­‰å¤±æ•—")
            
            # æ¸¬è©¦ 4: ä¸¦è¡ŒåŸ·è¡Œæ•ˆç‡
            avg_concurrent_time = total_duration / len(all_events)
            if avg_concurrent_time < 50:  # æ¯äº‹ä»¶ < 50ms
                tests_passed += 1
                print(f"  âœ… ä¸¦è¡ŒåŸ·è¡Œæ•ˆç‡è‰¯å¥½: {avg_concurrent_time:.1f}ms/äº‹ä»¶")
            else:
                print(f"  âŒ ä¸¦è¡ŒåŸ·è¡Œæ•ˆç‡éä½: {avg_concurrent_time:.1f}ms/äº‹ä»¶")
            
            self.scheme_metrics["isolation"] = {
                "concurrent_events": len(all_events),
                "scheme_counts": {str(k): v for k, v in scheme_event_counts.items()},
                "avg_latencies": {str(k): v for k, v in avg_latencies.items()},
                "concurrent_duration_ms": total_duration
            }
            
            success_rate = tests_passed / total_tests
            print(f"\nğŸ“Š æ–¹æ¡ˆæ•ˆèƒ½éš”é›¢æ¸¬è©¦çµæœ: {tests_passed}/{total_tests} ({success_rate:.1%})")
            
            self.test_results.append(("æ–¹æ¡ˆæ•ˆèƒ½éš”é›¢", success_rate >= 0.8))
            return success_rate >= 0.8
            
        except Exception as e:
            print(f"âŒ æ–¹æ¡ˆæ•ˆèƒ½éš”é›¢æ¸¬è©¦å¤±æ•—: {e}")
            self.test_results.append(("æ–¹æ¡ˆæ•ˆèƒ½éš”é›¢", False))
            return False
    
    def generate_test_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print("\n" + "=" * 70)
        print("ğŸ“Š éšæ®µäºŒ 2.4 å¤šæ–¹æ¡ˆæ¸¬è©¦æ”¯æ´æ¸¬è©¦å ±å‘Š")
        print("=" * 70)
        
        # ç¸½é«”çµæœ
        total_tests = len(self.test_results)
        passed_tests = sum(1 for _, result in self.test_results if result)
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        print(f"\nğŸ“‹ æ¸¬è©¦çµæœæ¦‚è¦½:")
        for test_name, result in self.test_results:
            status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
            print(f"   {status} {test_name}")
        
        print(f"\nğŸ“Š ç¸½é«”çµ±è¨ˆ:")
        print(f"   ç¸½æ¸¬è©¦æ•¸: {total_tests}")
        print(f"   é€šéæ¸¬è©¦: {passed_tests}")
        print(f"   å¤±æ•—æ¸¬è©¦: {total_tests - passed_tests}")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        
        # æ–¹æ¡ˆæ”¯æ´ç¸½çµ
        if self.scheme_metrics:
            print(f"\nâš¡ æ–¹æ¡ˆæ”¯æ´æŒ‡æ¨™ç¸½çµ:")
            
            if "differentiation" in self.scheme_metrics:
                diff_metrics = self.scheme_metrics["differentiation"]
                print(f"   æ–¹æ¡ˆå·®ç•°åŒ–:")
                print(f"     - æ¸¬è©¦äº‹ä»¶æ•¸: {diff_metrics['total_events']}")
                print(f"     - ç”Ÿæˆæ™‚é–“: {diff_metrics['generation_time_ms']:.1f}ms")
            
            if "switching" in self.scheme_metrics:
                switch_metrics = self.scheme_metrics["switching"]
                print(f"   æ–¹æ¡ˆåˆ‡æ›:")
                print(f"     - åˆ‡æ›æ¬¡æ•¸: {switch_metrics['switch_count']}")
                print(f"     - å¹³å‡åˆ‡æ›æ™‚é–“: {switch_metrics['avg_switch_time_ms']:.1f}ms")
            
            if "isolation" in self.scheme_metrics:
                iso_metrics = self.scheme_metrics["isolation"]
                print(f"   æ•ˆèƒ½éš”é›¢:")
                print(f"     - ä¸¦è¡Œäº‹ä»¶æ•¸: {iso_metrics['concurrent_events']}")
                print(f"     - åŸ·è¡Œæ™‚é–“: {iso_metrics['concurrent_duration_ms']:.1f}ms")
        
        # éšæ®µäºŒ 2.4 å®Œæˆåº¦è©•ä¼°
        print(f"\nğŸ¯ éšæ®µäºŒ 2.4 å®Œæˆåº¦è©•ä¼°:")
        
        feature_completion = {
            "æ–¹æ¡ˆåˆå§‹åŒ–åŠŸèƒ½": any(name == "æ–¹æ¡ˆåˆå§‹åŒ–" and result for name, result in self.test_results),
            "æ–¹æ¡ˆå·®ç•°åŒ–åŠŸèƒ½": any(name == "æ–¹æ¡ˆå·®ç•°åŒ–" and result for name, result in self.test_results),
            "æ–¹æ¡ˆåˆ‡æ›åŠŸèƒ½": any(name == "æ–¹æ¡ˆåˆ‡æ›" and result for name, result in self.test_results),
            "æ–¹æ¡ˆæ•ˆèƒ½éš”é›¢": any(name == "æ–¹æ¡ˆæ•ˆèƒ½éš”é›¢" and result for name, result in self.test_results)
        }
        
        completed_features = sum(feature_completion.values())
        total_features = len(feature_completion)
        
        for feature, completed in feature_completion.items():
            status = "âœ… å®Œæˆ" if completed else "âŒ æœªå®Œæˆ"
            print(f"   {status} {feature}")
        
        completion_rate = (completed_features / total_features * 100) if total_features > 0 else 0
        print(f"\n   éšæ®µå®Œæˆåº¦: {completed_features}/{total_features} ({completion_rate:.1f}%)")
        
        if success_rate >= 90.0:
            print(f"\nğŸ‰ éšæ®µäºŒ 2.4 å¤šæ–¹æ¡ˆæ¸¬è©¦æ”¯æ´å¯¦ä½œæˆåŠŸï¼")
            print(f"âœ¨ æ”¯æ´å››ç¨®åˆ‡æ›æ–¹æ¡ˆçš„å®Œæ•´æ¸¬è©¦æ¡†æ¶")
        elif success_rate >= 75.0:
            print(f"\nâš ï¸  éšæ®µäºŒ 2.4 åŸºæœ¬å®Œæˆï¼Œå»ºè­°å„ªåŒ–å¤±æ•—é …ç›®")
        else:
            print(f"\nâŒ éšæ®µäºŒ 2.4 å¯¦ä½œéœ€è¦æ”¹é€²")
        
        return success_rate >= 75.0


async def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹åŸ·è¡Œéšæ®µäºŒ 2.4 å¤šæ–¹æ¡ˆæ¸¬è©¦æ”¯æ´æ¸¬è©¦")
    
    tester = MultiSchemeSupportTester()
    
    # è¨­ç½®æ¸¬è©¦ç’°å¢ƒ
    if not await tester.setup_test_environment():
        print("âŒ æ¸¬è©¦ç’°å¢ƒè¨­ç½®å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ")
        return False
    
    # åŸ·è¡Œæ¸¬è©¦
    test_functions = [
        tester.test_scheme_initialization,
        tester.test_scheme_differentiation,
        tester.test_scheme_switching,
        tester.test_scheme_performance_isolation
    ]
    
    for test_func in test_functions:
        try:
            await test_func()
            await asyncio.sleep(0.5)  # çŸ­æš«ä¼‘æ¯
        except Exception as e:
            print(f"âŒ æ¸¬è©¦åŸ·è¡Œç•°å¸¸: {e}")
    
    # ç”Ÿæˆå ±å‘Š
    success = tester.generate_test_report()
    
    return success


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        exit_code = 0 if success else 1
        print(f"\næ¸¬è©¦å®Œæˆï¼Œé€€å‡ºç¢¼: {exit_code}")
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        exit_code = 130
    except Exception as e:
        print(f"\nğŸ’¥ æ¸¬è©¦åŸ·è¡ŒéŒ¯èª¤: {e}")
        exit_code = 1
    
    sys.exit(exit_code)