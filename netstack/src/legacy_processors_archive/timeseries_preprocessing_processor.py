"""
éšæ®µå››ï¼šæ™‚é–“åºåˆ—é è™•ç†å™¨

å°‡ä¿¡è™Ÿåˆ†æçµæœè½‰æ›ç‚ºå‰ç«¯å‹•ç•«å¯ç”¨çš„æ™‚é–“åºåˆ—æ•¸æ“šæ ¼å¼
ç¬¦åˆ @docs/stages/stage4-timeseries.md è¦ç¯„
"""

import json
import time
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, Optional

from src.shared_core.validation_snapshot_base import ValidationSnapshotBase, ValidationCheckHelper

# æ–°å¢ï¼šé‹è¡Œæ™‚æª¢æŸ¥çµ„ä»¶ (Phase 2)
from validation.runtime_architecture_checker import RuntimeArchitectureChecker, check_runtime_architecture
from validation.api_contract_validator import APIContractValidator, validate_api_contract
from validation.execution_flow_checker import ExecutionFlowChecker, check_execution_flow

logger = logging.getLogger(__name__)


class TimeseriesPreprocessingProcessor(ValidationSnapshotBase):
    """æ™‚é–“åºåˆ—é è™•ç†å™¨
    
    å°‡ä¿¡è™Ÿåˆ†æçš„è¤‡é›œæ•¸æ“šçµæ§‹è½‰æ›ç‚ºå‰ç«¯å‹•ç•«éœ€è¦çš„ enhanced_timeseries æ ¼å¼
    """
    
    def __init__(self, input_dir: str = "/app/data", output_dir: str = "/app/data"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        
        # Initialize ValidationSnapshotBase
        super().__init__(stage_number=4, stage_name="éšæ®µ4: æ™‚é–“åºåˆ—é è™•ç†", 
                         snapshot_dir="/app/data/validation_snapshots")
        
        # ğŸ”„ ä¿®æ”¹ï¼šå»ºç«‹å°ˆç”¨å­ç›®éŒ„ç”¨æ–¼éšæ®µå››è¼¸å‡º
        self.timeseries_preprocessing_dir = self.output_dir / "timeseries_preprocessing_outputs"
        self.timeseries_preprocessing_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿æŒå‘å¾Œå…¼å®¹ï¼Œenhanced_dir æŒ‡å‘æ–°çš„å­ç›®éŒ„
        self.enhanced_dir = self.timeseries_preprocessing_dir
        
        # åˆå§‹åŒ– sample_mode å±¬æ€§
        self.sample_mode = False  # é…ç½®ç‚ºå…¨é‡æ¨¡å¼
        
        # ğŸ¬ ç¬¦åˆ@docsè¦æ±‚ï¼šåˆå§‹åŒ–CronAnimationBuilder
        try:
            from services.animation.cron_animation_builder import CronAnimationBuilder
            self.cron_builder = CronAnimationBuilder(time_resolution=30)
            logger.info("ğŸ¬ CronAnimationBuilder åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ CronAnimationBuilder åˆå§‹åŒ–å¤±æ•—: {e}")
            self.cron_builder = None
        
        # ğŸ›¡ï¸ Phase 3 æ–°å¢ï¼šåˆå§‹åŒ–é©—è­‰æ¡†æ¶
        self.validation_enabled = False
        self.validation_adapter = None
        
        try:
            from validation.adapters.stage4_validation_adapter import Stage4ValidationAdapter
            self.validation_adapter = Stage4ValidationAdapter()
            self.validation_enabled = True
            logger.info("ğŸ›¡ï¸ Phase 3 Stage 4 é©—è­‰æ¡†æ¶åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"âš ï¸ Phase 3 é©—è­‰æ¡†æ¶åˆå§‹åŒ–å¤±æ•—: {e}")
            logger.warning("   ç¹¼çºŒä½¿ç”¨èˆŠç‰ˆé©—è­‰æ©Ÿåˆ¶")
        
        logger.info("âœ… æ™‚é–“åºåˆ—é è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  è¼¸å…¥ç›®éŒ„: {self.input_dir}")
        logger.info(f"  è¼¸å‡ºç›®éŒ„: {self.timeseries_preprocessing_dir}")
        logger.info("  ğŸ“ ä½¿ç”¨å°ˆç”¨å­ç›®éŒ„çµæ§‹")
        if self.validation_enabled:
            logger.info("  ğŸ›¡ï¸ Phase 3 é©—è­‰æ¡†æ¶: å·²å•Ÿç”¨")
        if self.cron_builder:
            logger.info("  ğŸ¬ CronAnimationBuilder: å·²å°±ç·’")      
    def extract_key_metrics(self, processing_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–éšæ®µ4é—œéµæŒ‡æ¨™"""
        # å¾è½‰æ›çµæœä¸­æå–é—œéµæŒ‡æ¨™
        conversion_stats = processing_results.get("conversion_statistics", {})
        constellation_data = processing_results.get("constellation_data", {})
        
        return {
            "è™•ç†ç¸½æ•¸": conversion_stats.get("total_processed", 0),
            "æˆåŠŸè½‰æ›": conversion_stats.get("successful_conversions", 0),
            "å¤±æ•—è½‰æ›": conversion_stats.get("failed_conversions", 0),
            "è½‰æ›ç‡": f"{conversion_stats.get('successful_conversions', 0) / max(conversion_stats.get('total_processed', 1), 1) * 100:.1f}%",
            "Starlinkè™•ç†": constellation_data.get("starlink", {}).get("satellites_processed", 0),
            "OneWebè™•ç†": constellation_data.get("oneweb", {}).get("satellites_processed", 0)
        }
    
    def run_validation_checks(self, processing_results: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œ Stage 4 é©—è­‰æª¢æŸ¥ - å°ˆæ³¨æ–¼æ™‚é–“åºåˆ—é è™•ç†å’Œå‰ç«¯å‹•ç•«æ•¸æ“šæº–å‚™ + Phase 3.5 å¯é…ç½®é©—è­‰ç´šåˆ¥"""
        
        # ğŸ¯ Phase 3.5: å°å…¥å¯é…ç½®é©—è­‰ç´šåˆ¥ç®¡ç†å™¨
        try:
            from pathlib import Path
            import sys
            
            from validation.managers.validation_level_manager import ValidationLevelManager
            
            validation_manager = ValidationLevelManager()
            validation_level = validation_manager.get_validation_level('stage4')
            
            # æ€§èƒ½ç›£æ§é–‹å§‹
            import time
            validation_start_time = time.time()
            
        except ImportError:
            # å›é€€åˆ°æ¨™æº–é©—è­‰ç´šåˆ¥
            validation_level = 'STANDARD'
            validation_start_time = time.time()
        
        metadata = processing_results.get('metadata', {})
        conversion_stats = processing_results.get("conversion_statistics", {})
        constellation_data = processing_results.get("constellation_data", {})
        
        checks = {}
        
        # ğŸ“Š æ ¹æ“šé©—è­‰ç´šåˆ¥æ±ºå®šæª¢æŸ¥é …ç›®
        if validation_level == 'FAST':
            # å¿«é€Ÿæ¨¡å¼ï¼šåªåŸ·è¡Œé—œéµæª¢æŸ¥
            critical_checks = [
                'è¼¸å…¥æ•¸æ“šå­˜åœ¨æ€§',
                'æ™‚é–“åºåˆ—è½‰æ›æˆåŠŸç‡',
                'å‰ç«¯å‹•ç•«æ•¸æ“šå®Œæ•´æ€§',
                'æ•¸æ“šçµæ§‹å®Œæ•´æ€§'
            ]
        elif validation_level == 'COMPREHENSIVE':
            # è©³ç´°æ¨¡å¼ï¼šåŸ·è¡Œæ‰€æœ‰æª¢æŸ¥ + é¡å¤–çš„æ·±åº¦æª¢æŸ¥
            critical_checks = [
                'è¼¸å…¥æ•¸æ“šå­˜åœ¨æ€§', 'æ™‚é–“åºåˆ—è½‰æ›æˆåŠŸç‡', 'å‰ç«¯å‹•ç•«æ•¸æ“šå®Œæ•´æ€§',
                'æ˜Ÿåº§æ•¸æ“šå¹³è¡¡æ€§', 'æª”æ¡ˆå¤§å°åˆç†æ€§', 'æ•¸æ“šçµæ§‹å®Œæ•´æ€§', 
                'è™•ç†æ™‚é–“åˆç†æ€§', 'å­¸è¡“æ¨™æº–åˆè¦æ€§', 'ä¿¡è™Ÿæ•¸æ“šå®Œæ•´æ€§',
                'æ™‚é–“æˆ³ä¸€è‡´æ€§é©—è­‰', 'çµ±è¨ˆç‰¹å¾µåˆç†æ€§'
            ]
        else:
            # æ¨™æº–æ¨¡å¼ï¼šåŸ·è¡Œå¤§éƒ¨åˆ†æª¢æŸ¥
            critical_checks = [
                'è¼¸å…¥æ•¸æ“šå­˜åœ¨æ€§', 'æ™‚é–“åºåˆ—è½‰æ›æˆåŠŸç‡', 'å‰ç«¯å‹•ç•«æ•¸æ“šå®Œæ•´æ€§',
                'æ˜Ÿåº§æ•¸æ“šå¹³è¡¡æ€§', 'æª”æ¡ˆå¤§å°åˆç†æ€§', 'æ•¸æ“šçµæ§‹å®Œæ•´æ€§',
                'è™•ç†æ™‚é–“åˆç†æ€§', 'å­¸è¡“æ¨™æº–åˆè¦æ€§', 'ä¿¡è™Ÿæ•¸æ“šå®Œæ•´æ€§',
                'æ™‚é–“æˆ³ä¸€è‡´æ€§é©—è­‰'
            ]
        
        # 1. è¼¸å…¥æ•¸æ“šå­˜åœ¨æ€§æª¢æŸ¥
        if 'è¼¸å…¥æ•¸æ“šå­˜åœ¨æ€§' in critical_checks:
            input_satellites = metadata.get('total_satellites', 0)
            checks["è¼¸å…¥æ•¸æ“šå­˜åœ¨æ€§"] = input_satellites > 0
        
        # 2. æ™‚é–“åºåˆ—è½‰æ›æˆåŠŸç‡æª¢æŸ¥ - ç¢ºä¿å¤§éƒ¨åˆ†è¡›æ˜ŸæˆåŠŸè½‰æ›ç‚ºå‰ç«¯æ ¼å¼
        if 'æ™‚é–“åºåˆ—è½‰æ›æˆåŠŸç‡' in critical_checks:
            total_processed = conversion_stats.get("total_processed", 0)
            successful_conversions = conversion_stats.get("successful_conversions", 0)
            conversion_rate = (successful_conversions / max(total_processed, 1)) * 100
            
            if self.sample_mode:
                checks["æ™‚é–“åºåˆ—è½‰æ›æˆåŠŸç‡"] = conversion_rate >= 70.0  # å–æ¨£æ¨¡å¼è¼ƒå¯¬é¬†
            else:
                checks["æ™‚é–“åºåˆ—è½‰æ›æˆåŠŸç‡"] = conversion_rate >= 85.0  # å…¨é‡æ¨¡å¼è¦æ±‚è¼ƒé«˜
        
        # 3. å‰ç«¯å‹•ç•«æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ - ç¢ºä¿åŒ…å«å‰ç«¯æ‰€éœ€çš„æ™‚é–“è»¸å’Œè»Œè·¡æ•¸æ“š
        if 'å‰ç«¯å‹•ç•«æ•¸æ“šå®Œæ•´æ€§' in critical_checks:
            animation_data_ok = True
            output_files = processing_results.get("output_files", {})
            if not output_files or len(output_files) == 0:
                animation_data_ok = False
            else:
                # æª¢æŸ¥æ˜¯å¦æœ‰ä¸»è¦çš„æ™‚é–“åºåˆ—æª”æ¡ˆ
                has_main_timeseries = any('animation_enhanced' in str(f) for f in output_files.values() if f)
                animation_data_ok = has_main_timeseries
            
            checks["å‰ç«¯å‹•ç•«æ•¸æ“šå®Œæ•´æ€§"] = animation_data_ok
        
        # 4. æ˜Ÿåº§æ•¸æ“šå¹³è¡¡æ€§æª¢æŸ¥ - ç¢ºä¿å…©å€‹æ˜Ÿåº§éƒ½æœ‰è½‰æ›çµæœ
        if 'æ˜Ÿåº§æ•¸æ“šå¹³è¡¡æ€§' in critical_checks:
            starlink_processed = constellation_data.get("starlink", {}).get("satellites_processed", 0)
            oneweb_processed = constellation_data.get("oneweb", {}).get("satellites_processed", 0)
            
            if self.sample_mode:
                checks["æ˜Ÿåº§æ•¸æ“šå¹³è¡¡æ€§"] = starlink_processed >= 5 and oneweb_processed >= 2
            else:
                checks["æ˜Ÿåº§æ•¸æ“šå¹³è¡¡æ€§"] = starlink_processed >= 200 and oneweb_processed >= 30
        
        # 5. æª”æ¡ˆå¤§å°åˆç†æ€§æª¢æŸ¥ - ç¢ºä¿è¼¸å‡ºæª”æ¡ˆåœ¨å‰ç«¯å¯æ¥å—ç¯„åœ
        if 'æª”æ¡ˆå¤§å°åˆç†æ€§' in critical_checks:
            file_size_reasonable = True
            total_size_mb = metadata.get('total_output_size_mb', 0)
            if total_size_mb > 0:
                if self.sample_mode:
                    file_size_reasonable = total_size_mb <= 20  # å–æ¨£æ¨¡å¼è¼ƒå°
                else:
                    # ğŸ¯ èª¿æ•´ï¼šé‡å°å¯¦éš›438é¡†è¡›æ˜Ÿï¼Œåˆç†ç¯„åœç‚º10-100MB
                    file_size_reasonable = 10 <= total_size_mb <= 100  # é©ä¸­æ•¸æ“šè¦æ¨¡
            
            checks["æª”æ¡ˆå¤§å°åˆç†æ€§"] = file_size_reasonable
        
        # 6. æ•¸æ“šçµæ§‹å®Œæ•´æ€§æª¢æŸ¥
        if 'æ•¸æ“šçµæ§‹å®Œæ•´æ€§' in critical_checks:
            required_fields = ['metadata', 'conversion_statistics', 'output_files']
            checks["æ•¸æ“šçµæ§‹å®Œæ•´æ€§"] = ValidationCheckHelper.check_data_completeness(
                processing_results, required_fields
            )
        
        # 7. è™•ç†æ™‚é–“æª¢æŸ¥ - æ™‚é–“åºåˆ—é è™•ç†æ‡‰è©²ç›¸å°å¿«é€Ÿ
        if 'è™•ç†æ™‚é–“åˆç†æ€§' in critical_checks:
            # å¿«é€Ÿæ¨¡å¼æœ‰æ›´åš´æ ¼çš„æ€§èƒ½è¦æ±‚
            if validation_level == 'FAST':
                max_time = 150 if self.sample_mode else 90
            else:
                max_time = 200 if self.sample_mode else 120  # å–æ¨£3.3åˆ†é˜ï¼Œå…¨é‡2åˆ†é˜
            checks["è™•ç†æ™‚é–“åˆç†æ€§"] = ValidationCheckHelper.check_processing_time(
                self.processing_duration, max_time
            )
        
        # 8. å­¸è¡“æ¨™æº–åˆè¦æª¢æŸ¥ - ç¢ºä¿ç¬¦åˆ academic_data_standards.md Grade A/B è¦æ±‚
        if 'å­¸è¡“æ¨™æº–åˆè¦æ€§' in critical_checks:
            academic_compliance_ok = True
            
            # æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†ä»»ä½•ç¦æ­¢çš„æ•¸æ“šè™•ç†æ–¹æ³•
            output_files = processing_results.get("output_files", {})
            for file_path in output_files.values():
                if file_path and Path(file_path).exists():
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                            # æª¢æŸ¥æ˜¯å¦åŒ…å«é•è¦çš„æ­£è¦åŒ–æ•¸æ“š
                            if 'rsrp_normalized' in content:
                                academic_compliance_ok = False
                                break
                            # æª¢æŸ¥æ˜¯å¦åŒ…å«ä»»æ„å£“ç¸®æ¯”ä¾‹
                            if 'compression_ratio' in content and '0.73' in content:
                                academic_compliance_ok = False
                                break
                    except:
                        pass  # å¦‚æœæ–‡ä»¶è®€å–å¤±æ•—ï¼Œè·³éæª¢æŸ¥
            
            checks["å­¸è¡“æ¨™æº–åˆè¦æ€§"] = academic_compliance_ok
        
        # 9. ä¿¡è™Ÿæ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ - ç¢ºä¿ä¿æŒåŸå§‹dBmå€¼
        if 'ä¿¡è™Ÿæ•¸æ“šå®Œæ•´æ€§' in critical_checks:
            signal_integrity_ok = True
            timeseries_data = processing_results.get("timeseries_data", {})
            satellites = timeseries_data.get("satellites", [])
            
            if satellites:
                # éš¨æ©Ÿæª¢æŸ¥å¹¾å€‹è¡›æ˜Ÿçš„ä¿¡è™Ÿæ•¸æ“š
                sample_size = min(3 if validation_level == 'FAST' else 5, len(satellites))
                for i in range(0, len(satellites), max(1, len(satellites) // sample_size)):
                    satellite = satellites[i]
                    signal_quality = satellite.get('signal_quality', {})
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰åŸå§‹RSRPå€¼ä¸¦ä¸”ä½¿ç”¨dBmå–®ä½
                    if 'statistics' in signal_quality:
                        rsrp_value = signal_quality['statistics'].get('mean_rsrp_dbm')
                        if rsrp_value is None or not isinstance(rsrp_value, (int, float)):
                            signal_integrity_ok = False
                            break
            
            checks["ä¿¡è™Ÿæ•¸æ“šå®Œæ•´æ€§"] = signal_integrity_ok
        
        # ===== Phase 3 å¢å¼·é©—è­‰ =====
        
        # 10. æ™‚é–“æˆ³ä¸€è‡´æ€§é©—è­‰ - UTCæ¨™æº–æ™‚é–“åˆè¦æ€§
        if 'æ™‚é–“æˆ³ä¸€è‡´æ€§é©—è­‰' in critical_checks:
            timestamp_consistency_result = self._validate_timestamp_consistency(processing_results)
            checks["æ™‚é–“æˆ³ä¸€è‡´æ€§é©—è­‰"] = timestamp_consistency_result.get("passed", False)
        
        # 11. çµ±è¨ˆç‰¹å¾µåˆç†æ€§é©—è­‰ - æ•¸æ“šå“è³ªè©•ä¼°ï¼ˆè©³ç´°æ¨¡å¼å°ˆç”¨ï¼‰
        if 'çµ±è¨ˆç‰¹å¾µåˆç†æ€§' in critical_checks:
            statistical_features_result = self._validate_statistical_features(processing_results)
            checks["çµ±è¨ˆç‰¹å¾µåˆç†æ€§"] = statistical_features_result.get("passed", False)
        
        # è¨ˆç®—é€šéçš„æª¢æŸ¥æ•¸é‡
        passed_checks = sum(1 for passed in checks.values() if passed)
        total_checks = len(checks)
        
        # ğŸ¯ Phase 3.5: è¨˜éŒ„é©—è­‰æ€§èƒ½æŒ‡æ¨™
        validation_end_time = time.time()
        validation_duration = validation_end_time - validation_start_time
        
        try:
            # æ›´æ–°æ€§èƒ½æŒ‡æ¨™
            validation_manager.update_performance_metrics('stage4', validation_duration, total_checks)
            
            # è‡ªé©æ‡‰èª¿æ•´ï¼ˆå¦‚æœæ€§èƒ½å¤ªå·®ï¼‰
            if validation_duration > 5.0 and validation_level != 'FAST':
                validation_manager.set_validation_level('stage4', 'FAST', reason='performance_auto_adjustment')
        except:
            # å¦‚æœæ€§èƒ½è¨˜éŒ„å¤±æ•—ï¼Œä¸å½±éŸ¿ä¸»è¦é©—è­‰æµç¨‹
            pass
        
        return {
            "passed": passed_checks == total_checks,
            "totalChecks": total_checks,
            "passedChecks": passed_checks,
            "failedChecks": total_checks - passed_checks,
            "criticalChecks": [
                {"name": name, "status": "passed" if checks.get(name, False) else "failed"}
                for name in critical_checks if name in checks
            ],
            "allChecks": checks,
            # Phase 3 å¢å¼·é©—è­‰è©³ç´°çµæœ
            "phase3_validation_details": {
                "timestamp_consistency": locals().get('timestamp_consistency_result', {}),
                "statistical_features": locals().get('statistical_features_result', {})
            },
            # ğŸ¯ Phase 3.5 æ–°å¢ï¼šé©—è­‰ç´šåˆ¥ä¿¡æ¯
            "validation_level_info": {
                "current_level": validation_level,
                "validation_duration_ms": round(validation_duration * 1000, 2),
                "checks_executed": list(checks.keys()),
                "performance_acceptable": validation_duration < 5.0
            },
            "summary": f"Stage 4 æ™‚é–“åºåˆ—é è™•ç†é©—è­‰: è½‰æ›æˆåŠŸç‡{conversion_stats.get('successful_conversions', 0)}/{conversion_stats.get('total_processed', 0)} ({((conversion_stats.get('successful_conversions', 0) / max(conversion_stats.get('total_processed', 1), 1)) * 100):.1f}%) - {passed_checks}/{total_checks}é …æª¢æŸ¥é€šé"
        }

    def _validate_timestamp_consistency(self, processing_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ™‚é–“æˆ³ä¸€è‡´æ€§é©—è­‰ - Phase 3 Task 3 æ–°å¢åŠŸèƒ½
        
        é©—è­‰æ™‚é–“åºåˆ—æ•¸æ“šçš„æ™‚é–“æˆ³æ˜¯å¦ä¸€è‡´ï¼š
        - UTCæ¨™æº–æ™‚é–“æ ¼å¼
        - æ™‚é–“åºåˆ—é€£çºŒæ€§
        - æ¡æ¨£é »ç‡ä¸€è‡´æ€§
        - æ™‚é–“åŒæ­¥ç²¾åº¦
        
        Args:
            processing_results: æ™‚åºé è™•ç†çµæœæ•¸æ“š
            
        Returns:
            Dict: æ™‚é–“æˆ³ä¸€è‡´æ€§é©—è­‰å ±å‘Š
            
        Raises:
            ValueError: å¦‚æœç™¼ç¾åš´é‡çš„æ™‚é–“æˆ³ä¸€è‡´æ€§å•é¡Œ
        """
        logger.info("â° åŸ·è¡Œæ™‚é–“æˆ³ä¸€è‡´æ€§é©—è­‰...")
        
        timeseries_data = processing_results.get("timeseries_data", {})
        satellites = timeseries_data.get("satellites", [])
        
        timestamp_report = {
            'validation_timestamp': datetime.now(timezone.utc).isoformat(),
            'total_satellites_checked': len(satellites),
            'timestamp_statistics': {
                'satellites_with_consistent_timestamps': 0,
                'satellites_with_timestamp_issues': 0,
                'consistency_percentage': 0.0
            },
            'timestamp_violations': [],
            'consistency_status': 'UNKNOWN'
        }
        
        # æ™‚é–“æˆ³æ¨™æº–å®šç¾©
        TIMESTAMP_STANDARDS = {
            'utc_timezone_required': True,
            'iso_8601_format_required': True,
            'max_time_gap_seconds': 600,         # æœ€å¤§æ™‚é–“é–“éš”10åˆ†é˜
            'min_time_gap_seconds': 30,          # æœ€å°æ™‚é–“é–“éš”30ç§’  
            'sampling_frequency_tolerance': 0.1,  # æ¡æ¨£é »ç‡å®¹å·®10%
            'max_timestamp_deviation_seconds': 5  # æœ€å¤§æ™‚é–“æˆ³åå·®5ç§’
        }
        
        consistent_satellites = 0
        issue_satellites = 0
        
        # æŠ½æ¨£æª¢æŸ¥è¡›æ˜Ÿçš„æ™‚é–“æˆ³ä¸€è‡´æ€§ï¼ˆæª¢æŸ¥å‰20é¡†ï¼‰
        sample_size = min(20, len(satellites))
        sample_satellites = satellites[:sample_size]
        
        for sat_data in sample_satellites:
            satellite_name = sat_data.get('name', 'Unknown')
            constellation = sat_data.get('constellation', '').lower()
            timeseries_points = sat_data.get('timeseries', [])
            
            if not timeseries_points:
                continue
            
            satellite_violations = []
            
            # 1. æª¢æŸ¥æ™‚é–“æˆ³æ ¼å¼å’Œæ™‚å€
            timestamps = []
            for i, point in enumerate(timeseries_points[:10]):  # æª¢æŸ¥å‰10å€‹é»
                timestamp_str = point.get('timestamp')
                
                if not timestamp_str:
                    satellite_violations.append({
                        'point_index': i,
                        'timestamp_violation': 'missing_timestamp',
                        'details': f'æ™‚é–“é»{i}ç¼ºå°‘æ™‚é–“æˆ³'
                    })
                    continue
                
                try:
                    # è§£ææ™‚é–“æˆ³ä¸¦æª¢æŸ¥æ ¼å¼
                    if timestamp_str.endswith('Z'):
                        dt = datetime.fromisoformat(timestamp_str[:-1]).replace(tzinfo=timezone.utc)
                    elif '+00:00' in timestamp_str:
                        dt = datetime.fromisoformat(timestamp_str)
                    else:
                        # ä¸ç¬¦åˆUTCæ ¼å¼
                        satellite_violations.append({
                            'point_index': i,
                            'timestamp_violation': 'non_utc_timezone',
                            'details': f'æ™‚é–“æˆ³{timestamp_str}ä¸æ˜¯UTCæ ¼å¼',
                            'expected': 'ISO 8601 UTC format with Z or +00:00'
                        })
                        continue
                    
                    timestamps.append(dt)
                    
                except Exception as e:
                    satellite_violations.append({
                        'point_index': i,
                        'timestamp_violation': 'invalid_timestamp_format',
                        'details': f'æ™‚é–“æˆ³{timestamp_str}æ ¼å¼ç„¡æ•ˆ: {str(e)}',
                        'expected': 'ISO 8601 format'
                    })
            
            # 2. æª¢æŸ¥æ™‚é–“åºåˆ—çš„é€£çºŒæ€§å’Œé–“éš”
            if len(timestamps) >= 2:
                time_intervals = []
                for i in range(1, len(timestamps)):
                    interval = (timestamps[i] - timestamps[i-1]).total_seconds()
                    time_intervals.append(interval)
                    
                    # æª¢æŸ¥æ™‚é–“é–“éš”æ˜¯å¦åœ¨åˆç†ç¯„åœå…§
                    if interval <= 0:
                        satellite_violations.append({
                            'point_index': i,
                            'timestamp_violation': 'negative_or_zero_time_interval',
                            'details': f'æ™‚é–“é»{i-1}åˆ°{i}çš„æ™‚é–“é–“éš”ç‚º{interval}ç§’',
                            'expected': 'æ­£æ™‚é–“é–“éš”'
                        })
                    elif interval > TIMESTAMP_STANDARDS['max_time_gap_seconds']:
                        satellite_violations.append({
                            'point_index': i,
                            'timestamp_violation': 'excessive_time_gap',
                            'details': f'æ™‚é–“é»{i-1}åˆ°{i}çš„æ™‚é–“é–“éš”éå¤§: {interval}ç§’',
                            'expected': f'< {TIMESTAMP_STANDARDS["max_time_gap_seconds"]}ç§’'
                        })
                    elif interval < TIMESTAMP_STANDARDS['min_time_gap_seconds']:
                        satellite_violations.append({
                            'point_index': i,
                            'timestamp_violation': 'insufficient_time_gap',
                            'details': f'æ™‚é–“é»{i-1}åˆ°{i}çš„æ™‚é–“é–“éš”éå°: {interval}ç§’',
                            'expected': f'> {TIMESTAMP_STANDARDS["min_time_gap_seconds"]}ç§’'
                        })
                
                # 3. æª¢æŸ¥æ¡æ¨£é »ç‡ä¸€è‡´æ€§
                if time_intervals:
                    avg_interval = sum(time_intervals) / len(time_intervals)
                    max_deviation = max(abs(interval - avg_interval) for interval in time_intervals)
                    max_allowed_deviation = avg_interval * TIMESTAMP_STANDARDS['sampling_frequency_tolerance']
                    
                    if max_deviation > max_allowed_deviation:
                        satellite_violations.append({
                            'timestamp_violation': 'inconsistent_sampling_frequency',
                            'details': f'æ¡æ¨£é »ç‡ä¸ä¸€è‡´ï¼Œæœ€å¤§åå·®{max_deviation:.1f}ç§’',
                            'average_interval': avg_interval,
                            'max_deviation': max_deviation,
                            'tolerance': max_allowed_deviation
                        })
            
            # ğŸš¨ ä¿®å¾©ï¼šç§»é™¤éŒ¯èª¤çš„æ™‚é–“æˆ³èˆ‡ç•¶å‰æ™‚é–“æ¯”è¼ƒ
            # åŸå› ï¼šæˆ‘å€‘è™•ç†çš„æ˜¯åŸºæ–¼TLE epochçš„æ­·å²è»Œé“æ•¸æ“šï¼Œèˆ‡åŸ·è¡Œæ™‚é–“ç„¡é—œ
            # æ™‚é–“æˆ³çš„åˆç†æ€§æ‡‰è©²åŸºæ–¼TLEæ•¸æ“šçš„æ™‚é–“åŸºæº–ï¼Œè€Œä¸æ˜¯ç•¶å‰æ™‚é–“
            
            # 4. æª¢æŸ¥æ™‚é–“æˆ³çš„å…§éƒ¨ä¸€è‡´æ€§ï¼ˆç›¸é„°æ™‚é–“é»çš„é–“éš”æ˜¯å¦åˆç†ï¼‰
            for i in range(len(timestamps) - 1):
                current_ts = timestamps[i]
                next_ts = timestamps[i + 1]
                interval = abs((next_ts - current_ts).total_seconds())
                
                # æª¢æŸ¥æ™‚é–“é–“éš”æ˜¯å¦åˆç†ï¼ˆæ‡‰è©²æ˜¯30ç§’å·¦å³ï¼‰
                if interval > 60:  # è¶…é1åˆ†é˜é–“éš”ä¸åˆç†
                    satellite_violations.append({
                        'point_index': i,
                        'timestamp_violation': 'irregular_time_interval',
                        'details': f'æ™‚é–“é»{i}åˆ°{i+1}é–“éš”ç•°å¸¸: {interval:.1f}ç§’',
                        'expected': 'ç´„30ç§’é–“éš”'
                    })
            
            # åˆ¤æ–·è©²è¡›æ˜Ÿçš„æ™‚é–“æˆ³ä¸€è‡´æ€§
            if len(satellite_violations) == 0:
                consistent_satellites += 1
            else:
                issue_satellites += 1
                timestamp_report['timestamp_violations'].append({
                    'satellite_name': satellite_name,
                    'constellation': constellation,
                    'violation_count': len(satellite_violations),
                    'violations': satellite_violations
                })
        
        # è¨ˆç®—ä¸€è‡´æ€§çµ±è¨ˆ
        consistency_rate = (consistent_satellites / sample_size * 100) if sample_size > 0 else 0
        
        timestamp_report['timestamp_statistics'] = {
            'satellites_with_consistent_timestamps': consistent_satellites,
            'satellites_with_timestamp_issues': issue_satellites,
            'consistency_percentage': consistency_rate
        }
        
        # ç¢ºå®šä¸€è‡´æ€§ç‹€æ…‹
        if consistency_rate >= 90 and len(timestamp_report['timestamp_violations']) <= 2:
            timestamp_report['consistency_status'] = 'PASS'
            logger.info(f"âœ… æ™‚é–“æˆ³ä¸€è‡´æ€§é©—è­‰é€šé: {consistency_rate:.2f}% ä¸€è‡´ç‡")
        else:
            timestamp_report['consistency_status'] = 'FAIL'
            logger.error(f"âŒ æ™‚é–“æˆ³ä¸€è‡´æ€§é©—è­‰å¤±æ•—: {consistency_rate:.2f}% ä¸€è‡´ç‡ï¼Œç™¼ç¾ {len(timestamp_report['timestamp_violations'])} å€‹å•é¡Œ")
            
            # å¦‚æœä¸€è‡´æ€§å•é¡Œåš´é‡ï¼Œæ‹‹å‡ºç•°å¸¸
            if consistency_rate < 75:
                raise ValueError(f"Academic Standards Violation: æ™‚é–“æˆ³ä¸€è‡´æ€§åš´é‡å•é¡Œ - ä¸€è‡´ç‡åƒ… {consistency_rate:.2f}%")
        
        return timestamp_report

    def _validate_statistical_features(self, processing_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        çµ±è¨ˆç‰¹å¾µåˆç†æ€§é©—è­‰ - Phase 3 Task 3 æ–°å¢åŠŸèƒ½
        
        é©—è­‰æ™‚åºæ•¸æ“šçš„çµ±è¨ˆç‰¹å¾µï¼š
        - å‡å€¼ã€æ–¹å·®ã€åˆ†ä½ˆåˆç†æ€§
        - ç•°å¸¸å€¼æª¢æ¸¬
        - çµ±è¨ˆä¸€è‡´æ€§æª¢æŸ¥
        - æ•¸æ“šå“è³ªè©•ä¼°
        
        Args:
            processing_results: æ™‚åºé è™•ç†çµæœæ•¸æ“š
            
        Returns:
            Dict: çµ±è¨ˆç‰¹å¾µé©—è­‰å ±å‘Š
            
        Raises:
            ValueError: å¦‚æœç™¼ç¾åš´é‡çš„çµ±è¨ˆç‰¹å¾µå•é¡Œ
        """
        logger.info("ğŸ“Š åŸ·è¡Œçµ±è¨ˆç‰¹å¾µåˆç†æ€§é©—è­‰...")
        
        timeseries_data = processing_results.get("timeseries_data", {})
        satellites = timeseries_data.get("satellites", [])
        
        stats_report = {
            'validation_timestamp': datetime.now(timezone.utc).isoformat(),
            'total_satellites_checked': len(satellites),
            'statistical_compliance': {
                'satellites_with_valid_statistics': 0,
                'satellites_with_statistical_issues': 0,
                'compliance_percentage': 0.0
            },
            'statistical_violations': [],
            'compliance_status': 'UNKNOWN'
        }
        
        # çµ±è¨ˆç‰¹å¾µåˆç†æ€§æ¨™æº–
        STATISTICAL_STANDARDS = {
            'rsrp_range_dbm': {
                'min': -140,
                'max': -50,
                'typical_mean': -95,
                'typical_std': 15
            },
            'elevation_range_deg': {
                'min': -90,
                'max': 90,
                'visible_min': 5,  # å¯è¦‹è¡›æ˜Ÿæœ€ä½ä»°è§’
                'visible_max': 90
            },
            'range_km': {
                'min': 200,
                'max': 3000,
                'typical_mean': 1000,
                'typical_std': 500
            },
            'outlier_threshold_z_score': 3.0,    # Z-scoreç•°å¸¸å€¼é–€æª»
            'min_data_points': 5,                # æœ€å°‘æ•¸æ“šé»æ•¸é‡
            'coefficient_variation_max': 1.0     # è®Šç•°ä¿‚æ•¸æœ€å¤§å€¼
        }
        
        valid_satellites = 0
        issue_satellites = 0
        
        # æŠ½æ¨£æª¢æŸ¥è¡›æ˜Ÿçš„çµ±è¨ˆç‰¹å¾µï¼ˆæª¢æŸ¥å‰15é¡†ï¼‰
        sample_size = min(15, len(satellites))
        sample_satellites = satellites[:sample_size]
        
        for sat_data in sample_satellites:
            satellite_name = sat_data.get('name', 'Unknown')
            constellation = sat_data.get('constellation', '').lower()
            signal_quality = sat_data.get('signal_quality', {})
            timeseries_points = sat_data.get('timeseries', [])
            
            if not signal_quality or not timeseries_points:
                continue
            
            satellite_violations = []
            
            # 1. æª¢æŸ¥ä¿¡è™Ÿçµ±è¨ˆç‰¹å¾µ
            statistics = signal_quality.get('statistics', {})
            
            if statistics:
                # æª¢æŸ¥RSRPçµ±è¨ˆå€¼
                mean_rsrp = statistics.get('mean_rsrp_dbm')
                std_rsrp = statistics.get('std_rsrp_dbm')
                min_rsrp = statistics.get('min_rsrp_dbm')
                max_rsrp = statistics.get('max_rsrp_dbm')
                
                if mean_rsrp is not None:
                    # æª¢æŸ¥RSRPå‡å€¼æ˜¯å¦åœ¨åˆç†ç¯„åœå…§
                    if not (STATISTICAL_STANDARDS['rsrp_range_dbm']['min'] <= mean_rsrp <= STATISTICAL_STANDARDS['rsrp_range_dbm']['max']):
                        satellite_violations.append({
                            'statistical_violation': 'rsrp_mean_out_of_range',
                            'details': f'RSRPå‡å€¼{mean_rsrp:.1f}dBmè¶…å‡ºåˆç†ç¯„åœ',
                            'expected_range': f"{STATISTICAL_STANDARDS['rsrp_range_dbm']['min']}-{STATISTICAL_STANDARDS['rsrp_range_dbm']['max']}dBm"
                        })
                    
                    # æª¢æŸ¥æ¨™æº–å·®æ˜¯å¦åˆç†
                    if std_rsrp is not None:
                        if std_rsrp < 0:
                            satellite_violations.append({
                                'statistical_violation': 'negative_standard_deviation',
                                'details': f'RSRPæ¨™æº–å·®ç‚ºè² å€¼: {std_rsrp:.1f}dBm'
                            })
                        elif std_rsrp > 50:  # æ¨™æº–å·®éå¤§
                            satellite_violations.append({
                                'statistical_violation': 'excessive_rsrp_standard_deviation',
                                'details': f'RSRPæ¨™æº–å·®éå¤§: {std_rsrp:.1f}dBm',
                                'expected': '< 50dBm'
                            })
                        
                        # æª¢æŸ¥è®Šç•°ä¿‚æ•¸
                        if mean_rsrp != 0:
                            cv = abs(std_rsrp / mean_rsrp)
                            if cv > STATISTICAL_STANDARDS['coefficient_variation_max']:
                                satellite_violations.append({
                                    'statistical_violation': 'high_coefficient_of_variation',
                                    'details': f'RSRPè®Šç•°ä¿‚æ•¸éé«˜: {cv:.2f}',
                                    'expected': f'< {STATISTICAL_STANDARDS["coefficient_variation_max"]}'
                                })
                
                # æª¢æŸ¥æœ€å¤§æœ€å°å€¼çš„é‚è¼¯ä¸€è‡´æ€§
                if min_rsrp is not None and max_rsrp is not None:
                    if min_rsrp > max_rsrp:
                        satellite_violations.append({
                            'statistical_violation': 'min_max_inconsistency',
                            'details': f'æœ€å°RSRP({min_rsrp:.1f})å¤§æ–¼æœ€å¤§RSRP({max_rsrp:.1f})',
                            'note': 'çµ±è¨ˆé‚è¼¯éŒ¯èª¤'
                        })
                    
                    rsrp_range = max_rsrp - min_rsrp
                    if rsrp_range > 60:  # RSRPç¯„åœéå¤§
                        satellite_violations.append({
                            'statistical_violation': 'excessive_rsrp_range',
                            'details': f'RSRPç¯„åœéå¤§: {rsrp_range:.1f}dBm',
                            'expected': '< 60dBm'
                        })
            
            # 2. æª¢æŸ¥æ™‚åºæ•¸æ“šé»çš„çµ±è¨ˆåˆ†ä½ˆ
            if len(timeseries_points) >= STATISTICAL_STANDARDS['min_data_points']:
                # æå–æ•¸å€¼æ•¸æ“šé€²è¡Œçµ±è¨ˆåˆ†æ
                rsrp_values = []
                elevation_values = []
                range_values = []
                
                for point in timeseries_points:
                    # æå–RSRPå€¼
                    if 'rsrp_dbm' in point and isinstance(point['rsrp_dbm'], (int, float)):
                        rsrp_values.append(point['rsrp_dbm'])
                    
                    # æå–ä»°è§’å€¼
                    if 'elevation_deg' in point and isinstance(point['elevation_deg'], (int, float)):
                        elevation_values.append(point['elevation_deg'])
                    
                    # æå–è·é›¢å€¼
                    if 'range_km' in point and isinstance(point['range_km'], (int, float)):
                        range_values.append(point['range_km'])
                
                # 3. ç•°å¸¸å€¼æª¢æ¸¬ï¼ˆä½¿ç”¨Z-scoreæ–¹æ³•ï¼‰
                for values, param_name, standards in [
                    (rsrp_values, 'RSRP', STATISTICAL_STANDARDS['rsrp_range_dbm']),
                    (elevation_values, 'ä»°è§’', STATISTICAL_STANDARDS['elevation_range_deg']),
                    (range_values, 'è·é›¢', STATISTICAL_STANDARDS['range_km'])
                ]:
                    if len(values) >= 3:  # è‡³å°‘éœ€è¦3å€‹æ•¸æ“šé»æ‰èƒ½è¨ˆç®—Z-score
                        import statistics
                        try:
                            mean_val = statistics.mean(values)
                            stdev_val = statistics.stdev(values) if len(values) > 1 else 0
                            
                            if stdev_val > 0:
                                outliers = []
                                for i, val in enumerate(values):
                                    z_score = abs((val - mean_val) / stdev_val)
                                    if z_score > STATISTICAL_STANDARDS['outlier_threshold_z_score']:
                                        outliers.append({'index': i, 'value': val, 'z_score': z_score})
                                
                                # å¦‚æœç•°å¸¸å€¼æ¯”ä¾‹éé«˜
                                outlier_ratio = len(outliers) / len(values)
                                if outlier_ratio > 0.1:  # è¶…é10%çš„ç•°å¸¸å€¼
                                    satellite_violations.append({
                                        'statistical_violation': f'{param_name}_excessive_outliers',
                                        'details': f'{param_name}ç•°å¸¸å€¼æ¯”ä¾‹éé«˜: {outlier_ratio:.1%}',
                                        'outlier_count': len(outliers),
                                        'total_points': len(values),
                                        'expected': '< 10%'
                                    })
                        except statistics.StatisticsError:
                            # çµ±è¨ˆè¨ˆç®—å¤±æ•—ï¼Œå¯èƒ½æ•¸æ“šå“è³ªæœ‰å•é¡Œ
                            satellite_violations.append({
                                'statistical_violation': f'{param_name}_statistical_calculation_failed',
                                'details': f'{param_name}çµ±è¨ˆè¨ˆç®—å¤±æ•—ï¼Œå¯èƒ½æ•¸æ“šå“è³ªæœ‰å•é¡Œ'
                            })
            else:
                satellite_violations.append({
                    'statistical_violation': 'insufficient_data_points',
                    'details': f'æ•¸æ“šé»æ•¸é‡ä¸è¶³: {len(timeseries_points)}',
                    'expected': f'>= {STATISTICAL_STANDARDS["min_data_points"]}'
                })
            
            # åˆ¤æ–·è©²è¡›æ˜Ÿçš„çµ±è¨ˆç‰¹å¾µåˆè¦æ€§
            if len(satellite_violations) == 0:
                valid_satellites += 1
            else:
                issue_satellites += 1
                stats_report['statistical_violations'].append({
                    'satellite_name': satellite_name,
                    'constellation': constellation,
                    'violation_count': len(satellite_violations),
                    'violations': satellite_violations
                })
        
        # è¨ˆç®—åˆè¦çµ±è¨ˆ
        compliance_rate = (valid_satellites / sample_size * 100) if sample_size > 0 else 0
        
        stats_report['statistical_compliance'] = {
            'satellites_with_valid_statistics': valid_satellites,
            'satellites_with_statistical_issues': issue_satellites,
            'compliance_percentage': compliance_rate
        }
        
        # ç¢ºå®šåˆè¦ç‹€æ…‹
        if compliance_rate >= 85 and len(stats_report['statistical_violations']) <= 3:
            stats_report['compliance_status'] = 'PASS'
            logger.info(f"âœ… çµ±è¨ˆç‰¹å¾µåˆç†æ€§é©—è­‰é€šé: {compliance_rate:.2f}% åˆè¦ç‡")
        else:
            stats_report['compliance_status'] = 'FAIL'
            logger.error(f"âŒ çµ±è¨ˆç‰¹å¾µåˆç†æ€§é©—è­‰å¤±æ•—: {compliance_rate:.2f}% åˆè¦ç‡ï¼Œç™¼ç¾ {len(stats_report['statistical_violations'])} å€‹å•é¡Œ")
            
            # å¦‚æœåˆè¦å•é¡Œåš´é‡ï¼Œæ‹‹å‡ºç•°å¸¸
            if compliance_rate < 70:
                raise ValueError(f"Academic Standards Violation: çµ±è¨ˆç‰¹å¾µåš´é‡ä¸åˆç† - åˆè¦ç‡åƒ… {compliance_rate:.2f}%")
        
        return stats_report
    
    def load_signal_analysis_output(self, signal_file: Optional[str] = None) -> Dict[str, Any]:
        """è¼‰å…¥ä¿¡è™Ÿåˆ†æè¼¸å‡ºæ•¸æ“š"""
        if signal_file is None:
            # ğŸ¯ æ›´æ–°ç‚ºæ–°çš„æª”æ¡ˆå‘½å
            signal_file = self.input_dir / "signal_quality_analysis_output.json"
        else:
            signal_file = Path(signal_file)
            
        logger.info(f"ğŸ“¥ è¼‰å…¥ä¿¡è™Ÿåˆ†ææ•¸æ“š: {signal_file}")
        
        if not signal_file.exists():
            raise FileNotFoundError(f"ä¿¡è™Ÿåˆ†æè¼¸å‡ºæª”æ¡ˆä¸å­˜åœ¨: {signal_file}")
            
        try:
            with open(signal_file, 'r', encoding='utf-8') as f:
                signal_data = json.load(f)
                
            # é©—è­‰æ•¸æ“šæ ¼å¼
            if 'constellations' not in signal_data:
                raise ValueError("ä¿¡è™Ÿåˆ†ææ•¸æ“šç¼ºå°‘ constellations æ¬„ä½")
                
            total_satellites = 0
            for constellation_name, constellation_data in signal_data['constellations'].items():
                satellites = constellation_data.get('satellites', [])
                total_satellites += len(satellites)
                logger.info(f"  {constellation_name}: {len(satellites)} é¡†è¡›æ˜Ÿ")
                
            logger.info(f"âœ… ä¿¡è™Ÿåˆ†ææ•¸æ“šè¼‰å…¥å®Œæˆ: ç¸½è¨ˆ {total_satellites} é¡†è¡›æ˜Ÿ")
            return signal_data
            
        except Exception as e:
            logger.error(f"è¼‰å…¥ä¿¡è™Ÿåˆ†ææ•¸æ“šå¤±æ•—: {e}")
            raise
            
    def convert_to_enhanced_timeseries(self, signal_data: Dict[str, Any]) -> Dict[str, Any]:
        """å°‡ä¿¡è™Ÿåˆ†ææ•¸æ“šè½‰æ›ç‚ºå¢å¼·æ™‚é–“åºåˆ—æ ¼å¼ - ä½¿ç”¨CronAnimationBuilder"""
        logger.info("ğŸ”„ é–‹å§‹æ™‚é–“åºåˆ—æ•¸æ“šè½‰æ›...")
        
        if not self.cron_builder:
            logger.error("âŒ CronAnimationBuilder æœªåˆå§‹åŒ–ï¼Œä½¿ç”¨å‚³çµ±è½‰æ›æ–¹æ³•")
            return self._legacy_convert_to_enhanced_timeseries(signal_data)
        
        try:
            # ğŸ¬ ä½¿ç”¨CronAnimationBuilderæŒ‰ç…§@docsè¦æ±‚è™•ç†
            logger.info("ğŸ¬ ä½¿ç”¨CronAnimationBuilderé€²è¡Œæ™‚é–“åºåˆ—è½‰æ›...")
            
            # 1. å»ºæ§‹è¡›æ˜Ÿè»Œè·¡
            constellations = signal_data.get('constellations', {})
            all_satellites = {}
            
            for const_name, const_data in constellations.items():
                satellites = const_data.get('satellites', [])
                for satellite in satellites:
                    sat_id = satellite.get('satellite_id')
                    if sat_id:
                        all_satellites[sat_id] = satellite
            
            tracks = self.cron_builder.build_satellite_tracks(all_satellites)
            
            # 2. å»ºæ§‹ä¿¡è™Ÿæ™‚é–“ç·š
            timelines = self.cron_builder.build_signal_timelines(signal_data)
            
            # 3. å»ºæ§‹æ›æ‰‹åºåˆ—
            sequences = self.cron_builder.build_handover_sequences(signal_data)
            
            # 4. å‰µå»ºå¢å¼·æ™‚é–“åºåˆ—æ ¼å¼
            enhanced_format = self.cron_builder.create_enhanced_timeseries_format(
                tracks, timelines, sequences
            )
            
            # 5. çµ±è¨ˆè™•ç†çµæœ
            conversion_statistics = {
                "total_satellites_processed": len(all_satellites),
                "successful_track_conversions": len(tracks),
                "total_constellations": len(timelines),
                "total_handover_events": sum(len(seq) for seq in sequences.values()),
                "conversion_success_rate": len(tracks) / len(all_satellites) * 100 if all_satellites else 0
            }
            
            logger.info(f"âœ… CronAnimationBuilderè½‰æ›å®Œæˆ:")
            logger.info(f"   ğŸ“Š è™•ç†è¡›æ˜Ÿ: {len(tracks)}/{len(all_satellites)}")
            logger.info(f"   ğŸŒŒ æ˜Ÿåº§æ•¸é‡: {len(timelines)}")
            logger.info(f"   ğŸ”„ æ›æ‰‹äº‹ä»¶: {conversion_statistics['total_handover_events']}")
            
            # 6. çµ„è£è¿”å›çµæœï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰
            conversion_results = {
                "enhanced_format": enhanced_format,
                "conversion_statistics": conversion_statistics,
                "builder_type": "CronAnimationBuilder",
                "processing_method": "cron_driven_animation_pipeline"
            }
            
            # ç‚ºäº†å‘å¾Œå…¼å®¹ï¼Œä¹Ÿæä¾›åˆ†æ˜Ÿåº§çš„çµæœ
            for const_name in constellations.keys():
                if const_name in ['starlink', 'oneweb']:
                    const_tracks = {k: v for k, v in tracks.items() 
                                  if v.get('constellation', '').lower() == const_name.lower()}
                    conversion_results[const_name] = {
                        "metadata": {
                            "constellation": const_name,
                            "processing_type": "cron_animation_builder",
                            "builder_version": "v1.0",
                            "total_satellites": len(const_tracks)
                        },
                        "animation_data": const_tracks
                    }
            
            return conversion_results
            
        except Exception as e:
            logger.error(f"âŒ CronAnimationBuilderè½‰æ›å¤±æ•—: {e}")
            logger.warning("âš ï¸ é™ç´šä½¿ç”¨å‚³çµ±è½‰æ›æ–¹æ³•")
            return self._legacy_convert_to_enhanced_timeseries(signal_data)

    def _legacy_convert_to_enhanced_timeseries(self, signal_data: Dict[str, Any]) -> Dict[str, Any]:
        """å‚³çµ±çš„æ™‚é–“åºåˆ—è½‰æ›æ–¹æ³• - å‘å¾Œå…¼å®¹"""
        logger.info("ğŸ“Š ä½¿ç”¨å‚³çµ±æ–¹æ³•é€²è¡Œæ™‚é–“åºåˆ—è½‰æ›...")
        
        conversion_results = {
            "starlink": None,
            "oneweb": None,
            "conversion_statistics": {
                "total_processed": 0,
                "successful_conversions": 0,
                "failed_conversions": 0
            }
        }
        
        constellations = signal_data.get('constellations', {})
        
        for const_name, const_data in constellations.items():
            if const_name not in ['starlink', 'oneweb']:
                continue
                
            satellites = const_data.get('satellites', [])
            logger.info(f"ğŸ“Š è™•ç† {const_name}: {len(satellites)} é¡†è¡›æ˜Ÿ")
            
            enhanced_satellites = []
            successful_count = 0
            failed_count = 0
            
            for satellite in satellites:
                try:
                    enhanced_satellite = self._convert_satellite_to_timeseries(satellite, const_name)
                    if enhanced_satellite:
                        enhanced_satellites.append(enhanced_satellite)
                        successful_count += 1
                    else:
                        failed_count += 1
                        
                except Exception as e:
                    sat_id = satellite.get('satellite_id', 'Unknown')
                    logger.warning(f"è¡›æ˜Ÿ {sat_id} è½‰æ›å¤±æ•—: {e}")
                    failed_count += 1
            
            # çµ„è£æ˜Ÿåº§æ•¸æ“š
            enhanced_constellation = {
                "metadata": {
                    "constellation": const_name,
                    "processing_type": "legacy_timeseries_preprocessing",
                    "generation_time": datetime.now(timezone.utc).isoformat(),
                    "source_data": "signal_event_analysis",
                    "total_satellites": len(satellites),
                    "successful_conversions": successful_count,
                    "failed_conversions": failed_count,
                    "conversion_rate": f"{successful_count/len(satellites)*100:.1f}%" if satellites else "0%"
                },
                "satellites": enhanced_satellites,
                "constellation_statistics": {
                    "total_satellites": len(enhanced_satellites),
                    "avg_visibility_windows": sum(len(s.get('visibility_windows', [])) for s in enhanced_satellites) / len(enhanced_satellites) if enhanced_satellites else 0,
                    "avg_signal_quality": sum(s.get('signal_quality', {}).get('statistics', {}).get('mean_rsrp_dbm', -150) for s in enhanced_satellites) / len(enhanced_satellites) if enhanced_satellites else 0
                }
            }
            
            conversion_results[const_name] = enhanced_constellation
            conversion_results["conversion_statistics"]["total_processed"] += len(satellites)
            conversion_results["conversion_statistics"]["successful_conversions"] += successful_count
            conversion_results["conversion_statistics"]["failed_conversions"] += failed_count
            
            logger.info(f"âœ… {const_name} è½‰æ›å®Œæˆ: {successful_count}/{len(satellites)} é¡†è¡›æ˜ŸæˆåŠŸ")
        
        total_processed = conversion_results["conversion_statistics"]["total_processed"]
        total_successful = conversion_results["conversion_statistics"]["successful_conversions"]
        
        logger.info(f"ğŸ¯ æ™‚é–“åºåˆ—è½‰æ›å®Œæˆ: {total_successful}/{total_processed} é¡†è¡›æ˜ŸæˆåŠŸè½‰æ›")
        
        return conversion_results

    def _convert_satellite_to_timeseries(self, satellite: Dict[str, Any], constellation: str) -> Optional[Dict[str, Any]]:
        """å°‡å–®é¡†è¡›æ˜Ÿè½‰æ›ç‚ºå¢å¼·æ™‚é–“åºåˆ—æ ¼å¼ - åš´æ ¼ç„¡é è¨­å€¼Grade Aæ¨™æº–"""
        
        # åŸºæœ¬è¡›æ˜Ÿä¿¡æ¯ - ğŸš¨ Grade A åš´æ ¼æª¢æŸ¥ï¼Œç„¡é è¨­å€¼å›é€€
        satellite_id = satellite.get('satellite_id', '')
        name = satellite.get('name', '')
        
        # NORAD ID åš´æ ¼æª¢æŸ¥
        if 'norad_id' not in satellite:
            logger.error(f"è¡›æ˜Ÿ {name or satellite_id} ç¼ºå°‘NORAD ID")
            raise ValueError(f"Academic Standards Violation: è¡›æ˜Ÿç¼ºå°‘NORADè­˜åˆ¥ç¢¼ - {name or satellite_id}")
        
        norad_id = satellite['norad_id']
        if norad_id <= 0:
            logger.error(f"è¡›æ˜Ÿ {name} NORAD ID ç„¡æ•ˆ: {norad_id}")
            raise ValueError(f"Academic Standards Violation: NORAD IDå¿…é ˆç‚ºæ­£æ•´æ•¸ - {name}: {norad_id}")
        
        enhanced_satellite = {
            "satellite_id": satellite_id,
            "name": name,
            "constellation": constellation,
            "norad_id": norad_id
        }
        
        # 1. è™•ç†è»Œé“æ•¸æ“š - ğŸš¨ Grade A åš´æ ¼æª¢æŸ¥è»Œé“åƒæ•¸
        orbit_data = satellite.get('orbit_data', {})
        if orbit_data:
            # æª¢æŸ¥é—œéµè»Œé“åƒæ•¸çš„å­˜åœ¨æ€§
            required_orbit_params = ['altitude', 'inclination', 'semi_major_axis', 'eccentricity', 'mean_motion']
            for param in required_orbit_params:
                if param not in orbit_data:
                    logger.error(f"è¡›æ˜Ÿ {name} ç¼ºå°‘è»Œé“åƒæ•¸: {param}")
                    raise ValueError(f"Academic Standards Violation: è»Œé“æ•¸æ“šä¸å®Œæ•´ï¼Œç¼ºå°‘{param} - {name}")
            
            # é©—è­‰è»Œé“åƒæ•¸çš„ç‰©ç†åˆç†æ€§
            altitude = orbit_data['altitude']
            if altitude <= 0 or altitude < 200:  # LEOæœ€ä½è»Œé“ç´„200km
                logger.error(f"è¡›æ˜Ÿ {name} è»Œé“é«˜åº¦ä¸åˆç†: {altitude}km")
                raise ValueError(f"Academic Standards Violation: LEOè¡›æ˜Ÿè»Œé“é«˜åº¦å¿…é ˆ>200km - {name}: {altitude}km")
            
            enhanced_satellite["orbit_parameters"] = {
                "altitude": altitude,
                "inclination": orbit_data['inclination'],
                "semi_major_axis": orbit_data['semi_major_axis'],
                "eccentricity": orbit_data['eccentricity'],
                "mean_motion": orbit_data['mean_motion']
            }
        
        # 2. è™•ç†ä½ç½®æ™‚é–“åºåˆ— - ğŸš¨ Grade A åš´æ ¼æª¢æŸ¥ä½ç½®æ•¸æ“š
        # ğŸ¯ é—œéµä¿®å¾©ï¼šå„ªå…ˆä½¿ç”¨Stage3çš„æ¨™æº–å­—æ®µï¼Œå…¼å®¹å¤šç¨®æ ¼å¼
        # æŸ¥æ‰¾é †åºï¼šposition_timeseries -> timeseries -> positions
        positions = (satellite.get('position_timeseries') or 
                    satellite.get('timeseries', []) or 
                    satellite.get('positions', []))
        if positions:
            enhanced_satellite["position_timeseries"] = []
            for i, pos in enumerate(positions):
                # é©é…æ–°çš„192é»æ™‚é–“åºåˆ—æ ¼å¼
                relative_obs = pos.get('relative_to_observer', {})
                geodetic = pos.get('geodetic', {})
                
                # ğŸš¨ Grade A æª¢æŸ¥ï¼šé—œéµä½ç½®åƒæ•¸ä¸å¯ç¼ºå¤±
                elevation_sources = [
                    relative_obs.get('elevation_deg'),
                    pos.get('elevation_deg')
                ]
                elevation_deg = next((e for e in elevation_sources if e is not None), None)
                
                if elevation_deg is None:
                    logger.error(f"è¡›æ˜Ÿ {name} ç¬¬{i}å€‹ä½ç½®é»ç¼ºå°‘ä»°è§’æ•¸æ“š")
                    raise ValueError(f"Academic Standards Violation: ä½ç½®æ•¸æ“šç¼ºå°‘ä»°è§’ä¿¡æ¯ - {name} at position {i}")
                
                # æª¢æŸ¥å…¶ä»–é—œéµåƒæ•¸
                azimuth_deg = relative_obs.get('azimuth_deg', pos.get('azimuth_deg'))
                range_km = relative_obs.get('range_km', pos.get('range_km'))
                
                if range_km is None or range_km <= 0:
                    logger.warning(f"è¡›æ˜Ÿ {name} ç¬¬{i}å€‹ä½ç½®é»è·é›¢æ•¸æ“šç•°å¸¸: {range_km}")
                    # å°æ–¼è·é›¢æ•¸æ“šï¼Œå…è¨±ä¸€å®šçš„å®¹å¿åº¦ï¼Œä½†è¨˜éŒ„è­¦å‘Š
                    range_km = range_km if range_km is not None else 0
                
                # ECI ä½ç½®è™•ç† - é¿å…é è¨­å€¼
                eci_position = pos.get('eci_position_km', [])
                if len(eci_position) < 3:
                    # å˜—è©¦å¾å…¶ä»–ä¾†æºç²å–
                    eci_x = pos.get('position_eci', {}).get('x')
                    eci_y = pos.get('position_eci', {}).get('y')
                    eci_z = pos.get('position_eci', {}).get('z')
                    if None in [eci_x, eci_y, eci_z]:
                        logger.warning(f"è¡›æ˜Ÿ {name} ç¬¬{i}å€‹ä½ç½®é»ECIåº§æ¨™ä¸å®Œæ•´")
                        eci_x = eci_y = eci_z = 0  # ä½œç‚ºæœ€å¾Œå›é€€
                    position_eci = {"x": eci_x, "y": eci_y, "z": eci_z}
                else:
                    position_eci = {
                        "x": eci_position[0],
                        "y": eci_position[1], 
                        "z": eci_position[2]
                    }
                
                # ECI é€Ÿåº¦è™•ç†
                eci_velocity = pos.get('eci_velocity_km_s', [])
                if len(eci_velocity) < 3:
                    velocity_eci = {"x": 0, "y": 0, "z": 0}  # é€Ÿåº¦ç¼ºå¤±æ™‚çš„å›é€€
                else:
                    velocity_eci = {
                        "x": eci_velocity[0],
                        "y": eci_velocity[1],
                        "z": eci_velocity[2]
                    }
                
                # åœ°ç†åº§æ¨™è™•ç† - ğŸš¨ ä¿®å¾©ç¡¬ç·¨ç¢¼é«˜åº¦é•è¦
                geodetic_data = {
                    "latitude_deg": geodetic.get('latitude_deg', 0),
                    "longitude_deg": geodetic.get('longitude_deg', 0),
                    "altitude_km": geodetic.get('altitude_km', altitude if 'altitude' in locals() else 550)  # ä½¿ç”¨å¯¦éš›è»Œé“é«˜åº¦
                }
                
                enhanced_pos = {
                    "time": pos.get('utc_time', pos.get('time', '')),
                    "time_offset_seconds": pos.get('time_index', 0) * 30,  # 30ç§’é–“éš”
                    "elevation_deg": elevation_deg,
                    "azimuth_deg": azimuth_deg or 0,
                    "range_km": range_km,
                    "is_visible": relative_obs.get('is_visible', pos.get('is_visible', False)),
                    "position_eci": position_eci,
                    "velocity_eci": velocity_eci,
                    "geodetic": geodetic_data
                }
                enhanced_satellite["position_timeseries"].append(enhanced_pos)
            logger.debug(f"  æˆåŠŸè™•ç† {len(positions)} å€‹æ™‚é–“é»çš„è»Œé“æ•¸æ“š")
        
        # 3. è™•ç†æ¨™æº–æ™‚é–“åºåˆ—ï¼ˆä¾†è‡ªåŸå§‹ timeseriesï¼‰
        timeseries = satellite.get('timeseries', [])
        if timeseries:
            enhanced_satellite["elevation_azimuth_timeseries"] = timeseries
        
        # 4. è™•ç†å¯è¦‹æ€§çª—å£
        visibility_windows = orbit_data.get('visibility_windows', [])
        if visibility_windows:
            enhanced_satellite["visibility_windows"] = visibility_windows
        
        # 5. è™•ç†ä¿¡è™Ÿå“è³ªæ•¸æ“š
        signal_quality = satellite.get('signal_quality', {})
        if signal_quality:
            enhanced_satellite["signal_quality"] = signal_quality
        
        # 6. è™•ç†äº‹ä»¶åˆ†æçµæœ
        if 'event_potential' in satellite:
            enhanced_satellite["event_analysis"] = {
                "event_potential": satellite.get('event_potential', {}),
                "supported_events": ["A4_intra_frequency", "A5_intra_frequency", "D2_beam_switch"],
                "standards_compliance": {
                    "A4": "3GPP TS 38.331 Section 5.5.4.5 - Neighbour becomes better than threshold",
                    "A5": "3GPP TS 38.331 Section 5.5.4.6 - SpCell worse and neighbour better",
                    "D2": "3GPP TS 38.331 Section 5.5.4.15a - Distance-based handover triggers"
                }
            }
        
        # 7. è™•ç†ç¶œåˆè©•åˆ† - Grade B å¯æ¥å—çš„0é è¨­å€¼
        if 'composite_score' in satellite:
            enhanced_satellite["performance_scores"] = {
                "composite_score": satellite.get('composite_score', 0),
                "geographic_score": satellite.get('geographic_relevance_score', 0),
                "handover_score": satellite.get('handover_suitability_score', 0)
            }
        
        # 8. æ·»åŠ æ™‚é–“åºåˆ—é è™•ç†æ¨™è¨˜
        enhanced_satellite["processing_metadata"] = {
            "processed_by_timeseries_preprocessing": True,
            "processing_time": datetime.now(timezone.utc).isoformat(),
            "data_source": "signal_event_analysis",
            "enhanced_features": [
                "position_timeseries",
                "elevation_azimuth_timeseries", 
                "visibility_windows",
                "signal_quality",
                "event_analysis",
                "performance_scores"
            ]
        }
        
        return enhanced_satellite

    def _create_animation_format(self, constellation_data: Dict[str, Any], constellation_name: str) -> Dict[str, Any]:
        """å‰µå»ºç¬¦åˆæ–‡æª”çš„å‹•ç•«æ•¸æ“šæ ¼å¼ - åŒæ™‚æ”¯æ´å‰ç«¯å‹•ç•«å’Œå¼·åŒ–å­¸ç¿’ç ”ç©¶"""
        satellites = constellation_data.get('satellites', [])
        
        # è¨ˆç®—å‹•ç•«åƒæ•¸
        total_frames = 192  # 96åˆ†é˜è»Œé“ï¼Œ30ç§’é–“éš”
        animation_fps = 60
        
        # è½‰æ›è¡›æ˜Ÿæ•¸æ“šç‚ºå‹•ç•«æ ¼å¼
        animation_satellites = {}
        for satellite in satellites:
            sat_id = satellite.get('satellite_id', '')
            if not sat_id:
                continue
                
            # å¾position_timeseriesæå–è»Œè·¡é»
            position_data = satellite.get('position_timeseries', [])
            track_points = []
            signal_timeline = []
            
            for i, pos in enumerate(position_data):
                # ğŸš¨ ACADEMIC GRADE A: åœ°ç†åº§æ¨™å¿…é ˆä¾†è‡ªçœŸå¯¦SGP4è¨ˆç®—ï¼Œç¦æ­¢é è¨­å€¼å›é€€
                geodetic = pos.get('geodetic', {})
                
                if 'latitude_deg' not in geodetic or 'longitude_deg' not in geodetic or 'altitude_km' not in geodetic:
                    logger.error(f"è¡›æ˜Ÿ {sat_id} ä½ç½®é» {i} ç¼ºå°‘å®Œæ•´åœ°ç†åº§æ¨™æ•¸æ“š")
                    raise ValueError(f"Academic Standards Violation: è¡›æ˜Ÿ {sat_id} åœ°ç†åº§æ¨™æ•¸æ“šä¸å®Œæ•´ï¼Œç¦æ­¢ä½¿ç”¨é è¨­å€¼")
                
                latitude_deg = geodetic['latitude_deg']
                longitude_deg = geodetic['longitude_deg']
                altitude_km = geodetic['altitude_km']
                
                # é©—è­‰åœ°ç†åº§æ¨™çš„ç‰©ç†åˆç†æ€§
                if not (-90 <= latitude_deg <= 90):
                    raise ValueError(f"Academic Standards Violation: è¡›æ˜Ÿ {sat_id} ç·¯åº¦å€¼è¶…å‡ºç¯„åœ: {latitude_deg}Â°")
                if not (-180 <= longitude_deg <= 180):
                    raise ValueError(f"Academic Standards Violation: è¡›æ˜Ÿ {sat_id} ç¶“åº¦å€¼è¶…å‡ºç¯„åœ: {longitude_deg}Â°")
                if not (200 <= altitude_km <= 2000):  # LEOè¡›æ˜Ÿåˆç†é«˜åº¦ç¯„åœ
                    raise ValueError(f"Academic Standards Violation: è¡›æ˜Ÿ {sat_id} é«˜åº¦å€¼è¶…å‡ºLEOç¯„åœ: {altitude_km}km")
                
                # è»Œè·¡é» - ä½¿ç”¨é©—è­‰éçš„çœŸå¯¦åº§æ¨™
                track_point = {
                    "time": i * 30,  # 30ç§’é–“éš”
                    "lat": latitude_deg,      # çœŸå¯¦ç·¯åº¦ï¼Œç„¡é è¨­å€¼
                    "lon": longitude_deg,     # çœŸå¯¦ç¶“åº¦ï¼Œç„¡é è¨­å€¼
                    "alt": altitude_km,       # çœŸå¯¦é«˜åº¦ï¼Œç„¡é è¨­å€¼
                    "elevation_deg": pos.get('elevation_deg', -90),  # ğŸ¯ ä¿ç•™ä»°è§’æ•¸æ“š
                    "visible": pos.get('is_visible', False)
                }
                track_points.append(track_point)
                
                # ä¿¡è™Ÿæ™‚é–“ç·š - ä¿æŒåŸå§‹ä¿¡è™Ÿå€¼ (Grade A è¦æ±‚)
                # å¾ä¿¡è™Ÿå“è³ªæ•¸æ“šä¸­ç²å–åŸå§‹RSRPå€¼
                satellite_signal_quality = satellite.get('signal_quality', {})
                original_rsrp_dbm = satellite_signal_quality.get('statistics', {}).get('mean_rsrp_dbm', -150)
                
                signal_point = {
                    "time": i * 30,
                    "rsrp_dbm": original_rsrp_dbm,  # ä¿æŒåŸå§‹dBmå€¼ï¼Œä¸æ­£è¦åŒ–
                    "signal_unit": "dBm",  # æ˜ç¢ºæ¨™ç¤ºç‰©ç†å–®ä½
                    "elevation_deg": pos.get('elevation_deg', -90),  # ä¿ç•™ä»°è§’ç”¨æ–¼å‰ç«¯è¨ˆç®—
                    "quality_color": "#00FF00" if pos.get('is_visible', False) else "#FF0000"
                }
                signal_timeline.append(signal_point)
            
            # è¨ˆç®—æ‘˜è¦çµ±è¨ˆ
            visible_points = [p for p in position_data if p.get('is_visible', False)]
            max_elevation = max((p.get('elevation_deg', -90) for p in position_data), default=-90)
            
            animation_satellites[sat_id] = {
                "track_points": track_points,
                "signal_timeline": signal_timeline,
                "summary": {
                    "max_elevation_deg": round(max_elevation, 1),
                    "total_visible_time_min": len(visible_points) * 0.5,  # 30ç§’ * é»æ•¸ / 60
                    "avg_signal_quality": "high" if max_elevation > 45 else "medium" if max_elevation > 15 else "low"
                }
            }
        
        # çµ„è£å®Œæ•´çš„å‹•ç•«æ•¸æ“šæ ¼å¼
        animation_data = {
            "metadata": {
                "constellation": constellation_name,
                "satellite_count": len(animation_satellites),
                "time_range": {
                    "start": "2025-08-14T00:00:00Z",
                    "end": "2025-08-14T06:00:00Z"
                },
                "animation_fps": animation_fps,
                "total_frames": total_frames,
                "stage": "stage4_timeseries",
                "data_integrity": "complete",  # ç„¡è³‡æ–™æ¸›é‡ï¼Œç¬¦åˆå­¸è¡“æ¨™æº–
                "processing_type": "animation_preprocessing",
                "research_data_included": True,  # ğŸ¯ æ¨™è¨˜åŒ…å«ç ”ç©¶æ•¸æ“š
                "academic_compliance": {
                    "geodetic_coordinates": "real_sgp4_calculated_no_fallback",
                    "grade_level": "A",
                    "zero_tolerance_policy": "enforced"
                }
            },
            "satellites": animation_satellites
        }
        
        return animation_data
        
    def save_enhanced_timeseries(self, conversion_results: Dict[str, Any]) -> Dict[str, str]:
        """ä¿å­˜å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“šåˆ°æ–‡ä»¶ - æ•´åˆCronAnimationBuilder"""
        logger.info("ğŸ’¾ ä¿å­˜å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š...")
        
        # ğŸš¨ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥ #7: CronAnimationBuilderé¡å‹å¼·åˆ¶æª¢æŸ¥
        try:
            from services.animation.cron_animation_builder import CronAnimationBuilder
            animation_builder = CronAnimationBuilder(str(self.timeseries_preprocessing_dir))
            
            assert isinstance(animation_builder, CronAnimationBuilder), \
                f"éŒ¯èª¤å‹•ç•«å»ºæ§‹å™¨é¡å‹: {type(animation_builder).__name__}, é æœŸ: CronAnimationBuilder"
            logger.info("âœ… CronAnimationBuilderé¡å‹æª¢æŸ¥é€šé")
            
        except ImportError as e:
            logger.error(f"ğŸš¨ CronAnimationBuilderåŒ¯å…¥å¤±æ•—: {e}")
            raise AssertionError(f"ç¼ºå°‘å¿…éœ€çš„CronAnimationBuilderä¾è³´: {e}")
        
        # ğŸ”„ ä¿®æ”¹ï¼šä½¿ç”¨å°ˆç”¨å­ç›®éŒ„
        # ç¢ºä¿å­ç›®éŒ„å­˜åœ¨
        self.timeseries_preprocessing_dir.mkdir(parents=True, exist_ok=True)
        
        output_files = {}
        
        # ğŸ¬ ä½¿ç”¨CronAnimationBuilderå‰µå»ºå®Œæ•´å‹•ç•«æ•¸æ“š
        try:
            enhanced_animation_data = animation_builder.create_enhanced_animation_format(conversion_results)
            
            # ğŸš¨ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥ #8: å‹•ç•«æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
            assert 'metadata' in enhanced_animation_data, "å‹•ç•«æ•¸æ“šç¼ºå°‘metadata"
            assert 'animation_data' in enhanced_animation_data, "å‹•ç•«æ•¸æ“šç¼ºå°‘animation_data"
            assert enhanced_animation_data['metadata'].get('builder_type') == 'CronAnimationBuilder', \
                f"å‹•ç•«å»ºæ§‹å™¨é¡å‹éŒ¯èª¤: {enhanced_animation_data['metadata'].get('builder_type')}"
            
            logger.info("âœ… CronAnimationBuilderå‹•ç•«æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥é€šé")
            
        except Exception as e:
            logger.error(f"ğŸš¨ CronAnimationBuilderè™•ç†å¤±æ•—: {e}")
            raise AssertionError(f"å‹•ç•«æ•¸æ“šå»ºæ§‹å¤±æ•—: {e}")
        
        # ğŸ¯ æ–‡æª”æŒ‡å®šçš„æª”æ¡ˆå‘½åè¦ç¯„
        ANIMATION_FILENAMES = {
            "starlink": "animation_enhanced_starlink.json",
            "oneweb": "animation_enhanced_oneweb.json"
        }
        
        for const_name in ['starlink', 'oneweb']:
            constellation_animation = enhanced_animation_data['animation_data'].get(const_name)
            if not constellation_animation:
                logger.info(f"âš ï¸ {const_name} ç„¡å‹•ç•«æ•¸æ“šï¼Œè·³é")
                continue
            
            # ä½¿ç”¨æ–‡æª”æŒ‡å®šçš„å‹•ç•«æª”æ¡ˆå‘½åï¼Œè¼¸å‡ºåˆ°å°ˆç”¨å­ç›®éŒ„
            filename = ANIMATION_FILENAMES[const_name]
            output_file = self.timeseries_preprocessing_dir / filename
            
            # ğŸš¨ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥ #9: æ˜Ÿåº§æ•¸æ“šå“è³ªæª¢æŸ¥
            total_satellites = constellation_animation.get('total_satellites', 0)
            total_frames = constellation_animation.get('frame_count', 0)
            
            assert total_satellites > 0, f"{const_name} æ˜Ÿåº§ç„¡æœ‰æ•ˆè¡›æ˜Ÿæ•¸æ“š"
            assert total_frames >= 96, f"{const_name} å¹€æ•¸ä¸è¶³: {total_frames}, éœ€è¦ >=96 å¹€ç¶­æŒè»Œé“é€±æœŸ"
            
            # æª¢æŸ¥å­¸è¡“åˆè¦æ€§
            academic_compliance = enhanced_animation_data['metadata'].get('academic_compliance', {})
            assert academic_compliance.get('grade_level') == 'A', \
                f"{const_name} å­¸è¡“åˆè¦ç­‰ç´šä¸è¶³: {academic_compliance.get('grade_level')}"
            assert academic_compliance.get('original_data_preserved') == True, \
                f"{const_name} åŸå§‹æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥å¤±æ•—"
            
            logger.info(f"âœ… {const_name} æ˜Ÿåº§æ•¸æ“šå“è³ªæª¢æŸ¥é€šé")
            
            # ä¿å­˜æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(constellation_animation, f, indent=2, ensure_ascii=False)
            
            file_size = output_file.stat().st_size
            output_files[const_name] = str(output_file)
            
            logger.info(f"âœ… {const_name} å‹•ç•«æ•¸æ“šå·²ä¿å­˜: {output_file}")
            logger.info(f"   æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.1f} MB")
            logger.info(f"   è¡›æ˜Ÿæ•¸é‡: {total_satellites} é¡†")
            logger.info(f"   å‹•ç•«å¹€æ•¸: {total_frames} å¹€")
        
        # ä¿å­˜å¢å¼·å¾Œçš„è½‰æ›çµ±è¨ˆï¼ˆåŒ…å«CronAnimationBuilderä¿¡æ¯ï¼‰
        enhanced_stats = {
            **conversion_results["conversion_statistics"],
            "animation_builder": {
                "type": "CronAnimationBuilder",
                "version": enhanced_animation_data['metadata'].get('version', 'unknown'),
                "cron_config": animation_builder.cron_config,
                "academic_compliance": enhanced_animation_data['metadata'].get('academic_compliance', {}),
                "total_animation_frames": enhanced_animation_data['metadata'].get('total_frames', 0),
                "animation_duration_min": enhanced_animation_data['metadata'].get('animation_duration_min', 0)
            },
            "runtime_checks": {
                "cron_animation_builder_verified": True,
                "data_integrity_verified": True,
                "academic_compliance_verified": True,
                "zero_tolerance_checks_passed": True
            }
        }
        
        stats_file = self.timeseries_preprocessing_dir / "conversion_statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(enhanced_stats, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š å¢å¼·è½‰æ›çµ±è¨ˆå·²ä¿å­˜: {stats_file}")
        logger.info("ğŸš¨ æ‰€æœ‰é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥å·²é€šé")
        
        return output_files
        
    def process_timeseries_preprocessing(self, signal_file: Optional[str] = None, save_output: bool = True) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„æ™‚é–“åºåˆ—é è™•ç†æµç¨‹ - v6.0 Phase 3 é©—è­‰æ¡†æ¶ç‰ˆæœ¬"""
        start_time = time.time()
        logger.info("ğŸš€ é–‹å§‹æ™‚é–“åºåˆ—é è™•ç† + Phase 3 é©—è­‰æ¡†æ¶")
        logger.info("=" * 60)
        
        # ğŸš¨ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥ #1: æ™‚é–“åºåˆ—è™•ç†å™¨é¡å‹å¼·åˆ¶æª¢æŸ¥
        assert isinstance(self, TimeseriesPreprocessingProcessor), \
            f"éŒ¯èª¤æ™‚é–“åºåˆ—è™•ç†å™¨é¡å‹: {type(self).__name__}, é æœŸ: TimeseriesPreprocessingProcessor"
        logger.info("âœ… æ™‚é–“åºåˆ—è™•ç†å™¨é¡å‹æª¢æŸ¥é€šé")
        
        # æ¸…ç†èˆŠé©—è­‰å¿«ç…§ (ç¢ºä¿ç”Ÿæˆæœ€æ–°é©—è­‰å¿«ç…§)
        if self.snapshot_file.exists():
            logger.info(f"ğŸ—‘ï¸ æ¸…ç†èˆŠé©—è­‰å¿«ç…§: {self.snapshot_file}")
            self.snapshot_file.unlink()
        
        # ğŸ”„ ä¿®æ”¹ï¼šæ¸…ç†å­ç›®éŒ„ä¸­çš„èˆŠè¼¸å‡ºæª”æ¡ˆ
        try:
            # æ¸…ç†å­ç›®éŒ„ä¸­çš„æ™‚é–“åºåˆ—æª”æ¡ˆ
            for file_pattern in ["animation_enhanced_starlink.json", "animation_enhanced_oneweb.json", "conversion_statistics.json"]:
                old_file = self.timeseries_preprocessing_dir / file_pattern
                if old_file.exists():
                    logger.info(f"ğŸ—‘ï¸ æ¸…ç†èˆŠæª”æ¡ˆ: {old_file}")
                    old_file.unlink()
                
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†å¤±æ•—ï¼Œç¹¼çºŒåŸ·è¡Œ: {e}")
        
        try:
            # 1. è¼‰å…¥ä¿¡è™Ÿåˆ†ææ•¸æ“š
            signal_data = self.load_signal_analysis_output(signal_file)
            
            # ğŸš¨ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥ #2: è¼¸å…¥æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
            assert 'constellations' in signal_data, "ç¼ºå°‘æ˜Ÿåº§æ•¸æ“šçµæ§‹"
            total_satellites = sum(len(const_data.get('satellites', [])) 
                                 for const_data in signal_data['constellations'].values())
            assert total_satellites > 100, \
                f"åˆ†æè¡›æ˜Ÿæ•¸é‡åš´é‡ä¸è¶³: {total_satellites}, å­¸è¡“ç ”ç©¶éœ€è¦ >100 é¡†è¡›æ˜Ÿ"
            logger.info(f"âœ… è¼¸å…¥æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥é€šé: {total_satellites} é¡†è¡›æ˜Ÿ")
            
            # ğŸ›¡ï¸ Phase 3 æ–°å¢ï¼šé è™•ç†é©—è­‰
            validation_context = {
                'stage_name': 'stage4_timeseries_preprocessing',
                'processing_start': datetime.now(timezone.utc).isoformat(),
                'input_source': signal_file or 'signal_quality_analysis_output.json',
                'timeseries_processing_parameters': {
                    'enhanced_format_conversion': True,
                    'animation_data_generation': True,
                    'temporal_consistency_required': True
                }
            }
            
            if self.validation_enabled and self.validation_adapter:
                try:
                    logger.info("ğŸ” åŸ·è¡Œé è™•ç†é©—è­‰ (æ™‚é–“åºåˆ—æ•¸æ“šçµæ§‹æª¢æŸ¥)...")
                    
                    # åŸ·è¡Œé è™•ç†é©—è­‰
                    import asyncio
                    pre_validation_result = asyncio.run(
                        self.validation_adapter.pre_process_validation(signal_data, validation_context)
                    )
                    
                    if not pre_validation_result.get('success', False):
                        error_msg = f"é è™•ç†é©—è­‰å¤±æ•—: {pre_validation_result.get('blocking_errors', [])}"
                        logger.error(f"ğŸš¨ {error_msg}")
                        raise ValueError(f"Phase 3 Validation Failed: {error_msg}")
                    
                    logger.info("âœ… é è™•ç†é©—è­‰é€šéï¼Œç¹¼çºŒæ™‚é–“åºåˆ—è½‰æ›...")
                    
                except Exception as e:
                    logger.error(f"ğŸš¨ Phase 3 é è™•ç†é©—è­‰ç•°å¸¸: {str(e)}")
                    if "Phase 3 Validation Failed" in str(e):
                        raise  # é‡æ–°æ‹‹å‡ºé©—è­‰å¤±æ•—éŒ¯èª¤
                    else:
                        logger.warning("   ä½¿ç”¨èˆŠç‰ˆé©—è­‰é‚è¼¯ç¹¼çºŒè™•ç†")
            
            # 2. è½‰æ›ç‚ºå¢å¼·æ™‚é–“åºåˆ—æ ¼å¼
            conversion_results = self.convert_to_enhanced_timeseries(signal_data)
            
            # ğŸš¨ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥ #3: æ™‚é–“åºåˆ—å®Œæ•´æ€§å¼·åˆ¶æª¢æŸ¥
            for const_name, const_data in conversion_results.items():
                if const_name in ['starlink', 'oneweb'] and const_data:
                    satellites = const_data.get('satellites', [])
                    for satellite in satellites[:3]:  # æª¢æŸ¥å‰3é¡†è¡›æ˜Ÿ
                        timeseries = satellite.get('position_timeseries', [])
                        assert len(timeseries) >= 96, \
                            f"{satellite.get('name', 'Unknown')} æ™‚é–“åºåˆ—é•·åº¦ä¸è¶³: {len(timeseries)}, éœ€è¦ >=96 é»ç¶­æŒè»Œé“é€±æœŸå®Œæ•´æ€§"
                        
                        # æª¢æŸ¥æ™‚é–“åºåˆ—æ•¸æ“šçµæ§‹
                        for i, point in enumerate(timeseries[:5]):  # æª¢æŸ¥å‰5å€‹é»
                            assert 'time' in point or 'time_offset_seconds' in point, \
                                f"{satellite.get('name')} æ™‚é–“é» {i} ç¼ºå°‘æ™‚é–“æ•¸æ“š"
                            assert 'elevation_deg' in point, \
                                f"{satellite.get('name')} æ™‚é–“é» {i} ç¼ºå°‘ä»°è§’æ•¸æ“š (å¼·åŒ–å­¸ç¿’å¿…éœ€)"
            logger.info("âœ… æ™‚é–“åºåˆ—å®Œæ•´æ€§æª¢æŸ¥é€šé")
            
            # 3. ä¿å­˜å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š
            output_files = {}
            if save_output:
                output_files = self.save_enhanced_timeseries(conversion_results)
                logger.info(f"ğŸ“ æ™‚é–“åºåˆ—é è™•ç†æ•¸æ“šå·²ä¿å­˜åˆ°: {self.timeseries_preprocessing_dir} (å°ˆç”¨å­ç›®éŒ„)")
            else:
                logger.info("ğŸš€ æ™‚é–“åºåˆ—é è™•ç†ä½¿ç”¨å…§å­˜å‚³éæ¨¡å¼ï¼Œæœªä¿å­˜æª”æ¡ˆ")
            
            # ğŸš¨ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥ #4: å­¸è¡“æ¨™æº–æ•¸æ“šç²¾åº¦æª¢æŸ¥  
            all_satellites = []
            for const_name in ['starlink', 'oneweb']:
                const_result = conversion_results.get(const_name)
                if const_result:
                    satellites = const_result.get('satellites', [])
                    all_satellites.extend(satellites)
                    
                    # æª¢æŸ¥æ•¸æ“šç²¾åº¦ç¬¦åˆå­¸è¡“æ¨™æº–
                    for satellite in satellites[:3]:  # æª¢æŸ¥ä»£è¡¨æ€§æ¨£æœ¬
                        signal_quality = satellite.get('signal_quality', {})
                        if 'statistics' in signal_quality:
                            rsrp_value = signal_quality['statistics'].get('mean_rsrp_dbm')
                            assert rsrp_value is not None and isinstance(rsrp_value, (int, float)), \
                                f"{satellite.get('name')} RSRPæ•¸æ“šé¡å‹éŒ¯èª¤ï¼Œå¿…é ˆä¿æŒåŸå§‹dBmæ•¸å€¼"
                            assert signal_quality['statistics'].get('signal_unit') == 'dBm', \
                                f"{satellite.get('name')} ä¿¡è™Ÿå–®ä½è¢«ç•°å¸¸ä¿®æ”¹ï¼Œå¿…é ˆä¿æŒ 'dBm'"
            
            logger.info("âœ… å­¸è¡“æ¨™æº–æ•¸æ“šç²¾åº¦æª¢æŸ¥é€šé")
            
            # ğŸš¨ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥ #5: å‰ç«¯æ€§èƒ½å„ªåŒ–åˆè¦æª¢æŸ¥
            total_output_size_mb = 0
            if output_files:
                total_output_size_mb = sum(
                    (Path(f).stat().st_size / (1024*1024)) 
                    for f in output_files.values() 
                    if Path(f).exists()
                )
                
                # æª¢æŸ¥æ€§èƒ½å„ªåŒ–ä¸çŠ§ç‰²æ•¸æ“šå®Œæ•´æ€§
                expected_min_size = 5 if self.sample_mode else 20  # æœ€å°åˆç†å¤§å°
                expected_max_size = 30 if self.sample_mode else 200  # æœ€å¤§åˆç†å¤§å°
                assert expected_min_size <= total_output_size_mb <= expected_max_size, \
                    f"è¼¸å‡ºæ–‡ä»¶å¤§å°ç•°å¸¸: {total_output_size_mb:.1f}MB, é æœŸç¯„åœ: {expected_min_size}-{expected_max_size}MB"
            logger.info("âœ… å‰ç«¯æ€§èƒ½å„ªåŒ–åˆè¦æª¢æŸ¥é€šé")
            
            # ğŸš¨ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥ #6: ç„¡ç°¡åŒ–è™•ç†é›¶å®¹å¿æª¢æŸ¥
            forbidden_processing_modes = [
                "arbitrary_downsampling", "fixed_compression_ratio", "uniform_quantization",
                "simplified_coordinates", "mock_timeseries", "estimated_positions"
            ]
            processor_methods = str(self.__class__).lower() + str(dir(self))
            for mode in forbidden_processing_modes:
                assert mode not in processor_methods, \
                    f"æª¢æ¸¬åˆ°ç¦ç”¨çš„ç°¡åŒ–è™•ç†æ¨¡å¼: {mode}"
            logger.info("âœ… ç„¡ç°¡åŒ–è™•ç†é›¶å®¹å¿æª¢æŸ¥é€šé")
            
            # æº–å‚™è™•ç†æŒ‡æ¨™
            end_time = time.time()
            processing_duration = end_time - start_time
            self.processing_duration = processing_duration
            
            processing_metrics = {
                'input_satellites': conversion_results["conversion_statistics"]["total_processed"],
                'processed_satellites': conversion_results["conversion_statistics"]["successful_conversions"],
                'failed_conversions': conversion_results["conversion_statistics"]["failed_conversions"],
                'processing_time': processing_duration,
                'processing_timestamp': datetime.now(timezone.utc).isoformat(),
                'output_size_mb': total_output_size_mb,
                'timeseries_conversion_completed': True,
                'runtime_checks_passed': True  # æ–°å¢ï¼šé‹è¡Œæ™‚æª¢æŸ¥ç‹€æ…‹
            }

            # ğŸ›¡ï¸ Phase 3 æ–°å¢ï¼šå¾Œè™•ç†é©—è­‰
            if self.validation_enabled and self.validation_adapter:
                try:
                    logger.info("ğŸ” åŸ·è¡Œå¾Œè™•ç†é©—è­‰ (æ™‚é–“åºåˆ—è½‰æ›çµæœæª¢æŸ¥)...")
                    
                    # æº–å‚™é©—è­‰æ•¸æ“šçµæ§‹
                    validation_output_data = {
                        'constellations': {
                            'starlink': conversion_results.get('starlink', {}),
                            'oneweb': conversion_results.get('oneweb', {})
                        },
                        'conversion_statistics': conversion_results.get('conversion_statistics', {})
                    }
                    
                    # åŸ·è¡Œå¾Œè™•ç†é©—è­‰
                    post_validation_result = asyncio.run(
                        self.validation_adapter.post_process_validation(validation_output_data, processing_metrics)
                    )
                    
                    # æª¢æŸ¥é©—è­‰çµæœ
                    if not post_validation_result.get('success', False):
                        error_msg = f"å¾Œè™•ç†é©—è­‰å¤±æ•—: {post_validation_result.get('error', 'æœªçŸ¥éŒ¯èª¤')}"
                        logger.error(f"ğŸš¨ {error_msg}")
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚ºå“è³ªé–€ç¦é˜»æ–·
                        if 'Quality gate blocked' in post_validation_result.get('error', ''):
                            raise ValueError(f"Phase 3 Quality Gate Blocked: {error_msg}")
                        else:
                            logger.warning("   å¾Œè™•ç†é©—è­‰å¤±æ•—ï¼Œä½†ç¹¼çºŒè™•ç† (é™ç´šæ¨¡å¼)")
                    else:
                        logger.info("âœ… å¾Œè™•ç†é©—è­‰é€šéï¼Œæ™‚é–“åºåˆ—è½‰æ›çµæœç¬¦åˆå­¸è¡“æ¨™æº–")
                        
                        # è¨˜éŒ„é©—è­‰æ‘˜è¦
                        academic_compliance = post_validation_result.get('academic_compliance', {})
                        if academic_compliance.get('compliant', False):
                            logger.info(f"ğŸ“ å­¸è¡“åˆè¦æ€§: Grade {academic_compliance.get('grade_level', 'Unknown')}")
                        else:
                            logger.warning(f"âš ï¸ å­¸è¡“åˆè¦æ€§å•é¡Œ: {len(academic_compliance.get('violations', []))} é …é•è¦")
                    
                    # å°‡é©—è­‰çµæœåŠ å…¥è™•ç†æŒ‡æ¨™
                    processing_metrics['validation_summary'] = post_validation_result
                    
                except Exception as e:
                    logger.error(f"ğŸš¨ Phase 3 å¾Œè™•ç†é©—è­‰ç•°å¸¸: {str(e)}")
                    if "Phase 3 Quality Gate Blocked" in str(e):
                        raise  # é‡æ–°æ‹‹å‡ºå“è³ªé–€ç¦é˜»æ–·éŒ¯èª¤
                    else:
                        logger.warning("   ä½¿ç”¨èˆŠç‰ˆé©—è­‰é‚è¼¯ç¹¼çºŒè™•ç†")
                        processing_metrics['validation_summary'] = {
                            'success': False,
                            'error': str(e),
                            'fallback_used': True
                        }

            # 4. çµ„è£è¿”å›çµæœ
            results = {
                "success": True,
                "processing_type": "timeseries_preprocessing",
                "processing_timestamp": datetime.now(timezone.utc).isoformat(),
                "input_source": "signal_quality_analysis_output.json",
                "output_directory": str(self.timeseries_preprocessing_dir),
                "output_files": output_files,
                "conversion_statistics": conversion_results["conversion_statistics"],
                "constellation_data": {
                    "starlink": {
                        "satellites_processed": len(conversion_results["starlink"]["satellites"]) if conversion_results["starlink"] else 0,
                        "output_file": output_files.get("starlink", None)
                    },
                    "oneweb": {
                        "satellites_processed": len(conversion_results["oneweb"]["satellites"]) if conversion_results["oneweb"] else 0,
                        "output_file": output_files.get("oneweb", None)
                    }
                },
                # ğŸ”§ ä¿®å¾©ï¼šæ·»åŠ timeseries_dataå­—æ®µä¾›Stage 5ä½¿ç”¨
                "timeseries_data": {
                    "satellites": all_satellites,
                    "metadata": {
                        "total_satellites": len(all_satellites),
                        "processing_complete": True,
                        "data_format": "enhanced_timeseries"
                    }
                },
                # ğŸ”§ æ·»åŠ metadataå…¼å®¹å­—æ®µ
                "metadata": {
                    "total_satellites": len(all_satellites),
                    "successful_conversions": conversion_results["conversion_statistics"]["successful_conversions"],
                    "failed_conversions": conversion_results["conversion_statistics"]["failed_conversions"],
                    "total_output_size_mb": total_output_size_mb,
                    "processing_metrics": processing_metrics,
                    "validation_summary": processing_metrics.get('validation_summary', None),
                    "academic_compliance": {
                        'phase3_validation': 'enabled' if self.validation_enabled else 'disabled',
                        'data_format_version': 'unified_v1.1_phase3',
                        'runtime_checks_enforced': True  # æ–°å¢ï¼šé›¶å®¹å¿æª¢æŸ¥ç‹€æ…‹
                    }
                }
            }
            
            # 5. ä¿å­˜é©—è­‰å¿«ç…§
            validation_success = self.save_validation_snapshot(results)
            if validation_success:
                logger.info("âœ… Stage 4 é©—è­‰å¿«ç…§å·²ä¿å­˜")
            else:
                logger.warning("âš ï¸ Stage 4 é©—è­‰å¿«ç…§ä¿å­˜å¤±æ•—")
            
            total_processed = results["conversion_statistics"]["total_processed"]
            total_successful = results["conversion_statistics"]["successful_conversions"]
            
            logger.info("=" * 60)
            logger.info("âœ… æ™‚é–“åºåˆ—é è™•ç†å®Œæˆ")
            logger.info(f"  è™•ç†çš„è¡›æ˜Ÿæ•¸: {total_processed}")
            logger.info(f"  æˆåŠŸè½‰æ›: {total_successful}")
            logger.info(f"  è½‰æ›ç‡: {total_successful/total_processed*100:.1f}%" if total_processed > 0 else "  è½‰æ›ç‡: 0%")
            logger.info(f"  è™•ç†æ™‚é–“: {processing_duration:.2f} ç§’")
            logger.info(f"  è¼¸å‡ºæª”æ¡ˆç¸½å¤§å°: {total_output_size_mb:.1f} MB")
            logger.info("  ğŸš¨ é›¶å®¹å¿é‹è¡Œæ™‚æª¢æŸ¥: å…¨éƒ¨é€šé")
            
            if output_files:
                logger.info(f"  è¼¸å‡ºæ–‡ä»¶:")
                for const, file_path in output_files.items():
                    logger.info(f"    {const}: {file_path}")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ Stage 4 æ™‚é–“åºåˆ—é è™•ç†å¤±æ•—: {e}")
            # ä¿å­˜éŒ¯èª¤å¿«ç…§
            error_data = {
                'error': str(e),
                'stage': 4,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'validation_enabled': self.validation_enabled,
                'runtime_checks_failed': 'assert' in str(e).lower()  # æª¢æ¸¬æ˜¯å¦ç‚ºé‹è¡Œæ™‚æª¢æŸ¥å¤±æ•—
            }
            self.save_validation_snapshot(error_data)
            raise