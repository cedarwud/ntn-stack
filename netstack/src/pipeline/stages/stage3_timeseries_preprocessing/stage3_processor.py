"""
Stage 3è™•ç†å™¨ - æ™‚é–“åºåˆ—é è™•ç†æ¨¡çµ„åŒ–ç‰ˆæœ¬

åŠŸèƒ½ï¼š
1. è¼‰å…¥Stage 2å¯è¦‹æ€§éæ¿¾è¼¸å‡º
2. è½‰æ›ç‚ºå‹•ç•«æ™‚é–“åºåˆ—æ ¼å¼
3. å»ºæ§‹å‰ç«¯å‹•ç•«æ•¸æ“šçµæ§‹
4. åŸ·è¡Œå­¸è¡“ç´šåˆ¥åˆè¦é©—è­‰
5. è¼¸å‡ºå¤šç¨®æ ¼å¼çš„è™•ç†çµæœ

æ¶æ§‹ï¼š
- ç¹¼æ‰¿BaseStageProcessoråŸºç¤æ¶æ§‹
- æ•´åˆ5å€‹å°ˆç”¨çµ„ä»¶å®Œæˆè¤‡é›œè™•ç†
- æ”¯æŒå¤šç¨®è¼¸å‡ºæ ¼å¼å’Œå­¸è¡“ç´šé©—è­‰
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime, timezone

from ...shared.base_processor import BaseStageProcessor
from .visibility_data_loader import VisibilityDataLoader
from .timeseries_converter import TimeseriesConverter
from .animation_builder import AnimationBuilder
from .academic_validator import AcademicValidator
from .output_formatter import OutputFormatter

logger = logging.getLogger(__name__)

class Stage3Processor(BaseStageProcessor):
    """Stage 3: æ™‚é–“åºåˆ—é è™•ç†å™¨ - é‡æ§‹ç‰ˆ"""
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–Stage 3è™•ç†å™¨"""
        # å‘¼å«åŸºç¤è™•ç†å™¨çš„åˆå§‹åŒ–
        super().__init__(stage_number=3, stage_name="timeseries_preprocessing", config=config)
        
        self.logger.info("ğŸ¬ åˆå§‹åŒ–Stage 3æ™‚é–“åºåˆ—é è™•ç†å™¨...")
        
        # è®€å–é…ç½®
        self.animation_mode = config.get('animation_mode', 'enhanced') if config else 'enhanced'
        self.web_optimization = config.get('web_optimization', True) if config else True
        self.academic_validation = config.get('academic_validation', True) if config else True
        self.output_formats = config.get('output_formats', ['enhanced_animation']) if config else ['enhanced_animation']
        self.time_compression_ratio = config.get('time_compression_ratio', 100) if config else 100
        
        # åˆå§‹åŒ–çµ„ä»¶
        try:
            self.data_loader = VisibilityDataLoader()
            self.timeseries_converter = TimeseriesConverter()
            self.animation_builder = AnimationBuilder()
            self.academic_validator = AcademicValidator()
            self.output_formatter = OutputFormatter()
            
            self.logger.info("âœ… Stage 3æ‰€æœ‰çµ„ä»¶åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"âŒ Stage 3çµ„ä»¶åˆå§‹åŒ–å¤±æ•—: {e}")
            raise RuntimeError(f"Stage 3åˆå§‹åŒ–å¤±æ•—: {e}")
        
        # è™•ç†çµ±è¨ˆ
        self.processing_stats = {
            "satellites_loaded": 0,
            "satellites_processed": 0,
            "animation_frames_created": 0,
            "constellations_animated": 0,
            "academic_validation_passed": 0,
            "output_formats_generated": 0
        }
    
    def validate_input(self, input_data: Any) -> bool:
        """
        é©—è­‰è¼¸å…¥æ•¸æ“š
        
        Stage 3éœ€è¦Stage 2çš„å¯è¦‹æ€§éæ¿¾è¼¸å‡º
        """
        self.logger.info("ğŸ” é©—è­‰Stage 3è¼¸å…¥æ•¸æ“š...")
        
        try:
            # æª¢æŸ¥Stage 2è¼¸å‡ºæ–‡ä»¶å­˜åœ¨æ€§
            stage2_data = self.data_loader.load_stage2_output()
            
            # é©—è­‰æ•¸æ“šå®Œæ•´æ€§
            validation_result = self.data_loader.validate_visibility_data_completeness(stage2_data)
            
            if not validation_result["overall_valid"]:
                self.logger.error("Stage 2å¯è¦‹æ€§æ•¸æ“šé©—è­‰å¤±æ•—:")
                for issue in validation_result["issues"]:
                    self.logger.error(f"  - {issue}")
                return False
            
            self.logger.info("âœ… Stage 2å¯è¦‹æ€§æ•¸æ“šé©—è­‰é€šé")
            return True
            
        except Exception as e:
            self.logger.error(f"è¼¸å…¥æ•¸æ“šé©—è­‰å¤±æ•—: {e}")
            return False
    
    def process(self, input_data: Any) -> Dict[str, Any]:
        """
        åŸ·è¡ŒStage 3çš„æ ¸å¿ƒè™•ç†é‚è¼¯
        
        è™•ç†æ­¥é©Ÿ:
        1. è¼‰å…¥Stage 2å¯è¦‹æ€§éæ¿¾è¼¸å‡º
        2. è½‰æ›ç‚ºå‹•ç•«æ™‚é–“åºåˆ—æ ¼å¼
        3. å»ºæ§‹å‰ç«¯å‹•ç•«æ•¸æ“šçµæ§‹
        4. åŸ·è¡Œå­¸è¡“ç´šåˆè¦é©—è­‰
        5. æ ¼å¼åŒ–å¤šç¨®è¼¸å‡ºæ ¼å¼
        """
        self.logger.info("ğŸ¬ é–‹å§‹Stage 3æ™‚é–“åºåˆ—é è™•ç†...")
        
        try:
            # æ­¥é©Ÿ1: è¼‰å…¥Stage 2æ•¸æ“š
            self.logger.info("ğŸ“¥ æ­¥é©Ÿ1: è¼‰å…¥Stage 2å¯è¦‹æ€§æ•¸æ“š")
            stage2_data = self.data_loader.load_stage2_output()
            
            load_stats = self.data_loader.get_load_statistics()
            self.processing_stats["satellites_loaded"] = load_stats["satellites_loaded"]
            
            # æ­¥é©Ÿ2: è½‰æ›ç‚ºæ™‚é–“åºåˆ—
            self.logger.info("â±ï¸ æ­¥é©Ÿ2: è½‰æ›ç‚ºå‹•ç•«æ™‚é–“åºåˆ—")
            satellites = stage2_data.get("satellites", [])
            timeseries_data = self.timeseries_converter.convert_visibility_to_timeseries(satellites)
            
            converter_stats = self.timeseries_converter.get_conversion_statistics()
            self.processing_stats["satellites_processed"] = converter_stats["total_satellites_processed"]
            
            # æ­¥é©Ÿ3: å»ºæ§‹å‹•ç•«æ•¸æ“š
            self.logger.info("ğŸ¨ æ­¥é©Ÿ3: å»ºæ§‹å‰ç«¯å‹•ç•«æ•¸æ“š")
            animation_data = self.animation_builder.build_animation_data(timeseries_data)
            
            builder_stats = self.animation_builder.get_build_statistics()
            self.processing_stats["animation_frames_created"] = builder_stats["total_frames"]
            self.processing_stats["constellations_animated"] = builder_stats["constellations_processed"]
            
            # æ­¥é©Ÿ4: å­¸è¡“ç´šé©—è­‰
            validation_passed = True
            if self.academic_validation:
                self.logger.info("ğŸ“ æ­¥é©Ÿ4: åŸ·è¡Œå­¸è¡“ç´šåˆè¦é©—è­‰")
                validation_result = self.academic_validator.validate_timeseries_academic_compliance(
                    timeseries_data,
                    animation_data
                )
                
                if not validation_result["overall_compliance"]:
                    self.logger.warning("å­¸è¡“ç´šé©—è­‰è­¦å‘Š:")
                    for issue in validation_result["compliance_issues"]:
                        self.logger.warning(f"  - {issue}")
                    validation_passed = False
                else:
                    self.processing_stats["academic_validation_passed"] = 1
            
            # æ­¥é©Ÿ5: æ ¼å¼åŒ–è¼¸å‡º
            self.logger.info("ğŸ“‹ æ­¥é©Ÿ5: æ ¼å¼åŒ–å¤šç¨®è¼¸å‡ºæ ¼å¼")
            formatted_results = {}
            
            for output_format in self.output_formats:
                try:
                    formatted_output = self.output_formatter.format_stage3_output(
                        timeseries_data=timeseries_data,
                        animation_data=animation_data,
                        stage2_metadata=stage2_data.get("metadata", {}),
                        processing_stats=self.processing_stats,
                        output_format=output_format,
                        validation_passed=validation_passed
                    )
                    
                    formatted_results[output_format] = formatted_output
                    self.processing_stats["output_formats_generated"] += 1
                    
                except Exception as e:
                    self.logger.error(f"æ ¼å¼åŒ–è¼¸å‡ºå¤±æ•— {output_format}: {e}")
                    continue
            
            if not formatted_results:
                raise ValueError("æ‰€æœ‰è¼¸å‡ºæ ¼å¼ç”Ÿæˆå¤±æ•—")
            
            # è¿”å›ä¸»è¦æ ¼å¼æˆ–ç¬¬ä¸€å€‹å¯ç”¨æ ¼å¼
            main_result = formatted_results.get('enhanced_animation') or next(iter(formatted_results.values()))
            
            # æ·»åŠ é¡å¤–æ ¼å¼åˆ°metadataä¸­
            if len(formatted_results) > 1:
                main_result["metadata"]["additional_formats"] = {
                    fmt: result["metadata"]["output_summary"] 
                    for fmt, result in formatted_results.items() 
                    if fmt != 'enhanced_animation'
                }
            
            self.logger.info(f"âœ… Stage 3è™•ç†å®Œæˆ: {self.processing_stats['satellites_processed']} é¡†è¡›æ˜Ÿ, "
                           f"{self.processing_stats['animation_frames_created']} å‹•ç•«å¹€")
            
            return main_result
            
        except Exception as e:
            self.logger.error(f"Stage 3è™•ç†å¤±æ•—: {e}")
            raise RuntimeError(f"Stage 3æ™‚é–“åºåˆ—é è™•ç†å¤±æ•—: {e}")
    
    def validate_output(self, output_data: Dict[str, Any]) -> bool:
        """é©—è­‰è¼¸å‡ºæ•¸æ“šçš„æœ‰æ•ˆæ€§"""
        self.logger.info("ğŸ” é©—è­‰Stage 3è¼¸å‡ºæ•¸æ“š...")
        
        try:
            # æª¢æŸ¥åŸºæœ¬çµæ§‹
            if not isinstance(output_data, dict):
                self.logger.error("è¼¸å‡ºæ•¸æ“šå¿…é ˆæ˜¯å­—å…¸æ ¼å¼")
                return False
            
            if "data" not in output_data or "metadata" not in output_data:
                self.logger.error("è¼¸å‡ºæ•¸æ“šç¼ºå°‘å¿…è¦çš„dataæˆ–metadataæ¬„ä½")
                return False
            
            # æª¢æŸ¥å‹•ç•«æ•¸æ“š
            data_section = output_data["data"]
            
            # é©—è­‰æ™‚é–“åºåˆ—æ•¸æ“š
            if "timeseries_data" not in data_section:
                self.logger.error("è¼¸å‡ºæ•¸æ“šç¼ºå°‘æ™‚é–“åºåˆ—æ•¸æ“š")
                return False
            
            timeseries_data = data_section["timeseries_data"]
            satellites = timeseries_data.get("satellites", [])
            
            if not satellites:
                self.logger.error("è¼¸å‡ºæ•¸æ“šä¸­ç„¡è¡›æ˜Ÿæ™‚é–“åºåˆ—æ•¸æ“š")
                return False
            
            # é©—è­‰å‹•ç•«æ•¸æ“š
            if "animation_data" not in data_section:
                self.logger.error("è¼¸å‡ºæ•¸æ“šç¼ºå°‘å‹•ç•«æ•¸æ“š")
                return False
            
            animation_data = data_section["animation_data"]
            
            # æª¢æŸ¥é—œéµå‹•ç•«çµ„ä»¶
            required_animation_components = ["global_timeline", "constellation_animations"]
            for component in required_animation_components:
                if component not in animation_data:
                    self.logger.error(f"å‹•ç•«æ•¸æ“šç¼ºå°‘ {component} çµ„ä»¶")
                    return False
            
            # æª¢æŸ¥metadataå®Œæ•´æ€§
            metadata = output_data["metadata"]
            required_fields = ["stage_number", "stage_name", "processing_timestamp", "output_summary"]
            
            for field in required_fields:
                if field not in metadata:
                    self.logger.error(f"metadataç¼ºå°‘å¿…è¦æ¬„ä½: {field}")
                    return False
            
            # ä½¿ç”¨å­¸è¡“é©—è­‰å™¨é€²è¡Œæ·±åº¦æª¢æŸ¥
            if self.academic_validation:
                validation_result = self.academic_validator.validate_timeseries_academic_compliance(
                    timeseries_data, animation_data
                )
                
                if not validation_result["overall_compliance"]:
                    self.logger.error("å­¸è¡“ç´šåˆè¦é©—è­‰å¤±æ•—:")
                    for issue in validation_result["compliance_issues"]:
                        self.logger.error(f"  - {issue}")
                    return False
            
            self.logger.info("âœ… Stage 3è¼¸å‡ºæ•¸æ“šé©—è­‰é€šé")
            return True
            
        except Exception as e:
            self.logger.error(f"è¼¸å‡ºæ•¸æ“šé©—è­‰å¤±æ•—: {e}")
            return False
    
    def save_results(self, results: Dict[str, Any]) -> str:
        """ä¿å­˜è™•ç†çµæœåˆ°æ–‡ä»¶"""
        try:
            # æ§‹å»ºè¼¸å‡ºæ–‡ä»¶è·¯å¾‘
            output_file = self.output_dir / "timeseries_preprocessing_output.json"
            
            # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            output_file.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜çµæœåˆ°JSONæ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"ğŸ’¾ Stage 3çµæœå·²ä¿å­˜: {output_file}")
            
            # ä¿å­˜è™•ç†çµ±è¨ˆåˆ°å–®ç¨æ–‡ä»¶
            stats_file = self.output_dir / "stage3_processing_stats.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "processing_statistics": self.processing_stats,
                    "loader_statistics": self.data_loader.get_load_statistics(),
                    "converter_statistics": self.timeseries_converter.get_conversion_statistics(),
                    "builder_statistics": self.animation_builder.get_build_statistics(),
                    "validator_statistics": self.academic_validator.get_validation_statistics(),
                    "formatter_statistics": self.output_formatter.get_formatting_statistics(),
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }, f, indent=2, ensure_ascii=False)
            
            # å¦‚æœæœ‰å¤šç¨®æ ¼å¼ï¼Œä¿å­˜é¡å¤–æ ¼å¼
            additional_formats = results.get("metadata", {}).get("additional_formats", {})
            if additional_formats:
                for format_name, format_summary in additional_formats.items():
                    format_file = self.output_dir / f"timeseries_preprocessing_output_{format_name}.json"
                    # é€™è£¡æ‡‰è©²ä¿å­˜å®Œæ•´çš„æ ¼å¼æ•¸æ“šï¼Œä½†ç”±æ–¼å·²ç¶“åœ¨ä¸»çµæœä¸­ï¼Œæš«æ™‚è¨˜éŒ„æ‘˜è¦
                    with open(format_file.with_suffix('.summary.json'), 'w', encoding='utf-8') as f:
                        json.dump(format_summary, f, indent=2, ensure_ascii=False)
            
            return str(output_file)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜Stage 3çµæœå¤±æ•—: {e}")
            raise IOError(f"ç„¡æ³•ä¿å­˜Stage 3çµæœ: {e}")
    
    def extract_key_metrics(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–é—œéµæŒ‡æ¨™"""
        try:
            metadata = results.get("metadata", {})
            data_section = results.get("data", {})
            timeseries_data = data_section.get("timeseries_data", {})
            animation_data = data_section.get("animation_data", {})
            
            # è¨ˆç®—æ™‚é–“åºåˆ—çµ±è¨ˆ
            satellites = timeseries_data.get("satellites", [])
            total_timeseries_points = sum(
                len(sat.get("enhanced_timeseries", [])) for sat in satellites
            )
            
            # è¨ˆç®—å‹•ç•«çµ±è¨ˆ
            constellation_animations = animation_data.get("constellation_animations", {})
            total_animation_frames = sum(
                len(const_anim.get("keyframes", [])) 
                for const_anim in constellation_animations.values()
            )
            
            # è¨ˆç®—è¦†è“‹ç¯„åœçµ±è¨ˆ
            coverage_analysis = animation_data.get("coverage_analysis", {})
            
            key_metrics = {
                "total_satellites_processed": len(satellites),
                "total_timeseries_points": total_timeseries_points,
                "total_animation_frames": total_animation_frames,
                "constellations_animated": len(constellation_animations),
                "processing_duration": self.processing_duration,
                "animation_config": {
                    "mode": self.animation_mode,
                    "web_optimization": self.web_optimization,
                    "time_compression_ratio": self.time_compression_ratio
                },
                "data_quality_metrics": {
                    "academic_validation_passed": self.processing_stats["academic_validation_passed"],
                    "output_formats_generated": self.processing_stats["output_formats_generated"],
                    "avg_timeseries_points_per_satellite": total_timeseries_points / len(satellites) if satellites else 0
                },
                "coverage_metrics": {
                    "total_coverage_events": coverage_analysis.get("total_coverage_events", 0),
                    "coverage_percentage": coverage_analysis.get("overall_coverage_percentage", 0),
                    "handover_scenarios": len(coverage_analysis.get("handover_scenarios", []))
                },
                "performance_metrics": {
                    "animation_compression_ratio": self._calculate_compression_ratio(animation_data),
                    "web_optimization_savings": self._calculate_optimization_savings(animation_data)
                }
            }
            
            return key_metrics
            
        except Exception as e:
            self.logger.error(f"æå–é—œéµæŒ‡æ¨™å¤±æ•—: {e}")
            return {"error": f"æŒ‡æ¨™æå–å¤±æ•—: {e}"}
    
    def run_validation_checks(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œé©—è­‰æª¢æŸ¥"""
        try:
            validation_result = {
                "passed": True,
                "totalChecks": 0,
                "passedChecks": 0,
                "failedChecks": 0,
                "criticalChecks": [],
                "allChecks": {},
                "validation_level_info": {
                    "level": "COMPREHENSIVE",
                    "academic_grade": "A",
                    "framework": "unified_pipeline_v2"
                }
            }
            
            checks = [
                ("data_structure_check", self._check_data_structure(results)),
                ("timeseries_completeness_check", self._check_timeseries_completeness(results)),
                ("animation_integrity_check", self._check_animation_integrity(results)),
                ("metadata_completeness_check", self._check_metadata_completeness(results)),
                ("academic_compliance_check", self._check_academic_compliance(results)),
                ("temporal_consistency_check", self._check_temporal_consistency(results)),
                ("web_optimization_check", self._check_web_optimization(results))
            ]
            
            for check_name, check_result in checks:
                validation_result["allChecks"][check_name] = check_result
                validation_result["totalChecks"] += 1
                
                if check_result:
                    validation_result["passedChecks"] += 1
                else:
                    validation_result["failedChecks"] += 1
                    validation_result["criticalChecks"].append({
                        "check": check_name,
                        "status": "FAILED"
                    })
            
            # æ•´é«”é€šéç‹€æ…‹
            if validation_result["failedChecks"] > 0:
                validation_result["passed"] = False
            
            return validation_result
            
        except Exception as e:
            self.logger.error(f"é©—è­‰æª¢æŸ¥å¤±æ•—: {e}")
            return {
                "passed": False,
                "error": f"é©—è­‰æª¢æŸ¥ç•°å¸¸: {e}",
                "totalChecks": 0,
                "passedChecks": 0,
                "failedChecks": 1
            }
    
    # === è¼”åŠ©æ–¹æ³• ===
    
    def _calculate_compression_ratio(self, animation_data: Dict[str, Any]) -> float:
        """è¨ˆç®—å‹•ç•«å£“ç¸®æ¯”ç‡"""
        try:
            global_timeline = animation_data.get("global_timeline", {})
            total_timepoints = global_timeline.get("total_timepoints", 0)
            compressed_keyframes = global_timeline.get("compressed_keyframes", 0)
            
            if compressed_keyframes == 0:
                return 0.0
                
            return total_timepoints / compressed_keyframes
            
        except Exception:
            return 0.0
    
    def _calculate_optimization_savings(self, animation_data: Dict[str, Any]) -> float:
        """è¨ˆç®—ç¶²é å„ªåŒ–ç¯€çœæ¯”ä¾‹"""
        try:
            optimization_stats = animation_data.get("optimization_stats", {})
            original_size = optimization_stats.get("original_data_size", 0)
            optimized_size = optimization_stats.get("optimized_data_size", 0)
            
            if original_size == 0:
                return 0.0
                
            return ((original_size - optimized_size) / original_size) * 100
            
        except Exception:
            return 0.0
    
    # === é©—è­‰æª¢æŸ¥æ–¹æ³• ===
    
    def _check_data_structure(self, results: Dict[str, Any]) -> bool:
        """æª¢æŸ¥æ•¸æ“šçµæ§‹å®Œæ•´æ€§"""
        required_keys = ["data", "metadata"]
        data_keys = ["timeseries_data", "animation_data"]
        
        if not all(key in results for key in required_keys):
            return False
            
        data_section = results.get("data", {})
        return all(key in data_section for key in data_keys)
    
    def _check_timeseries_completeness(self, results: Dict[str, Any]) -> bool:
        """æª¢æŸ¥æ™‚é–“åºåˆ—æ•¸æ“šå®Œæ•´æ€§"""
        timeseries_data = results.get("data", {}).get("timeseries_data", {})
        satellites = timeseries_data.get("satellites", [])
        
        if not satellites:
            return False
            
        # æª¢æŸ¥æ¯é¡†è¡›æ˜Ÿéƒ½æœ‰è¶³å¤ çš„æ™‚é–“åºåˆ—é»
        for satellite in satellites:
            timeseries = satellite.get("enhanced_timeseries", [])
            if len(timeseries) < 50:  # æœ€å°‘50å€‹æ™‚é–“é»
                return False
                
        return True
    
    def _check_animation_integrity(self, results: Dict[str, Any]) -> bool:
        """æª¢æŸ¥å‹•ç•«æ•¸æ“šå®Œæ•´æ€§"""
        animation_data = results.get("data", {}).get("animation_data", {})
        
        required_components = ["global_timeline", "constellation_animations", "coverage_analysis"]
        
        return all(component in animation_data for component in required_components)
    
    def _check_metadata_completeness(self, results: Dict[str, Any]) -> bool:
        """æª¢æŸ¥metadataå®Œæ•´æ€§"""
        metadata = results.get("metadata", {})
        required_fields = [
            "stage_number", "stage_name", "processing_timestamp", 
            "output_summary", "data_format_version", "processing_statistics"
        ]
        
        return all(field in metadata for field in required_fields)
    
    def _check_academic_compliance(self, results: Dict[str, Any]) -> bool:
        """æª¢æŸ¥å­¸è¡“æ¨™æº–åˆè¦æ€§"""
        academic_compliance = results.get("metadata", {}).get("academic_compliance", {})
        
        return (
            academic_compliance.get("grade") == "A" and
            academic_compliance.get("validation_passed") == True and
            academic_compliance.get("no_simplified_algorithms") == True
        )
    
    def _check_temporal_consistency(self, results: Dict[str, Any]) -> bool:
        """æª¢æŸ¥æ™‚é–“ä¸€è‡´æ€§"""
        try:
            timeseries_data = results.get("data", {}).get("timeseries_data", {})
            satellites = timeseries_data.get("satellites", [])
            
            # æª¢æŸ¥æ™‚é–“åºåˆ—æ™‚é–“æˆ³éå¢
            for satellite in satellites[:3]:  # æª¢æŸ¥å‰3é¡†è¡›æ˜Ÿ
                timeseries = satellite.get("enhanced_timeseries", [])
                if len(timeseries) < 2:
                    continue
                    
                prev_timestamp = None
                for point in timeseries[:10]:  # æª¢æŸ¥å‰10å€‹é»
                    timestamp = point.get("timestamp")
                    if prev_timestamp and timestamp <= prev_timestamp:
                        return False
                    prev_timestamp = timestamp
                    
            return True
            
        except Exception:
            return False
    
    def _check_web_optimization(self, results: Dict[str, Any]) -> bool:
        """æª¢æŸ¥ç¶²é å„ªåŒ–æ•ˆæœ"""
        if not self.web_optimization:
            return True  # å¦‚æœæ²’é–‹å•Ÿå„ªåŒ–ï¼Œè¦–ç‚ºé€šé
            
        animation_data = results.get("data", {}).get("animation_data", {})
        optimization_stats = animation_data.get("optimization_stats", {})
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å„ªåŒ–çµ±è¨ˆ
        return "optimized_data_size" in optimization_stats and "original_data_size" in optimization_stats