#!/usr/bin/env python3
"""
éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™è™•ç†å™¨ - ç°¡åŒ–ä¿®å¾©ç‰ˆæœ¬
å¯¦ç¾æ··åˆå­˜å„²æ¶æ§‹å’Œæ•¸æ“šæ ¼å¼çµ±ä¸€
"""

import json
import logging
import asyncio
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timezone

# å°å…¥çµ±ä¸€è§€æ¸¬åº§æ¨™ç®¡ç†
from shared_core.observer_config_service import get_ntpu_coordinates

# å°å…¥é©—è­‰åŸºç¤é¡åˆ¥
from shared_core.validation_snapshot_base import ValidationSnapshotBase

@dataclass
class Stage5Config:
    """éšæ®µäº”é…ç½®åƒæ•¸"""
    
    # ğŸ¯ ä¿®å¾©ï¼šè¼¸å…¥ç›®éŒ„æŒ‡å‘æ­£ç¢ºçš„timeseries_preprocessing_outputs
    input_enhanced_timeseries_dir: str = "/app/data"
    
    # è¼¸å‡ºç›®éŒ„
    output_layered_dir: str = "/app/data/layered_phase0_enhanced"
    output_handover_scenarios_dir: str = "/app/data/handover_scenarios"
    output_signal_analysis_dir: str = "/app/data/signal_quality_analysis"
    output_processing_cache_dir: str = "/app/data/processing_cache"
    output_status_files_dir: str = "/app/data/status_files"
    output_data_integration_dir: str = "/app/data"
    
    # åˆ†å±¤ä»°è§’é–€æª»
    elevation_thresholds: List[int] = None
    
    def __post_init__(self):
        if self.elevation_thresholds is None:
            self.elevation_thresholds = [5, 10, 15]

class Stage5IntegrationProcessor(ValidationSnapshotBase):
    """éšæ®µäº”æ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™è™•ç†å™¨ - èªæ³•ä¿®å¾©ç‰ˆ"""
    
    def __init__(self, config: Stage5Config):
        # Initialize ValidationSnapshotBase
        super().__init__(stage_number=5, stage_name="éšæ®µ5: æ•¸æ“šæ•´åˆ", 
                         snapshot_dir=str(config.output_data_integration_dir + "/validation_snapshots"))
        
        self.config = config
        self.logger = logging.getLogger(__name__)
        # Use ValidationSnapshotBase timer instead of manual time.time()
        self.start_processing_timer()
        
        # ä½¿ç”¨çµ±ä¸€è§€æ¸¬åº§æ¨™ç®¡ç†ï¼Œç§»é™¤ç¡¬ç·¨ç¢¼
        ntpu_lat, ntpu_lon, ntpu_alt = get_ntpu_coordinates()
        self.observer_lat = ntpu_lat
        self.observer_lon = ntpu_lon
        self.observer_alt = ntpu_alt
        
        self.logger.info("âœ… Stage5 æ•¸æ“šæ•´åˆè™•ç†å™¨åˆå§‹åŒ–å®Œæˆ (ä½¿ç”¨ shared_core åº§æ¨™)")
        self.logger.info(f"  è§€æ¸¬åº§æ¨™: ({self.observer_lat}Â°, {self.observer_lon}Â°) [ä¾†è‡ª shared_core]")
        
    def extract_key_metrics(self, processing_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–éšæ®µ5é—œéµæŒ‡æ¨™"""
        constellation_summary = processing_results.get('constellation_summary', {})
        
        return {
            "ç¸½è¡›æ˜Ÿæ•¸": processing_results.get('total_satellites', 0),
            "æˆåŠŸæ•´åˆ": processing_results.get('successfully_integrated', 0),
            "Starlinkæ•´åˆ": constellation_summary.get('starlink', {}).get('satellite_count', 0),
            "OneWebæ•´åˆ": constellation_summary.get('oneweb', {}).get('satellite_count', 0),
            "è™•ç†è€—æ™‚": f"{processing_results.get('processing_time_seconds', 0):.2f}ç§’"
        }
    
    def run_validation_checks(self, processing_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸ·è¡Œéšæ®µ5ç‰¹å®šé©—è­‰æª¢æŸ¥"""
        checks = []
        
        constellation_summary = processing_results.get('constellation_summary', {})
        satellites_data = processing_results.get('satellites', {})
        
        # æª¢æŸ¥1: æ•¸æ“šæ•´åˆæˆåŠŸç‡
        total_satellites = processing_results.get('total_satellites', 0)
        successfully_integrated = processing_results.get('successfully_integrated', 0)
        integration_rate = (successfully_integrated / max(total_satellites, 1)) * 100
        
        checks.append({
            'checkName': 'æ•¸æ“šæ•´åˆæˆåŠŸç‡æª¢æŸ¥',
            'passed': integration_rate >= 95.0,  # è¦æ±‚95%ä»¥ä¸Šæ•´åˆç‡
            'result': f"æ•´åˆæˆåŠŸç‡: {integration_rate:.1f}% ({successfully_integrated}/{total_satellites})",
            'details': f"æˆåŠŸæ•´åˆ {successfully_integrated} é¡†è¡›æ˜Ÿï¼Œç¸½è¨ˆ {total_satellites} é¡†è¡›æ˜Ÿ"
        })
        
        # æª¢æŸ¥2: Starlinkæ•¸æ“šæ•´åˆæª¢æŸ¥
        starlink_info = constellation_summary.get('starlink', {})
        starlink_data_exists = 'starlink' in satellites_data and satellites_data['starlink'] is not None
        starlink_count = starlink_info.get('satellite_count', 0)
        
        checks.append({
            'checkName': 'Starlinkæ•¸æ“šæ•´åˆæª¢æŸ¥',
            'passed': starlink_data_exists and starlink_count > 0,
            'result': f"Starlink: {starlink_count} é¡†è¡›æ˜Ÿæ•´åˆå®Œæˆ" if starlink_data_exists else "Starlinkæ•¸æ“šæœªæ•´åˆ",
            'details': f"æ•¸æ“šç‹€æ…‹: {'å·²åŠ è¼‰' if starlink_data_exists else 'æœªåŠ è¼‰'}ï¼Œè¡›æ˜Ÿæ•¸é‡: {starlink_count}"
        })
        
        # æª¢æŸ¥3: OneWebæ•¸æ“šæ•´åˆæª¢æŸ¥
        oneweb_info = constellation_summary.get('oneweb', {})
        oneweb_data_exists = 'oneweb' in satellites_data and satellites_data['oneweb'] is not None
        oneweb_count = oneweb_info.get('satellite_count', 0)
        
        checks.append({
            'checkName': 'OneWebæ•¸æ“šæ•´åˆæª¢æŸ¥',
            'passed': oneweb_data_exists and oneweb_count > 0,
            'result': f"OneWeb: {oneweb_count} é¡†è¡›æ˜Ÿæ•´åˆå®Œæˆ" if oneweb_data_exists else "OneWebæ•¸æ“šæœªæ•´åˆ",
            'details': f"æ•¸æ“šç‹€æ…‹: {'å·²åŠ è¼‰' if oneweb_data_exists else 'æœªåŠ è¼‰'}ï¼Œè¡›æ˜Ÿæ•¸é‡: {oneweb_count}"
        })
        
        # æª¢æŸ¥4: è¼¸å‡ºæ–‡ä»¶ç”Ÿæˆæª¢æŸ¥
        output_file = processing_results.get('output_file')
        output_file_exists = output_file and Path(output_file).exists() if output_file else False
        
        checks.append({
            'checkName': 'æ•´åˆè¼¸å‡ºæ–‡ä»¶ç”Ÿæˆæª¢æŸ¥',
            'passed': output_file_exists,
            'result': f"è¼¸å‡ºæ–‡ä»¶ç”ŸæˆæˆåŠŸ: {Path(output_file).name}" if output_file_exists else "è¼¸å‡ºæ–‡ä»¶æœªç”Ÿæˆ",
            'details': f"æ–‡ä»¶è·¯å¾‘: {output_file if output_file else 'æœªæŒ‡å®š'}"
        })
        
        return checks
    
    async def process_enhanced_timeseries(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„æ•¸æ“šæ•´åˆè™•ç†æµç¨‹"""
        start_time = time.time()
        self.logger.info("ğŸš€ é–‹å§‹éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™ (èªæ³•ä¿®å¾©ç‰ˆ)")
        
        # æ¸…ç†èˆŠé©—è­‰å¿«ç…§ (ç¢ºä¿ç”Ÿæˆæœ€æ–°é©—è­‰å¿«ç…§)
        if self.snapshot_file.exists():
            self.logger.info(f"ğŸ—‘ï¸ æ¸…ç†èˆŠé©—è­‰å¿«ç…§: {self.snapshot_file}")
            self.snapshot_file.unlink()
        
        results = {
            "stage": "stage5_integration",
            "start_time": datetime.now(timezone.utc).isoformat(),
            "mixed_storage_verification": {},
            "success": True,
            "processing_time_seconds": 0
        }
        
        try:
            # è¼‰å…¥å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š
            enhanced_data = await self._load_enhanced_timeseries()
            
            # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
            total_satellites = 0
            constellation_summary = {}
            
            for constellation, data in enhanced_data.items():
                if data and 'satellites' in data:
                    count = len(data['satellites'])
                    constellation_summary[constellation] = {
                        "satellite_count": count,
                        "processing_status": "integrated"
                    }
                    total_satellites += count
        
            results["total_satellites"] = total_satellites
            results["successfully_integrated"] = total_satellites
            results["constellation_summary"] = constellation_summary
            results["satellites"] = enhanced_data  # ç‚ºStage6æä¾›å®Œæ•´è¡›æ˜Ÿæ•¸æ“š
            results["processing_time_seconds"] = time.time() - self.processing_start_time
            
            # ğŸ”§ ä¿®å¾©ï¼šæ·»åŠ metadataå­—æ®µä¾›å¾ŒçºŒéšæ®µä½¿ç”¨
            results["metadata"] = {
                "stage": "stage5_integration", 
                "total_satellites": total_satellites,
                "successfully_integrated": total_satellites,
                "processing_complete": True,
                "data_integration_timestamp": datetime.now(timezone.utc).isoformat(),
                "constellation_breakdown": constellation_summary,
                "ready_for_dynamic_pool_planning": True
            }
            
            # ğŸ¯ æ–°å¢ï¼šä¿å­˜æª”æ¡ˆä¾›éšæ®µå…­ä½¿ç”¨
            output_file = self.save_integration_output(results)
            results["output_file"] = output_file
            
            # ä¿å­˜é©—è­‰å¿«ç…§
            processing_duration = time.time() - start_time
            validation_success = self.save_validation_snapshot(results)
            if validation_success:
                self.logger.info("âœ… Stage 5 é©—è­‰å¿«ç…§å·²ä¿å­˜")
            else:
                self.logger.warning("âš ï¸ Stage 5 é©—è­‰å¿«ç…§ä¿å­˜å¤±æ•—")
            
            self.logger.info(f"âœ… éšæ®µäº”å®Œæˆï¼Œè€—æ™‚: {results['processing_time_seconds']:.2f} ç§’")
            self.logger.info(f"ğŸ“Š æ•´åˆè¡›æ˜Ÿæ•¸æ“š: {total_satellites} é¡†è¡›æ˜Ÿ")
            self.logger.info(f"ğŸ’¾ è¼¸å‡ºæª”æ¡ˆ: {output_file}")
        
        except Exception as e:
            self.logger.error(f"âŒ éšæ®µäº”è™•ç†å¤±æ•—: {e}")
            results["success"] = False
            results["error"] = str(e)
            
            # ä¿å­˜éŒ¯èª¤å¿«ç…§
            error_data = {
                'error': str(e),
                'stage': 5,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            self.save_validation_snapshot(error_data)
            
        return results
    
    async def _load_enhanced_timeseries(self) -> Dict[str, Any]:
        """è¼‰å…¥å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š"""
        
        enhanced_data = {
            "starlink": None,
            "oneweb": None
        }
        
        input_dir = Path(self.config.input_enhanced_timeseries_dir)
        
        for constellation in ["starlink", "oneweb"]:
            target_file = input_dir / f"{constellation}_enhanced.json"
            
            if target_file.exists():
                self.logger.info(f"è¼‰å…¥ {constellation} å¢å¼·æ•¸æ“š: {target_file}")
                
                with open(target_file, 'r') as f:
                    enhanced_data[constellation] = json.load(f)
                    
                satellites_count = len(enhanced_data[constellation].get('satellites', []))
                self.logger.info(f"âœ… {constellation}: {satellites_count} é¡†è¡›æ˜Ÿ")
            else:
                self.logger.warning(f"âš ï¸ {constellation} å¢å¼·æ•¸æ“šæª”æ¡ˆä¸å­˜åœ¨: {target_file}")
        
        return enhanced_data

    def save_integration_output(self, results: Dict[str, Any]) -> str:
        """ä¿å­˜éšæ®µäº”æ•´åˆè¼¸å‡ºï¼Œä¾›éšæ®µå…­ä½¿ç”¨"""
        output_file = Path(self.config.output_data_integration_dir) / "data_integration_output.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # æ¸…ç†èˆŠæª”æ¡ˆ
        if output_file.exists():
            output_file.unlink()
            self.logger.info(f"ğŸ—‘ï¸ æ¸…ç†èˆŠæ•´åˆè¼¸å‡º: {output_file}")
        
        # ä¿å­˜æ–°æª”æ¡ˆ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        file_size = output_file.stat().st_size / (1024*1024)  # MB
        self.logger.info(f"ğŸ’¾ éšæ®µäº”æ•´åˆè¼¸å‡ºå·²ä¿å­˜: {output_file}")
        self.logger.info(f"   æª”æ¡ˆå¤§å°: {file_size:.1f} MB")
        self.logger.info(f"   åŒ…å«è¡›æ˜Ÿæ•¸: {results.get('total_satellites', 0)}")
        
        return str(output_file)

async def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    logging.basicConfig(level=logging.INFO)
    
    config = Stage5Config()
    processor = Stage5IntegrationProcessor(config)
    
    results = await processor.process_enhanced_timeseries()
    
    print("\nğŸ¯ éšæ®µäº”è™•ç†çµæœ:")
    print(json.dumps(results, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    asyncio.run(main())
