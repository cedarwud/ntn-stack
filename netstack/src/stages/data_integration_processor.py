#!/usr/bin/env python3
"""
éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™è™•ç†å™¨ - ç°¡åŒ–ä¿®å¾©ç‰ˆæœ¬
å¯¦ç¾æ··åˆå­˜å„²æ¶æ§‹å’Œæ•¸æ“šæ ¼å¼çµ±ä¸€
"""

import json
import logging
import asyncio
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timezone

# å°å…¥çµ±ä¸€è§€æ¸¬åº§æ¨™ç®¡ç†
from shared_core.observer_config_service import get_ntpu_coordinates

# å°å…¥é©—è­‰åŸºç¤é¡åˆ¥
from shared_core.validation_snapshot_base import ValidationSnapshotBase

@dataclass
class Stage5Config:
    """éšæ®µäº”é…ç½®åƒæ•¸"""
    
    # ğŸ¯ ä¿®å¾©ï¼šè¼¸å…¥ç›®éŒ„æŒ‡å‘æ­£ç¢ºçš„timeseries_preprocessing_outputs
    input_enhanced_timeseries_dir: str = "/app/data"
    
    # è¼¸å‡ºç›®éŒ„
    output_layered_dir: str = "/app/data/layered_phase0_enhanced"
    output_handover_scenarios_dir: str = "/app/data/handover_scenarios"
    output_signal_analysis_dir: str = "/app/data/signal_quality_analysis"
    output_processing_cache_dir: str = "/app/data/processing_cache"
    output_status_files_dir: str = "/app/data/status_files"
    output_data_integration_dir: str = "/app/data"
    
    # åˆ†å±¤ä»°è§’é–€æª»
    elevation_thresholds: List[int] = None
    
    def __post_init__(self):
        if self.elevation_thresholds is None:
            self.elevation_thresholds = [5, 10, 15]

class Stage5IntegrationProcessor(ValidationSnapshotBase):
    """éšæ®µäº”æ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™è™•ç†å™¨ - èªæ³•ä¿®å¾©ç‰ˆ"""
    
    def __init__(self, config: Stage5Config):
        # Initialize ValidationSnapshotBase
        super().__init__(stage_number=5, stage_name="éšæ®µ5: æ•¸æ“šæ•´åˆ", 
                         snapshot_dir=str(config.output_data_integration_dir + "/validation_snapshots"))
        
        self.config = config
        self.logger = logging.getLogger(__name__)
        # Use ValidationSnapshotBase timer instead of manual time.time()
        self.start_processing_timer()
        
        # ä½¿ç”¨çµ±ä¸€è§€æ¸¬åº§æ¨™ç®¡ç†ï¼Œç§»é™¤ç¡¬ç·¨ç¢¼
        ntpu_lat, ntpu_lon, ntpu_alt = get_ntpu_coordinates()
        self.observer_lat = ntpu_lat
        self.observer_lon = ntpu_lon
        self.observer_alt = ntpu_alt
        
        self.logger.info("âœ… Stage5 æ•¸æ“šæ•´åˆè™•ç†å™¨åˆå§‹åŒ–å®Œæˆ (ä½¿ç”¨ shared_core åº§æ¨™)")
        self.logger.info(f"  è§€æ¸¬åº§æ¨™: ({self.observer_lat}Â°, {self.observer_lon}Â°) [ä¾†è‡ª shared_core]")
        
    def extract_key_metrics(self, processing_results: Dict[str, Any]) -> Dict[str, Any]:
        """æå–éšæ®µ5é—œéµæŒ‡æ¨™"""
        constellation_summary = processing_results.get('constellation_summary', {})
        
        return {
            "ç¸½è¡›æ˜Ÿæ•¸": processing_results.get('total_satellites', 0),
            "æˆåŠŸæ•´åˆ": processing_results.get('successfully_integrated', 0),
            "Starlinkæ•´åˆ": constellation_summary.get('starlink', {}).get('satellite_count', 0),
            "OneWebæ•´åˆ": constellation_summary.get('oneweb', {}).get('satellite_count', 0),
            "è™•ç†è€—æ™‚": f"{processing_results.get('processing_time_seconds', 0):.2f}ç§’"
        }
    
    def run_validation_checks(self, processing_results: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œ Stage 5 é©—è­‰æª¢æŸ¥ - å°ˆæ³¨æ–¼æ•¸æ“šæ•´åˆå’Œæ··åˆå­˜å„²æ¶æ§‹æº–ç¢ºæ€§"""
        metadata = processing_results.get('metadata', {})
        constellation_summary = processing_results.get('constellation_summary', {})
        satellites_data = processing_results.get('satellites', {})
        
        checks = {}
        
        # 1. è¼¸å…¥æ•¸æ“šå­˜åœ¨æ€§æª¢æŸ¥
        input_satellites = metadata.get('input_satellites', 0)
        checks["è¼¸å…¥æ•¸æ“šå­˜åœ¨æ€§"] = input_satellites > 0
        
        # 2. æ•¸æ“šæ•´åˆæˆåŠŸç‡æª¢æŸ¥ - ç¢ºä¿å¤§éƒ¨åˆ†æ•¸æ“šæˆåŠŸæ•´åˆ
        total_satellites = processing_results.get('total_satellites', 0)
        successfully_integrated = processing_results.get('successfully_integrated', 0)
        integration_rate = (successfully_integrated / max(total_satellites, 1)) * 100
        
        if self.sample_mode:
            checks["æ•¸æ“šæ•´åˆæˆåŠŸç‡"] = integration_rate >= 90.0  # å–æ¨£æ¨¡å¼90%
        else:
            checks["æ•¸æ“šæ•´åˆæˆåŠŸç‡"] = integration_rate >= 95.0  # å…¨é‡æ¨¡å¼95%
        
        # 3. PostgreSQLçµæ§‹åŒ–æ•¸æ“šæª¢æŸ¥ - ç¢ºä¿é—œéµçµæ§‹åŒ–æ•¸æ“šæ­£ç¢ºå­˜å„²
        postgresql_data_ok = True
        required_pg_tables = ['satellite_metadata', 'signal_statistics', 'event_summaries']
        pg_summary = processing_results.get('postgresql_summary', {})
        
        for table in required_pg_tables:
            if table not in pg_summary or pg_summary[table].get('record_count', 0) == 0:
                postgresql_data_ok = False
                break
        
        checks["PostgreSQLçµæ§‹åŒ–æ•¸æ“š"] = postgresql_data_ok
        
        # 4. Docker Volumeæª”æ¡ˆå­˜å„²æª¢æŸ¥ - ç¢ºä¿å¤§å‹æ™‚é–“åºåˆ—æª”æ¡ˆæ­£ç¢ºä¿å­˜
        volume_files_ok = True
        output_file = processing_results.get('output_file')
        if output_file:
            from pathlib import Path
            volume_files_ok = Path(output_file).exists()
        else:
            volume_files_ok = False
        
        checks["Volumeæª”æ¡ˆå­˜å„²"] = volume_files_ok
        
        # 5. æ··åˆå­˜å„²æ¶æ§‹å¹³è¡¡æ€§æª¢æŸ¥ - ç¢ºä¿PostgreSQLå’ŒVolumeçš„æ•¸æ“šåˆ†ä½ˆåˆç†
        pg_size_mb = metadata.get('postgresql_size_mb', 0)
        volume_size_mb = metadata.get('volume_size_mb', 0)
        
        storage_balance_ok = True
        if pg_size_mb > 0 and volume_size_mb > 0:
            total_size = pg_size_mb + volume_size_mb
            pg_ratio = pg_size_mb / total_size
            # PostgreSQLæ‡‰ä½”15-25%ï¼ŒVolumeä½”75-85%ï¼ˆæ ¹æ“šæ–‡æª”ï¼šPostgreSQL ~65MB, Volume ~300MBï¼‰
            storage_balance_ok = 0.10 <= pg_ratio <= 0.30
        else:
            storage_balance_ok = False
        
        checks["æ··åˆå­˜å„²æ¶æ§‹å¹³è¡¡æ€§"] = storage_balance_ok
        
        # 6. æ˜Ÿåº§æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥ - ç¢ºä¿å…©å€‹æ˜Ÿåº§éƒ½æˆåŠŸæ•´åˆ
        starlink_integrated = 'starlink' in satellites_data and constellation_summary.get('starlink', {}).get('satellite_count', 0) > 0
        oneweb_integrated = 'oneweb' in satellites_data and constellation_summary.get('oneweb', {}).get('satellite_count', 0) > 0
        
        checks["æ˜Ÿåº§æ•¸æ“šå®Œæ•´æ€§"] = starlink_integrated and oneweb_integrated
        
        # 7. æ•¸æ“šçµæ§‹å®Œæ•´æ€§æª¢æŸ¥
        required_fields = ['metadata', 'constellation_summary', 'postgresql_summary', 'output_file']
        checks["æ•¸æ“šçµæ§‹å®Œæ•´æ€§"] = ValidationCheckHelper.check_data_completeness(
            processing_results, required_fields
        )
        
        # 8. è™•ç†æ™‚é–“æª¢æŸ¥ - æ•¸æ“šæ•´åˆéœ€è¦ä¸€å®šæ™‚é–“ä½†ä¸æ‡‰éé•·
        max_time = 300 if self.sample_mode else 180  # å–æ¨£5åˆ†é˜ï¼Œå…¨é‡3åˆ†é˜
        checks["è™•ç†æ™‚é–“åˆç†æ€§"] = ValidationCheckHelper.check_processing_time(
            self.processing_duration, max_time
        )
        
        # è¨ˆç®—é€šéçš„æª¢æŸ¥æ•¸é‡
        passed_checks = sum(1 for passed in checks.values() if passed)
        total_checks = len(checks)
        
        return {
            "passed": passed_checks == total_checks,
            "totalChecks": total_checks,
            "passedChecks": passed_checks,
            "failedChecks": total_checks - passed_checks,
            "criticalChecks": [
                {"name": "æ•¸æ“šæ•´åˆæˆåŠŸç‡", "status": "passed" if checks["æ•¸æ“šæ•´åˆæˆåŠŸç‡"] else "failed"},
                {"name": "PostgreSQLçµæ§‹åŒ–æ•¸æ“š", "status": "passed" if checks["PostgreSQLçµæ§‹åŒ–æ•¸æ“š"] else "failed"},
                {"name": "Volumeæª”æ¡ˆå­˜å„²", "status": "passed" if checks["Volumeæª”æ¡ˆå­˜å„²"] else "failed"},
                {"name": "æ··åˆå­˜å„²æ¶æ§‹å¹³è¡¡æ€§", "status": "passed" if checks["æ··åˆå­˜å„²æ¶æ§‹å¹³è¡¡æ€§"] else "failed"}
            ],
            "allChecks": checks
        }
    
    async def process_enhanced_timeseries(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„æ•¸æ“šæ•´åˆè™•ç†æµç¨‹"""
        start_time = time.time()
        self.logger.info("ğŸš€ é–‹å§‹éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™ (èªæ³•ä¿®å¾©ç‰ˆ)")
        
        # æ¸…ç†èˆŠé©—è­‰å¿«ç…§ (ç¢ºä¿ç”Ÿæˆæœ€æ–°é©—è­‰å¿«ç…§)
        if self.snapshot_file.exists():
            self.logger.info(f"ğŸ—‘ï¸ æ¸…ç†èˆŠé©—è­‰å¿«ç…§: {self.snapshot_file}")
            self.snapshot_file.unlink()
        
        results = {
            "stage": "stage5_integration",
            "start_time": datetime.now(timezone.utc).isoformat(),
            "mixed_storage_verification": {},
            "success": True,
            "processing_time_seconds": 0
        }
        
        try:
            # è¼‰å…¥å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š
            enhanced_data = await self._load_enhanced_timeseries()
            
            # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
            total_satellites = 0
            constellation_summary = {}
            
            for constellation, data in enhanced_data.items():
                if data and 'satellites' in data:
                    count = len(data['satellites'])
                    constellation_summary[constellation] = {
                        "satellite_count": count,
                        "processing_status": "integrated"
                    }
                    total_satellites += count
        
            results["total_satellites"] = total_satellites
            results["successfully_integrated"] = total_satellites
            results["constellation_summary"] = constellation_summary
            results["satellites"] = enhanced_data  # ç‚ºStage6æä¾›å®Œæ•´è¡›æ˜Ÿæ•¸æ“š
            results["processing_time_seconds"] = time.time() - self.processing_start_time
            
            # ğŸ”§ ä¿®å¾©ï¼šæ·»åŠ metadataå­—æ®µä¾›å¾ŒçºŒéšæ®µä½¿ç”¨
            results["metadata"] = {
                "stage": "stage5_integration", 
                "total_satellites": total_satellites,
                "successfully_integrated": total_satellites,
                "processing_complete": True,
                "data_integration_timestamp": datetime.now(timezone.utc).isoformat(),
                "constellation_breakdown": constellation_summary,
                "ready_for_dynamic_pool_planning": True
            }
            
            # ğŸ¯ æ–°å¢ï¼šä¿å­˜æª”æ¡ˆä¾›éšæ®µå…­ä½¿ç”¨
            output_file = self.save_integration_output(results)
            results["output_file"] = output_file
            
            # ä¿å­˜é©—è­‰å¿«ç…§
            processing_duration = time.time() - start_time
            validation_success = self.save_validation_snapshot(results)
            if validation_success:
                self.logger.info("âœ… Stage 5 é©—è­‰å¿«ç…§å·²ä¿å­˜")
            else:
                self.logger.warning("âš ï¸ Stage 5 é©—è­‰å¿«ç…§ä¿å­˜å¤±æ•—")
            
            self.logger.info(f"âœ… éšæ®µäº”å®Œæˆï¼Œè€—æ™‚: {results['processing_time_seconds']:.2f} ç§’")
            self.logger.info(f"ğŸ“Š æ•´åˆè¡›æ˜Ÿæ•¸æ“š: {total_satellites} é¡†è¡›æ˜Ÿ")
            self.logger.info(f"ğŸ’¾ è¼¸å‡ºæª”æ¡ˆ: {output_file}")
        
        except Exception as e:
            self.logger.error(f"âŒ éšæ®µäº”è™•ç†å¤±æ•—: {e}")
            results["success"] = False
            results["error"] = str(e)
            
            # ä¿å­˜éŒ¯èª¤å¿«ç…§
            error_data = {
                'error': str(e),
                'stage': 5,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }
            self.save_validation_snapshot(error_data)
            
        return results
    
    async def _load_enhanced_timeseries(self) -> Dict[str, Any]:
        """è¼‰å…¥å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š"""
        
        enhanced_data = {
            "starlink": None,
            "oneweb": None
        }
        
        input_dir = Path(self.config.input_enhanced_timeseries_dir)
        
        for constellation in ["starlink", "oneweb"]:
            target_file = input_dir / f"{constellation}_enhanced.json"
            
            if target_file.exists():
                self.logger.info(f"è¼‰å…¥ {constellation} å¢å¼·æ•¸æ“š: {target_file}")
                
                with open(target_file, 'r') as f:
                    enhanced_data[constellation] = json.load(f)
                    
                satellites_count = len(enhanced_data[constellation].get('satellites', []))
                self.logger.info(f"âœ… {constellation}: {satellites_count} é¡†è¡›æ˜Ÿ")
            else:
                self.logger.warning(f"âš ï¸ {constellation} å¢å¼·æ•¸æ“šæª”æ¡ˆä¸å­˜åœ¨: {target_file}")
        
        return enhanced_data

    def save_integration_output(self, results: Dict[str, Any]) -> str:
        """ä¿å­˜éšæ®µäº”æ•´åˆè¼¸å‡ºï¼Œä¾›éšæ®µå…­ä½¿ç”¨"""
        output_file = Path(self.config.output_data_integration_dir) / "data_integration_output.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        # æ¸…ç†èˆŠæª”æ¡ˆ
        if output_file.exists():
            output_file.unlink()
            self.logger.info(f"ğŸ—‘ï¸ æ¸…ç†èˆŠæ•´åˆè¼¸å‡º: {output_file}")
        
        # ä¿å­˜æ–°æª”æ¡ˆ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        file_size = output_file.stat().st_size / (1024*1024)  # MB
        self.logger.info(f"ğŸ’¾ éšæ®µäº”æ•´åˆè¼¸å‡ºå·²ä¿å­˜: {output_file}")
        self.logger.info(f"   æª”æ¡ˆå¤§å°: {file_size:.1f} MB")
        self.logger.info(f"   åŒ…å«è¡›æ˜Ÿæ•¸: {results.get('total_satellites', 0)}")
        
        return str(output_file)

async def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    logging.basicConfig(level=logging.INFO)
    
    config = Stage5Config()
    processor = Stage5IntegrationProcessor(config)
    
    results = await processor.process_enhanced_timeseries()
    
    print("\nğŸ¯ éšæ®µäº”è™•ç†çµæœ:")
    print(json.dumps(results, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    asyncio.run(main())
