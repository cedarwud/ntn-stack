#!/usr/bin/env python3
"""
éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™è™•ç†å™¨
å¯¦ç¾æ··åˆå­˜å„²æ¶æ§‹å’Œæ•¸æ“šæ ¼å¼çµ±ä¸€
"""

import json
import logging
import asyncio
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime, timezone

# è³‡æ–™åº«é€£æ¥
import asyncpg
import psycopg2

@dataclass
class Stage5Config:
    """éšæ®µäº”é…ç½®åƒæ•¸"""
    
    # è¼¸å…¥ç›®éŒ„
    input_enhanced_timeseries_dir: str = "/app/data/timeseries_preprocessing_outputs"
    
    # è¼¸å‡ºç›®éŒ„
    output_layered_dir: str = "/app/data/layered_phase0_enhanced"
    output_handover_scenarios_dir: str = "/app/data/handover_scenarios"
    output_signal_analysis_dir: str = "/app/data/signal_quality_analysis"
    output_processing_cache_dir: str = "/app/data/processing_cache"
    output_status_files_dir: str = "/app/data/status_files"
    output_data_integration_dir: str = "/app/data/data_integration_outputs"
    
    # PostgreSQL é…ç½®
    postgres_host: str = "netstack-postgres"
    postgres_port: int = 5432
    postgres_user: str = "netstack_user"
    postgres_password: str = "netstack_password"
    postgres_database: str = "netstack_db"
    
    # åˆ†å±¤ä»°è§’é–€æª»
    elevation_thresholds: List[int] = None
    
    def __post_init__(self):
        if self.elevation_thresholds is None:
            self.elevation_thresholds = [5, 10, 15]

class Stage5IntegrationProcessor:
    """éšæ®µäº”æ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™è™•ç†å™¨"""
    
    def __init__(self, config: Stage5Config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.processing_start_time = time.time()
        
        # ğŸ”§ é‡æ§‹ï¼šä½¿ç”¨çµ±ä¸€è§€æ¸¬é…ç½®æœå‹™ï¼ˆæ¶ˆé™¤ç¡¬ç·¨ç¢¼åº§æ¨™ï¼‰
        try:
            from shared_core.observer_config_service import get_ntpu_coordinates
            self.observer_lat, self.observer_lon, self.observer_alt = get_ntpu_coordinates()
            self.logger.info("âœ… Stage5ä½¿ç”¨çµ±ä¸€è§€æ¸¬é…ç½®æœå‹™")
        except Exception as e:
            self.logger.error(f"è§€æ¸¬é…ç½®è¼‰å…¥å¤±æ•—: {e}")
            raise RuntimeError("ç„¡æ³•è¼‰å…¥è§€æ¸¬é»é…ç½®ï¼Œè«‹æª¢æŸ¥shared_coreé…ç½®")
        
        # åˆå§‹åŒ–çµ±ä¸€ç®¡ç†å™¨ (é‡æ§‹æ”¹é€²)
        from shared_core.elevation_threshold_manager import get_elevation_threshold_manager
        from shared_core.signal_quality_cache import get_signal_quality_cache
        
        self.elevation_manager = get_elevation_threshold_manager()
        self.signal_cache = get_signal_quality_cache()
        
        self.logger.info("âœ… Stage5 æ•¸æ“šæ•´åˆè™•ç†å™¨åˆå§‹åŒ–å®Œæˆ (v3.1é‡æ§‹ç‰ˆ)")
        self.logger.info(f"  ğŸ“ è§€æ¸¬åº§æ¨™: ({self.observer_lat}Â°, {self.observer_lon}Â°) - çµ±ä¸€é…ç½®")
        self.logger.info("  ğŸ”§ çµ±ä¸€ä»°è§’é–€æª»ç®¡ç†å™¨å·²å•Ÿç”¨")
        self.logger.info("  ğŸ”§ ä¿¡è™Ÿå“è³ªç·©å­˜å·²å•Ÿç”¨")
        
    async def process_enhanced_timeseries(self) -> Dict[str, Any]:
        """è™•ç†å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“šä¸¦å¯¦ç¾æ··åˆå­˜å„²æ¶æ§‹"""
        
        self.logger.info("ğŸš€ é–‹å§‹éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™")
        
        results = {
            "stage": "stage5_integration",
            "start_time": datetime.now(timezone.utc).isoformat(),
            "postgresql_integration": {},
            "layered_data_enhancement": {},
            "handover_scenarios": {},
            "signal_quality_analysis": {},
            "processing_cache": {},
            "status_files": {},
            "mixed_storage_verification": {}
        }
        
        try:
            # 1. è¼‰å…¥å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š
            enhanced_data = await self._load_enhanced_timeseries()
            
            # 2. PostgreSQL æ•¸æ“šæ•´åˆ
            results["postgresql_integration"] = await self._integrate_postgresql_data(enhanced_data)
            
            # 3. ç”Ÿæˆåˆ†å±¤æ•¸æ“šå¢å¼·
            results["layered_data_enhancement"] = await self._generate_layered_data(enhanced_data)
            
            # 4. ç”Ÿæˆæ›æ‰‹å ´æ™¯å°ˆç”¨æ•¸æ“š
            results["handover_scenarios"] = await self._generate_handover_scenarios(enhanced_data)
            
            # 5. å‰µå»ºä¿¡è™Ÿå“è³ªåˆ†æç›®éŒ„çµæ§‹ï¼ˆæ•¸æ“šæ•´åˆï¼Œä¸é‡è¤‡è¨ˆç®—ï¼‰
            results["signal_quality_analysis"] = await self._setup_signal_analysis_structure(enhanced_data)
            
            # 6. å‰µå»ºè™•ç†ç·©å­˜
            results["processing_cache"] = await self._create_processing_cache(enhanced_data)
            
            # 7. ç”Ÿæˆç‹€æ…‹æ–‡ä»¶
            results["status_files"] = await self._create_status_files()
            
            # 8. é©—è­‰æ··åˆå­˜å„²è¨ªå•æ¨¡å¼
            results["mixed_storage_verification"] = await self._verify_mixed_storage_access()
            
            # è¨»ï¼šéšæ®µå…­å‹•æ…‹æ± è¦åŠƒå·²ç¨ç«‹åŸ·è¡Œï¼Œä¸åœ¨éšæ®µäº”ä¸­èª¿ç”¨
            
            # æ·»åŠ è¡›æ˜Ÿçµ±è¨ˆä¿¡æ¯ä¾›Stage6ä½¿ç”¨
            total_satellites = 0
            constellation_summary = {}
            
            for constellation, data in enhanced_data.items():
                if data and 'satellites' in data:
                    count = len(data['satellites'])
                    constellation_summary[constellation] = {
                        "satellite_count": count,
                        "processing_status": "integrated"
                    }
                    total_satellites += count
            
            results["total_satellites"] = total_satellites
            results["successfully_integrated"] = total_satellites
            results["constellation_summary"] = constellation_summary
            results["satellites"] = enhanced_data  # ç‚ºStage6æä¾›å®Œæ•´è¡›æ˜Ÿæ•¸æ“š
            results["success"] = True
            results["processing_time_seconds"] = time.time() - self.processing_start_time
            
            self.logger.info(f"âœ… éšæ®µäº”å®Œæˆï¼Œè€—æ™‚: {results['processing_time_seconds']:.2f} ç§’")
            self.logger.info(f"ğŸ“Š æ•´åˆè¡›æ˜Ÿæ•¸æ“š: {total_satellites} é¡†è¡›æ˜Ÿ")
            
        except Exception as e:
            self.logger.error(f"âŒ éšæ®µäº”è™•ç†å¤±æ•—: {e}")
            results["success"] = False
            results["error"] = str(e)
            
        return results
    
    async def _load_enhanced_timeseries(self) -> Dict[str, Any]:
        """è¼‰å…¥å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š"""
        
        enhanced_data = {
            "starlink": None,
            "oneweb": None
        }
        
        input_dir = Path(self.config.input_enhanced_timeseries_dir)
        
        for constellation in ["starlink", "oneweb"]:
            # ä½¿ç”¨å›ºå®šæª”æ¡ˆåï¼Œç¬¦åˆStage4çš„æ¨™æº–åŒ–å‘½å
            target_file = input_dir / f"{constellation}_enhanced.json"
            
            if target_file.exists():
                self.logger.info(f"è¼‰å…¥ {constellation} å¢å¼·æ•¸æ“š: {target_file}")
                
                with open(target_file, 'r') as f:
                    enhanced_data[constellation] = json.load(f)
                    
                satellites_count = len(enhanced_data[constellation].get('satellites', []))
                self.logger.info(f"âœ… {constellation}: {satellites_count} é¡†è¡›æ˜Ÿ")
            else:
                self.logger.warning(f"âš ï¸ {constellation} å¢å¼·æ•¸æ“šæª”æ¡ˆä¸å­˜åœ¨: {target_file}")
        
        return enhanced_data
    
    async def _integrate_postgresql_data(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ•´åˆæ•¸æ“šåˆ°PostgreSQL"""
        
        self.logger.info("ğŸ˜ é–‹å§‹PostgreSQLæ•¸æ“šæ•´åˆ")
        
        integration_results = {
            "satellite_metadata_inserted": 0,
            "orbital_parameters_inserted": 0,
            "handover_scores_inserted": 0,
            "constellation_stats_updated": 0
        }
    
        try:
            # å»ºç«‹è³‡æ–™åº«é€£æ¥
            conn = psycopg2.connect(
                host=self.config.postgres_host,
                port=self.config.postgres_port,
                user=self.config.postgres_user,
                password=self.config.postgres_password,
                database=self.config.postgres_database
            )
            cur = conn.cursor()
            
            for constellation, data in enhanced_data.items():
                if not data:
                    continue
                    
                satellites = data.get('satellites', [])
                metadata = data.get('metadata', {})
                
                for satellite in satellites:
                    satellite_id = satellite.get('satellite_id')
                    
                    if not satellite_id:
                        continue
                    
                    # æ’å…¥è¡›æ˜ŸåŸºç¤è³‡è¨Š
                    cur.execute("""
                        INSERT INTO satellite_metadata 
                        (satellite_id, constellation, active) 
                        VALUES (%s, %s, %s)
                        ON CONFLICT (satellite_id) DO UPDATE SET
                        constellation = EXCLUDED.constellation,
                        active = EXCLUDED.active
                    """, (satellite_id, constellation, True))
                    
                    integration_results["satellite_metadata_inserted"] += 1
                    
                    # æ’å…¥è»Œé“åƒæ•¸ï¼ˆå¾ç¬¬ä¸€å€‹æ™‚é–“é»ä¼°ç®—ï¼‰
                    timeseries_data = satellite.get('position_timeseries', satellite.get('timeseries', []))
                    if timeseries_data:
                        first_point = timeseries_data[0]
                        
                        # å¾range_kmä¼°ç®—é«˜åº¦ï¼ˆæ¸›å»åœ°çƒåŠå¾‘ç´„6371kmï¼‰
                        range_km = first_point.get('range_km', 7000)
                        estimated_altitude = max(range_km - 6371, 400)  # æœ€ä½400km
                        
                        cur.execute("""
                            INSERT INTO orbital_parameters 
                            (satellite_id, altitude_km) 
                            VALUES (%s, %s)
                            ON CONFLICT DO NOTHING
                        """, (satellite_id, estimated_altitude))
                        
                        integration_results["orbital_parameters_inserted"] += 1
                
                # æ›´æ–°æ˜Ÿåº§çµ±è¨ˆ
                cur.execute("""
                    INSERT INTO constellation_statistics 
                    (constellation, total_satellites, active_satellites) 
                    VALUES (%s, %s, %s)
                    ON CONFLICT DO NOTHING
                """, (constellation, len(satellites), len(satellites)))
                
                integration_results["constellation_stats_updated"] += 1
            
            conn.commit()
            cur.close()
            conn.close()
            
            self.logger.info(f"âœ… PostgreSQLæ•´åˆå®Œæˆ: {integration_results}")
            
        except Exception as e:
            self.logger.error(f"âŒ PostgreSQLæ•´åˆå¤±æ•—: {e}")
            integration_results["error"] = str(e)
        
        return integration_results
    
    async def _generate_layered_data(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆåˆ†å±¤æ•¸æ“šå¢å¼· (é‡æ§‹ç‰ˆ)
        
        ä½¿ç”¨çµ±ä¸€ä»°è§’é–€æª»ç®¡ç†å™¨ï¼Œç§»é™¤é‡è¤‡çš„ä»°è§’é‚è¼¯
        """
        
        self.logger.info("ğŸ”„ ç”Ÿæˆåˆ†å±¤ä»°è§’æ•¸æ“š (é‡æ§‹ç‰ˆ)")
        self.logger.info("  ğŸ”§ ä½¿ç”¨çµ±ä¸€ä»°è§’é–€æª»ç®¡ç†å™¨")
        
        # å°å…¥çµ±ä¸€ä»°è§’ç®¡ç†å™¨
        from shared_core.elevation_threshold_manager import get_elevation_threshold_manager
        elevation_manager = get_elevation_threshold_manager()
        
        # ä½¿ç”¨çµ±ä¸€ç®¡ç†å™¨çš„åˆ†å±¤é–€æª»
        layered_thresholds = elevation_manager.get_layered_thresholds()
        self.logger.info(f"  ğŸ“ åˆ†å±¤é–€æª»: {layered_thresholds}")
        
        layered_results = {}
        
        for threshold in layered_thresholds:
            threshold_dir = Path(self.config.output_layered_dir) / f"elevation_{threshold}deg"
            threshold_dir.mkdir(parents=True, exist_ok=True)
            
            layered_results[f"elevation_{threshold}deg"] = {}
            
            for constellation, data in enhanced_data.items():
                if not data:
                    continue
                
                self.logger.info(f"  ğŸ“¡ è™•ç† {constellation} æ˜Ÿåº§ @ {threshold}Â° é–€æª»")
                
                # ä½¿ç”¨çµ±ä¸€ä»°è§’ç®¡ç†å™¨é€²è¡Œæ¿¾æ³¢
                satellites = data.get('satellites', [])
                
                # èª¿ç”¨çµ±ä¸€ç®¡ç†å™¨çš„æ¿¾æ³¢æ–¹æ³•
                filtered_satellites = elevation_manager.filter_satellites_by_elevation(
                    satellites, constellation, threshold
                )
                
                # ç”Ÿæˆåˆ†å±¤æ•¸æ“šæª”æ¡ˆ
                layered_data = {
                    "metadata": {
                        **data.get('metadata', {}),
                        "elevation_threshold_deg": threshold,
                        "filtered_satellites_count": len(filtered_satellites),
                        "processing_method": "unified_elevation_threshold_manager",
                        "constellation_min_threshold": elevation_manager.get_min_elevation(constellation),
                        "constellation_optimal_threshold": elevation_manager.get_optimal_elevation(constellation),
                        "stage5_processing_time": datetime.now(timezone.utc).isoformat(),
                        "refactoring_notes": "Using unified elevation threshold manager, removed duplicate logic"
                    },
                    "satellites": filtered_satellites
                }
                
                output_file = threshold_dir / f"{constellation}_with_3gpp_events.json"
                
                with open(output_file, 'w') as f:
                    json.dump(layered_data, f, indent=2)
                
                file_size_mb = output_file.stat().st_size / (1024 * 1024)
                
                layered_results[f"elevation_{threshold}deg"][constellation] = {
                    "file_path": str(output_file),
                    "satellites_count": len(filtered_satellites),
                    "file_size_mb": round(file_size_mb, 2),
                    "filtering_method": "unified_elevation_threshold_manager",
                    "retention_stats": {
                        sat.get('elevation_filter_info', {}) 
                        for sat in filtered_satellites 
                        if 'elevation_filter_info' in sat
                    } if filtered_satellites else []
                }
                
                self.logger.info(f"âœ… {constellation} {threshold}åº¦: {len(filtered_satellites)} é¡†è¡›æ˜Ÿ, {file_size_mb:.1f}MB")
                
                # è¨˜éŒ„æ¿¾æ³¢çµ±è¨ˆ
                if filtered_satellites and 'elevation_filter_info' in filtered_satellites[0]:
                    total_retention = sum(
                        sat.get('elevation_filter_info', {}).get('retention_rate', 0) 
                        for sat in filtered_satellites
                    ) / len(filtered_satellites)
                    self.logger.info(f"    ğŸ“Š å¹³å‡é»ä¿ç•™ç‡: {total_retention*100:.1f}%")
        
        self.logger.info("âœ… é‡æ§‹ç‰ˆåˆ†å±¤æ•¸æ“šç”Ÿæˆå®Œæˆ")
        self.logger.info("  ğŸ¯ æ”¹é€²: ç§»é™¤é‡è¤‡ä»°è§’é‚è¼¯ï¼Œçµ±ä¸€ä½¿ç”¨ä»°è§’ç®¡ç†å™¨")
        
        return layered_results
    
    async def _generate_handover_scenarios(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ›æ‰‹å ´æ™¯å°ˆç”¨æ•¸æ“š"""
        
        self.logger.info("ğŸ”„ ç”Ÿæˆæ›æ‰‹å ´æ™¯æ•¸æ“š")
        
        scenarios_dir = Path(self.config.output_handover_scenarios_dir)
        scenarios_dir.mkdir(parents=True, exist_ok=True)
        
        scenario_results = {}
        
        # A4äº‹ä»¶æ™‚é–“è»¸ç”Ÿæˆ
        a4_timeline = await self._generate_a4_event_timeline(enhanced_data)
        a4_file = scenarios_dir / "a4_event_timeline.json"
        with open(a4_file, 'w') as f:
            json.dump(a4_timeline, f, indent=2)
        
        scenario_results["a4_events"] = {
            "file_path": str(a4_file),
            "events_count": len(a4_timeline.get('events', [])),
            "file_size_mb": round(a4_file.stat().st_size / (1024 * 1024), 2)
        }
        
        # A5äº‹ä»¶æ™‚é–“è»¸ç”Ÿæˆ
        a5_timeline = await self._generate_a5_event_timeline(enhanced_data)
        a5_file = scenarios_dir / "a5_event_timeline.json"
        with open(a5_file, 'w') as f:
            json.dump(a5_timeline, f, indent=2)
        
        scenario_results["a5_events"] = {
            "file_path": str(a5_file),
            "events_count": len(a5_timeline.get('events', [])),
            "file_size_mb": round(a5_file.stat().st_size / (1024 * 1024), 2)
        }
        
        # D2äº‹ä»¶æ™‚é–“è»¸ç”Ÿæˆ
        d2_timeline = await self._generate_d2_event_timeline(enhanced_data)
        d2_file = scenarios_dir / "d2_event_timeline.json"
        with open(d2_file, 'w') as f:
            json.dump(d2_timeline, f, indent=2)
        
        scenario_results["d2_events"] = {
            "file_path": str(d2_file),
            "events_count": len(d2_timeline.get('events', [])),
            "file_size_mb": round(d2_file.stat().st_size / (1024 * 1024), 2)
        }
        
        # æœ€ä½³æ›æ‰‹æ™‚é–“çª—å£åˆ†æ
        optimal_windows = await self._generate_optimal_handover_windows(enhanced_data)
        windows_file = scenarios_dir / "optimal_handover_windows.json"
        with open(windows_file, 'w') as f:
            json.dump(optimal_windows, f, indent=2)
        
        scenario_results["optimal_windows"] = {
            "file_path": str(windows_file),
            "windows_count": len(optimal_windows.get('windows', [])),
            "file_size_mb": round(windows_file.stat().st_size / (1024 * 1024), 2)
        }
        
        return scenario_results
        
        async def _generate_a4_event_timeline(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆA4äº‹ä»¶æ™‚é–“è»¸"""
        
        a4_threshold = -80.0  # dBm
        a4_hysteresis = 3.0   # dB
        
        events = []
        
        for constellation, data in enhanced_data.items():
            if not data:
                continue
                
            for satellite in data.get('satellites', []):
                satellite_id = satellite.get('satellite_id')
                
                for point in satellite.get('timeseries', []):
                    rsrp = point.get('rsrp_dbm')
                    
                    if rsrp and rsrp > a4_threshold:
                        events.append({
                            "satellite_id": satellite_id,
                            "constellation": constellation,
                            "trigger_time": point.get('time'),
                            "rsrp_dbm": rsrp,
                            "threshold_dbm": a4_threshold,
                            "hysteresis_db": a4_hysteresis,
                            "event_type": "a4_trigger",
                            "elevation_deg": point.get('elevation_deg'),
                            "azimuth_deg": point.get('azimuth_deg')
                        })
        
        return {
            "metadata": {
                "event_type": "A4_neighbor_better_than_threshold",
                "threshold_dbm": a4_threshold,
                "hysteresis_db": a4_hysteresis,
                "total_events": len(events),
                "generation_time": datetime.now(timezone.utc).isoformat()
            },
            "events": events
        }
        
        async def _generate_a5_event_timeline(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆA5äº‹ä»¶æ™‚é–“è»¸"""
        
        serving_threshold = -72.0  # dBm
        neighbor_threshold = -70.0  # dBm
        
        events = []
        
        # ç°¡åŒ–çš„A5äº‹ä»¶æª¢æ¸¬é‚è¼¯
        for constellation, data in enhanced_data.items():
            if not data:
                continue
                
            satellites = data.get('satellites', [])
            
            for i, satellite in enumerate(satellites):
                satellite_id = satellite.get('satellite_id')
                
                for point in satellite.get('timeseries', []):
                    rsrp = point.get('rsrp_dbm')
                    
                    if rsrp and rsrp < serving_threshold:
                        # å°‹æ‰¾å¯èƒ½çš„é„°å±…è¡›æ˜Ÿ
                        neighbor_count = 0
                        for j, neighbor in enumerate(satellites):
                            if i != j:  # ä¸æ˜¯åŒä¸€é¡†è¡›æ˜Ÿ
                                neighbor_rsrp = None
                                # å°‹æ‰¾ç›¸åŒæ™‚é–“é»
                                for neighbor_point in neighbor.get('timeseries', []):
                                    if neighbor_point.get('time') == point.get('time'):
                                        neighbor_rsrp = neighbor_point.get('rsrp_dbm')
                                        break
                                
                                if neighbor_rsrp and neighbor_rsrp > neighbor_threshold:
                                    neighbor_count += 1
                        
                        if neighbor_count > 0:
                            events.append({
                                "serving_satellite_id": satellite_id,
                                "constellation": constellation,
                                "trigger_time": point.get('time'),
                                "serving_rsrp_dbm": rsrp,
                                "serving_threshold_dbm": serving_threshold,
                                "neighbor_threshold_dbm": neighbor_threshold,
                                "qualified_neighbors": neighbor_count,
                                "event_type": "a5_serving_poor_neighbor_good",
                                "elevation_deg": point.get('elevation_deg')
                            })
        
        return {
            "metadata": {
                "event_type": "A5_serving_poor_neighbor_good",
                "serving_threshold_dbm": serving_threshold,
                "neighbor_threshold_dbm": neighbor_threshold,
                "total_events": len(events),
                "generation_time": datetime.now(timezone.utc).isoformat()
            },
            "events": events
        }
        
        async def _generate_d2_event_timeline(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆD2äº‹ä»¶æ™‚é–“è»¸"""
        
        distance_threshold_km = 2000.0
        
        events = []
        
        for constellation, data in enhanced_data.items():
            if not data:
                continue
                
            for satellite in data.get('satellites', []):
                satellite_id = satellite.get('satellite_id')
                
                for point in satellite.get('timeseries', []):
                    distance = point.get('range_km')
                    
                    if distance and distance < distance_threshold_km:
                        events.append({
                            "satellite_id": satellite_id,
                            "constellation": constellation,
                            "trigger_time": point.get('time'),
                            "distance_km": distance,
                            "threshold_km": distance_threshold_km,
                            "event_type": "d2_distance_trigger",
                            "elevation_deg": point.get('elevation_deg'),
                            "ue_latitude": self.observer_lat,  # NTPUä½ç½®ï¼ˆçµ±ä¸€é…ç½®ï¼‰
                            "ue_longitude": self.observer_lon
                        })
        
        return {
            "metadata": {
                "event_type": "D2_distance_based",
                "distance_threshold_km": distance_threshold_km,
                "observer_location": {"lat": self.observer_lat, "lon": self.observer_lon},
                "total_events": len(events),
                "generation_time": datetime.now(timezone.utc).isoformat()
            },
            "events": events
        }
        
        async def _generate_optimal_handover_windows(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ä½³æ›æ‰‹æ™‚é–“çª—å£åˆ†æ"""
        
        windows = []
        
        for constellation, data in enhanced_data.items():
            if not data:
                continue
            
            satellites = data.get('satellites', [])
            
            # ç°¡åŒ–çš„æœ€ä½³çª—å£æª¢æ¸¬
            for satellite in satellites:
            satellite_id = satellite.get('satellite_id')
            timeseries = satellite.get('timeseries', [])
            
            # å°‹æ‰¾ä¿¡è™Ÿå“è³ªè‰¯å¥½çš„æ™‚é–“çª—å£
            good_periods = []
            current_window = None
            
            for point in timeseries:
                rsrp = point.get('rsrp_dbm', -120)
                elevation = point.get('elevation_deg', 0)
                
                if rsrp > -85 and elevation > 10:  # è‰¯å¥½ä¿¡è™Ÿæ¢ä»¶
                    if current_window is None:
                        current_window = {
                            "start_time": point.get('time'),
                            "start_rsrp": rsrp,
                            "start_elevation": elevation
                        }
                    current_window["end_time"] = point.get('time')
                    current_window["end_rsrp"] = rsrp
                    current_window["end_elevation"] = elevation
                else:
                    if current_window:
                        good_periods.append(current_window)
                        current_window = None
            
            if current_window:
                good_periods.append(current_window)
            
            for period in good_periods:
                windows.append({
                    "satellite_id": satellite_id,
                    "constellation": constellation,
                    "window_start": period["start_time"],
                    "window_end": period["end_time"],
                    "window_quality": "optimal",
                    "min_rsrp_dbm": min(period["start_rsrp"], period["end_rsrp"]),
                    "max_elevation_deg": max(period["start_elevation"], period["end_elevation"])
                })
        
        return {
        "metadata": {
            "analysis_type": "optimal_handover_windows",
            "quality_criteria": {
                "min_rsrp_dbm": -85,
                "min_elevation_deg": 10
            },
            "total_windows": len(windows),
            "generation_time": datetime.now(timezone.utc).isoformat()
        },
        "windows": windows
        }
    
    async def _setup_signal_analysis_structure(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """è¨­ç½®ä¿¡è™Ÿå“è³ªåˆ†æç›®éŒ„çµæ§‹ï¼ˆä¸é‡è¤‡éšæ®µä¸‰çš„è¨ˆç®—ï¼‰"""
        
        self.logger.info("ğŸ“ è¨­ç½®ä¿¡è™Ÿå“è³ªåˆ†æç›®éŒ„çµæ§‹")
        
        analysis_dir = Path(self.config.output_signal_analysis_dir)
        analysis_dir.mkdir(parents=True, exist_ok=True)
        
        # å‰µå»ºåŸºæœ¬çµæ§‹æ–‡ä»¶ï¼ˆå¼•ç”¨éšæ®µä¸‰çš„è¨ˆç®—çµæœï¼Œä¸é‡è¤‡è¨ˆç®—ï¼‰
        structure_info = {
        "metadata": {
            "data_type": "signal_analysis_structure_setup",
            "note": "ä¿¡è™Ÿå“è³ªè¨ˆç®—å·²åœ¨éšæ®µä¸‰å®Œæˆï¼Œæ­¤è™•åƒ…è¨­ç½®ç›®éŒ„çµæ§‹",
            "stage3_reference": "signal_quality_analysisåœ¨stage3_signal_event_analysis_output.jsonä¸­",
            "generation_time": datetime.now(timezone.utc).isoformat()
        },
        "directory_structure": {
            "analysis_dir": str(analysis_dir),
            "available_for_future_analysis": True
        }
        }
        
        # ä¿å­˜çµæ§‹ä¿¡æ¯
        structure_file = analysis_dir / "analysis_structure_info.json"
        with open(structure_file, 'w') as f:
        json.dump(structure_info, f, indent=2, ensure_ascii=False)
        
        self.logger.info("âœ… ä¿¡è™Ÿå“è³ªåˆ†æç›®éŒ„çµæ§‹è¨­ç½®å®Œæˆï¼ˆé¿å…èˆ‡éšæ®µä¸‰é‡è¤‡ï¼‰")
        
        return {
        "setup_completed": True,
        "structure_file": str(structure_file),
        "note": "Signal quality analysis completed in Stage 3"
        }
    
    async def _create_processing_cache(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """å‰µå»ºè™•ç†ç·©å­˜å„ªåŒ–"""
        
        self.logger.info("ğŸ’¾ å‰µå»ºè™•ç†ç·©å­˜")
        
        cache_dir = Path(self.config.output_processing_cache_dir)
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        cache_results = {}
        
        # ç·©å­˜åŸºæœ¬çµ±è¨ˆä¿¡æ¯
        cache_stats = {
        "total_satellites": 0,
        "constellations": {}
        }
        
        for constellation, data in enhanced_data.items():
        if not data or not isinstance(data, dict):
            continue
            
        satellites = data.get('satellites', [])
        satellite_count = len(satellites)
        cache_stats["total_satellites"] += satellite_count
        
        cache_stats["constellations"][constellation] = {
            "satellite_count": satellite_count,
            "has_position_data": any('position_timeseries' in sat for sat in satellites),
            "has_signal_data": any('signal_quality' in sat for sat in satellites)
        }
        
        # ä¿å­˜ç·©å­˜çµ±è¨ˆ
        stats_file = cache_dir / "processing_statistics.json"
        with open(stats_file, 'w') as f:
        json.dump(cache_stats, f, indent=2, ensure_ascii=False)
        
        cache_results["statistics"] = {
        "file_path": str(stats_file),
        "total_satellites": cache_stats["total_satellites"],
        "file_size_kb": round(stats_file.stat().st_size / 1024, 2)
        }
        
        self.logger.info(f"âœ… è™•ç†ç·©å­˜å‰µå»ºå®Œæˆï¼š{cache_stats['total_satellites']} é¡†è¡›æ˜Ÿçµ±è¨ˆ")
        
        return cache_results
    
    async def _create_status_files(self) -> Dict[str, Any]:
        """å‰µå»ºç‹€æ…‹è¿½è¹¤ç³»çµ±"""
        
        self.logger.info("ğŸ“‹ å‰µå»ºç‹€æ…‹æ–‡ä»¶")
        
        status_dir = Path(self.config.output_status_files_dir)
        status_dir.mkdir(parents=True, exist_ok=True)
        
        status_results = {}
        
        # å»ºæ§‹æ™‚é–“æˆ³
        build_timestamp = {
        "stage5_completion_time": datetime.now(timezone.utc).isoformat(),
        "data_ready": True,
        "processing_completed": True
        }
        
        timestamp_file = status_dir / "build_timestamp.json"
        with open(timestamp_file, 'w') as f:
        json.dump(build_timestamp, f, indent=2, ensure_ascii=False)
        
        status_results["build_timestamp"] = {
        "file_path": str(timestamp_file),
        "status": "completed"
        }
        
        self.logger.info("âœ… ç‹€æ…‹æ–‡ä»¶å‰µå»ºå®Œæˆ")
        
        return status_results
    
    async def _verify_mixed_storage_access(self) -> Dict[str, Any]:
        """é©—è­‰æ··åˆå­˜å„²è¨ªå•æ¨¡å¼"""
        
        self.logger.info("ğŸ” é©—è­‰æ··åˆå­˜å„²è¨ªå•æ¨¡å¼")
        
        verification_results = {
        "postgresql_access": {
            "available": True,
            "note": "PostgreSQL connection will be verified at runtime"
        },
        "volume_access": {
            "available": True,
            "enhanced_timeseries_exists": Path(self.config.input_enhanced_timeseries_dir).exists(),
            "layered_data_exists": Path(self.config.output_layered_dir).exists()
        },
        "mixed_storage_ready": True
        }
        
        self.logger.info("âœ… æ··åˆå­˜å„²è¨ªå•é©—è­‰å®Œæˆ")
        
        return verification_results

async def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    logging.basicConfig(level=logging.INFO)
    
    config = Stage5Config()
    processor = Stage5IntegrationProcessor(config)
    
    results = await processor.process_enhanced_timeseries()
    
    print("\nğŸ¯ éšæ®µäº”è™•ç†çµæœ:")
    print(json.dumps(results, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    asyncio.run(main())
