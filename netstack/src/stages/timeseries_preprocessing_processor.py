#!/usr/bin/env python3
"""
æ™‚é–“åºåˆ—é è™•ç†å™¨
==============================

è·è²¬ï¼šå°‡ä¿¡è™Ÿåˆ†æçµæœè½‰æ›ç‚ºå¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š
è¼¸å…¥ï¼šsignal_event_analysis_output.json
è¼¸å‡ºï¼štimeseries_preprocessing_outputs/ ç›®éŒ„ä¸­çš„æ ¼å¼åŒ–æ•¸æ“š
è™•ç†å°è±¡ï¼š554é¡†ç¶“éä¿¡è™Ÿåˆ†æçš„è¡›æ˜Ÿ
"""

import os
import sys
import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional

logger = logging.getLogger(__name__)

class TimeseriesPreprocessingProcessor:
    """æ™‚é–“åºåˆ—é è™•ç†å™¨
    
    å°‡ä¿¡è™Ÿåˆ†æçš„è¤‡é›œæ•¸æ“šçµæ§‹è½‰æ›ç‚ºå¾ŒçºŒè™•ç†éœ€è¦çš„ enhanced_timeseries æ ¼å¼
    """
    
    def __init__(self, input_dir: str = "/app/data", output_dir: str = "/app/data"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        # ä½¿ç”¨æ¨™æº–åŒ–ç›®éŒ„çµæ§‹ï¼Œç¬¦åˆåŠŸèƒ½æè¿°æ€§å‘½å
        self.enhanced_dir = self.output_dir / "timeseries_preprocessing_outputs"
        self.enhanced_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("âœ… æ™‚é–“åºåˆ—é è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  è¼¸å…¥ç›®éŒ„: {self.input_dir}")
        logger.info(f"  å¢å¼·æ™‚é–“åºåˆ—è¼¸å‡º: {self.enhanced_dir}")
        
    def load_signal_analysis_output(self, signal_file: Optional[str] = None) -> Dict[str, Any]:
        """è¼‰å…¥ä¿¡è™Ÿåˆ†æè¼¸å‡ºæ•¸æ“š"""
        if signal_file is None:
            # ä½¿ç”¨åŠŸèƒ½æè¿°æ€§æª”æ¡ˆè·¯å¾‘ï¼Œç¬¦åˆå‘½åè¦ç¯„
            signal_file = self.input_dir / "signal_analysis_outputs" / "signal_event_analysis_output.json"
        else:
            signal_file = Path(signal_file)
            
        logger.info(f"ğŸ“¥ è¼‰å…¥ä¿¡è™Ÿåˆ†ææ•¸æ“š: {signal_file}")
        
        if not signal_file.exists():
            raise FileNotFoundError(f"ä¿¡è™Ÿåˆ†æè¼¸å‡ºæª”æ¡ˆä¸å­˜åœ¨: {signal_file}")
            
        try:
            with open(signal_file, 'r', encoding='utf-8') as f:
                signal_data = json.load(f)
                
            # é©—è­‰æ•¸æ“šæ ¼å¼
            if 'constellations' not in signal_data:
                raise ValueError("ä¿¡è™Ÿåˆ†ææ•¸æ“šç¼ºå°‘ constellations æ¬„ä½")
                
            total_satellites = 0
            for constellation_name, constellation_data in signal_data['constellations'].items():
                satellites = constellation_data.get('satellites', [])
                total_satellites += len(satellites)
                logger.info(f"  {constellation_name}: {len(satellites)} é¡†è¡›æ˜Ÿ")
                
            logger.info(f"âœ… ä¿¡è™Ÿåˆ†ææ•¸æ“šè¼‰å…¥å®Œæˆ: ç¸½è¨ˆ {total_satellites} é¡†è¡›æ˜Ÿ")
            return signal_data
            
        except Exception as e:
            logger.error(f"è¼‰å…¥ä¿¡è™Ÿåˆ†ææ•¸æ“šå¤±æ•—: {e}")
            raise
            
    def convert_to_enhanced_timeseries(self, signal_data: Dict[str, Any]) -> Dict[str, Any]:
        """å°‡ä¿¡è™Ÿåˆ†ææ•¸æ“šè½‰æ›ç‚ºå¢å¼·æ™‚é–“åºåˆ—æ ¼å¼"""
        logger.info("ğŸ”„ é–‹å§‹æ™‚é–“åºåˆ—æ•¸æ“šè½‰æ›...")
        
        conversion_results = {
            "starlink": None,
            "oneweb": None,
            "conversion_statistics": {
                "total_processed": 0,
                "successful_conversions": 0,
                "failed_conversions": 0
            }
        }
        
        constellations = signal_data.get('constellations', {})
        
        for const_name, const_data in constellations.items():
            if const_name not in ['starlink', 'oneweb']:
                continue
                
            satellites = const_data.get('satellites', [])
            logger.info(f"ğŸ“Š è™•ç† {const_name}: {len(satellites)} é¡†è¡›æ˜Ÿ")
            
            enhanced_satellites = []
            successful_count = 0
            failed_count = 0
            
            for satellite in satellites:
                try:
                    enhanced_satellite = self._convert_satellite_to_timeseries(satellite, const_name)
                    if enhanced_satellite:
                        enhanced_satellites.append(enhanced_satellite)
                        successful_count += 1
                    else:
                        failed_count += 1
                        
                except Exception as e:
                    sat_id = satellite.get('satellite_id', 'Unknown')
                    logger.warning(f"è¡›æ˜Ÿ {sat_id} è½‰æ›å¤±æ•—: {e}")
                    failed_count += 1
            
            # çµ„è£æ˜Ÿåº§æ•¸æ“š
            enhanced_constellation = {
                "metadata": {
                    "constellation": const_name,
                    "processing_type": "timeseries_preprocessing",
                    "generation_time": datetime.now(timezone.utc).isoformat(),
                    "source_data": "signal_event_analysis",
                    "total_satellites": len(satellites),
                    "successful_conversions": successful_count,
                    "failed_conversions": failed_count,
                    "conversion_rate": f"{successful_count/len(satellites)*100:.1f}%" if satellites else "0%"
                },
                "satellites": enhanced_satellites,
                "constellation_statistics": {
                    "total_satellites": len(enhanced_satellites),
                    "avg_visibility_windows": sum(len(s.get('visibility_windows', [])) for s in enhanced_satellites) / len(enhanced_satellites) if enhanced_satellites else 0,
                    "avg_signal_quality": sum(s.get('signal_quality', {}).get('statistics', {}).get('mean_rsrp_dbm', -150) for s in enhanced_satellites) / len(enhanced_satellites) if enhanced_satellites else 0
                }
            }
            
            conversion_results[const_name] = enhanced_constellation
            conversion_results["conversion_statistics"]["total_processed"] += len(satellites)
            conversion_results["conversion_statistics"]["successful_conversions"] += successful_count
            conversion_results["conversion_statistics"]["failed_conversions"] += failed_count
            
            logger.info(f"âœ… {const_name} è½‰æ›å®Œæˆ: {successful_count}/{len(satellites)} é¡†è¡›æ˜ŸæˆåŠŸ")
        
        total_processed = conversion_results["conversion_statistics"]["total_processed"]
        total_successful = conversion_results["conversion_statistics"]["successful_conversions"]
        
        logger.info(f"ğŸ¯ æ™‚é–“åºåˆ—è½‰æ›å®Œæˆ: {total_successful}/{total_processed} é¡†è¡›æ˜ŸæˆåŠŸè½‰æ›")
        
        return conversion_results
        
    def _convert_satellite_to_timeseries(self, satellite: Dict[str, Any], constellation: str) -> Optional[Dict[str, Any]]:
        """å°‡å–®é¡†è¡›æ˜Ÿè½‰æ›ç‚ºå¢å¼·æ™‚é–“åºåˆ—æ ¼å¼"""
        
        # åŸºæœ¬è¡›æ˜Ÿä¿¡æ¯
        enhanced_satellite = {
            "satellite_id": satellite.get('satellite_id', ''),
            "name": satellite.get('name', ''),
            "constellation": constellation,
            "norad_id": satellite.get('norad_id', 0)
        }
        
        # 1. è™•ç†è»Œé“æ•¸æ“š
        orbit_data = satellite.get('orbit_data', {})
        if orbit_data:
            enhanced_satellite["orbit_parameters"] = {
                "altitude": orbit_data.get('altitude', 0),
                "inclination": orbit_data.get('inclination', 0),
                "semi_major_axis": orbit_data.get('semi_major_axis', 0),
                "eccentricity": orbit_data.get('eccentricity', 0),
                "mean_motion": orbit_data.get('mean_motion', 0)
            }
        
        # 2. è™•ç†ä½ç½®æ™‚é–“åºåˆ—
        # ğŸ¯ é—œéµä¿®å¾©ï¼šStage 3 ç¾åœ¨è¼¸å‡º "position_timeseries" è€Œä¸æ˜¯ "positions"
        # éœ€è¦å…¼å®¹å…©ç¨®å­—æ®µåç¨±
        positions = satellite.get('position_timeseries', satellite.get('positions', []))
        if positions:
            enhanced_satellite["position_timeseries"] = []
            for pos in positions:
                # å…¼å®¹ä¸åŒçš„æ•¸æ“šæ ¼å¼
                enhanced_pos = {
                    "time": pos.get('time', ''),
                    "time_offset_seconds": pos.get('time_offset_seconds', 0),
                    "elevation_deg": pos.get('elevation_deg', -999),
                    "azimuth_deg": pos.get('azimuth_deg', 0),
                    "range_km": pos.get('range_km', 0),
                    "is_visible": pos.get('is_visible', False),
                    "position_eci": pos.get('position_eci', {}),
                    "velocity_eci": pos.get('velocity_eci', {})
                }
                enhanced_satellite["position_timeseries"].append(enhanced_pos)
            logger.debug(f"  æˆåŠŸè™•ç† {len(positions)} å€‹æ™‚é–“é»çš„è»Œé“æ•¸æ“š")
        
        # 3. è™•ç†ç°¡åŒ–æ™‚é–“åºåˆ—ï¼ˆä¾†è‡ªåŸå§‹ timeseriesï¼‰
        timeseries = satellite.get('timeseries', [])
        if timeseries:
            enhanced_satellite["elevation_azimuth_timeseries"] = timeseries
        
        # 4. è™•ç†å¯è¦‹æ€§çª—å£
        visibility_windows = orbit_data.get('visibility_windows', [])
        if visibility_windows:
            enhanced_satellite["visibility_windows"] = visibility_windows
        
        # 5. è™•ç†ä¿¡è™Ÿå“è³ªæ•¸æ“š
        signal_quality = satellite.get('signal_quality', {})
        if signal_quality:
            enhanced_satellite["signal_quality"] = signal_quality
        
        # 6. è™•ç†äº‹ä»¶åˆ†æçµæœ
        if 'event_potential' in satellite:
            enhanced_satellite["event_analysis"] = {
                "event_potential": satellite.get('event_potential', {}),
                "supported_events": ["A4_intra_frequency", "A5_intra_frequency", "D2_beam_switch"]
            }
        
        # 7. è™•ç†ç¶œåˆè©•åˆ†
        if 'composite_score' in satellite:
            enhanced_satellite["performance_scores"] = {
                "composite_score": satellite.get('composite_score', 0),
                "geographic_score": satellite.get('geographic_relevance_score', 0),
                "handover_score": satellite.get('handover_suitability_score', 0)
            }
        
        # 8. æ·»åŠ æ™‚é–“åºåˆ—é è™•ç†æ¨™è¨˜
        enhanced_satellite["processing_metadata"] = {
            "processed_by_timeseries_preprocessing": True,
            "processing_time": datetime.now(timezone.utc).isoformat(),
            "data_source": "signal_event_analysis",
            "enhanced_features": [
                "position_timeseries",
                "elevation_azimuth_timeseries", 
                "visibility_windows",
                "signal_quality",
                "event_analysis",
                "performance_scores"
            ]
        }
        
        return enhanced_satellite
        
    def save_enhanced_timeseries(self, conversion_results: Dict[str, Any]) -> Dict[str, str]:
        """ä¿å­˜å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“šåˆ°æ–‡ä»¶"""
        logger.info("ğŸ’¾ ä¿å­˜å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š...")
        
        output_files = {}
        
        # ä½¿ç”¨å›ºå®šæª”æ¡ˆåï¼Œç¬¦åˆæ¨™æº–åŒ–å‘½åè¦ç¯„
        FIXED_FILENAMES = {
            "starlink": "starlink_enhanced.json",
            "oneweb": "oneweb_enhanced.json"
        }
        
        for const_name in ['starlink', 'oneweb']:
            if conversion_results[const_name] is None:
                continue
                
            # ä½¿ç”¨å›ºå®šæª”æ¡ˆåè€Œéå‹•æ…‹å‘½å
            filename = FIXED_FILENAMES[const_name]
            output_file = self.enhanced_dir / filename
            
            # å°‡çµ±è¨ˆä¿¡æ¯æ·»åŠ åˆ°æª”æ¡ˆå…§å®¹ä¸­
            constellation_data = conversion_results[const_name].copy()
            satellite_count = len(constellation_data['satellites'])
            
            # åœ¨ metadata ä¸­è¨˜éŒ„çµ±è¨ˆä¿¡æ¯
            constellation_data["metadata"]["satellite_count"] = satellite_count
            constellation_data["metadata"]["filename_standard"] = "fixed_naming"
            constellation_data["metadata"]["previous_dynamic_name"] = f"{const_name}_enhanced_{satellite_count}sats.json"
            
            # ä¿å­˜æ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(constellation_data, f, indent=2, ensure_ascii=False)
            
            file_size = output_file.stat().st_size
            output_files[const_name] = str(output_file)
            
            logger.info(f"âœ… {const_name} æ•¸æ“šå·²ä¿å­˜: {output_file}")
            logger.info(f"   æ–‡ä»¶å¤§å°: {file_size / (1024*1024):.1f} MB")
            logger.info(f"   è¡›æ˜Ÿæ•¸é‡: {satellite_count} é¡† (è¨˜éŒ„åœ¨æª”æ¡ˆå…§)")
        
        # ä¿å­˜è½‰æ›çµ±è¨ˆ
        stats_file = self.enhanced_dir / "conversion_statistics.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(conversion_results["conversion_statistics"], f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š è½‰æ›çµ±è¨ˆå·²ä¿å­˜: {stats_file}")
        
        return output_files
        
    def process_timeseries_preprocessing(self, signal_file: Optional[str] = None, save_output: bool = True) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„æ™‚é–“åºåˆ—é è™•ç†æµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹æ™‚é–“åºåˆ—é è™•ç†")
        
        # 1. è¼‰å…¥ä¿¡è™Ÿåˆ†ææ•¸æ“š
        signal_data = self.load_signal_analysis_output(signal_file)
        
        # 2. è½‰æ›ç‚ºå¢å¼·æ™‚é–“åºåˆ—æ ¼å¼
        conversion_results = self.convert_to_enhanced_timeseries(signal_data)
        
        # 3. ä¿å­˜å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š
        output_files = {}
        if save_output:
            output_files = self.save_enhanced_timeseries(conversion_results)
            logger.info(f"ğŸ“ æ™‚é–“åºåˆ—é è™•ç†æ•¸æ“šå·²ä¿å­˜åˆ°: {self.enhanced_dir}")
        else:
            logger.info("ğŸš€ æ™‚é–“åºåˆ—é è™•ç†ä½¿ç”¨å…§å­˜å‚³éæ¨¡å¼ï¼Œæœªä¿å­˜æª”æ¡ˆ")
        
        # 4. çµ„è£è¿”å›çµæœ
        results = {
            "success": True,
            "processing_type": "timeseries_preprocessing",
            "processing_timestamp": datetime.now(timezone.utc).isoformat(),
            "input_source": "signal_event_analysis_output.json",
            "output_directory": str(self.enhanced_dir),
            "output_files": output_files,
            "conversion_statistics": conversion_results["conversion_statistics"],
            "constellation_data": {
                "starlink": {
                    "satellites_processed": len(conversion_results["starlink"]["satellites"]) if conversion_results["starlink"] else 0,
                    "output_file": output_files.get("starlink", None)
                },
                "oneweb": {
                    "satellites_processed": len(conversion_results["oneweb"]["satellites"]) if conversion_results["oneweb"] else 0,
                    "output_file": output_files.get("oneweb", None)
                }
            }
        }
        
        total_processed = results["conversion_statistics"]["total_processed"]
        total_successful = results["conversion_statistics"]["successful_conversions"]
        
        logger.info("âœ… æ™‚é–“åºåˆ—é è™•ç†å®Œæˆ")
        logger.info(f"  è™•ç†çš„è¡›æ˜Ÿæ•¸: {total_processed}")
        logger.info(f"  æˆåŠŸè½‰æ›: {total_successful}")
        logger.info(f"  è½‰æ›ç‡: {total_successful/total_processed*100:.1f}%" if total_processed > 0 else "  è½‰æ›ç‡: 0%")
        
        if output_files:
            logger.info(f"  è¼¸å‡ºæ–‡ä»¶:")
            for const, file_path in output_files.items():
                logger.info(f"    {const}: {file_path}")
        
        return results

def main():
    """ä¸»å‡½æ•¸"""
    logging.basicConfig(level=logging.INFO, 
                       format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    
    logger.info("============================================================")
    logger.info("æ™‚é–“åºåˆ—é è™•ç†")
    logger.info("============================================================")
    
    try:
        processor = TimeseriesPreprocessingProcessor()
        result = processor.process_timeseries_preprocessing()
        
        logger.info("ğŸ‰ æ™‚é–“åºåˆ—é è™•ç†æˆåŠŸå®Œæˆï¼")
        return True
        
    except Exception as e:
        logger.error(f"âŒ æ™‚é–“åºåˆ—é è™•ç†å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)