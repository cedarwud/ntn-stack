#!/usr/bin/env python3
"""
ğŸš€ LEOæ ¸å¿ƒç³»çµ±ä¸»æµç¨‹æ§åˆ¶å™¨
==========================

çµ±ä¸€å…­éšæ®µè™•ç†æµç¨‹çš„ä¸»è¦æ§åˆ¶å™¨ï¼Œæ•´åˆæ‰€æœ‰éšæ®µä¸¦ç¢ºä¿æ­£ç¢ºçš„åŸ·è¡Œé †åºï¼š
1. TLEè¼‰å…¥èˆ‡SGP4è»Œé“è¨ˆç®—  
2. æ™ºèƒ½è¡›æ˜Ÿç¯©é¸
3. ä¿¡è™Ÿå“è³ªåˆ†æèˆ‡3GPPäº‹ä»¶è™•ç†
4. æ™‚é–“åºåˆ—é è™•ç†
5. æ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™
6. å‹•æ…‹è¡›æ˜Ÿæ± è¦åŠƒ (å¢å¼·ç‰ˆï¼Œæ•´åˆæ¨¡æ“¬é€€ç«å„ªåŒ–)

ç‰¹è‰²ï¼š
- ç´”å…­æ¨¡çµ„æ¶æ§‹ï¼Œä¿æŒç³»çµ±å®Œæ•´æ€§
- æ•´åˆshared_coreæŠ€è¡“æ£§å’Œæ¨¡æ“¬é€€ç«æ¼”ç®—æ³•
- å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œç‹€æ…‹è¿½è¹¤
- æ”¯æ´å–®æ¨¡çµ„åŸ·è¡Œå’Œå®Œæ•´æµç¨‹åŸ·è¡Œ
"""

import asyncio
import json
import logging
import time
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

# å°å…¥æ‰€æœ‰éšæ®µè™•ç†å™¨
sys.path.insert(0, '/home/sat/ntn-stack/netstack/src')

from stages.tle_orbital_calculation_processor import Stage1TLEProcessor as TLEOrbitalCalculationProcessor
from stages.intelligent_satellite_filter_processor import IntelligentSatelliteFilterProcessor  
from stages.signal_quality_analysis_processor import SignalQualityAnalysisProcessor
from stages.timeseries_preprocessing_processor import TimeseriesPreprocessingProcessor
from stages.data_integration_processor import DataIntegrationProcessor, DataIntegrationConfig
from stages.enhanced_dynamic_pool_planner import EnhancedDynamicPoolPlanner, create_enhanced_dynamic_pool_planner

@dataclass
class PipelineConfig:
    """ä¸»æµç¨‹é…ç½®"""
    # åŸºç¤ç›®éŒ„é…ç½®
    base_data_dir: str = "/home/sat/ntn-stack/netstack/data"
    
    # è¼¸å‡ºç›®éŒ„é…ç½® - åŠŸèƒ½æè¿°æ€§å‘½å
    tle_calculation_output_dir: str = "tle_calculation_outputs"
    intelligent_filtering_output_dir: str = "intelligent_filtering_outputs" 
    signal_analysis_output_dir: str = "signal_analysis_outputs"
    timeseries_preprocessing_output_dir: str = "timeseries_preprocessing_outputs"
    data_integration_output_dir: str = "data_integration_outputs"
    dynamic_pool_planning_output_dir: str = "dynamic_pool_planning_outputs"
    
    # åŸ·è¡Œé…ç½®
    enable_processor_validation: bool = True
    enable_intermediate_cleanup: bool = False
    save_all_intermediate_results: bool = True
    
    # å¢å¼·åŠŸèƒ½é…ç½®
    enable_enhanced_dynamic_pool_planner: bool = True
    enable_performance_monitoring: bool = True

class LEOMainPipelineController:
    """LEOæ ¸å¿ƒç³»çµ±ä¸»æµç¨‹æ§åˆ¶å™¨"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.pipeline_start_time = time.time()
        
        # è™•ç†å™¨åŸ·è¡Œç‹€æ…‹è¿½è¹¤
        self.processor_status = {
            'tle_orbital_calculation': {'completed': False, 'output_file': None, 'processing_time': 0.0},
            'intelligent_satellite_filter': {'completed': False, 'output_file': None, 'processing_time': 0.0},
            'signal_quality_analysis': {'completed': False, 'output_file': None, 'processing_time': 0.0},
            'timeseries_preprocessing': {'completed': False, 'output_file': None, 'processing_time': 0.0},
            'data_integration': {'completed': False, 'output_file': None, 'processing_time': 0.0},
            'dynamic_pool_planning': {'completed': False, 'output_file': None, 'processing_time': 0.0}
        }
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self._setup_output_directories()
        
        self.logger.info("ğŸš€ LEOæ ¸å¿ƒç³»çµ±ä¸»æµç¨‹æ§åˆ¶å™¨åˆå§‹åŒ–å®Œæˆ")
        self.logger.info("ğŸ“‹ å…­æ¨¡çµ„æµç¨‹æ¶æ§‹ï¼šTLEè¼‰å…¥â†’æ™ºèƒ½ç¯©é¸â†’ä¿¡è™Ÿåˆ†æâ†’æ™‚é–“åºåˆ—â†’æ•¸æ“šæ•´åˆâ†’å‹•æ…‹æ± è¦åŠƒ")
        
    def _setup_output_directories(self):
        """è¨­ç½®è¼¸å‡ºç›®éŒ„çµæ§‹"""
        base_path = Path(self.config.base_data_dir)
        base_path.mkdir(exist_ok=True)
        
        # åŠŸèƒ½æè¿°æ€§ç›®éŒ„å‘½å
        output_dirs = [
            'tle_calculation_outputs', 'intelligent_filtering_outputs', 'signal_analysis_outputs',
            'timeseries_preprocessing_outputs', 'data_integration_outputs', 'dynamic_pool_planning_outputs'
        ]
        for output_dir in output_dirs:
            (base_path / output_dir).mkdir(exist_ok=True)
    
    async def execute_complete_pipeline(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„å…­æ¨¡çµ„æµç¨‹"""
        try:
            self.logger.info("ğŸŒŸ é–‹å§‹åŸ·è¡Œå®Œæ•´LEOæ ¸å¿ƒç³»çµ±å…­æ¨¡çµ„æµç¨‹")
            self.logger.info("=" * 80)
            
            pipeline_results = {
                'pipeline_start_time': datetime.now(timezone.utc).isoformat(),
                'pipeline_architecture': 'six_processor_enhanced_with_simulated_annealing',
                'technology_stack': [
                    'shared_core_data_models',
                    'auto_cleanup_manager',
                    'incremental_update_manager', 
                    'simulated_annealing_optimizer'
                ],
                'processors': {},
                'pipeline_success': False,
                'total_processing_time': 0.0
            }
            
            # 1. TLEè¼‰å…¥èˆ‡SGP4è»Œé“è¨ˆç®—
            tle_result = await self._execute_tle_orbital_calculation()
            pipeline_results['processors']['tle_orbital_calculation'] = tle_result
            
            if not tle_result['success']:
                raise Exception("TLEè»Œé“è¨ˆç®—åŸ·è¡Œå¤±æ•—")
            
            # 2. æ™ºèƒ½è¡›æ˜Ÿç¯©é¸ (v3.0è¨˜æ†¶é«”å‚³éæ¨¡å¼)
            filter_result = await self._execute_intelligent_satellite_filter_memory(tle_result['satellite_data'])
            pipeline_results['processors']['intelligent_satellite_filter'] = filter_result
            
            if not filter_result['success']:
                raise Exception("æ™ºèƒ½è¡›æ˜Ÿç¯©é¸åŸ·è¡Œå¤±æ•—")
            
            # 3. ä¿¡è™Ÿå“è³ªåˆ†æèˆ‡3GPPäº‹ä»¶è™•ç† (æ··åˆæ¨¡å¼ï¼šè¨˜æ†¶é«”è¼¸å…¥ï¼Œæª”æ¡ˆè¼¸å‡º)
            signal_result = await self._execute_signal_quality_analysis_hybrid(filter_result['satellite_data'])
            pipeline_results['processors']['signal_quality_analysis'] = signal_result
            
            if not signal_result['success']:
                raise Exception("ä¿¡è™Ÿå“è³ªåˆ†æåŸ·è¡Œå¤±æ•—")
            
            # 4. æ™‚é–“åºåˆ—é è™•ç†
            timeseries_result = await self._execute_timeseries_preprocessing(signal_result['output_file'])
            pipeline_results['processors']['timeseries_preprocessing'] = timeseries_result
            
            if not timeseries_result['success']:
                raise Exception("æ™‚é–“åºåˆ—é è™•ç†åŸ·è¡Œå¤±æ•—")
            
            # 5. æ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™
            integration_result = await self._execute_data_integration(timeseries_result['output_file'])
            pipeline_results['processors']['data_integration'] = integration_result
            
            if not integration_result['success']:
                raise Exception("æ•¸æ“šæ•´åˆåŸ·è¡Œå¤±æ•—")
            
            # 6. å‹•æ…‹è¡›æ˜Ÿæ± è¦åŠƒ (å¢å¼·ç‰ˆï¼Œæ•´åˆæ¨¡æ“¬é€€ç«å„ªåŒ–)
            pool_result = await self._execute_dynamic_pool_planning(integration_result['output_file'])
            pipeline_results['processors']['dynamic_pool_planning'] = pool_result
            
            if not pool_result['success']:
                raise Exception("å‹•æ…‹æ± è¦åŠƒåŸ·è¡Œå¤±æ•—")
            
            self.logger.info("ğŸ‰ å…­æ¨¡çµ„å®Œæ•´æµç¨‹åŸ·è¡ŒæˆåŠŸï¼")
            self.logger.info("ğŸ“Š æ¨¡çµ„1-2: è¨˜æ†¶é«”å‚³éæ¨¡å¼ (å¤§æ•¸æ“šé«˜æ•ˆ)")
            self.logger.info("ğŸ“Š æ¨¡çµ„3-6: æª”æ¡ˆå„²å­˜æ¨¡å¼ (ç¯©é¸å¾Œæ•¸æ“š)")
            
            # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
            total_time = time.time() - self.pipeline_start_time
            pipeline_results['total_processing_time'] = round(total_time, 2)
            pipeline_results['pipeline_success'] = True
            pipeline_results['pipeline_completion_time'] = datetime.now(timezone.utc).isoformat()
            
            # ç”Ÿæˆæµç¨‹ç¸½çµ
            pipeline_results['pipeline_summary'] = self._generate_pipeline_summary()
            
            self.logger.info("ğŸ‰ å®Œæ•´LEOæ ¸å¿ƒç³»çµ±å…­æ¨¡çµ„æµç¨‹åŸ·è¡ŒæˆåŠŸï¼")
            self.logger.info(f"â±ï¸ ç¸½è™•ç†æ™‚é–“: {total_time:.2f} ç§’")
            self.logger.info("=" * 80)
            
            return pipeline_results
            
        except Exception as e:
            self.logger.error(f"âŒ ä¸»æµç¨‹åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'pipeline_success': False,
                'error': str(e),
                'total_processing_time': time.time() - self.pipeline_start_time,
                'completed_processors': [k for k, v in self.processor_status.items() if v['completed']]
            }
    
    async def _execute_tle_orbital_calculation(self) -> Dict[str, Any]:
        """åŸ·è¡ŒTLEè¼‰å…¥èˆ‡SGP4è»Œé“è¨ˆç®—"""
        self.logger.info("ğŸ›°ï¸ åŸ·è¡Œ TLEè¼‰å…¥èˆ‡SGP4è»Œé“è¨ˆç®—")
        
        try:
            processor_start_time = time.time()
            
            processor = TLEOrbitalCalculationProcessor()
            tle_data = processor.process_tle_orbital_calculation()  # è¿”å›è¨˜æ†¶é«”æ•¸æ“š
            
            processing_time = time.time() - processor_start_time
            
            # v3.0è¨˜æ†¶é«”å‚³éæ¨¡å¼ï¼šä¸éœ€è¦æª”æ¡ˆè·¯å¾‘
            self.processor_status['tle_orbital_calculation'] = {
                'completed': True,
                'output_file': None,  # è¨˜æ†¶é«”å‚³éæ¨¡å¼
                'processing_time': processing_time
            }
            
            self.logger.info(f"âœ… TLEè»Œé“è¨ˆç®—å®Œæˆ - è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
            
            return {
                'success': True,
                'processor': 'tle_orbital_calculation',
                'satellite_data': tle_data,  # å‚³éè¨˜æ†¶é«”æ•¸æ“š
                'processing_time': processing_time,
                'processor_description': 'TLEè¼‰å…¥èˆ‡SGP4è»Œé“è¨ˆç®—'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ TLEè»Œé“è¨ˆç®—åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'success': False,
                'processor': 'tle_orbital_calculation',
                'error': str(e)
            }
    
    async def _execute_intelligent_satellite_filter_memory(self, tle_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œæ™ºèƒ½è¡›æ˜Ÿç¯©é¸ (v3.0è¨˜æ†¶é«”å‚³éæ¨¡å¼)"""
        self.logger.info("ğŸ¯ åŸ·è¡Œ æ™ºèƒ½è¡›æ˜Ÿç¯©é¸")
        
        try:
            processor_start_time = time.time()
            
            processor = IntelligentSatelliteFilterProcessor()
            # ä½¿ç”¨è¨˜æ†¶é«”å‚³éæ¨¡å¼ï¼Œä¸å„²å­˜æª”æ¡ˆ
            filter_data = processor.process_intelligent_filtering(orbital_data=tle_data, save_output=False)
            
            processing_time = time.time() - processor_start_time
            
            # v3.0è¨˜æ†¶é«”å‚³éæ¨¡å¼ï¼šä¸éœ€è¦æª”æ¡ˆè·¯å¾‘
            
            self.processor_status['intelligent_satellite_filter'] = {
                'completed': True,
                'output_file': None,  # è¨˜æ†¶é«”å‚³éæ¨¡å¼
                'processing_time': processing_time
            }
            
            self.logger.info(f"âœ… æ™ºèƒ½è¡›æ˜Ÿç¯©é¸å®Œæˆ - è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
            
            return {
                'success': True,
                'processor': 'intelligent_satellite_filter', 
                'satellite_data': filter_data,  # å‚³éè¨˜æ†¶é«”æ•¸æ“š
                'processing_time': processing_time,
                'processor_description': 'æ™ºèƒ½è¡›æ˜Ÿç¯©é¸'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ æ™ºèƒ½è¡›æ˜Ÿç¯©é¸åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'success': False,
                'processor': 'intelligent_satellite_filter',
                'error': str(e)
            }
    
    async def _execute_signal_quality_analysis_hybrid(self, filter_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œä¿¡è™Ÿå“è³ªåˆ†æèˆ‡3GPPäº‹ä»¶è™•ç† (æ··åˆæ¨¡å¼ï¼šè¨˜æ†¶é«”è¼¸å…¥ï¼Œæª”æ¡ˆè¼¸å‡º)"""
        self.logger.info("ğŸ“¡ åŸ·è¡Œ ä¿¡è™Ÿå“è³ªåˆ†æèˆ‡3GPPäº‹ä»¶è™•ç† (æ··åˆæ¨¡å¼)")
        
        try:
            processor_start_time = time.time()
            
            processor = SignalQualityAnalysisProcessor()
            # ä½¿ç”¨æ··åˆæ¨¡å¼ï¼šå‚³å…¥è¨˜æ†¶é«”æ•¸æ“šï¼Œä¸¦è¦æ±‚ä¿å­˜è¼¸å‡ºæª”æ¡ˆ
            result = processor.process_signal_quality_analysis(filter_data=filter_data, save_output=True)
            
            processing_time = time.time() - processor_start_time
            
            # ä¿¡è™Ÿåˆ†æçš„è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ - åŠŸèƒ½æè¿°æ€§å‘½å
            signal_actual_output = Path(self.config.base_data_dir) / "signal_event_analysis_output.json"
            output_file = Path(self.config.base_data_dir) / self.config.signal_analysis_output_dir / "signal_event_analysis_output.json"
            
            # å¦‚æœæª”æ¡ˆåœ¨dataæ ¹ç›®éŒ„ï¼Œç§»å‹•åˆ°æ­£ç¢ºçš„è¼¸å‡ºç›®éŒ„
            if signal_actual_output.exists() and not output_file.exists():
                output_file.parent.mkdir(parents=True, exist_ok=True)
                signal_actual_output.rename(output_file)
            
            self.processor_status['signal_quality_analysis'] = {
                'completed': True,
                'output_file': str(output_file),
                'processing_time': processing_time
            }
            
            self.logger.info(f"âœ… ä¿¡è™Ÿå“è³ªåˆ†æ (æ··åˆæ¨¡å¼) å®Œæˆ - è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
            
            return {
                'success': True,
                'processor': 'signal_quality_analysis_hybrid',
                'output_file': str(output_file),
                'processing_time': processing_time,
                'processor_description': 'ä¿¡è™Ÿå“è³ªåˆ†æèˆ‡3GPPäº‹ä»¶è™•ç† (æ··åˆæ¨¡å¼)'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿¡è™Ÿå“è³ªåˆ†æ (æ··åˆæ¨¡å¼) åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'success': False,
                'processor': 'signal_quality_analysis_hybrid',
                'error': str(e)
            }
    
    async def _execute_stage3(self, stage2_output: str) -> Dict[str, Any]:
        """åŸ·è¡ŒStage3: ä¿¡è™Ÿå“è³ªåˆ†æèˆ‡3GPPäº‹ä»¶è™•ç†"""
        self.logger.info("ğŸ“¡ åŸ·è¡Œ Stage3: ä¿¡è™Ÿå“è³ªåˆ†æèˆ‡3GPPäº‹ä»¶è™•ç†")
        
        try:
            stage_start_time = time.time()
            
            processor = Stage3SignalProcessor()
            result = processor.process_stage3(stage2_output)
            
            processing_time = time.time() - stage_start_time
            
            output_file = Path(self.config.base_data_dir) / self.config.signal_analysis_output_dir / "signal_event_analysis_output.json"
            
            self.stage_status['stage3'] = {
                'completed': True,
                'output_file': str(output_file),
                'processing_time': processing_time
            }
            
            self.logger.info(f"âœ… Stage3 å®Œæˆ - è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
            
            return {
                'success': True,
                'stage': 'stage3',
                'output_file': str(output_file),
                'processing_time': processing_time,
                'stage_description': 'ä¿¡è™Ÿå“è³ªåˆ†æèˆ‡3GPPäº‹ä»¶è™•ç†'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Stage3 åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'success': False,
                'stage': 'stage3',
                'error': str(e)
            }
    
    async def _execute_timeseries_preprocessing(self, signal_analysis_output: str) -> Dict[str, Any]:
        """åŸ·è¡ŒStage4: æ™‚é–“åºåˆ—é è™•ç†"""
        self.logger.info("â° åŸ·è¡Œ Stage4: æ™‚é–“åºåˆ—é è™•ç†")
        
        try:
            stage_start_time = time.time()
            
            processor = Stage4TimeseriesProcessor()
            result = processor.process_stage4(signal_analysis_output)
            
            processing_time = time.time() - stage_start_time
            
            output_file = Path(self.config.base_data_dir) / self.config.timeseries_preprocessing_output_dir / "enhanced_timeseries_output.json"
            
            self.stage_status['stage4'] = {
                'completed': True,
                'output_file': str(output_file),
                'processing_time': processing_time
            }
            
            self.logger.info(f"âœ… Stage4 å®Œæˆ - è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
            
            return {
                'success': True,
                'stage': 'stage4',
                'output_file': str(output_file),
                'processing_time': processing_time,
                'stage_description': 'æ™‚é–“åºåˆ—é è™•ç†'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Stage4 åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'success': False,
                'stage': 'stage4',
                'error': str(e)
            }
    
    async def _execute_data_integration(self, timeseries_output: str) -> Dict[str, Any]:
        """åŸ·è¡ŒStage5: æ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™"""
        self.logger.info("ğŸ”— åŸ·è¡Œ Stage5: æ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™")
        
        try:
            stage_start_time = time.time()
            
            # é…ç½®Stage5
            stage5_config = Stage5Config()
            processor = Stage5IntegrationProcessor(stage5_config)
            
            result = await processor.process_enhanced_timeseries()
            
            processing_time = time.time() - stage_start_time
            
            output_file = Path(self.config.base_data_dir) / self.config.data_integration_output_dir / "data_integration_output.json"
            
            # ä¿å­˜Stage5çµæœ
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            self.stage_status['stage5'] = {
                'completed': True,
                'output_file': str(output_file),
                'processing_time': processing_time
            }
            
            self.logger.info(f"âœ… Stage5 å®Œæˆ - è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
            
            return {
                'success': True,
                'stage': 'stage5',
                'output_file': str(output_file),
                'processing_time': processing_time,
                'stage_description': 'æ•¸æ“šæ•´åˆèˆ‡æ¥å£æº–å‚™'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Stage5 åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'success': False,
                'stage': 'stage5',
                'error': str(e)
            }
    
    async def _execute_dynamic_pool_planning(self, integration_output: str) -> Dict[str, Any]:
        """åŸ·è¡ŒStage6: å‹•æ…‹è¡›æ˜Ÿæ± è¦åŠƒ (å¢å¼·ç‰ˆï¼Œæ•´åˆæ¨¡æ“¬é€€ç«å„ªåŒ–)"""
        self.logger.info("ğŸ§  åŸ·è¡Œ Stage6: å‹•æ…‹è¡›æ˜Ÿæ± è¦åŠƒ (å¢å¼·ç‰ˆ)")
        
        try:
            stage_start_time = time.time()
            
            # å‰µå»ºå¢å¼·ç‰ˆStage6è™•ç†å™¨
            processor = create_enhanced_stage6_processor()
            
            output_file = Path(self.config.base_data_dir) / self.config.dynamic_pool_planning_output_dir / "enhanced_dynamic_pools_output.json"
            
            # åŸ·è¡ŒStage6è™•ç†
            result = processor.process(integration_output, str(output_file))
            
            processing_time = time.time() - stage_start_time
            
            self.stage_status['stage6'] = {
                'completed': True,
                'output_file': str(output_file),
                'processing_time': processing_time
            }
            
            self.logger.info(f"âœ… Stage6 (å¢å¼·ç‰ˆ) å®Œæˆ - è™•ç†æ™‚é–“: {processing_time:.2f}ç§’")
            self.logger.info("ğŸ¯ æ¨¡æ“¬é€€ç«å„ªåŒ–æ¼”ç®—æ³•å·²æˆåŠŸæ•´åˆ")
            
            return {
                'success': True,
                'stage': 'stage6_enhanced',
                'output_file': str(output_file),
                'processing_time': processing_time,
                'stage_description': 'å‹•æ…‹è¡›æ˜Ÿæ± è¦åŠƒ (å¢å¼·ç‰ˆ - æ¨¡æ“¬é€€ç«å„ªåŒ–)',
                'solution_details': result.get('solution', {}),
                'technology_enhancements': [
                    'shared_core_data_models',
                    'simulated_annealing_optimizer',
                    'auto_cleanup_manager',
                    'incremental_update_manager'
                ]
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Stage6 åŸ·è¡Œå¤±æ•—: {e}")
            return {
                'success': False,
                'stage': 'stage6_enhanced',
                'error': str(e)
            }
    
    def _generate_pipeline_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµç¨‹ç¸½çµå ±å‘Š"""
        total_processing_time = sum(stage['processing_time'] for stage in self.stage_status.values())
        completed_stages = sum(1 for stage in self.stage_status.values() if stage['completed'])
        
        return {
            'pipeline_architecture': 'six_stage_enhanced_leo_core_system',
            'completed_stages': completed_stages,
            'total_stages': 6,
            'pipeline_success_rate': f"{(completed_stages / 6) * 100:.1f}%",
            'total_processing_time_seconds': round(total_processing_time, 2),
            'stage_breakdown': {
                stage: {
                    'completed': status['completed'],
                    'processing_time': status['processing_time'],
                    'output_file': status['output_file']
                }
                for stage, status in self.stage_status.items()
            },
            'technology_highlights': [
                'å…­éšæ®µå®Œæ•´æµç¨‹æ¶æ§‹',
                'shared_coreçµ±ä¸€æ•¸æ“šæ¨¡å‹',
                'æ¨¡æ“¬é€€ç«å„ªåŒ–æ¼”ç®—æ³•',
                'æ™ºèƒ½æ¸…ç†ç®¡ç†ç³»çµ±',
                'å¢é‡æ›´æ–°ç®¡ç†æ©Ÿåˆ¶'
            ],
            'performance_metrics': {
                'average_stage_time': round(total_processing_time / 6, 2),
                'fastest_stage': min(self.stage_status.items(), 
                                   key=lambda x: x[1]['processing_time'] if x[1]['completed'] else float('inf'))[0],
                'slowest_stage': max(self.stage_status.items(),
                                   key=lambda x: x[1]['processing_time'] if x[1]['completed'] else 0)[0]
            }
        }
    
    async def execute_single_stage(self, stage_name: str, input_file: Optional[str] = None) -> Dict[str, Any]:
        """åŸ·è¡Œå–®ä¸€éšæ®µ (ç”¨æ–¼æ¸¬è©¦å’Œèª¿è©¦)"""
        self.logger.info(f"ğŸ”§ åŸ·è¡Œå–®ä¸€éšæ®µ: {stage_name}")
        
        if stage_name == 'stage1':
            return await self._execute_stage1()
        elif stage_name == 'stage2':
            # å–®éšæ®µæ¨¡å¼ä¸‹ä¸æ”¯æ´Stage2ï¼Œå› ç‚ºå®ƒéœ€è¦Stage1çš„è¨˜æ†¶é«”æ•¸æ“š
            raise ValueError("Stage2 åœ¨å–®éšæ®µæ¨¡å¼ä¸‹ä¸æ”¯æ´ï¼Œéœ€è¦Stage1çš„è¨˜æ†¶é«”æ•¸æ“šã€‚è«‹ä½¿ç”¨å®Œæ•´æµç¨‹æ¨¡å¼ã€‚")
        elif stage_name == 'stage3':
            if not input_file:
                raise ValueError("Stage3 éœ€è¦æä¾› stage2 è¼¸å‡ºæª”æ¡ˆ")
            return await self._execute_stage3(input_file)
        elif stage_name == 'stage4':
            if not input_file:
                raise ValueError("Stage4 éœ€è¦æä¾› stage3 è¼¸å‡ºæª”æ¡ˆ")
            return await self._execute_timeseries_preprocessing(input_file)
        elif stage_name == 'stage5':
            if not input_file:
                raise ValueError("Stage5 éœ€è¦æä¾› stage4 è¼¸å‡ºæª”æ¡ˆ")
            return await self._execute_data_integration(input_file)
        elif stage_name == 'stage6':
            if not input_file:
                raise ValueError("Stage6 éœ€è¦æä¾› stage5 è¼¸å‡ºæª”æ¡ˆ")
            return await self._execute_dynamic_pool_planning(input_file)
        else:
            raise ValueError(f"æœªçŸ¥çš„éšæ®µåç¨±: {stage_name}")
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """ç²å–æµç¨‹ç‹€æ…‹"""
        return {
            'pipeline_running_time': time.time() - self.pipeline_start_time,
            'stage_status': self.stage_status,
            'completed_stages': [k for k, v in self.stage_status.items() if v['completed']],
            'pending_stages': [k for k, v in self.stage_status.items() if not v['completed']]
        }

# ä¾¿åˆ©å‡½æ•¸
def create_leo_main_pipeline(base_data_dir: str = "/home/sat/ntn-stack/netstack/data") -> LEOMainPipelineController:
    """å‰µå»ºLEOä¸»æµç¨‹æ§åˆ¶å™¨å¯¦ä¾‹"""
    config = PipelineConfig(base_data_dir=base_data_dir)
    return LEOMainPipelineController(config)

async def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LEOæ ¸å¿ƒç³»çµ±ä¸»æµç¨‹æ§åˆ¶å™¨")
    parser.add_argument("--mode", choices=['full', 'single'], default='full', 
                       help="åŸ·è¡Œæ¨¡å¼: full=å®Œæ•´æµç¨‹, single=å–®ä¸€éšæ®µ")
    parser.add_argument("--stage", choices=['stage1', 'stage2', 'stage3', 'stage4', 'stage5', 'stage6'],
                       help="å–®ä¸€éšæ®µæ¨¡å¼ä¸‹æŒ‡å®šè¦åŸ·è¡Œçš„éšæ®µ")
    parser.add_argument("--input", help="å–®ä¸€éšæ®µæ¨¡å¼ä¸‹çš„è¼¸å…¥æª”æ¡ˆ")
    parser.add_argument("--data-dir", default="/home/sat/ntn-stack/netstack/data",
                       help="æ•¸æ“šç›®éŒ„è·¯å¾‘")
    
    args = parser.parse_args()
    
    # è¨­ç½®æ—¥èªŒ
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # å‰µå»ºä¸»æµç¨‹æ§åˆ¶å™¨
    pipeline = create_leo_main_pipeline(args.data_dir)
    
    try:
        if args.mode == 'full':
            # åŸ·è¡Œå®Œæ•´æµç¨‹
            result = await pipeline.execute_complete_pipeline()
            print("\nğŸ¯ å®Œæ•´æµç¨‹åŸ·è¡Œçµæœ:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
            
        elif args.mode == 'single':
            if not args.stage:
                print("âŒ å–®ä¸€éšæ®µæ¨¡å¼éœ€è¦æŒ‡å®š --stage åƒæ•¸")
                sys.exit(1)
                
            # åŸ·è¡Œå–®ä¸€éšæ®µ
            result = await pipeline.execute_single_stage(args.stage, args.input)
            print(f"\nğŸ¯ {args.stage} åŸ·è¡Œçµæœ:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
        
        if result.get('success', False) or result.get('pipeline_success', False):
            print("\nâœ… åŸ·è¡ŒæˆåŠŸå®Œæˆï¼")
            sys.exit(0)
        else:
            print("\nâŒ åŸ·è¡Œå¤±æ•—ï¼")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nğŸ’¥ ä¸»æµç¨‹åŸ·è¡Œç•°å¸¸: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())