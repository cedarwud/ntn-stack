# ğŸš€ Phase 2 RLæ“´å±•ç³»çµ± - è©³ç´°å¯¦æ–½è¨ˆåŠƒ

**ç‰ˆæœ¬**: v2.0  
**æ›´æ–°æ—¥æœŸ**: 2025-08-15  
**ç‹€æ…‹**: è¦åŠƒéšæ®µ (ç­‰å¾…Phase 1å®Œæˆ)

---

## ğŸ“‹ åŸ·è¡Œå‰æ

### âœ… Phase 1å®Œæˆç¢ºèªæ¸…å–®
- [ ] **å…¨é‡8736é¡†è¡›æ˜Ÿè»Œé“è¨ˆç®—å®Œæˆ**
- [ ] **å¯è¦‹æ€§åˆè¦é”åˆ°>70%** (æ¸¬è©¦æ¨¡å¼) æˆ– >90% (æ­£å¼æ¨¡å¼)
- [ ] **Phase 1æœ€çµ‚å ±å‘Šç”Ÿæˆ** (`/tmp/phase1_outputs/phase1_final_report.json`)
- [ ] **å€™é¸è¡›æ˜Ÿæ± æœ€ä½³åŒ–å®Œæˆ** (205é¡†Starlink + 63é¡†OneWeb æˆ–æ›´ä½³çµæœ)
- [ ] **A4/A5/D2äº‹ä»¶æª¢æ¸¬æ¡†æ¶é©—è­‰**

### ğŸ“Š Phase 1è¼¸å‡ºä¾è³´
```bash
# å¿…è¦çš„Phase 1è¼¸å‡ºæ–‡ä»¶
/tmp/phase1_outputs/
â”œâ”€â”€ stage1_tle_loading_results.json      # TLEè¼‰å…¥çµ±è¨ˆ
â”œâ”€â”€ stage2_filtering_results.json        # å€™é¸è¡›æ˜Ÿ+è»Œé“ä½ç½®
â”œâ”€â”€ stage3_event_analysis_results.json   # A4/A5/D2äº‹ä»¶
â”œâ”€â”€ stage4_optimization_results.json     # æœ€ä½³è¡›æ˜Ÿæ± 
â””â”€â”€ phase1_final_report.json            # å®Œæ•´å ±å‘Š
```

---

## ğŸ—“ï¸ è©³ç´°æ™‚ç¨‹è¦åŠƒ

### ğŸ“… Week 1: åŸºç¤è¨­æ–½å»ºè¨­ (5å¤©)

#### Day 1: ç’°å¢ƒè¨­ç½®å’Œä¾è³´å®‰è£
```bash
# ä»»å‹™æ¸…å–®
- [ ] å‰µå»ºPhase 2ç›®éŒ„çµæ§‹
- [ ] å®‰è£RLæ¡†æ¶ä¾è³´ (PyTorch, Gymnasium, Stable-Baselines3)
- [ ] è¨­ç½®å¯¦é©—è¿½è¹¤ (Weights & Biases)
- [ ] å»ºç«‹Gitåˆ†æ”¯å’Œç‰ˆæœ¬æ§åˆ¶
```

**æŠ€è¡“è¦æ ¼**:
```bash
# ä¾è³´å®‰è£
pip install torch gymnasium stable-baselines3[extra]
pip install wandb ray[rllib] mlflow
pip install pandas numpy scikit-learn plotly

# ç›®éŒ„å‰µå»º
cd /home/sat/ntn-stack/leo_restructure/phase2_rl_expansion
mkdir -p ml1_data_collector ml2_model_trainer ml3_inference_engine
mkdir -p performance_monitor integration experiments docs
```

#### Day 2: Phase 1æ•¸æ“šé©é…å™¨é–‹ç™¼
```python
# æ ¸å¿ƒæ–‡ä»¶: ml1_data_collector/phase1_data_adapter.py
class Phase1DataAdapter:
    def __init__(self, phase1_output_dir="/tmp/phase1_outputs"):
        self.output_dir = Path(phase1_output_dir)
        self.satellite_pools = None
        self.orbital_positions = None
        self.handover_events = None
    
    def validate_phase1_completion(self) -> bool:
        """é©—è­‰Phase 1æ˜¯å¦å®Œæˆä¸”æ•¸æ“šå®Œæ•´"""
        required_files = [
            "stage4_optimization_results.json",
            "stage2_filtering_results.json", 
            "stage3_event_analysis_results.json",
            "phase1_final_report.json"
        ]
        
        for file in required_files:
            if not (self.output_dir / file).exists():
                raise FileNotFoundError(f"Phase 1è¼¸å‡ºç¼ºå¤±: {file}")
        
        # é©—è­‰å¯è¦‹æ€§åˆè¦
        final_report = self.load_final_report()
        visibility_compliance = final_report['final_results']['optimal_satellite_pools']['visibility_compliance']
        
        if visibility_compliance < 0.7:  # æœ€ä½è¦æ±‚70%
            raise ValueError(f"å¯è¦‹æ€§åˆè¦ä¸è¶³: {visibility_compliance:.1%} < 70%")
        
        return True
    
    def extract_rl_training_data(self) -> Dict:
        """æå–RLè¨“ç·´æ‰€éœ€çš„æ‰€æœ‰æ•¸æ“š"""
        return {
            'satellite_pools': self.load_optimal_pools(),
            'orbital_trajectories': self.extract_orbital_trajectories(),
            'handover_scenarios': self.extract_handover_scenarios(),
            'visibility_constraints': self.extract_constraints(),
            'performance_baselines': self.extract_baselines()
        }
```

#### Day 3: LEOè¡›æ˜ŸRLç’°å¢ƒè¨­è¨ˆ
```python
# æ ¸å¿ƒæ–‡ä»¶: ml2_model_trainer/leo_environment.py
import gymnasium as gym
from gymnasium import spaces
import numpy as np

class LEOSatelliteEnv(gym.Env):
    """åŸºæ–¼Phase 1çœŸå¯¦æ•¸æ“šçš„LEOè¡›æ˜Ÿæ›æ‰‹ç’°å¢ƒ"""
    
    def __init__(self, phase1_data: Dict):
        super().__init__()
        
        # åŸºæ–¼Phase 1æ•¸æ“šåˆå§‹åŒ–
        self.satellite_pools = phase1_data['satellite_pools']
        self.orbital_data = phase1_data['orbital_trajectories']
        self.constraints = phase1_data['visibility_constraints']
        
        # ç‹€æ…‹ç©ºé–“å®šç¾© (åŸºæ–¼Phase 1å¯¦éš›æ•¸æ“šç¶­åº¦)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(self._get_state_dimension(),), 
            dtype=np.float32
        )
        
        # å‹•ä½œç©ºé–“å®šç¾© (åŸºæ–¼å€™é¸è¡›æ˜Ÿæ•¸é‡)
        max_candidates = max(len(self.satellite_pools['starlink']), 
                           len(self.satellite_pools['oneweb']))
        self.action_space = spaces.Discrete(max_candidates + 1)  # +1 for STAY
        
        # åˆå§‹åŒ–ç‹€æ…‹
        self.reset()
    
    def _get_state_dimension(self) -> int:
        """è¨ˆç®—ç‹€æ…‹å‘é‡ç¶­åº¦"""
        return (
            1 +          # ç•¶å‰è¡›æ˜ŸRSRP
            1 +          # ç•¶å‰è¡›æ˜Ÿä»°è§’  
            1 +          # ç•¶å‰è¡›æ˜Ÿè·é›¢
            10 +         # æœ€å¤š10å€‹å€™é¸è¡›æ˜Ÿçš„RSRP
            10 +         # æœ€å¤š10å€‹å€™é¸è¡›æ˜Ÿçš„ä»°è§’
            5 +          # æ›æ‰‹æ­·å²ç‰¹å¾µ
            3 +          # ç¶²è·¯è² è¼‰ç‰¹å¾µ
            2            # æ™‚é–“ç‰¹å¾µ
        )  # ç¸½è¨ˆ: 33ç¶­ç‹€æ…‹ç©ºé–“
    
    def step(self, action):
        """åŸ·è¡Œå‹•ä½œä¸¦è¨ˆç®—çå‹µ"""
        # æ ¹æ“šPhase 1ç´„æŸé©—è­‰å‹•ä½œåˆæ³•æ€§
        if not self._is_action_valid(action):
            return self.state, -1.0, True, True, {"error": "é•åPhase 1ç´„æŸ"}
        
        # åŸ·è¡Œå‹•ä½œ
        next_state = self._execute_action(action)
        reward = self._compute_reward(self.state, action, next_state)
        done = self._is_episode_done()
        
        self.state = next_state
        return self.state, reward, done, False, {}
    
    def _compute_reward(self, state, action, next_state) -> float:
        """åŸºæ–¼Phase 1æ•¸æ“šçš„çå‹µå‡½æ•¸"""
        reward = 0.0
        
        # 1. ä¿¡è™Ÿå“è³ªæ”¹å–„ (åŸºæ–¼Phase 1çš„RSRPè¨ˆç®—)
        rsrp_improvement = next_state[0] - state[0]
        reward += 0.3 * np.tanh(rsrp_improvement / 10.0)  # æ­£è¦åŒ–
        
        # 2. å¯è¦‹æ€§åˆè¦ (åŸºæ–¼Phase 1çš„ä»°è§’ç´„æŸ)
        elevation = next_state[1]
        if elevation >= 5.0:  # Starlinké–¾å€¼
            reward += 0.25
        elif elevation >= 0.0:
            reward += 0.1 * (elevation / 5.0)  # ç·šæ€§çå‹µ
        else:
            reward -= 0.5  # é‡æ‡²ç½°è² ä»°è§’
        
        # 3. æ›æ‰‹æˆæœ¬
        if action > 0:  # éSTAYå‹•ä½œ
            reward -= 0.1
        
        return reward
```

#### Day 4: åŸºç¤çå‹µå‡½æ•¸å¯¦ç¾
```python
# æ ¸å¿ƒæ–‡ä»¶: ml2_model_trainer/reward_function.py
class Phase1AwareRewardFunction:
    """åŸºæ–¼Phase 1ç´„æŸå’Œç›®æ¨™çš„çå‹µå‡½æ•¸"""
    
    def __init__(self, phase1_constraints: Dict):
        self.constraints = phase1_constraints
        self.weights = {
            'signal_quality': 0.3,
            'visibility_compliance': 0.25, 
            'handover_cost': 0.1,
            'qos_satisfaction': 0.2,
            'constraint_violation': 0.15
        }
    
    def compute(self, state, action, next_state, context) -> float:
        """è¨ˆç®—å¤šç›®æ¨™çå‹µ"""
        components = {}
        
        # 1. ä¿¡è™Ÿå“è³ªæ”¹å–„
        components['signal_quality'] = self._signal_quality_reward(state, next_state)
        
        # 2. Phase 1å¯è¦‹æ€§åˆè¦
        components['visibility_compliance'] = self._visibility_reward(next_state, context)
        
        # 3. æ›æ‰‹æˆæœ¬
        components['handover_cost'] = self._handover_cost_penalty(action)
        
        # 4. QoSæ»¿è¶³åº¦
        components['qos_satisfaction'] = self._qos_reward(next_state, context)
        
        # 5. Phase 1ç´„æŸé•åæ‡²ç½°
        components['constraint_violation'] = self._constraint_penalty(next_state, context)
        
        # åŠ æ¬Šç¸½å’Œ
        total_reward = sum(
            self.weights[component] * value 
            for component, value in components.items()
        )
        
        return total_reward, components  # è¿”å›ç¸½çå‹µå’Œçµ„ä»¶åˆ†è§£
```

#### Day 5: æ•¸æ“šé©—è­‰å’Œåˆæ­¥æ¸¬è©¦
```python
# æ ¸å¿ƒæ–‡ä»¶: ml1_data_collector/data_validator.py
class Phase1DataValidator:
    """Phase 1æ•¸æ“šå“è³ªé©—è­‰å™¨"""
    
    def validate_complete_pipeline(self, phase1_data: Dict) -> ValidationReport:
        """å®Œæ•´æ•¸æ“šç®¡é“é©—è­‰"""
        report = ValidationReport()
        
        # 1. æ•¸æ“šå®Œæ•´æ€§æª¢æŸ¥
        report.add_check("æ•¸æ“šå®Œæ•´æ€§", self._check_data_completeness(phase1_data))
        
        # 2. æ•¸æ“šä¸€è‡´æ€§æª¢æŸ¥
        report.add_check("æ•¸æ“šä¸€è‡´æ€§", self._check_data_consistency(phase1_data))
        
        # 3. ç´„æŸæ»¿è¶³æª¢æŸ¥
        report.add_check("ç´„æŸæ»¿è¶³", self._check_constraint_satisfaction(phase1_data))
        
        # 4. è¨“ç·´æ•¸æ“šå……è¶³æ€§æª¢æŸ¥
        report.add_check("è¨“ç·´æ•¸æ“šå……è¶³æ€§", self._check_training_data_sufficiency(phase1_data))
        
        return report
    
    def _check_constraint_satisfaction(self, data) -> bool:
        """æª¢æŸ¥Phase 1ç´„æŸæ˜¯å¦æ»¿è¶³"""
        pools = data['satellite_pools']
        
        # æª¢æŸ¥è¡›æ˜Ÿæ± å¤§å°
        starlink_count = len(pools.get('starlink_satellites', []))
        oneweb_count = len(pools.get('oneweb_satellites', []))
        
        # æª¢æŸ¥å¯è¦‹æ€§åˆè¦
        visibility_compliance = pools.get('visibility_compliance', 0.0)
        
        return (
            starlink_count >= 50 and  # æœ€ä½å€™é¸æ•¸é‡
            oneweb_count >= 20 and
            visibility_compliance >= 0.7  # æœ€ä½åˆè¦è¦æ±‚
        )
```

---

### ğŸ“… Week 2: æ·±åº¦å­¸ç¿’æ¨¡å‹é–‹ç™¼ (5å¤©)

#### Day 6-7: DQNç¶²è·¯æ¶æ§‹å¯¦ç¾
```python
# æ ¸å¿ƒæ–‡ä»¶: ml2_model_trainer/dqn_trainer.py
import torch
import torch.nn as nn
import torch.nn.functional as F

class LEOSatelliteDQN(nn.Module):
    """å°ˆç‚ºLEOè¡›æ˜Ÿæ›æ‰‹è¨­è¨ˆçš„DQNç¶²è·¯"""
    
    def __init__(self, state_dim: int, action_dim: int):
        super().__init__()
        
        # ç‹€æ…‹ç·¨ç¢¼å™¨ (è™•ç†33ç¶­ç‹€æ…‹å‘é‡)
        self.state_encoder = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.LayerNorm(128),
            nn.Dropout(0.1),
            
            nn.Linear(128, 64),
            nn.ReLU(), 
            nn.LayerNorm(64)
        )
        
        # è¡›æ˜Ÿç‰¹å¾µç·¨ç¢¼å™¨ (è™•ç†å€™é¸è¡›æ˜Ÿç‰¹å¾µ)
        self.satellite_encoder = nn.Sequential(
            nn.Linear(20, 32),  # 10å€‹å€™é¸çš„RSRP+ä»°è§’
            nn.ReLU(),
            nn.Linear(32, 16)
        )
        
        # æ±ºç­–é ­ (Qå€¼é æ¸¬)
        self.q_head = nn.Sequential(
            nn.Linear(64 + 16, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim)
        )
    
    def forward(self, state):
        # åˆ†é›¢ä¸åŒé¡å‹çš„ç‰¹å¾µ
        general_features = state[:, :13]  # å‰13ç¶­ç‚ºä¸€èˆ¬ç‰¹å¾µ
        satellite_features = state[:, 13:33]  # å¾Œ20ç¶­ç‚ºè¡›æ˜Ÿç‰¹å¾µ
        
        # ç·¨ç¢¼ç‰¹å¾µ
        general_encoded = self.state_encoder(general_features)
        satellite_encoded = self.satellite_encoder(satellite_features)
        
        # æ‹¼æ¥ç‰¹å¾µ
        combined_features = torch.cat([general_encoded, satellite_encoded], dim=1)
        
        # é æ¸¬Qå€¼
        q_values = self.q_head(combined_features)
        
        return q_values

class DQNTrainer:
    """DQNè¨“ç·´å™¨"""
    
    def __init__(self, env, config):
        self.env = env
        self.config = config
        
        # ç¶²è·¯åˆå§‹åŒ–
        state_dim = env.observation_space.shape[0]
        action_dim = env.action_space.n
        
        self.q_network = LEOSatelliteDQN(state_dim, action_dim)
        self.target_network = LEOSatelliteDQN(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=config.learning_rate)
        
        # ç¶“é©—å›æ”¾
        self.replay_buffer = ReplayBuffer(config.buffer_size)
        
        # è¨“ç·´çµ±è¨ˆ
        self.episode_rewards = []
        self.training_losses = []
    
    def train(self, num_episodes: int):
        """åŸ·è¡ŒDQNè¨“ç·´"""
        for episode in range(num_episodes):
            episode_reward = self._train_episode()
            self.episode_rewards.append(episode_reward)
            
            # å®šæœŸæ›´æ–°ç›®æ¨™ç¶²è·¯
            if episode % self.config.target_update_freq == 0:
                self._update_target_network()
            
            # è¨˜éŒ„å’Œå¯è¦–åŒ–
            if episode % 100 == 0:
                self._log_training_progress(episode)
    
    def _train_episode(self) -> float:
        """è¨“ç·´å–®å€‹episode"""
        state, _ = self.env.reset()
        episode_reward = 0.0
        done = False
        
        while not done:
            # é¸æ“‡å‹•ä½œ (Îµ-greedy)
            action = self._select_action(state)
            
            # åŸ·è¡Œå‹•ä½œ
            next_state, reward, done, truncated, info = self.env.step(action)
            done = done or truncated
            
            # å­˜å„²ç¶“é©—
            self.replay_buffer.push(state, action, reward, next_state, done)
            
            # è¨“ç·´ç¶²è·¯
            if len(self.replay_buffer) > self.config.batch_size:
                loss = self._train_step()
                self.training_losses.append(loss)
            
            state = next_state
            episode_reward += reward
        
        return episode_reward
```

#### Day 8-9: ç¶“é©—å›æ”¾å’Œè¨“ç·´å„ªåŒ–
```python
# æ ¸å¿ƒæ–‡ä»¶: ml2_model_trainer/replay_buffer.py
import random
from collections import deque, namedtuple
import numpy as np

Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])

class PrioritizedReplayBuffer:
    """å„ªå…ˆç¶“é©—å›æ”¾ç·©è¡å€"""
    
    def __init__(self, capacity: int, alpha: float = 0.6):
        self.capacity = capacity
        self.alpha = alpha  # å„ªå…ˆç´šæŒ‡æ•¸
        self.beta = 0.4     # é‡è¦æ€§æ¡æ¨£æŒ‡æ•¸
        self.beta_increment = 0.001
        
        self.buffer = []
        self.priorities = deque(maxlen=capacity)
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ æ–°ç¶“é©—"""
        experience = Experience(state, action, reward, next_state, done)
        
        # æ–°ç¶“é©—ç²å¾—æœ€å¤§å„ªå…ˆç´š
        max_priority = max(self.priorities) if self.priorities else 1.0
        
        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
            self.priorities.append(max_priority)
        else:
            self.buffer[self.position] = experience
            self.priorities[self.position] = max_priority
        
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size: int):
        """åŸºæ–¼å„ªå…ˆç´šæ¡æ¨£"""
        if len(self.buffer) < batch_size:
            return None
        
        # è¨ˆç®—æ¡æ¨£æ¦‚ç‡
        priorities = np.array(self.priorities)
        probabilities = priorities ** self.alpha
        probabilities /= probabilities.sum()
        
        # æ¡æ¨£ç´¢å¼•
        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)
        
        # è¨ˆç®—é‡è¦æ€§æ¬Šé‡
        weights = (len(self.buffer) * probabilities[indices]) ** (-self.beta)
        weights /= weights.max()
        
        # æå–æ‰¹æ¬¡æ•¸æ“š
        batch = [self.buffer[idx] for idx in indices]
        
        # æ›´æ–°Î²
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        return batch, indices, weights
    
    def update_priorities(self, indices, priorities):
        """æ›´æ–°å„ªå…ˆç´š"""
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority + 1e-6  # é¿å…é›¶å„ªå…ˆç´š
```

#### Day 10: è¨“ç·´ç›£æ§å’Œå¯¦é©—è¿½è¹¤
```python
# æ ¸å¿ƒæ–‡ä»¶: ml2_model_trainer/training_monitor.py
import wandb
import matplotlib.pyplot as plt
from typing import Dict, List

class TrainingMonitor:
    """è¨“ç·´éç¨‹ç›£æ§å’Œå¯è¦–åŒ–"""
    
    def __init__(self, project_name: str, config: Dict):
        # åˆå§‹åŒ–Weights & Biases
        wandb.init(project=project_name, config=config)
        
        self.metrics_history = {
            'episode_rewards': [],
            'training_losses': [],
            'q_values': [],
            'exploration_rate': [],
            'phase1_compliance_rate': []
        }
    
    def log_episode(self, episode: int, metrics: Dict):
        """è¨˜éŒ„å–®å€‹episodeçš„æŒ‡æ¨™"""
        # æ›´æ–°æ­·å²è¨˜éŒ„
        for key, value in metrics.items():
            if key in self.metrics_history:
                self.metrics_history[key].append(value)
        
        # è¨˜éŒ„åˆ°wandb
        wandb.log({
            "episode": episode,
            **metrics
        })
        
        # ç”Ÿæˆå¯è¦–åŒ–åœ–è¡¨
        if episode % 500 == 0:
            self._generate_training_plots(episode)
    
    def log_phase1_compliance(self, episode: int, compliance_metrics: Dict):
        """è¨˜éŒ„Phase 1ç´„æŸåˆè¦æ€§"""
        wandb.log({
            "episode": episode,
            "phase1/visibility_compliance": compliance_metrics.get('visibility_compliance', 0),
            "phase1/constraint_violations": compliance_metrics.get('violations', 0),
            "phase1/reward_from_constraints": compliance_metrics.get('constraint_reward', 0)
        })
    
    def _generate_training_plots(self, episode: int):
        """ç”Ÿæˆè¨“ç·´éç¨‹å¯è¦–åŒ–åœ–è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. Episodeçå‹µè¶¨å‹¢
        axes[0, 0].plot(self.metrics_history['episode_rewards'])
        axes[0, 0].set_title('Episode Rewards')
        axes[0, 0].set_xlabel('Episode')
        axes[0, 0].set_ylabel('Reward')
        
        # 2. è¨“ç·´æå¤±
        if self.metrics_history['training_losses']:
            axes[0, 1].plot(self.metrics_history['training_losses'])
            axes[0, 1].set_title('Training Loss')
            axes[0, 1].set_xlabel('Training Step')
            axes[0, 1].set_ylabel('Loss')
        
        # 3. Qå€¼çµ±è¨ˆ
        if self.metrics_history['q_values']:
            axes[1, 0].plot(self.metrics_history['q_values'])
            axes[1, 0].set_title('Average Q-Values')
            axes[1, 0].set_xlabel('Episode')
            axes[1, 0].set_ylabel('Q-Value')
        
        # 4. Phase 1åˆè¦ç‡
        if self.metrics_history['phase1_compliance_rate']:
            axes[1, 1].plot(self.metrics_history['phase1_compliance_rate'])
            axes[1, 1].set_title('Phase 1 Compliance Rate')
            axes[1, 1].set_xlabel('Episode')
            axes[1, 1].set_ylabel('Compliance Rate')
        
        plt.tight_layout()
        
        # ä¿å­˜ä¸¦ä¸Šå‚³åˆ°wandb
        plt.savefig(f'/tmp/training_plots_episode_{episode}.png')
        wandb.log({"training_plots": wandb.Image(f'/tmp/training_plots_episode_{episode}.png')})
        plt.close()
```

---

### ğŸ“… Week 3-4: é«˜ç´šæ¼”ç®—æ³•å’Œå¤šç›®æ¨™å„ªåŒ– (10å¤©)

#### Day 11-15: PPOå’ŒSACæ¼”ç®—æ³•å¯¦ç¾
```python
# æ ¸å¿ƒæ–‡ä»¶: ml2_model_trainer/ppo_trainer.py
import torch
import torch.nn as nn
from torch.distributions import Categorical

class PPOAgent:
    """Proximal Policy Optimizationæ™ºèƒ½é«”"""
    
    def __init__(self, state_dim, action_dim, config):
        self.config = config
        
        # Actor-Criticç¶²è·¯
        self.actor_critic = ActorCriticNetwork(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.actor_critic.parameters(), lr=config.learning_rate)
        
        # PPOåƒæ•¸
        self.epsilon = config.ppo_epsilon
        self.value_loss_coef = config.value_loss_coef
        self.entropy_coef = config.entropy_coef
    
    def select_action(self, state):
        """é¸æ“‡å‹•ä½œä¸¦è¿”å›å‹•ä½œã€logæ¦‚ç‡ã€ç‹€æ…‹å€¼"""
        with torch.no_grad():
            logits, value = self.actor_critic(state)
            dist = Categorical(logits=logits)
            action = dist.sample()
            log_prob = dist.log_prob(action)
        
        return action.item(), log_prob.item(), value.item()
    
    def update(self, trajectories):
        """ä½¿ç”¨æ”¶é›†çš„è»Œè·¡æ›´æ–°ç­–ç•¥"""
        states, actions, rewards, log_probs, values, dones = trajectories
        
        # è¨ˆç®—å„ªå‹¢å’Œå›å ±
        advantages, returns = self._compute_gae(rewards, values, dones)
        
        # å¤šæ¬¡æ›´æ–°ç­–ç•¥
        for _ in range(self.config.ppo_epochs):
            # å‰å‘å‚³æ’­
            logits, current_values = self.actor_critic(states)
            dist = Categorical(logits=logits)
            current_log_probs = dist.log_prob(actions)
            entropy = dist.entropy().mean()
            
            # è¨ˆç®—PPOæå¤±
            ratio = torch.exp(current_log_probs - log_probs)
            surr1 = ratio * advantages
            surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages
            
            actor_loss = -torch.min(surr1, surr2).mean()
            critic_loss = nn.MSELoss()(current_values.squeeze(), returns)
            
            total_loss = (
                actor_loss + 
                self.value_loss_coef * critic_loss - 
                self.entropy_coef * entropy
            )
            
            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)
            self.optimizer.step()
        
        return {
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'entropy': entropy.item()
        }
```

#### Day 16-20: å¤šç›®æ¨™å„ªåŒ–å’Œè¶…åƒæ•¸èª¿å„ª
```python
# æ ¸å¿ƒæ–‡ä»¶: ml2_model_trainer/multi_objective_optimizer.py
import optuna
from typing import Dict, List, Callable

class MultiObjectiveRLOptimizer:
    """å¤šç›®æ¨™RLå„ªåŒ–å™¨"""
    
    def __init__(self, objectives: List[str], constraints: Dict):
        self.objectives = objectives  # ['reward', 'phase1_compliance', 'stability']
        self.constraints = constraints
        self.study = optuna.create_study(
            directions=['maximize'] * len(objectives),
            study_name='leo_satellite_rl_optimization'
        )
    
    def objective_function(self, trial):
        """å¤šç›®æ¨™å„ªåŒ–ç›®æ¨™å‡½æ•¸"""
        # æ¡æ¨£è¶…åƒæ•¸
        config = self._sample_hyperparameters(trial)
        
        # è¨“ç·´æ¨¡å‹
        results = self._train_with_config(config)
        
        # è¨ˆç®—å¤šå€‹ç›®æ¨™
        objectives = []
        
        # ç›®æ¨™1: å¹³å‡çå‹µ
        objectives.append(results['avg_reward'])
        
        # ç›®æ¨™2: Phase 1ç´„æŸåˆè¦ç‡
        objectives.append(results['phase1_compliance_rate'])
        
        # ç›®æ¨™3: è¨“ç·´ç©©å®šæ€§ (è² çš„æ¨™æº–å·®)
        objectives.append(-results['reward_std'])
        
        return objectives
    
    def _sample_hyperparameters(self, trial) -> Dict:
        """æ¡æ¨£è¶…åƒæ•¸ç©ºé–“"""
        return {
            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),
            'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128, 256]),
            'hidden_size': trial.suggest_categorical('hidden_size', [64, 128, 256, 512]),
            'epsilon_decay': trial.suggest_float('epsilon_decay', 0.9, 0.999),
            'reward_weights': {
                'signal_quality': trial.suggest_float('w_signal', 0.1, 0.5),
                'visibility_compliance': trial.suggest_float('w_visibility', 0.2, 0.4),
                'handover_cost': trial.suggest_float('w_cost', 0.05, 0.15),
                'qos_satisfaction': trial.suggest_float('w_qos', 0.1, 0.3)
            }
        }
    
    def optimize(self, n_trials: int = 100):
        """åŸ·è¡Œå¤šç›®æ¨™å„ªåŒ–"""
        self.study.optimize(self.objective_function, n_trials=n_trials)
        
        # åˆ†æParetoå‰æ²¿
        pareto_trials = self._extract_pareto_front()
        
        return pareto_trials
    
    def _extract_pareto_front(self) -> List[Dict]:
        """æå–Paretoæœ€å„ªè§£"""
        # ç²å–æ‰€æœ‰è©¦é©—çµæœ
        trials = self.study.trials
        
        # æå–ç›®æ¨™å€¼
        objectives_matrix = []
        for trial in trials:
            if trial.state == optuna.trial.TrialState.COMPLETE:
                objectives_matrix.append(trial.values)
        
        objectives_matrix = np.array(objectives_matrix)
        
        # è¨ˆç®—Paretoå‰æ²¿
        pareto_indices = self._compute_pareto_front(objectives_matrix)
        
        pareto_trials = [trials[i] for i in pareto_indices]
        
        return pareto_trials
```

---

### ğŸ“… Week 5-6: æ•´åˆæ¸¬è©¦å’Œæ€§èƒ½è©•ä¼° (10å¤©)

#### Day 21-25: A/Bæ¸¬è©¦æ¡†æ¶é–‹ç™¼
```python
# æ ¸å¿ƒæ–‡ä»¶: performance_monitor/ab_testing.py
class ABTestFramework:
    """A/Bæ¸¬è©¦æ¡†æ¶ - RL vs å‚³çµ±æ¼”ç®—æ³•"""
    
    def __init__(self, phase1_data: Dict):
        self.phase1_data = phase1_data
        self.baseline_algorithm = TraditionalHandoverAlgorithm(phase1_data)
        self.rl_algorithm = None  # å°‡è¼‰å…¥è¨“ç·´å¥½çš„RLæ¨¡å‹
        
        self.test_scenarios = self._generate_test_scenarios()
        self.results = {'rl': [], 'baseline': []}
    
    def run_ab_test(self, rl_model_path: str, num_episodes: int = 1000):
        """åŸ·è¡ŒA/Bæ¸¬è©¦"""
        # è¼‰å…¥RLæ¨¡å‹
        self.rl_algorithm = self._load_rl_model(rl_model_path)
        
        print(f"ğŸ§ª é–‹å§‹A/Bæ¸¬è©¦ - {num_episodes} episodes")
        
        for episode in range(num_episodes):
            scenario = random.choice(self.test_scenarios)
            
            # æ¸¬è©¦åŸºæº–æ¼”ç®—æ³•
            baseline_result = self._test_algorithm(self.baseline_algorithm, scenario)
            self.results['baseline'].append(baseline_result)
            
            # æ¸¬è©¦RLæ¼”ç®—æ³•
            rl_result = self._test_algorithm(self.rl_algorithm, scenario)
            self.results['rl'].append(rl_result)
            
            if episode % 100 == 0:
                self._log_intermediate_results(episode)
        
        # ç”Ÿæˆæœ€çµ‚å ±å‘Š
        return self._generate_ab_report()
    
    def _generate_ab_report(self) -> Dict:
        """ç”ŸæˆA/Bæ¸¬è©¦å ±å‘Š"""
        report = {
            'summary': {},
            'detailed_metrics': {},
            'statistical_significance': {},
            'phase1_compliance': {}
        }
        
        # è¨ˆç®—çµ±è¨ˆæŒ‡æ¨™
        for algorithm in ['rl', 'baseline']:
            results = self.results[algorithm]
            
            report['summary'][algorithm] = {
                'avg_reward': np.mean([r['total_reward'] for r in results]),
                'avg_handover_delay': np.mean([r['handover_delay'] for r in results]),
                'success_rate': np.mean([r['success'] for r in results]),
                'phase1_compliance_rate': np.mean([r['phase1_compliant'] for r in results])
            }
        
        # çµ±è¨ˆé¡¯è‘—æ€§æ¸¬è©¦
        rl_rewards = [r['total_reward'] for r in self.results['rl']]
        baseline_rewards = [r['total_reward'] for r in self.results['baseline']]
        
        from scipy import stats
        t_stat, p_value = stats.ttest_ind(rl_rewards, baseline_rewards)
        
        report['statistical_significance'] = {
            't_statistic': t_stat,
            'p_value': p_value,
            'significant': p_value < 0.05
        }
        
        return report
```

#### Day 26-30: æ€§èƒ½ç›£æ§å„€è¡¨æ¿é–‹ç™¼
```python
# æ ¸å¿ƒæ–‡ä»¶: performance_monitor/dashboard_generator.py
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import streamlit as st

class RLPerformanceDashboard:
    """RLç³»çµ±æ€§èƒ½ç›£æ§å„€è¡¨æ¿"""
    
    def __init__(self):
        self.data_loader = None
        self.metrics_cache = {}
    
    def create_dashboard(self):
        """å‰µå»ºStreamlitå„€è¡¨æ¿"""
        st.set_page_config(
            page_title="LEOè¡›æ˜ŸRLç³»çµ±ç›£æ§", 
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        st.title("ğŸ›°ï¸ LEOè¡›æ˜ŸRLæ›æ‰‹æ±ºç­–ç³»çµ±ç›£æ§")
        st.markdown("åŸºæ–¼Phase 1çœŸå¯¦è¡›æ˜Ÿæ± çš„RLæ€§èƒ½ç›£æ§")
        
        # å´é‚Šæ¬„æ§åˆ¶
        self._create_sidebar()
        
        # ä¸»è¦å…§å®¹å€åŸŸ
        col1, col2 = st.columns(2)
        
        with col1:
            self._create_realtime_metrics()
            self._create_phase1_compliance_chart()
        
        with col2:
            self._create_ab_test_comparison()
            self._create_handover_performance_chart()
        
        # åº•éƒ¨è©³ç´°åˆ†æ
        st.markdown("## è©³ç´°æ€§èƒ½åˆ†æ")
        self._create_detailed_analysis()
    
    def _create_realtime_metrics(self):
        """å‰µå»ºå¯¦æ™‚æŒ‡æ¨™å¡ç‰‡"""
        st.markdown("### ğŸ”¥ å¯¦æ™‚æ€§èƒ½æŒ‡æ¨™")
        
        # è¼‰å…¥æœ€æ–°æŒ‡æ¨™
        metrics = self._load_latest_metrics()
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="RLå¹³å‡çå‹µ",
                value=f"{metrics.get('rl_avg_reward', 0):.2f}",
                delta=f"{metrics.get('rl_reward_delta', 0):.2f}"
            )
        
        with col2:
            st.metric(
                label="æ›æ‰‹æˆåŠŸç‡",
                value=f"{metrics.get('handover_success_rate', 0):.1%}",
                delta=f"{metrics.get('success_rate_delta', 0):.1%}"
            )
        
        with col3:
            st.metric(
                label="Phase 1åˆè¦ç‡", 
                value=f"{metrics.get('phase1_compliance', 0):.1%}",
                delta=f"{metrics.get('compliance_delta', 0):.1%}"
            )
        
        with col4:
            st.metric(
                label="å¹³å‡å»¶é²",
                value=f"{metrics.get('avg_latency', 0):.0f}ms",
                delta=f"{metrics.get('latency_delta', 0):.0f}ms"
            )
    
    def _create_ab_test_comparison(self):
        """å‰µå»ºA/Bæ¸¬è©¦æ¯”è¼ƒåœ–è¡¨"""
        st.markdown("### ğŸ“Š A/Bæ¸¬è©¦æ¯”è¼ƒ (RL vs å‚³çµ±)")
        
        # è¼‰å…¥A/Bæ¸¬è©¦æ•¸æ“š
        ab_data = self._load_ab_test_data()
        
        # å‰µå»ºæ¯”è¼ƒåœ–è¡¨
        fig = go.Figure()
        
        algorithms = ['RLæ¼”ç®—æ³•', 'å‚³çµ±æ¼”ç®—æ³•']
        metrics = ['å¹³å‡çå‹µ', 'æˆåŠŸç‡', 'å»¶é²(ms)', 'Phase1åˆè¦ç‡']
        
        rl_values = [
            ab_data.get('rl_avg_reward', 0),
            ab_data.get('rl_success_rate', 0) * 100,
            ab_data.get('rl_avg_latency', 0),
            ab_data.get('rl_phase1_compliance', 0) * 100
        ]
        
        baseline_values = [
            ab_data.get('baseline_avg_reward', 0),
            ab_data.get('baseline_success_rate', 0) * 100,
            ab_data.get('baseline_avg_latency', 0),
            ab_data.get('baseline_phase1_compliance', 0) * 100
        ]
        
        fig.add_trace(go.Bar(
            name='RLæ¼”ç®—æ³•',
            x=metrics,
            y=rl_values,
            marker_color='lightblue'
        ))
        
        fig.add_trace(go.Bar(
            name='å‚³çµ±æ¼”ç®—æ³•',
            x=metrics,
            y=baseline_values,
            marker_color='lightcoral'
        ))
        
        fig.update_layout(
            title='æ€§èƒ½æŒ‡æ¨™æ¯”è¼ƒ',
            xaxis_title='æŒ‡æ¨™',
            yaxis_title='æ•¸å€¼',
            barmode='group'
        )
        
        st.plotly_chart(fig, use_container_width=True)
```

---

## ğŸ¯ æˆåŠŸæ¨™æº–å’Œé©—æ”¶æ¢ä»¶

### ğŸ“Š é‡åŒ–æŒ‡æ¨™è¦æ±‚

| éšæ®µ | æŒ‡æ¨™ | åŸºæº–å€¼ | RLç›®æ¨™ | é©—æ”¶æ¨™æº– |
|------|------|--------|--------|----------|
| **Week 1-2** | ç’°å¢ƒæ­å»º | N/A | 100% | æ‰€æœ‰ç’°å¢ƒçµ„ä»¶æ­£å¸¸é‹è¡Œ |
| **Week 3-4** | DQNæ”¶æ–‚ | éš¨æ©Ÿç­–ç•¥ | >åŸºæº–30% | è¨“ç·´1000 episodeså¾Œæ”¶æ–‚ |
| **Week 5-6** | å¤šæ¼”ç®—æ³•å°æ¯” | DQNåŸºæº– | PPO/SAC >DQN 10% | çµ±è¨ˆé¡¯è‘—æ€§p<0.05 |
| **Week 7-8** | A/Bæ¸¬è©¦ | å‚³çµ±æ¼”ç®—æ³• | >å‚³çµ±20% | å„é …æŒ‡æ¨™å‡æœ‰æå‡ |

### ğŸ† æœ€çµ‚é©—æ”¶æ¨™æº–

1. **æŠ€è¡“æŒ‡æ¨™**
   - RLæ¼”ç®—æ³•å¹³å‡çå‹µ > å‚³çµ±æ¼”ç®—æ³•20%
   - æ›æ‰‹æˆåŠŸç‡ >95%
   - å¹³å‡å»¶é² <500ms
   - Phase 1ç´„æŸåˆè¦ç‡ >99%

2. **ç³»çµ±æ•´åˆ**
   - èˆ‡Phase 1æ•¸æ“šç„¡ç¸«æ•´åˆ
   - APIéŸ¿æ‡‰æ™‚é–“ <100ms
   - ç³»çµ±ç©©å®šé‹è¡Œ24å°æ™‚ç„¡æ•…éšœ

3. **ç ”ç©¶åƒ¹å€¼**
   - å®Œæ•´çš„å¯¦é©—å ±å‘Šå’Œè«–æ–‡è‰ç¨¿
   - å¯é‡ç¾çš„å¯¦é©—çµæœ
   - é–‹æºä»£ç¢¼å’Œæ–‡æª”

---

## ğŸš¨ é¢¨éšªè©•ä¼°å’Œæ‡‰å°ç­–ç•¥

### âš ï¸ é«˜é¢¨éšªé …ç›®

1. **Phase 1æ•¸æ“šä¸è¶³**
   - **é¢¨éšª**: å¯è¦‹æ€§åˆè¦<70%ï¼Œç¼ºä¹è¶³å¤ è¨“ç·´æ•¸æ“š
   - **æ‡‰å°**: é–‹ç™¼æ•¸æ“šå¢å¼·æŠ€è¡“ï¼Œåˆæˆè¨“ç·´å ´æ™¯

2. **RLè¨“ç·´ä¸æ”¶æ–‚**
   - **é¢¨éšª**: è¤‡é›œç’°å¢ƒå°è‡´è¨“ç·´ä¸ç©©å®š
   - **æ‡‰å°**: èª²ç¨‹å­¸ç¿’ï¼Œæ¼¸é€²å¼è¤‡é›œåº¦æå‡

3. **å¯¦æ™‚æ€§èƒ½ä¸é”æ¨™**
   - **é¢¨éšª**: æ¨ç†å»¶é²éé«˜ï¼Œç„¡æ³•æ»¿è¶³å¯¦æ™‚è¦æ±‚
   - **æ‡‰å°**: æ¨¡å‹å£“ç¸®ã€ç¡¬é«”åŠ é€Ÿ

### ğŸ›¡ï¸ ç·©è§£æªæ–½

```python
# æ•¸æ“šå¢å¼·ç­–ç•¥
class DataAugmentation:
    def augment_phase1_data(self, original_data):
        """åŸºæ–¼Phase 1æ•¸æ“šç”Ÿæˆæ›´å¤šè¨“ç·´å ´æ™¯"""
        augmented_scenarios = []
        
        # 1. æ™‚é–“çª—å£æ“´å±•
        augmented_scenarios.extend(self._temporal_augmentation(original_data))
        
        # 2. å™ªè²æ³¨å…¥
        augmented_scenarios.extend(self._noise_injection(original_data))
        
        # 3. æ¥µç«¯å ´æ™¯åˆæˆ
        augmented_scenarios.extend(self._extreme_scenario_synthesis(original_data))
        
        return augmented_scenarios

# èª²ç¨‹å­¸ç¿’ç­–ç•¥  
class CurriculumLearning:
    def __init__(self):
        self.stages = [
            {'name': 'basic', 'max_satellites': 5, 'duration': 1000},
            {'name': 'intermediate', 'max_satellites': 10, 'duration': 2000}, 
            {'name': 'advanced', 'max_satellites': 20, 'duration': 3000},
            {'name': 'full', 'max_satellites': None, 'duration': 5000}
        ]
    
    def get_current_stage(self, episode):
        """æ ¹æ“šepisodeæ•¸æ±ºå®šç•¶å‰è¨“ç·´éšæ®µ"""
        cumulative_duration = 0
        for stage in self.stages:
            cumulative_duration += stage['duration']
            if episode < cumulative_duration:
                return stage
        return self.stages[-1]  # æœ€çµ‚éšæ®µ
```

---

## ğŸ“ˆ é æœŸæˆæœ

### ğŸ“ å­¸è¡“æˆæœ
1. **é ‚ç´šæœƒè­°è«–æ–‡** (IEEE ICC/GLOBECOM, ACM MobiCom)
2. **æœŸåˆŠè«–æ–‡** (IEEE TWC, IEEE JSAC)
3. **é–‹æºé …ç›®** (GitHub, å®Œæ•´æ–‡æª”)

### ğŸ­ å¯¦ç”¨åƒ¹å€¼
1. **å¯éƒ¨ç½²çš„RLæ›æ‰‹ç³»çµ±**
2. **æ€§èƒ½åŸºæº–æ¸¬è©¦å¥—ä»¶**
3. **å¤šæ˜Ÿåº§é©æ‡‰æ¡†æ¶**

### ğŸ’¡ æŠ€è¡“å‰µæ–°
1. **ç´„æŸæ„ŸçŸ¥RLæ¼”ç®—æ³•**
2. **å¤šç›®æ¨™è¡›æ˜Ÿæ›æ‰‹å„ªåŒ–**
3. **Phase 1-2ç„¡ç¸«æ•´åˆæ¶æ§‹**

---

**Phase 2å¯¦æ–½è¨ˆåŠƒå°‡ç¢ºä¿LEOè¡›æ˜ŸRLç³»çµ±çš„æˆåŠŸé–‹ç™¼å’Œéƒ¨ç½²ï¼Œå¯¦ç¾å­¸è¡“åƒ¹å€¼å’Œå¯¦ç”¨åƒ¹å€¼çš„é›™é‡çªç ´ï¼** ğŸš€

---

*è¨ˆåŠƒåˆ¶å®š: 2025-08-15*  
*ä¸‹æ¬¡æ›´æ–°: Phase 1å®Œæˆå¾Œ*