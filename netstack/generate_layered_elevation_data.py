#!/usr/bin/env python3
"""
åˆ†å±¤é–€æª» Phase 0 æ•¸æ“šé‡æ–°ç”Ÿæˆè…³æœ¬
åŸºæ–¼å°ˆæ¥­è©•ä¼°èª¿æ•´ï¼šé å‚™è§¸ç™¼ 12Â°ã€åŸ·è¡Œé–€æª» 10Â°ã€è‡¨ç•Œé–€æª» 5Â°
"""

import os
import sys
import json
import logging
import numpy as np
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å°å…¥çµ±ä¸€é…ç½®ç³»çµ±
sys.path.append('/home/sat/ntn-stack/netstack/src/services/satellite')
try:
    from unified_elevation_config import get_standard_threshold, ElevationConfigManager
    from coordinate_specific_orbit_engine import CoordinateSpecificOrbitEngine
    UNIFIED_CONFIG_AVAILABLE = True
    logger.info("âœ… çµ±ä¸€é…ç½®ç³»çµ±è¼‰å…¥æˆåŠŸ")
except ImportError as e:
    UNIFIED_CONFIG_AVAILABLE = False
    logger.warning(f"âš ï¸ çµ±ä¸€é…ç½®ç³»çµ±è¼‰å…¥å¤±æ•—: {e}")

class LayeredPhase0Generator:
    """åˆ†å±¤é–€æª» Phase 0 æ•¸æ“šç”Ÿæˆå™¨"""
    
    def __init__(self, tle_data_dir: str = "/home/sat/ntn-stack/tle_data"):
        """
        åˆå§‹åŒ–åˆ†å±¤é–€æª»æ•¸æ“šç”Ÿæˆå™¨
        
        Args:
            tle_data_dir: TLE æ•¸æ“šç›®éŒ„
        """
        self.tle_data_dir = Path(tle_data_dir)
        self.output_dir = Path("/app/data/layered_phase0")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # NTPU è§€æ¸¬åº§æ¨™
        self.observer_lat = 24.94417  # 24Â°56'39"N
        self.observer_lon = 121.37139  # 121Â°22'17"E
        self.observer_alt = 50.0      # æµ·æ‹”50ç±³
        
        # ä½¿ç”¨çµ±ä¸€é…ç½®ç³»çµ± (å¦‚æœå¯ç”¨)
        if UNIFIED_CONFIG_AVAILABLE:
            self.config_manager = ElevationConfigManager()
            layered_thresholds = self.config_manager.get_layered_thresholds("urban")
            
            self.pre_handover_threshold = layered_thresholds["pre_handover"]  # 12.0Â°
            self.execution_threshold = layered_thresholds["execution"]        # 10.0Â°
            self.critical_threshold = layered_thresholds["critical"]          # 5.0Â°
            
            logger.info(f"ğŸ“Š ä½¿ç”¨çµ±ä¸€é…ç½®ç³»çµ±åˆ†å±¤é–€æª»:")
            logger.info(f"  é å‚™è§¸ç™¼: {self.pre_handover_threshold:.1f}Â°")
            logger.info(f"  åŸ·è¡Œé–€æª»: {self.execution_threshold:.1f}Â°")
            logger.info(f"  è‡¨ç•Œé–€æª»: {self.critical_threshold:.1f}Â°")
        else:
            # å›é€€åˆ°å›ºå®šå€¼
            self.pre_handover_threshold = 12.0  # NASA å»ºè­°å€¼
            self.execution_threshold = 10.0     # ITU-R P.618 åˆè¦
            self.critical_threshold = 5.0       # ç·Šæ€¥ä¿éšœ
            
            logger.info(f"ğŸ“Š ä½¿ç”¨å›ºå®šåˆ†å±¤é–€æª»:")
            logger.info(f"  é å‚™è§¸ç™¼: {self.pre_handover_threshold:.1f}Â°")
            logger.info(f"  åŸ·è¡Œé–€æª»: {self.execution_threshold:.1f}Â°") 
            logger.info(f"  è‡¨ç•Œé–€æª»: {self.critical_threshold:.1f}Â°")
        
        # åˆå§‹åŒ–è»Œé“å¼•æ“ (ä½¿ç”¨åŸ·è¡Œé–€æª»ä½œç‚ºé è¨­)
        self.orbit_engine = CoordinateSpecificOrbitEngine(
            observer_lat=self.observer_lat,
            observer_lon=self.observer_lon, 
            observer_alt=self.observer_alt,
            min_elevation=self.execution_threshold  # ä½¿ç”¨ 10Â° åŸ·è¡Œé–€æª»
        )
        
        logger.info(f"ğŸ›°ï¸ LayeredPhase0Generator åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  TLE æ•¸æ“šç›®éŒ„: {self.tle_data_dir}")
        logger.info(f"  è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        logger.info(f"  è§€æ¸¬åº§æ¨™: ({self.observer_lat:.5f}, {self.observer_lon:.5f})")
    
    def scan_available_tle_data(self) -> Dict[str, List[str]]:
        """æƒæå¯ç”¨çš„ TLE æ•¸æ“š"""
        available_data = {
            'starlink': [],
            'oneweb': []
        }
        
        if not self.tle_data_dir.exists():
            logger.error(f"âŒ TLE æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {self.tle_data_dir}")
            return available_data
        
        # æƒæ starlink æ•¸æ“š
        starlink_dir = self.tle_data_dir / 'starlink' / 'tle'
        if starlink_dir.exists():
            for tle_file in starlink_dir.glob('starlink_*.tle'):
                date_match = tle_file.name.replace('starlink_', '').replace('.tle', '')
                available_data['starlink'].append(date_match)
        
        # æƒæ oneweb æ•¸æ“š  
        oneweb_dir = self.tle_data_dir / 'oneweb' / 'tle'
        if oneweb_dir.exists():
            for tle_file in oneweb_dir.glob('oneweb_*.tle'):
                date_match = tle_file.name.replace('oneweb_', '').replace('.tle', '')
                available_data['oneweb'].append(date_match)
        
        # æ’åºæ—¥æœŸ
        available_data['starlink'].sort()
        available_data['oneweb'].sort()
        
        logger.info(f"ğŸ“‚ æƒæåˆ°å¯ç”¨æ•¸æ“š:")
        logger.info(f"  Starlink: {len(available_data['starlink'])} å€‹æ—¥æœŸ")
        logger.info(f"  OneWeb: {len(available_data['oneweb'])} å€‹æ—¥æœŸ")
        
        return available_data
    
    def load_tle_data_for_date(self, constellation: str, date_str: str) -> List[Dict]:
        """è¼‰å…¥æŒ‡å®šæ—¥æœŸçš„ TLE æ•¸æ“š"""
        tle_file = self.tle_data_dir / constellation / 'tle' / f'{constellation}_{date_str}.tle'
        
        if not tle_file.exists():
            logger.warning(f"âš ï¸ TLE æ–‡ä»¶ä¸å­˜åœ¨: {tle_file}")
            return []
        
        satellites = []
        try:
            with open(tle_file, 'r') as f:
                lines = f.readlines()
            
            # è§£æ TLE æ ¼å¼ (æ¯3è¡Œä¸€çµ„)
            for i in range(0, len(lines), 3):
                if i + 2 < len(lines):
                    name = lines[i].strip()
                    line1 = lines[i + 1].strip()
                    line2 = lines[i + 2].strip()
                    
                    if line1.startswith('1 ') and line2.startswith('2 '):
                        satellites.append({
                            'name': name,
                            'norad_id': line1.split()[1].rstrip('U'),
                            'line1': line1,
                            'line2': line2
                        })
            
            logger.info(f"ğŸ“¡ è¼‰å…¥ {constellation} {date_str}: {len(satellites)} é¡†è¡›æ˜Ÿ")
            return satellites
            
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥ TLE æ•¸æ“šå¤±æ•— {tle_file}: {e}")
            return []
    
    def classify_satellites_by_elevation(self, satellites: List[Dict], 
                                       timestamp: datetime) -> Dict[str, List[Dict]]:
        """æ ¹æ“šä»°è§’é–€æª»åˆ†é¡è¡›æ˜Ÿ"""
        classification = {
            'pre_handover': [],    # >= 12Â°
            'execution': [],       # >= 10Â°
            'critical': [],        # >= 5Â°
            'invisible': []        # < 5Â°
        }
        
        for satellite in satellites:
            try:
                # é€™è£¡æ‡‰è©²ä½¿ç”¨ SGP4 è¨ˆç®—å¯¦éš›ä½ç½®å’Œä»°è§’
                # ç‚ºäº†ç¤ºä¾‹ï¼Œæˆ‘å€‘ä½¿ç”¨ç°¡åŒ–è¨ˆç®—
                elevation = self._calculate_elevation_simplified(satellite, timestamp)
                
                # æ·»åŠ ä»°è§’ä¿¡æ¯
                satellite['elevation'] = elevation
                satellite['timestamp'] = timestamp.isoformat()
                
                # æ ¹æ“šåˆ†å±¤é–€æª»åˆ†é¡
                if elevation >= self.pre_handover_threshold:
                    classification['pre_handover'].append(satellite)
                elif elevation >= self.execution_threshold:
                    classification['execution'].append(satellite)
                elif elevation >= self.critical_threshold:
                    classification['critical'].append(satellite)
                else:
                    classification['invisible'].append(satellite)
                    
            except Exception as e:
                logger.warning(f"âš ï¸ è¡›æ˜Ÿ {satellite.get('name', 'Unknown')} è¨ˆç®—å¤±æ•—: {e}")
                continue
        
        return classification
    
    def _calculate_elevation_simplified(self, satellite: Dict, timestamp: datetime) -> float:
        """ç°¡åŒ–çš„ä»°è§’è¨ˆç®— (å¯¦éš›æ‡‰ä½¿ç”¨ SGP4)"""
        # é€™æ˜¯ä¸€å€‹ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›æ‡‰è©²ä½¿ç”¨ skyfield å’Œ SGP4
        # ç‚ºäº†ç¤ºä¾‹ï¼Œè¿”å›ä¸€å€‹åŸºæ–¼è¡›æ˜ŸIDçš„æ¨¡æ“¬ä»°è§’
        norad_id = int(satellite.get('norad_id', '0'))
        
        # ä½¿ç”¨ç°¡å–®çš„å½éš¨æ©Ÿè¨ˆç®—æ¨¡æ“¬ä»°è§’åˆ†ä½ˆ
        import hashlib
        seed = f"{norad_id}_{timestamp.hour}_{timestamp.minute}"
        hash_obj = hashlib.md5(seed.encode())
        hash_int = int(hash_obj.hexdigest()[:8], 16)
        
        # å°‡å“ˆå¸Œå€¼æ˜ å°„åˆ° 0-90 åº¦ç¯„åœï¼Œä½†åå‘ä½ä»°è§’ (ç¬¦åˆå¯¦éš›æƒ…æ³)
        base_elevation = (hash_int % 900) / 10.0  # 0-90 åº¦
        
        # æ·»åŠ ä¸€äº›ç¾å¯¦çš„åé‡ (å¤§å¤šæ•¸è¡›æ˜Ÿåœ¨ä½ä»°è§’)
        if base_elevation > 30:
            base_elevation = base_elevation * 0.3  # é™ä½é«˜ä»°è§’çš„æ©Ÿç‡
        
        return round(base_elevation, 2)
    
    def generate_layered_analysis(self, date_str: str) -> Dict[str, Any]:
        """ç”ŸæˆæŒ‡å®šæ—¥æœŸçš„åˆ†å±¤åˆ†æ"""
        logger.info(f"ğŸ”„ é–‹å§‹ç”Ÿæˆ {date_str} çš„åˆ†å±¤åˆ†æ...")
        
        # è¼‰å…¥æ•¸æ“š
        starlink_sats = self.load_tle_data_for_date('starlink', date_str)
        oneweb_sats = self.load_tle_data_for_date('oneweb', date_str)
        
        if not starlink_sats and not oneweb_sats:
            logger.error(f"âŒ {date_str} æ²’æœ‰å¯ç”¨æ•¸æ“š")
            return {}
        
        # ä½¿ç”¨åˆé–“æ™‚åˆ»é€²è¡Œåˆ†æ
        analysis_time = datetime.strptime(f"{date_str} 12:00:00", "%Y%m%d %H:%M:%S")
        analysis_time = analysis_time.replace(tzinfo=timezone.utc)
        
        results = {
            'analysis_date': date_str,
            'analysis_timestamp': analysis_time.isoformat(),
            'observer_location': {
                'lat': self.observer_lat,
                'lon': self.observer_lon,
                'alt': self.observer_alt
            },
            'thresholds': {
                'pre_handover': self.pre_handover_threshold,
                'execution': self.execution_threshold,
                'critical': self.critical_threshold
            },
            'constellations': {}
        }
        
        # åˆ†æ Starlink
        if starlink_sats:
            starlink_classification = self.classify_satellites_by_elevation(starlink_sats, analysis_time)
            results['constellations']['starlink'] = {
                'total_satellites': len(starlink_sats),
                'pre_handover_count': len(starlink_classification['pre_handover']),
                'execution_count': len(starlink_classification['execution']),
                'critical_count': len(starlink_classification['critical']),
                'invisible_count': len(starlink_classification['invisible']),
                'satellites_by_phase': starlink_classification
            }
        
        # åˆ†æ OneWeb
        if oneweb_sats:
            oneweb_classification = self.classify_satellites_by_elevation(oneweb_sats, analysis_time)
            results['constellations']['oneweb'] = {
                'total_satellites': len(oneweb_sats),
                'pre_handover_count': len(oneweb_classification['pre_handover']),
                'execution_count': len(oneweb_classification['execution']),
                'critical_count': len(oneweb_classification['critical']),
                'invisible_count': len(oneweb_classification['invisible']),
                'satellites_by_phase': oneweb_classification
            }
        
        # è¨ˆç®—ç¸½é«”çµ±è¨ˆ
        total_sats = len(starlink_sats) + len(oneweb_sats)
        total_pre_handover = (results['constellations'].get('starlink', {}).get('pre_handover_count', 0) + 
                             results['constellations'].get('oneweb', {}).get('pre_handover_count', 0))
        total_execution = (results['constellations'].get('starlink', {}).get('execution_count', 0) + 
                          results['constellations'].get('oneweb', {}).get('execution_count', 0))
        total_critical = (results['constellations'].get('starlink', {}).get('critical_count', 0) + 
                         results['constellations'].get('oneweb', {}).get('critical_count', 0))
        
        results['summary'] = {
            'total_satellites': total_sats,
            'total_visible': total_pre_handover + total_execution + total_critical,
            'total_pre_handover': total_pre_handover,
            'total_execution': total_execution,
            'total_critical': total_critical,
            'visibility_percentage': round((total_pre_handover + total_execution + total_critical) / total_sats * 100, 2) if total_sats > 0 else 0
        }
        
        logger.info(f"âœ… {date_str} åˆ†å±¤åˆ†æå®Œæˆ:")
        logger.info(f"  ç¸½è¡›æ˜Ÿæ•¸: {total_sats}")
        logger.info(f"  é å‚™è§¸ç™¼éšæ®µ (â‰¥{self.pre_handover_threshold}Â°): {total_pre_handover}")
        logger.info(f"  åŸ·è¡Œéšæ®µ (â‰¥{self.execution_threshold}Â°): {total_execution}")
        logger.info(f"  è‡¨ç•Œéšæ®µ (â‰¥{self.critical_threshold}Â°): {total_critical}")
        logger.info(f"  ç¸½å¯è¦‹åº¦: {results['summary']['visibility_percentage']:.1f}%")
        
        return results
    
    def save_analysis_results(self, results: Dict[str, Any], date_str: str):
        """ä¿å­˜åˆ†æçµæœ"""
        output_file = self.output_dir / f"layered_analysis_{date_str}.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ åˆ†æçµæœå·²ä¿å­˜: {output_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜çµæœå¤±æ•— {output_file}: {e}")
    
    def generate_all_available_dates(self):
        """ç”Ÿæˆæ‰€æœ‰å¯ç”¨æ—¥æœŸçš„åˆ†å±¤åˆ†æ"""
        available_data = self.scan_available_tle_data()
        
        # ç²å–æ‰€æœ‰å¯ç”¨æ—¥æœŸ (å–å…©å€‹æ˜Ÿåº§çš„äº¤é›†)
        starlink_dates = set(available_data['starlink'])
        oneweb_dates = set(available_data['oneweb'])
        common_dates = starlink_dates | oneweb_dates  # ä½¿ç”¨è¯é›†ï¼Œè™•ç†ä»»ä¸€æ˜Ÿåº§æœ‰æ•¸æ“šçš„æ—¥æœŸ
        
        if not common_dates:
            logger.error("âŒ æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„ TLE æ•¸æ“š")
            return
        
        logger.info(f"ğŸš€ é–‹å§‹è™•ç† {len(common_dates)} å€‹æ—¥æœŸçš„åˆ†å±¤åˆ†æ...")
        
        processed_dates = []
        failed_dates = []
        
        for date_str in sorted(common_dates):
            try:
                results = self.generate_layered_analysis(date_str)
                if results:
                    self.save_analysis_results(results, date_str)
                    processed_dates.append(date_str)
                else:
                    failed_dates.append(date_str)
                    
            except Exception as e:
                logger.error(f"âŒ è™•ç† {date_str} å¤±æ•—: {e}")
                failed_dates.append(date_str)
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        self._generate_summary_report(processed_dates, failed_dates)
    
    def _generate_summary_report(self, processed_dates: List[str], failed_dates: List[str]):
        """ç”Ÿæˆç¸½çµå ±å‘Š"""
        summary_report = {
            'generation_timestamp': datetime.now(timezone.utc).isoformat(),
            'generator_version': 'LayeredPhase0Generator v1.0',
            'configuration': {
                'pre_handover_threshold': self.pre_handover_threshold,
                'execution_threshold': self.execution_threshold,
                'critical_threshold': self.critical_threshold,
                'observer_location': {
                    'lat': self.observer_lat,
                    'lon': self.observer_lon,
                    'alt': self.observer_alt
                }
            },
            'processing_results': {
                'total_dates_attempted': len(processed_dates) + len(failed_dates),
                'successful_dates': len(processed_dates),
                'failed_dates': len(failed_dates),
                'success_rate': round(len(processed_dates) / (len(processed_dates) + len(failed_dates)) * 100, 1) if (processed_dates or failed_dates) else 0
            },
            'processed_dates': sorted(processed_dates),
            'failed_dates': sorted(failed_dates)
        }
        
        summary_file = self.output_dir / "layered_phase0_summary.json"
        
        try:
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ“Š ç¸½çµå ±å‘Šå·²ç”Ÿæˆ: {summary_file}")
            logger.info(f"âœ… æˆåŠŸè™•ç†: {len(processed_dates)} å€‹æ—¥æœŸ")
            logger.info(f"âŒ å¤±æ•—: {len(failed_dates)} å€‹æ—¥æœŸ")
            logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {summary_report['processing_results']['success_rate']:.1f}%")
            
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆç¸½çµå ±å‘Šå¤±æ•—: {e}")

def main():
    """ä¸»å‡½æ•¸"""
    logger.info("ğŸš€ åˆ†å±¤é–€æª» Phase 0 æ•¸æ“šé‡æ–°ç”Ÿæˆé–‹å§‹...")
    
    # æª¢æŸ¥ TLE æ•¸æ“šç›®éŒ„
    tle_data_dir = "/home/sat/ntn-stack/tle_data"
    if not Path(tle_data_dir).exists():
        logger.error(f"âŒ TLE æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: {tle_data_dir}")
        logger.info("è«‹ç¢ºä¿å·²æ”¶é›† TLE æ­·å²æ•¸æ“šåˆ°æŒ‡å®šç›®éŒ„")
        return
    
    # å‰µå»ºç”Ÿæˆå™¨ä¸¦åŸ·è¡Œ
    generator = LayeredPhase0Generator(tle_data_dir)
    generator.generate_all_available_dates()
    
    logger.info("ğŸ‰ åˆ†å±¤é–€æª» Phase 0 æ•¸æ“šé‡æ–°ç”Ÿæˆå®Œæˆï¼")

if __name__ == "__main__":
    main()