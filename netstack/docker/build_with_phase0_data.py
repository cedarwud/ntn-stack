#!/usr/bin/env python3
"""
Docker å»ºç½®æ™‚é è™•ç†åŠŸèƒ½ - Phase 0 å¢å¼·ç‰ˆ
æ”¯æ´æ‰‹å‹•æ”¶é›†çš„ TLE æ­·å²æ•¸æ“šé è™•ç†
"""

import os
import sys
import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Phase0DataPreprocessor:
    """Phase 0 æ•¸æ“šé è™•ç†å™¨"""
    
    def __init__(self, tle_data_dir: str = "/app/tle_data", output_dir: str = "/app/data"):
        """
        åˆå§‹åŒ–é è™•ç†å™¨
        
        Args:
            tle_data_dir: TLE æ•¸æ“šæ ¹ç›®éŒ„
            output_dir: è¼¸å‡ºç›®éŒ„
        """
        self.tle_data_dir = Path(tle_data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.supported_constellations = ['starlink', 'oneweb']
        
        logger.info(f"Phase0DataPreprocessor åˆå§‹åŒ–")
        logger.info(f"  TLE æ•¸æ“šç›®éŒ„: {self.tle_data_dir}")
        logger.info(f"  è¼¸å‡ºç›®éŒ„: {self.output_dir}")
    
    def scan_available_data(self) -> Dict[str, Any]:
        """æƒæå¯ç”¨çš„æ•¸æ“š"""
        scan_result = {
            'total_constellations': 0,
            'total_files': 0,
            'total_satellites': 0,
            'constellations': {},
            'overall_date_range': {
                'start': None,
                'end': None
            }
        }
        
        all_dates = []
        
        for constellation in self.supported_constellations:
            tle_dir = self.tle_data_dir / constellation / "tle"
            json_dir = self.tle_data_dir / constellation / "json"
            
            constellation_data = {
                'name': constellation,
                'files': 0,
                'satellites': 0,
                'dates': [],
                'dual_format_count': 0,
                'data_quality': 'unknown'
            }
            
            if tle_dir.exists():
                import glob
                import re
                
                # æƒæ TLE æ–‡ä»¶
                tle_files = glob.glob(str(tle_dir / f"{constellation}_*.tle"))
                
                for tle_file in tle_files:
                    match = re.search(r'(\d{8})\.tle$', tle_file)
                    if match:
                        date_str = match.group(1)
                        file_path = Path(tle_file)
                        
                        if file_path.exists() and file_path.stat().st_size > 0:
                            constellation_data['files'] += 1
                            constellation_data['dates'].append(date_str)
                            all_dates.append(date_str)
                            
                            # è¨ˆç®—è¡›æ˜Ÿæ•¸é‡ï¼ˆä¼°ç®—ï¼šæ¯3è¡Œä¸€é¡†è¡›æ˜Ÿï¼‰
                            try:
                                with open(file_path, 'r') as f:
                                    lines = len([l for l in f if l.strip()])
                                satellite_count = lines // 3
                                constellation_data['satellites'] += satellite_count
                            except:
                                pass
                            
                            # æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰çš„ JSON æ–‡ä»¶
                            json_file = json_dir / f"{constellation}_{date_str}.json"
                            if json_file.exists() and json_file.stat().st_size > 0:
                                constellation_data['dual_format_count'] += 1
                
                # æ’åºæ—¥æœŸ
                constellation_data['dates'].sort()
                
                # è©•ä¼°æ•¸æ“šå“è³ª
                if constellation_data['files'] > 0:
                    dual_format_rate = (constellation_data['dual_format_count'] / 
                                      constellation_data['files']) * 100
                    
                    if dual_format_rate >= 80 and constellation_data['files'] >= 1:
                        constellation_data['data_quality'] = 'excellent'
                    elif dual_format_rate >= 50:
                        constellation_data['data_quality'] = 'good'
                    elif constellation_data['files'] >= 1:
                        constellation_data['data_quality'] = 'fair'
                    else:
                        constellation_data['data_quality'] = 'poor'
                else:
                    constellation_data['data_quality'] = 'missing'
            
            scan_result['constellations'][constellation] = constellation_data
            scan_result['total_constellations'] += 1 if constellation_data['files'] > 0 else 0
            scan_result['total_files'] += constellation_data['files']
            scan_result['total_satellites'] += constellation_data['satellites']
        
        # è¨ˆç®—æ•´é«”æ—¥æœŸç¯„åœ
        if all_dates:
            all_dates.sort()
            scan_result['overall_date_range']['start'] = all_dates[0]
            scan_result['overall_date_range']['end'] = all_dates[-1]
        
        return scan_result
    
    def generate_build_time_config(self, scan_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå»ºç½®æ™‚é…ç½®"""
        config = {
            'generated_at': datetime.now(timezone.utc).isoformat(),
            'build_environment': 'docker',
            'phase0_version': '1.0.0',
            'data_source': 'local_manual_collection',
            'scan_result': scan_result,
            'runtime_settings': {
                'default_constellation': 'starlink',
                'data_validation_level': 'strict',
                'cache_enabled': True,
                'precomputed_data_available': True
            }
        }
        
        # æ ¹æ“šå¯ç”¨æ•¸æ“šèª¿æ•´è¨­ç½®
        if scan_result['total_constellations'] >= 2:
            config['runtime_settings']['multi_constellation_support'] = True
        
        if scan_result['overall_date_range']['start']:
            config['runtime_settings']['historical_data_range'] = scan_result['overall_date_range']
        
        return config
    
    def generate_rl_training_dataset(self, scan_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆ RL è¨“ç·´æ•¸æ“šé›†metadata"""
        rl_dataset = {
            'dataset_type': 'satellite_handover_rl_training',
            'generated_at': datetime.now(timezone.utc).isoformat(),
            'constellations': {},
            'training_parameters': {
                'observation_space_size': 0,
                'action_space_size': 0,
                'episode_length_minutes': 45,
                'reward_function': 'handover_efficiency'
            }
        }
        
        total_episodes = 0
        
        for constellation, data in scan_result['constellations'].items():
            if data['files'] > 0:
                # æ¯å¤©å¯ä»¥ç”Ÿæˆå¤šå€‹è¨“ç·´episode
                episodes_per_day = 24  # æ¯å°æ™‚ä¸€å€‹episode
                constellation_episodes = data['files'] * episodes_per_day
                
                rl_dataset['constellations'][constellation] = {
                    'available_days': data['files'],
                    'satellite_count': data['satellites'],
                    'episodes_count': constellation_episodes,
                    'data_quality': data['data_quality']
                }
                
                total_episodes += constellation_episodes
        
        rl_dataset['training_parameters']['total_episodes'] = total_episodes
        
        # æ ¹æ“šå¯ç”¨æ•¸æ“šèª¿æ•´observation space
        if scan_result['total_satellites'] > 0:
            # å‡è¨­æ¯æ¬¡è§€æ¸¬æœ€å¤šè¿½è¹¤10é¡†è¡›æ˜Ÿ
            max_tracked_satellites = min(10, scan_result['total_satellites'])
            features_per_satellite = 6  # lat, lon, elevation, azimuth, distance, signal_strength
            rl_dataset['training_parameters']['observation_space_size'] = max_tracked_satellites * features_per_satellite
            
            # å‹•ä½œç©ºé–“ï¼šé¸æ“‡ç›®æ¨™è¡›æ˜Ÿ + æ˜¯å¦åˆ‡æ›
            rl_dataset['training_parameters']['action_space_size'] = max_tracked_satellites + 1
        
        return rl_dataset
    
    def export_build_artifacts(self, scan_result: Dict[str, Any]) -> List[str]:
        """å°å‡ºå»ºç½®ç”¢ç‰©"""
        artifacts = []
        
        try:
            # 1. ç”Ÿæˆå»ºç½®é…ç½®
            build_config = self.generate_build_time_config(scan_result)
            config_path = self.output_dir / "phase0_build_config.json"
            
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(build_config, f, indent=2, ensure_ascii=False)
            
            artifacts.append(str(config_path))
            logger.info(f"âœ… å»ºç½®é…ç½®å·²ç”Ÿæˆ: {config_path}")
            
            # 2. ç”Ÿæˆ RL è¨“ç·´æ•¸æ“šé›†metadata
            rl_dataset = self.generate_rl_training_dataset(scan_result)
            rl_path = self.output_dir / "phase0_rl_dataset_metadata.json"
            
            with open(rl_path, 'w', encoding='utf-8') as f:
                json.dump(rl_dataset, f, indent=2, ensure_ascii=False)
            
            artifacts.append(str(rl_path))
            logger.info(f"âœ… RL æ•¸æ“šé›†metadataå·²ç”Ÿæˆ: {rl_path}")
            
            # 3. ç”Ÿæˆæ•¸æ“šæ‘˜è¦å ±å‘Š
            summary_report = {
                'phase0_data_summary': {
                    'total_constellations': scan_result['total_constellations'],
                    'total_files': scan_result['total_files'],
                    'total_satellites': scan_result['total_satellites'],
                    'date_range': scan_result['overall_date_range'],
                    'constellation_details': scan_result['constellations']
                },
                'build_recommendations': []
            }
            
            # æ·»åŠ å»ºè­°
            if scan_result['total_constellations'] < 2:
                summary_report['build_recommendations'].append("Consider collecting data for both Starlink and OneWeb")
            
            if scan_result['total_files'] < 5:
                summary_report['build_recommendations'].append("More historical data days recommended for better RL training")
            
            for const, data in scan_result['constellations'].items():
                if data['data_quality'] in ['poor', 'missing']:
                    summary_report['build_recommendations'].append(f"Improve data quality for {const}")
            
            summary_path = self.output_dir / "phase0_data_summary.json"
            
            with open(summary_path, 'w', encoding='utf-8') as f:
                json.dump(summary_report, f, indent=2, ensure_ascii=False)
            
            artifacts.append(str(summary_path))
            logger.info(f"âœ… æ•¸æ“šæ‘˜è¦å ±å‘Šå·²ç”Ÿæˆ: {summary_path}")
            
            # 4. ç”Ÿæˆç’°å¢ƒè®Šæ•¸æ–‡ä»¶ï¼ˆä¾› Docker ä½¿ç”¨ï¼‰
            env_vars = {
                'PHASE0_DATA_AVAILABLE': 'true',
                'PHASE0_TOTAL_CONSTELLATIONS': str(scan_result['total_constellations']),
                'PHASE0_TOTAL_FILES': str(scan_result['total_files']),
                'PHASE0_TOTAL_SATELLITES': str(scan_result['total_satellites']),
                'PHASE0_DATE_START': scan_result['overall_date_range']['start'] or '',
                'PHASE0_DATE_END': scan_result['overall_date_range']['end'] or '',
                'PHASE0_BUILD_TIMESTAMP': datetime.now(timezone.utc).isoformat()
            }
            
            env_path = self.output_dir / "phase0.env"
            
            with open(env_path, 'w') as f:
                for key, value in env_vars.items():
                    f.write(f"{key}={value}\n")
            
            artifacts.append(str(env_path))
            logger.info(f"âœ… ç’°å¢ƒè®Šæ•¸æ–‡ä»¶å·²ç”Ÿæˆ: {env_path}")
            
        except Exception as e:
            logger.error(f"å°å‡ºå»ºç½®ç”¢ç‰©å¤±æ•—: {e}")
            raise
        
        return artifacts
    
    def run_build_preprocessing(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„å»ºç½®é è™•ç†"""
        logger.info("ğŸš€ é–‹å§‹ Phase 0 å»ºç½®é è™•ç†...")
        
        try:
            # 1. æƒææ•¸æ“š
            logger.info("ğŸ“Š æƒæå¯ç”¨æ•¸æ“š...")
            scan_result = self.scan_available_data()
            
            logger.info("ğŸ“‹ æƒæçµæœ:")
            logger.info(f"  - æ˜Ÿåº§æ•¸: {scan_result['total_constellations']}")
            logger.info(f"  - æ–‡ä»¶æ•¸: {scan_result['total_files']}")
            logger.info(f"  - è¡›æ˜Ÿæ•¸: {scan_result['total_satellites']}")
            logger.info(f"  - æ—¥æœŸç¯„åœ: {scan_result['overall_date_range']['start']} - {scan_result['overall_date_range']['end']}")
            
            # 2. ç”Ÿæˆå»ºç½®ç”¢ç‰©
            logger.info("ğŸ“¦ ç”Ÿæˆå»ºç½®ç”¢ç‰©...")
            artifacts = self.export_build_artifacts(scan_result)
            
            # 3. ç”Ÿæˆè™•ç†çµæœ
            processing_result = {
                'success': True,
                'scan_result': scan_result,
                'artifacts': artifacts,
                'processing_time': datetime.now(timezone.utc).isoformat(),
                'recommendations': []
            }
            
            # æ·»åŠ å»ºè­°
            if scan_result['total_files'] == 0:
                processing_result['recommendations'].append("No TLE data files found - container will use fallback data")
            elif scan_result['total_files'] < 3:
                processing_result['recommendations'].append("Limited historical data - consider collecting more days")
            
            logger.info("âœ… Phase 0 å»ºç½®é è™•ç†å®Œæˆ")
            return processing_result
            
        except Exception as e:
            logger.error(f"å»ºç½®é è™•ç†å¤±æ•—: {e}")
            return {
                'success': False,
                'error': str(e),
                'processing_time': datetime.now(timezone.utc).isoformat()
            }

def main():
    """ä¸»ç¨‹åº"""
    print("ğŸ³ Phase 0 Docker å»ºç½®é è™•ç†ç³»çµ±")
    print("=" * 50)
    
    # å¾ç’°å¢ƒè®Šæ•¸æˆ–å‘½ä»¤è¡Œåƒæ•¸ç²å–é…ç½®
    tle_data_dir = os.environ.get('TLE_DATA_DIR', '/app/tle_data')
    output_dir = os.environ.get('OUTPUT_DIR', '/app/data')
    
    # æª¢æŸ¥å‘½ä»¤è¡Œåƒæ•¸
    if len(sys.argv) > 1:
        if '--tle-data-dir' in sys.argv:
            idx = sys.argv.index('--tle-data-dir')
            if idx + 1 < len(sys.argv):
                tle_data_dir = sys.argv[idx + 1]
        
        if '--output-dir' in sys.argv:
            idx = sys.argv.index('--output-dir')
            if idx + 1 < len(sys.argv):
                output_dir = sys.argv[idx + 1]
    
    print(f"ğŸ“‚ TLE æ•¸æ“šç›®éŒ„: {tle_data_dir}")
    print(f"ğŸ“ è¼¸å‡ºç›®éŒ„: {output_dir}")
    
    # åˆå§‹åŒ–é è™•ç†å™¨
    preprocessor = Phase0DataPreprocessor(tle_data_dir, output_dir)
    
    # åŸ·è¡Œé è™•ç†
    result = preprocessor.run_build_preprocessing()
    
    # é¡¯ç¤ºçµæœ
    if result['success']:
        print("\nğŸ‰ å»ºç½®é è™•ç†æˆåŠŸå®Œæˆ")
        print(f"ğŸ“Š æƒæåˆ° {result['scan_result']['total_files']} å€‹æ•¸æ“šæ–‡ä»¶")
        print(f"ğŸ›°ï¸ ç¸½è¨ˆ {result['scan_result']['total_satellites']} é¡†è¡›æ˜Ÿ")
        print(f"ğŸ“¦ ç”Ÿæˆ {len(result['artifacts'])} å€‹å»ºç½®ç”¢ç‰©")
        
        if result['recommendations']:
            print("\nğŸ’¡ å»ºè­°:")
            for rec in result['recommendations']:
                print(f"  - {rec}")
    else:
        print(f"\nâŒ å»ºç½®é è™•ç†å¤±æ•—: {result['error']}")
        sys.exit(1)

if __name__ == "__main__":
    main()