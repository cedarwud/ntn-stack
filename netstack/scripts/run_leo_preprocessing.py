#!/usr/bin/env python3
"""
LEOè¡›æ˜Ÿå…­éšæ®µæ•¸æ“šé è™•ç†ä¸»åŸ·è¡Œè…³æœ¬
================================
çµ±ä¸€æ¨™æº–ç‰ˆæœ¬ - æ•´åˆæ‰€æœ‰ä¿®å¾©å¾Œçš„è™•ç†é‚è¼¯

Author: NTN Stack Team
Version: 4.0.0
Date: 2025-09-04

è™•ç†æµç¨‹ï¼š
1. TLEè¼‰å…¥èˆ‡SGP4è»Œé“è¨ˆç®—
2. æ™ºèƒ½è¡›æ˜Ÿå¯è¦‹æ€§ç¯©é¸
3. ä¿¡è™Ÿå“è³ªåˆ†æèˆ‡3GPPäº‹ä»¶
4. æ™‚é–“åºåˆ—é è™•ç†
5. æ•¸æ“šæ•´åˆ
6. å‹•æ…‹è¡›æ˜Ÿæ± è¦åŠƒ
"""

import sys
import os
import json
import time
import logging
import asyncio
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, Optional

# ç¢ºä¿èƒ½æ‰¾åˆ°æ¨¡çµ„
sys.path.insert(0, '/app')
sys.path.insert(0, '/app/src')

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class LEOPreprocessingPipeline:
    """LEOè¡›æ˜Ÿæ•¸æ“šé è™•ç†ç®¡ç·š"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–è™•ç†ç®¡ç·š
        
        Args:
            config: å¯é¸é…ç½®åƒæ•¸
        """
        self.config = config or {}
        self.data_dir = Path(self.config.get('data_dir', '/app/data'))
        self.tle_dir = Path(self.config.get('tle_dir', '/app/tle_data'))
        self.sample_mode = self.config.get('sample_mode', False)
        self.results = {}
        
        # ç¢ºä¿æ•¸æ“šç›®éŒ„å­˜åœ¨
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("âœ… LEOé è™•ç†ç®¡ç·šåˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  æ•¸æ“šç›®éŒ„: {self.data_dir}")
        logger.info(f"  TLEç›®éŒ„: {self.tle_dir}")
        logger.info(f"  è™•ç†æ¨¡å¼: {'å–æ¨£' if self.sample_mode else 'å…¨é‡'}")
    
    def cleanup_previous_outputs(self) -> int:
        """æ¸…ç†èˆŠè¼¸å‡ºæª”æ¡ˆ"""
        logger.info("ğŸ—‘ï¸ æ¸…ç†èˆŠè¼¸å‡ºæª”æ¡ˆ...")
        
        try:
            from stages.dynamic_pool_planner import EnhancedDynamicPoolPlanner
            temp_planner = EnhancedDynamicPoolPlanner({'cleanup_only': True})
            cleaned_count = temp_planner.cleanup_all_stage6_outputs()
            logger.info(f"âœ… æ¸…ç†å®Œæˆ: {cleaned_count} é …ç›®å·²æ¸…ç†")
            return cleaned_count
        except Exception as e:
            logger.warning(f"âš ï¸ æ¸…ç†è­¦å‘Š: {e}")
            return 0
    
    def run_stage1_tle_loading(self) -> bool:
        """éšæ®µä¸€ï¼šTLEè¼‰å…¥èˆ‡SGP4è¨ˆç®—"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“¡ éšæ®µä¸€ï¼šTLEè¼‰å…¥èˆ‡SGP4è»Œé“è¨ˆç®—")
        logger.info("-"*60)
        
        try:
            from stages.orbital_calculation_processor import Stage1TLEProcessor
            
            stage1 = Stage1TLEProcessor(
                tle_data_dir=str(self.tle_dir),
                output_dir=str(self.data_dir),
                sample_mode=self.sample_mode
            )
            
            self.results['stage1'] = stage1.process_tle_orbital_calculation()
            
            if not self.results['stage1']:
                logger.error("âŒ éšæ®µä¸€å¤±æ•—")
                return False
                
            total_sats = self.results['stage1']['metadata']['total_satellites']
            logger.info(f"âœ… éšæ®µä¸€å®Œæˆ: {total_sats} é¡†è¡›æ˜Ÿè¼‰å…¥")
            return True
            
        except Exception as e:
            logger.error(f"âŒ éšæ®µä¸€éŒ¯èª¤: {e}")
            return False
    
    def run_stage2_filtering(self) -> bool:
        """éšæ®µäºŒï¼šæ™ºèƒ½è¡›æ˜Ÿç¯©é¸"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ éšæ®µäºŒï¼šæ™ºèƒ½è¡›æ˜Ÿç¯©é¸")
        logger.info("-"*60)
        
        try:
            from stages.satellite_visibility_filter_processor import SatelliteVisibilityFilterProcessor
            
            stage2 = SatelliteVisibilityFilterProcessor(
                input_dir=str(self.data_dir),
                output_dir=str(self.data_dir),
                sample_mode=self.sample_mode
            )
            
            self.results['stage2'] = stage2.process_intelligent_filtering(
                orbital_data=self.results['stage1'],
                save_output=True
            )
            
            if not self.results['stage2']:
                logger.error("âŒ éšæ®µäºŒå¤±æ•—")
                return False
            
            # è¨ˆç®—ç¯©é¸æ•¸é‡
            filtered_count = self._count_filtered_satellites(self.results['stage2'])
            logger.info(f"âœ… éšæ®µäºŒå®Œæˆ: {filtered_count} é¡†è¡›æ˜Ÿé€šéç¯©é¸")
            return True
            
        except Exception as e:
            logger.error(f"âŒ éšæ®µäºŒéŒ¯èª¤: {e}")
            return False
    
    def run_stage3_signal_analysis(self) -> bool:
        """éšæ®µä¸‰ï¼šä¿¡è™Ÿå“è³ªåˆ†æ"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“¡ éšæ®µä¸‰ï¼šä¿¡è™Ÿå“è³ªåˆ†æèˆ‡3GPPäº‹ä»¶")
        logger.info("-"*60)
        
        try:
            from stages.signal_analysis_processor import SignalQualityAnalysisProcessor
            
            stage3 = SignalQualityAnalysisProcessor(
                input_dir=str(self.data_dir),
                output_dir=str(self.data_dir)
            )
            
            self.results['stage3'] = stage3.process_signal_quality_analysis(
                filtering_data=self.results['stage2'],
                save_output=True
            )
            
            if not self.results['stage3']:
                logger.error("âŒ éšæ®µä¸‰å¤±æ•—")
                return False
            
            event_count = self._count_3gpp_events(self.results['stage3'])
            logger.info(f"âœ… éšæ®µä¸‰å®Œæˆ: {event_count} å€‹3GPPäº‹ä»¶")
            return True
            
        except Exception as e:
            logger.error(f"âŒ éšæ®µä¸‰éŒ¯èª¤: {e}")
            return False
    
    def run_stage4_timeseries(self) -> bool:
        """éšæ®µå››ï¼šæ™‚é–“åºåˆ—é è™•ç†"""
        logger.info("\n" + "="*60)
        logger.info("â° éšæ®µå››ï¼šæ™‚é–“åºåˆ—é è™•ç†")
        logger.info("-"*60)
        
        try:
            from stages.timeseries_preprocessing_processor import TimeseriesPreprocessingProcessor
            
            stage4 = TimeseriesPreprocessingProcessor(
                input_dir=str(self.data_dir),
                output_dir=str(self.data_dir)
            )
            
            signal_file = self.data_dir / 'stage3_signal_event_analysis_output.json'
            self.results['stage4'] = stage4.process_timeseries_preprocessing(
                signal_file=str(signal_file),
                save_output=True
            )
            
            if not self.results['stage4']:
                logger.error("âŒ éšæ®µå››å¤±æ•—")
                return False
            
            ts_count = self._count_timeseries_satellites(self.results['stage4'])
            logger.info(f"âœ… éšæ®µå››å®Œæˆ: {ts_count} é¡†è¡›æ˜Ÿæ™‚é–“åºåˆ—")
            return True
            
        except Exception as e:
            logger.error(f"âŒ éšæ®µå››éŒ¯èª¤: {e}")
            return False
    
    def run_stage5_integration(self) -> bool:
        """éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆ"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ”„ éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆ")
        logger.info("-"*60)
        
        try:
            from stages.data_integration_processor import Stage5IntegrationProcessor, Stage5Config
            
            stage5_config = Stage5Config(
                input_enhanced_timeseries_dir=str(self.data_dir),
                output_data_integration_dir=str(self.data_dir),
                elevation_thresholds=[5, 10, 15]
            )
            
            stage5 = Stage5IntegrationProcessor(stage5_config)
            
            # ä½¿ç”¨asyncioåŸ·è¡Œasyncæ–¹æ³•
            self.results['stage5'] = asyncio.run(stage5.process_enhanced_timeseries())
            
            if not self.results['stage5']:
                logger.error("âŒ éšæ®µäº”å¤±æ•—")
                return False
            
            integrated_count = self.results['stage5'].get('metadata', {}).get('total_satellites', 0)
            logger.info(f"âœ… éšæ®µäº”å®Œæˆ: {integrated_count} é¡†è¡›æ˜Ÿæ•´åˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ éšæ®µäº”éŒ¯èª¤: {e}")
            return False
    
    def run_stage6_dynamic_pool(self) -> bool:
        """éšæ®µå…­ï¼šå‹•æ…‹æ± è¦åŠƒ"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ¯ éšæ®µå…­ï¼šå‹•æ…‹æ± è¦åŠƒ")
        logger.info("-"*60)
        
        try:
            from stages.dynamic_pool_planner import EnhancedDynamicPoolPlanner
            
            stage6_config = {
                'input_dir': str(self.data_dir),
                'output_dir': str(self.data_dir)
            }
            
            stage6 = EnhancedDynamicPoolPlanner(stage6_config)
            
            output_file = self.data_dir / 'enhanced_dynamic_pools_output.json'
            self.results['stage6'] = stage6.process(
                input_data=self.results['stage5'],
                output_file=str(output_file)
            )
            
            if not self.results['stage6']:
                logger.error("âŒ éšæ®µå…­å¤±æ•—")
                return False
            
            pool_stats = self._extract_pool_stats(self.results['stage6'])
            logger.info(f"âœ… éšæ®µå…­å®Œæˆ: ç¸½è¨ˆ {pool_stats['total']} é¡†è¡›æ˜Ÿ")
            logger.info(f"   - Starlink: {pool_stats['starlink']} é¡†")
            logger.info(f"   - OneWeb: {pool_stats['oneweb']} é¡†")
            return True
            
        except Exception as e:
            logger.error(f"âŒ éšæ®µå…­éŒ¯èª¤: {e}")
            return False
    
    def _count_filtered_satellites(self, data: Dict) -> int:
        """è¨ˆç®—ç¯©é¸å¾Œè¡›æ˜Ÿæ•¸é‡"""
        count = 0
        if 'constellations' in data:
            for const_data in data['constellations'].values():
                count += const_data.get('satellite_count', 0)
        elif 'metadata' in data:
            count = data['metadata'].get('total_satellites', 0)
        return count
    
    def _count_3gpp_events(self, data: Dict) -> int:
        """è¨ˆç®—3GPPäº‹ä»¶æ•¸é‡"""
        if 'gpp_events' in data:
            return len(data['gpp_events'].get('all_events', []))
        elif 'metadata' in data:
            return data['metadata'].get('total_3gpp_events', 0)
        return 0
    
    def _count_timeseries_satellites(self, data: Dict) -> int:
        """è¨ˆç®—æ™‚é–“åºåˆ—è¡›æ˜Ÿæ•¸é‡"""
        if 'timeseries_data' in data:
            return len(data['timeseries_data'].get('satellites', []))
        elif 'metadata' in data:
            return data['metadata'].get('total_satellites', 0)
        return 0
    
    def _extract_pool_stats(self, data: Dict) -> Dict[str, int]:
        """æå–è¡›æ˜Ÿæ± çµ±è¨ˆ"""
        pool_data = data.get('dynamic_satellite_pool', {})
        
        # è™•ç†å¯èƒ½æ˜¯æ•´æ•¸æˆ–åˆ—è¡¨çš„æƒ…æ³
        def extract_count(value):
            return len(value) if isinstance(value, list) else value
        
        return {
            'total': pool_data.get('total_selected', 0),
            'starlink': extract_count(pool_data.get('starlink_satellites', 0)),
            'oneweb': extract_count(pool_data.get('oneweb_satellites', 0))
        }
    
    def save_final_report(self, elapsed_time: float):
        """
        [REMOVED] æœ€çµ‚å ±å‘Šç”Ÿæˆå·²ç§»é™¤
        åŸå› ï¼šèˆ‡Dockeræ—¥èªŒå’Œé©—è­‰å¿«ç…§åŠŸèƒ½é‡è¤‡
        æ›¿ä»£æ–¹æ¡ˆï¼šä½¿ç”¨ docker logs netstack-api æŸ¥çœ‹åŸ·è¡Œæ—¥èªŒ
                ä½¿ç”¨ validation_snapshots/ ä¸­çš„é©—è­‰å ±å‘Š
        """
        pass
    
    def run_pipeline(self, skip_stages=None) -> bool:
        """åŸ·è¡Œå®Œæ•´è™•ç†ç®¡ç·š"""
        print("\n" + "="*80)
        print("ğŸš€ LEOè¡›æ˜Ÿå…­éšæ®µæ•¸æ“šé è™•ç†ç³»çµ±")
        print("="*80)
        print(f"é–‹å§‹æ™‚é–“: {datetime.now(timezone.utc).isoformat()}")
        print(f"è™•ç†æ¨¡å¼: {'å–æ¨£æ¨¡å¼' if self.sample_mode else 'å…¨é‡æ¨¡å¼'}")
        print("="*80)
        
        start_time = time.time()
        skip_stages = skip_stages or []
        executed_stages = []
        
        try:
            # å°å…¥é©—è­‰å¼•æ“ (ä¿®å¾©å°å…¥è·¯å¾‘ - ä½¿ç”¨å®¹å™¨å…§çš„æ­£ç¢ºè·¯å¾‘)
            import sys
            sys.path.insert(0, '/app/src')
            from shared_core.validation_engine import PipelineValidationEngine
            validator = PipelineValidationEngine(str(self.data_dir))
            
            # æ¸…ç†èˆŠè¼¸å‡º
            self.cleanup_previous_outputs()
            
            # ä¾åºåŸ·è¡Œå…­éšæ®µ
            stages = [
                (1, "éšæ®µä¸€", self.run_stage1_tle_loading),
                (2, "éšæ®µäºŒ", self.run_stage2_filtering),
                (3, "éšæ®µä¸‰", self.run_stage3_signal_analysis),
                (4, "éšæ®µå››", self.run_stage4_timeseries),
                (5, "éšæ®µäº”", self.run_stage5_integration),
                (6, "éšæ®µå…­", self.run_stage6_dynamic_pool)
            ]
            
            for stage_num, stage_name, stage_func in stages:
                if stage_num in skip_stages:
                    logger.info(f"â­ï¸ è·³é{stage_name}")
                    continue
                    
                logger.info(f"ğŸš€ é–‹å§‹åŸ·è¡Œ{stage_name}")
                
                # åŸ·è¡Œéšæ®µ
                if not stage_func():
                    logger.error(f"âŒ {stage_name}åŸ·è¡Œå¤±æ•—ï¼Œè™•ç†ä¸­æ­¢")
                    return False
                
                # è¨˜éŒ„å·²åŸ·è¡Œéšæ®µ
                executed_stages.append(stage_num)
                
                # è‡ªå‹•é©—è­‰éšæ®µè¼¸å‡º
                logger.info(f"ğŸ“Š è‡ªå‹•é©—è­‰{stage_name}è¼¸å‡º...")
                validation_result = validator.validate_stage(stage_num)
                
                if validation_result.result.value == "passed":
                    logger.info(f"âœ… {stage_name}é©—è­‰é€šé ({validation_result.passed_checks}/{validation_result.total_checks})")
                    
                elif validation_result.result.value == "missing":
                    logger.error(f"âŒ {stage_name}é©—è­‰å¿«ç…§ç¼ºå¤±")
                    logger.error(f"   éŒ¯èª¤: {validation_result.error_message}")
                    logger.error("ğŸ›‘ é©—è­‰å¤±æ•—ï¼Œåœæ­¢ç®¡é“åŸ·è¡Œ (Fail-Fast)")
                    return False
                    
                else:  # failed
                    logger.error(f"âŒ {stage_name}é©—è­‰å¤±æ•— ({validation_result.failed_checks}/{validation_result.total_checks})")
                    logger.error(f"   é—œéµå¤±æ•—: {', '.join(validation_result.critical_failures)}")
                    if validation_result.error_message:
                        logger.error(f"   éŒ¯èª¤è©³æƒ…: {validation_result.error_message}")
                    
                    # Fail-Fast: ç«‹å³åœæ­¢
                    logger.error("ğŸ›‘ é©—è­‰å¤±æ•—ï¼Œåœæ­¢ç®¡é“åŸ·è¡Œ (Fail-Fast)")
                    return False
                
                logger.info(f"ğŸ¯ {stage_name}åŸ·è¡Œä¸¦é©—è­‰å®Œæˆ")
            
            # è™•ç†å®Œæˆ
            elapsed_time = time.time() - start_time
            
            print("\n" + "="*80)
            print("ğŸ“Š LEOè¡›æ˜Ÿé è™•ç†å®Œæˆç¸½çµ")
            print("="*80)
            print(f"âœ… æ‰€æœ‰éšæ®µæˆåŠŸå®Œæˆï¼")
            print(f"â±ï¸ ç¸½è€—æ™‚: {elapsed_time:.2f} ç§’ ({elapsed_time/60:.2f} åˆ†é˜)")
            print(f"ğŸ“‹ åŸ·è¡Œéšæ®µ: {executed_stages}")
            print(f"ğŸ›¡ï¸ è‡ªå‹•é©—è­‰: æ‰€æœ‰éšæ®µé©—è­‰é€šé")
            print("="*80)
            
            # ç§»é™¤é‡è¤‡çš„å ±å‘Šç”Ÿæˆ - ä½¿ç”¨Dockeræ—¥èªŒå’Œé©—è­‰å¿«ç…§å·²è¶³å¤ 
            
            # ç”Ÿæˆé©—è­‰å ±å‘Š
            validation_report = validator.generate_validation_report(executed_stages)
            validation_report_path = self.data_dir / "validation_snapshots" / "pipeline_validation_report.json"
            validation_report_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(validation_report_path, 'w', encoding='utf-8') as f:
                json.dump(validation_report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“ ç®¡é“é©—è­‰å ±å‘Šå·²ä¿å­˜: {validation_report_path}")
            
            return True
            
        except Exception as e:
            logger.error(f"\nâŒ ç®¡ç·šåŸ·è¡ŒéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='LEOè¡›æ˜Ÿå…­éšæ®µæ•¸æ“šé è™•ç†ç³»çµ±',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹:
  # å…¨é‡è™•ç†
  python run_leo_preprocessing.py
  
  # å–æ¨£æ¨¡å¼è™•ç†
  python run_leo_preprocessing.py --sample-mode
  
  # æŒ‡å®šæ•¸æ“šç›®éŒ„
  python run_leo_preprocessing.py --data-dir /custom/data
  
  # è·³ééƒ¨åˆ†éšæ®µï¼ˆé–‹ç™¼æ¸¬è©¦ç”¨ï¼‰
  python run_leo_preprocessing.py --skip-stages 1 2
        """
    )
    
    parser.add_argument(
        '--data-dir',
        default='/app/data',
        help='æ•¸æ“šç›®éŒ„è·¯å¾‘ (é è¨­: /app/data)'
    )
    
    parser.add_argument(
        '--tle-dir',
        default='/app/tle_data',
        help='TLEæ•¸æ“šç›®éŒ„è·¯å¾‘ (é è¨­: /app/tle_data)'
    )
    
    parser.add_argument(
        '--sample-mode',
        action='store_true',
        help='ä½¿ç”¨å–æ¨£æ¨¡å¼ï¼ˆè™•ç†å°‘é‡è¡›æ˜Ÿç”¨æ–¼æ¸¬è©¦ï¼‰'
    )
    
    parser.add_argument(
        '--skip-stages',
        nargs='+',
        type=int,
        choices=[1, 2, 3, 4, 5, 6],
        help='è·³éæŒ‡å®šéšæ®µï¼ˆåƒ…ä¾›é–‹ç™¼æ¸¬è©¦ï¼‰'
    )
    
    args = parser.parse_args()
    
    # é…ç½®åƒæ•¸
    config = {
        'data_dir': args.data_dir,
        'tle_dir': args.tle_dir,
        'sample_mode': args.sample_mode
    }
    
    # å‰µå»ºç®¡ç·šä¸¦åŸ·è¡Œ
    pipeline = LEOPreprocessingPipeline(config)
    
    # å¦‚æœæŒ‡å®šè·³ééšæ®µï¼Œçµ¦å‡ºè­¦å‘Š
    if args.skip_stages:
        logger.warning(f"âš ï¸ æ³¨æ„ï¼šå°‡è·³ééšæ®µ {args.skip_stages}")
        logger.warning("æ­¤åŠŸèƒ½åƒ…ä¾›é–‹ç™¼æ¸¬è©¦ï¼Œç”Ÿç”¢ç’°å¢ƒè«‹åŸ·è¡Œå®Œæ•´æµç¨‹")
    
    # åŸ·è¡Œç®¡ç·š
    success = pipeline.run_pipeline(skip_stages=args.skip_stages)
    
    # è¿”å›ç‹€æ…‹ç¢¼
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    main()