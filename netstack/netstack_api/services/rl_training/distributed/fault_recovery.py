"""
æ•…éšœæ¢å¾©æ©Ÿåˆ¶ - Phase 4 æ ¸å¿ƒçµ„ä»¶

å¯¦ç¾åˆ†ä½ˆå¼è¨“ç·´ç’°å¢ƒä¸­çš„æ•…éšœæª¢æ¸¬å’Œæ¢å¾©ï¼š
- ç¯€é»æ•…éšœæª¢æ¸¬
- ä»»å‹™è‡ªå‹•æ¢å¾©
- ç³»çµ±ç‹€æ…‹æ¢å¾©
- æ•¸æ“šä¸€è‡´æ€§ä¿è­‰

Features:
- å¯¦æ™‚æ•…éšœæª¢æ¸¬
- è‡ªå‹•æ•…éšœæ¢å¾©
- ä»»å‹™ç‹€æ…‹ä¿è­·
- ç³»çµ±å¥å£¯æ€§ä¿è­‰
"""

import asyncio
import logging
import time
import json
import hashlib
from typing import Dict, List, Optional, Any, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
import copy
import pickle
import os

from .node_coordinator import NodeInfo, NodeStatus, NodeType, TrainingTask

logger = logging.getLogger(__name__)


class FailureType(Enum):
    """æ•…éšœé¡å‹"""
    NODE_TIMEOUT = "node_timeout"
    NODE_CRASH = "node_crash"
    TASK_FAILURE = "task_failure"
    NETWORK_PARTITION = "network_partition"
    RESOURCE_EXHAUSTION = "resource_exhaustion"
    CORRUPTION = "corruption"


class RecoveryStrategy(Enum):
    """æ¢å¾©ç­–ç•¥"""
    IMMEDIATE = "immediate"        # ç«‹å³æ¢å¾©
    DELAYED = "delayed"           # å»¶é²æ¢å¾©
    MANUAL = "manual"             # æ‰‹å‹•æ¢å¾©
    GRACEFUL = "graceful"         # å„ªé›…æ¢å¾©
    AGGRESSIVE = "aggressive"     # æ¿€é€²æ¢å¾©


@dataclass
class FailureEvent:
    """æ•…éšœäº‹ä»¶"""
    failure_id: str
    failure_type: FailureType
    affected_node: str
    affected_tasks: List[str]
    detection_time: datetime
    description: str
    severity: str  # low, medium, high, critical
    recovery_strategy: RecoveryStrategy
    recovery_attempts: int = 0
    max_recovery_attempts: int = 3
    resolved: bool = False
    resolution_time: Optional[datetime] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        return {
            "failure_id": self.failure_id,
            "failure_type": self.failure_type.value,
            "affected_node": self.affected_node,
            "affected_tasks": self.affected_tasks,
            "detection_time": self.detection_time.isoformat(),
            "description": self.description,
            "severity": self.severity,
            "recovery_strategy": self.recovery_strategy.value,
            "recovery_attempts": self.recovery_attempts,
            "max_recovery_attempts": self.max_recovery_attempts,
            "resolved": self.resolved,
            "resolution_time": self.resolution_time.isoformat() if self.resolution_time else None
        }


@dataclass
class SystemSnapshot:
    """ç³»çµ±å¿«ç…§"""
    snapshot_id: str
    timestamp: datetime
    nodes: Dict[str, NodeInfo]
    tasks: Dict[str, TrainingTask]
    system_state: Dict[str, Any]
    checksum: str
    
    def calculate_checksum(self) -> str:
        """è¨ˆç®—æ ¡é©—å’Œ"""
        data = {
            "nodes": {k: v.to_dict() for k, v in self.nodes.items()},
            "tasks": {k: v.to_dict() for k, v in self.tasks.items()},
            "system_state": self.system_state
        }
        
        json_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()


@dataclass
class RecoveryPlan:
    """æ¢å¾©è¨ˆåŠƒ"""
    plan_id: str
    failure_event: FailureEvent
    recovery_steps: List[Dict[str, Any]]
    estimated_duration: int
    priority: int
    dependencies: List[str]
    created_at: datetime
    
    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        return {
            "plan_id": self.plan_id,
            "failure_event": self.failure_event.to_dict(),
            "recovery_steps": self.recovery_steps,
            "estimated_duration": self.estimated_duration,
            "priority": self.priority,
            "dependencies": self.dependencies,
            "created_at": self.created_at.isoformat()
        }


class FaultRecovery:
    """
    æ•…éšœæ¢å¾©æ©Ÿåˆ¶
    
    å¯¦ç¾åˆ†ä½ˆå¼è¨“ç·´ç’°å¢ƒä¸­çš„æ•…éšœæª¢æ¸¬å’Œè‡ªå‹•æ¢å¾©ï¼Œ
    ä¿è­‰ç³»çµ±çš„é«˜å¯ç”¨æ€§å’Œæ•¸æ“šä¸€è‡´æ€§ã€‚
    """
    
    def __init__(self,
                 detection_interval: int = 10,
                 recovery_timeout: int = 300,
                 max_recovery_attempts: int = 3,
                 snapshot_interval: int = 60,
                 enable_auto_recovery: bool = True):
        """
        åˆå§‹åŒ–æ•…éšœæ¢å¾©æ©Ÿåˆ¶
        
        Args:
            detection_interval: æ•…éšœæª¢æ¸¬é–“éš”ï¼ˆç§’ï¼‰
            recovery_timeout: æ¢å¾©è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰
            max_recovery_attempts: æœ€å¤§æ¢å¾©å˜—è©¦æ¬¡æ•¸
            snapshot_interval: å¿«ç…§é–“éš”ï¼ˆç§’ï¼‰
            enable_auto_recovery: æ˜¯å¦å•Ÿç”¨è‡ªå‹•æ¢å¾©
        """
        self.detection_interval = detection_interval
        self.recovery_timeout = recovery_timeout
        self.max_recovery_attempts = max_recovery_attempts
        self.snapshot_interval = snapshot_interval
        self.enable_auto_recovery = enable_auto_recovery
        
        # æ•…éšœç®¡ç†
        self.failure_events: Dict[str, FailureEvent] = {}
        self.recovery_plans: Dict[str, RecoveryPlan] = {}
        self.active_recoveries: Set[str] = set()
        
        # ç³»çµ±å¿«ç…§
        self.snapshots: Dict[str, SystemSnapshot] = {}
        self.snapshot_history: List[str] = []
        self.max_snapshots = 10
        
        # ç›£æ§ç‹€æ…‹
        self.monitored_nodes: Dict[str, NodeInfo] = {}
        self.monitored_tasks: Dict[str, TrainingTask] = {}
        
        # é‹è¡Œç‹€æ…‹
        self.is_running = False
        self.detection_task = None
        self.recovery_task = None
        self.snapshot_task = None
        
        # çµ±è¨ˆä¿¡æ¯
        self.stats = {
            "total_failures": 0,
            "resolved_failures": 0,
            "failed_recoveries": 0,
            "average_recovery_time": 0.0,
            "system_uptime": 0.0,
            "last_failure": None,
            "start_time": datetime.now()
        }
        
        self.logger = logger
        
    async def start(self):
        """å•Ÿå‹•æ•…éšœæ¢å¾©æ©Ÿåˆ¶"""
        try:
            self.logger.info("ğŸš€ å•Ÿå‹•æ•…éšœæ¢å¾©æ©Ÿåˆ¶...")
            
            # å‰µå»ºå¿«ç…§ç›®éŒ„
            self.snapshot_dir = "/tmp/phase4_snapshots"
            os.makedirs(self.snapshot_dir, exist_ok=True)
            
            # å•Ÿå‹•ç›£æ§ä»»å‹™
            self.detection_task = asyncio.create_task(self._failure_detection_loop())
            self.recovery_task = asyncio.create_task(self._recovery_loop())
            self.snapshot_task = asyncio.create_task(self._snapshot_loop())
            
            self.is_running = True
            self.stats["start_time"] = datetime.now()
            
            self.logger.info("âœ… æ•…éšœæ¢å¾©æ©Ÿåˆ¶å•Ÿå‹•æˆåŠŸ")
            
        except Exception as e:
            self.logger.error(f"âŒ æ•…éšœæ¢å¾©æ©Ÿåˆ¶å•Ÿå‹•å¤±æ•—: {e}")
            raise
    
    async def stop(self):
        """åœæ­¢æ•…éšœæ¢å¾©æ©Ÿåˆ¶"""
        try:
            self.logger.info("ğŸ›‘ åœæ­¢æ•…éšœæ¢å¾©æ©Ÿåˆ¶...")
            
            self.is_running = False
            
            # åœæ­¢ç›£æ§ä»»å‹™
            for task in [self.detection_task, self.recovery_task, self.snapshot_task]:
                if task:
                    task.cancel()
                    try:
                        await task
                    except asyncio.CancelledError:
                        pass
            
            # ä¿å­˜æœ€å¾Œä¸€å€‹å¿«ç…§
            if self.monitored_nodes or self.monitored_tasks:
                await self._create_snapshot("final_snapshot")
            
            self.logger.info("âœ… æ•…éšœæ¢å¾©æ©Ÿåˆ¶å·²åœæ­¢")
            
        except Exception as e:
            self.logger.error(f"âŒ åœæ­¢æ•…éšœæ¢å¾©æ©Ÿåˆ¶å¤±æ•—: {e}")
    
    async def update_system_state(self, 
                                nodes: Dict[str, NodeInfo], 
                                tasks: Dict[str, TrainingTask]):
        """æ›´æ–°ç³»çµ±ç‹€æ…‹"""
        self.monitored_nodes = copy.deepcopy(nodes)
        self.monitored_tasks = copy.deepcopy(tasks)
    
    async def _failure_detection_loop(self):
        """æ•…éšœæª¢æ¸¬å¾ªç’°"""
        while self.is_running:
            try:
                # æª¢æ¸¬ç¯€é»æ•…éšœ
                await self._detect_node_failures()
                
                # æª¢æ¸¬ä»»å‹™æ•…éšœ
                await self._detect_task_failures()
                
                # æª¢æ¸¬ç¶²çµ¡åˆ†å€
                await self._detect_network_partitions()
                
                # æª¢æ¸¬è³‡æºè€—ç›¡
                await self._detect_resource_exhaustion()
                
                await asyncio.sleep(self.detection_interval)
                
            except Exception as e:
                self.logger.error(f"âŒ æ•…éšœæª¢æ¸¬å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(5)
    
    async def _detect_node_failures(self):
        """æª¢æ¸¬ç¯€é»æ•…éšœ"""
        try:
            current_time = datetime.now()
            
            for node_id, node in self.monitored_nodes.items():
                # æª¢æ¸¬è¶…æ™‚
                if node.last_heartbeat:
                    time_since_heartbeat = (current_time - node.last_heartbeat).total_seconds()
                    if time_since_heartbeat > 90:  # 90ç§’è¶…æ™‚
                        await self._handle_node_timeout(node_id, time_since_heartbeat)
                
                # æª¢æ¸¬ç‹€æ…‹ç•°å¸¸
                if node.status == NodeStatus.ERROR:
                    await self._handle_node_error(node_id)
                
                # æª¢æ¸¬è² è¼‰ç•°å¸¸
                if node.current_load > 1.0:
                    await self._handle_node_overload(node_id, node.current_load)
                    
        except Exception as e:
            self.logger.error(f"âŒ ç¯€é»æ•…éšœæª¢æ¸¬å¤±æ•—: {e}")
    
    async def _detect_task_failures(self):
        """æª¢æ¸¬ä»»å‹™æ•…éšœ"""
        try:
            current_time = datetime.now()
            
            for task_id, task in self.monitored_tasks.items():
                # æª¢æ¸¬é•·æ™‚é–“é‹è¡Œçš„ä»»å‹™
                if task.status == "running":
                    runtime = (current_time - task.created_at).total_seconds()
                    if runtime > 3600:  # 1å°æ™‚è¶…æ™‚
                        await self._handle_task_timeout(task_id, runtime)
                
                # æª¢æ¸¬å¤±æ•—ä»»å‹™
                if task.status == "failed":
                    await self._handle_task_failure(task_id)
                    
        except Exception as e:
            self.logger.error(f"âŒ ä»»å‹™æ•…éšœæª¢æ¸¬å¤±æ•—: {e}")
    
    async def _detect_network_partitions(self):
        """æª¢æ¸¬ç¶²çµ¡åˆ†å€"""
        try:
            # æª¢æ¸¬ç¯€é»é–“çš„é€£æ¥ç‹€æ…‹
            # é€™è£¡å¯ä»¥å¯¦ç¾ ping æ¸¬è©¦æˆ–å…¶ä»–ç¶²çµ¡æª¢æ¸¬
            pass
            
        except Exception as e:
            self.logger.error(f"âŒ ç¶²çµ¡åˆ†å€æª¢æ¸¬å¤±æ•—: {e}")
    
    async def _detect_resource_exhaustion(self):
        """æª¢æ¸¬è³‡æºè€—ç›¡"""
        try:
            # æª¢æ¸¬ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³
            # é€™è£¡å¯ä»¥å¯¦ç¾ CPUã€å…§å­˜ã€ç£ç›¤ä½¿ç”¨ç‡æª¢æ¸¬
            pass
            
        except Exception as e:
            self.logger.error(f"âŒ è³‡æºè€—ç›¡æª¢æ¸¬å¤±æ•—: {e}")
    
    async def _handle_node_timeout(self, node_id: str, timeout_duration: float):
        """è™•ç†ç¯€é»è¶…æ™‚"""
        failure_id = f"node_timeout_{node_id}_{int(time.time())}"
        
        # æ‰¾åˆ°å—å½±éŸ¿çš„ä»»å‹™
        affected_tasks = [
            task_id for task_id, task in self.monitored_tasks.items()
            if task.assigned_node == node_id and task.status in ["assigned", "running"]
        ]
        
        failure_event = FailureEvent(
            failure_id=failure_id,
            failure_type=FailureType.NODE_TIMEOUT,
            affected_node=node_id,
            affected_tasks=affected_tasks,
            detection_time=datetime.now(),
            description=f"Node {node_id} timeout after {timeout_duration:.1f} seconds",
            severity="high",
            recovery_strategy=RecoveryStrategy.IMMEDIATE
        )
        
        await self._register_failure(failure_event)
    
    async def _handle_node_error(self, node_id: str):
        """è™•ç†ç¯€é»éŒ¯èª¤"""
        failure_id = f"node_error_{node_id}_{int(time.time())}"
        
        affected_tasks = [
            task_id for task_id, task in self.monitored_tasks.items()
            if task.assigned_node == node_id and task.status in ["assigned", "running"]
        ]
        
        failure_event = FailureEvent(
            failure_id=failure_id,
            failure_type=FailureType.NODE_CRASH,
            affected_node=node_id,
            affected_tasks=affected_tasks,
            detection_time=datetime.now(),
            description=f"Node {node_id} in error state",
            severity="high",
            recovery_strategy=RecoveryStrategy.GRACEFUL
        )
        
        await self._register_failure(failure_event)
    
    async def _handle_node_overload(self, node_id: str, load: float):
        """è™•ç†ç¯€é»éè¼‰"""
        failure_id = f"node_overload_{node_id}_{int(time.time())}"
        
        failure_event = FailureEvent(
            failure_id=failure_id,
            failure_type=FailureType.RESOURCE_EXHAUSTION,
            affected_node=node_id,
            affected_tasks=[],
            detection_time=datetime.now(),
            description=f"Node {node_id} overloaded (load: {load:.2f})",
            severity="medium",
            recovery_strategy=RecoveryStrategy.DELAYED
        )
        
        await self._register_failure(failure_event)
    
    async def _handle_task_timeout(self, task_id: str, runtime: float):
        """è™•ç†ä»»å‹™è¶…æ™‚"""
        failure_id = f"task_timeout_{task_id}_{int(time.time())}"
        
        task = self.monitored_tasks[task_id]
        
        failure_event = FailureEvent(
            failure_id=failure_id,
            failure_type=FailureType.TASK_FAILURE,
            affected_node=task.assigned_node or "unknown",
            affected_tasks=[task_id],
            detection_time=datetime.now(),
            description=f"Task {task_id} timeout after {runtime:.1f} seconds",
            severity="medium",
            recovery_strategy=RecoveryStrategy.IMMEDIATE
        )
        
        await self._register_failure(failure_event)
    
    async def _handle_task_failure(self, task_id: str):
        """è™•ç†ä»»å‹™å¤±æ•—"""
        failure_id = f"task_failure_{task_id}_{int(time.time())}"
        
        task = self.monitored_tasks[task_id]
        
        failure_event = FailureEvent(
            failure_id=failure_id,
            failure_type=FailureType.TASK_FAILURE,
            affected_node=task.assigned_node or "unknown",
            affected_tasks=[task_id],
            detection_time=datetime.now(),
            description=f"Task {task_id} failed",
            severity="low",
            recovery_strategy=RecoveryStrategy.IMMEDIATE
        )
        
        await self._register_failure(failure_event)
    
    async def _register_failure(self, failure_event: FailureEvent):
        """è¨»å†Šæ•…éšœäº‹ä»¶"""
        self.failure_events[failure_event.failure_id] = failure_event
        self.stats["total_failures"] += 1
        self.stats["last_failure"] = failure_event.detection_time
        
        self.logger.warning(f"âš ï¸ æ•…éšœäº‹ä»¶è¨»å†Š: {failure_event.failure_id} - {failure_event.description}")
        
        # å¦‚æœå•Ÿç”¨è‡ªå‹•æ¢å¾©ï¼Œå‰µå»ºæ¢å¾©è¨ˆåŠƒ
        if self.enable_auto_recovery:
            await self._create_recovery_plan(failure_event)
    
    async def _create_recovery_plan(self, failure_event: FailureEvent):
        """å‰µå»ºæ¢å¾©è¨ˆåŠƒ"""
        try:
            plan_id = f"recovery_plan_{failure_event.failure_id}"
            
            # æ ¹æ“šæ•…éšœé¡å‹å‰µå»ºä¸åŒçš„æ¢å¾©æ­¥é©Ÿ
            recovery_steps = []
            
            if failure_event.failure_type == FailureType.NODE_TIMEOUT:
                recovery_steps = [
                    {"step": "verify_node_status", "node_id": failure_event.affected_node},
                    {"step": "reassign_tasks", "tasks": failure_event.affected_tasks},
                    {"step": "remove_failed_node", "node_id": failure_event.affected_node}
                ]
            elif failure_event.failure_type == FailureType.TASK_FAILURE:
                recovery_steps = [
                    {"step": "analyze_task_failure", "task_id": failure_event.affected_tasks[0]},
                    {"step": "restart_task", "task_id": failure_event.affected_tasks[0]},
                    {"step": "monitor_task_progress", "task_id": failure_event.affected_tasks[0]}
                ]
            elif failure_event.failure_type == FailureType.RESOURCE_EXHAUSTION:
                recovery_steps = [
                    {"step": "redistribute_load", "node_id": failure_event.affected_node},
                    {"step": "scale_resources", "node_id": failure_event.affected_node},
                    {"step": "monitor_resource_usage", "node_id": failure_event.affected_node}
                ]
            
            # ä¼°ç®—æ¢å¾©æ™‚é–“
            estimated_duration = len(recovery_steps) * 30  # æ¯æ­¥é©Ÿ 30 ç§’
            
            recovery_plan = RecoveryPlan(
                plan_id=plan_id,
                failure_event=failure_event,
                recovery_steps=recovery_steps,
                estimated_duration=estimated_duration,
                priority=self._calculate_recovery_priority(failure_event),
                dependencies=[],
                created_at=datetime.now()
            )
            
            self.recovery_plans[plan_id] = recovery_plan
            
            self.logger.info(f"ğŸ“‹ æ¢å¾©è¨ˆåŠƒå‰µå»º: {plan_id}")
            
        except Exception as e:
            self.logger.error(f"âŒ æ¢å¾©è¨ˆåŠƒå‰µå»ºå¤±æ•—: {e}")
    
    def _calculate_recovery_priority(self, failure_event: FailureEvent) -> int:
        """è¨ˆç®—æ¢å¾©å„ªå…ˆç´š"""
        priority = 0
        
        # æ ¹æ“šåš´é‡ç¨‹åº¦èª¿æ•´å„ªå…ˆç´š
        severity_weights = {
            "low": 1,
            "medium": 2,
            "high": 3,
            "critical": 4
        }
        
        priority += severity_weights.get(failure_event.severity, 1) * 10
        
        # æ ¹æ“šå—å½±éŸ¿çš„ä»»å‹™æ•¸é‡èª¿æ•´
        priority += len(failure_event.affected_tasks) * 5
        
        # æ ¹æ“šæ•…éšœé¡å‹èª¿æ•´
        type_weights = {
            FailureType.NODE_TIMEOUT: 15,
            FailureType.NODE_CRASH: 20,
            FailureType.TASK_FAILURE: 5,
            FailureType.NETWORK_PARTITION: 25,
            FailureType.RESOURCE_EXHAUSTION: 10
        }
        
        priority += type_weights.get(failure_event.failure_type, 5)
        
        return priority
    
    async def _recovery_loop(self):
        """æ¢å¾©å¾ªç’°"""
        while self.is_running:
            try:
                # åŸ·è¡Œæ¢å¾©è¨ˆåŠƒ
                await self._execute_recovery_plans()
                
                # æ¸…ç†å·²å®Œæˆçš„æ¢å¾©
                await self._cleanup_completed_recoveries()
                
                await asyncio.sleep(5)
                
            except Exception as e:
                self.logger.error(f"âŒ æ¢å¾©å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(5)
    
    async def _execute_recovery_plans(self):
        """åŸ·è¡Œæ¢å¾©è¨ˆåŠƒ"""
        try:
            # æŒ‰å„ªå…ˆç´šæ’åºæ¢å¾©è¨ˆåŠƒ
            sorted_plans = sorted(
                self.recovery_plans.values(),
                key=lambda p: p.priority,
                reverse=True
            )
            
            for plan in sorted_plans:
                if plan.plan_id in self.active_recoveries:
                    continue
                
                failure_event = plan.failure_event
                
                # æª¢æŸ¥æ˜¯å¦è¶…éæœ€å¤§æ¢å¾©å˜—è©¦æ¬¡æ•¸
                if failure_event.recovery_attempts >= failure_event.max_recovery_attempts:
                    self.logger.error(f"âŒ æ¢å¾©å¤±æ•—ï¼Œè¶…éæœ€å¤§å˜—è©¦æ¬¡æ•¸: {plan.plan_id}")
                    failure_event.resolved = False
                    self.stats["failed_recoveries"] += 1
                    continue
                
                # åŸ·è¡Œæ¢å¾©
                self.active_recoveries.add(plan.plan_id)
                asyncio.create_task(self._execute_single_recovery(plan))
                
        except Exception as e:
            self.logger.error(f"âŒ æ¢å¾©è¨ˆåŠƒåŸ·è¡Œå¤±æ•—: {e}")
    
    async def _execute_single_recovery(self, plan: RecoveryPlan):
        """åŸ·è¡Œå–®å€‹æ¢å¾©è¨ˆåŠƒ"""
        try:
            self.logger.info(f"ğŸ”§ é–‹å§‹åŸ·è¡Œæ¢å¾©è¨ˆåŠƒ: {plan.plan_id}")
            
            start_time = datetime.now()
            plan.failure_event.recovery_attempts += 1
            
            # é€æ­¥åŸ·è¡Œæ¢å¾©æ­¥é©Ÿ
            for i, step in enumerate(plan.recovery_steps):
                try:
                    self.logger.info(f"ğŸ”§ åŸ·è¡Œæ¢å¾©æ­¥é©Ÿ {i+1}/{len(plan.recovery_steps)}: {step['step']}")
                    
                    # åŸ·è¡Œå…·é«”çš„æ¢å¾©æ­¥é©Ÿ
                    success = await self._execute_recovery_step(step)
                    
                    if not success:
                        self.logger.error(f"âŒ æ¢å¾©æ­¥é©Ÿå¤±æ•—: {step['step']}")
                        break
                    
                    await asyncio.sleep(1)  # æ­¥é©Ÿé–“éš”
                    
                except Exception as e:
                    self.logger.error(f"âŒ æ¢å¾©æ­¥é©ŸåŸ·è¡ŒéŒ¯èª¤: {e}")
                    break
            else:
                # æ‰€æœ‰æ­¥é©ŸæˆåŠŸå®Œæˆ
                plan.failure_event.resolved = True
                plan.failure_event.resolution_time = datetime.now()
                
                # æ›´æ–°çµ±è¨ˆ
                self.stats["resolved_failures"] += 1
                recovery_time = (datetime.now() - start_time).total_seconds()
                
                # è¨ˆç®—å¹³å‡æ¢å¾©æ™‚é–“
                current_avg = self.stats["average_recovery_time"]
                resolved_count = self.stats["resolved_failures"]
                self.stats["average_recovery_time"] = (
                    (current_avg * (resolved_count - 1) + recovery_time) / resolved_count
                )
                
                self.logger.info(f"âœ… æ¢å¾©è¨ˆåŠƒå®Œæˆ: {plan.plan_id} (è€—æ™‚: {recovery_time:.1f}s)")
            
        except Exception as e:
            self.logger.error(f"âŒ æ¢å¾©è¨ˆåŠƒåŸ·è¡Œå¤±æ•—: {e}")
        finally:
            self.active_recoveries.discard(plan.plan_id)
    
    async def _execute_recovery_step(self, step: Dict[str, Any]) -> bool:
        """åŸ·è¡Œæ¢å¾©æ­¥é©Ÿ"""
        try:
            step_type = step["step"]
            
            if step_type == "verify_node_status":
                return await self._verify_node_status(step["node_id"])
            elif step_type == "reassign_tasks":
                return await self._reassign_tasks(step["tasks"])
            elif step_type == "remove_failed_node":
                return await self._remove_failed_node(step["node_id"])
            elif step_type == "analyze_task_failure":
                return await self._analyze_task_failure(step["task_id"])
            elif step_type == "restart_task":
                return await self._restart_task(step["task_id"])
            elif step_type == "monitor_task_progress":
                return await self._monitor_task_progress(step["task_id"])
            elif step_type == "redistribute_load":
                return await self._redistribute_load(step["node_id"])
            elif step_type == "scale_resources":
                return await self._scale_resources(step["node_id"])
            elif step_type == "monitor_resource_usage":
                return await self._monitor_resource_usage(step["node_id"])
            else:
                self.logger.warning(f"âš ï¸ æœªçŸ¥çš„æ¢å¾©æ­¥é©Ÿé¡å‹: {step_type}")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ æ¢å¾©æ­¥é©ŸåŸ·è¡Œå¤±æ•—: {e}")
            return False
    
    async def _verify_node_status(self, node_id: str) -> bool:
        """é©—è­‰ç¯€é»ç‹€æ…‹"""
        # æ¨¡æ“¬ç¯€é»ç‹€æ…‹é©—è­‰
        self.logger.info(f"ğŸ” é©—è­‰ç¯€é»ç‹€æ…‹: {node_id}")
        await asyncio.sleep(1)
        return True
    
    async def _reassign_tasks(self, task_ids: List[str]) -> bool:
        """é‡æ–°åˆ†é…ä»»å‹™"""
        self.logger.info(f"ğŸ”„ é‡æ–°åˆ†é…ä»»å‹™: {task_ids}")
        await asyncio.sleep(2)
        return True
    
    async def _remove_failed_node(self, node_id: str) -> bool:
        """ç§»é™¤å¤±æ•—ç¯€é»"""
        self.logger.info(f"ğŸ—‘ï¸ ç§»é™¤å¤±æ•—ç¯€é»: {node_id}")
        await asyncio.sleep(1)
        return True
    
    async def _analyze_task_failure(self, task_id: str) -> bool:
        """åˆ†æä»»å‹™å¤±æ•—åŸå› """
        self.logger.info(f"ğŸ” åˆ†æä»»å‹™å¤±æ•—: {task_id}")
        await asyncio.sleep(1)
        return True
    
    async def _restart_task(self, task_id: str) -> bool:
        """é‡å•Ÿä»»å‹™"""
        self.logger.info(f"ğŸ”„ é‡å•Ÿä»»å‹™: {task_id}")
        await asyncio.sleep(2)
        return True
    
    async def _monitor_task_progress(self, task_id: str) -> bool:
        """ç›£æ§ä»»å‹™é€²åº¦"""
        self.logger.info(f"ğŸ“Š ç›£æ§ä»»å‹™é€²åº¦: {task_id}")
        await asyncio.sleep(1)
        return True
    
    async def _redistribute_load(self, node_id: str) -> bool:
        """é‡æ–°åˆ†é…è² è¼‰"""
        self.logger.info(f"âš–ï¸ é‡æ–°åˆ†é…è² è¼‰: {node_id}")
        await asyncio.sleep(2)
        return True
    
    async def _scale_resources(self, node_id: str) -> bool:
        """æ“´å±•è³‡æº"""
        self.logger.info(f"ğŸ“ˆ æ“´å±•è³‡æº: {node_id}")
        await asyncio.sleep(3)
        return True
    
    async def _monitor_resource_usage(self, node_id: str) -> bool:
        """ç›£æ§è³‡æºä½¿ç”¨"""
        self.logger.info(f"ğŸ“Š ç›£æ§è³‡æºä½¿ç”¨: {node_id}")
        await asyncio.sleep(1)
        return True
    
    async def _cleanup_completed_recoveries(self):
        """æ¸…ç†å·²å®Œæˆçš„æ¢å¾©"""
        try:
            completed_plans = [
                plan_id for plan_id, plan in self.recovery_plans.items()
                if plan.failure_event.resolved or 
                   plan.failure_event.recovery_attempts >= plan.failure_event.max_recovery_attempts
            ]
            
            for plan_id in completed_plans:
                del self.recovery_plans[plan_id]
                
        except Exception as e:
            self.logger.error(f"âŒ æ¸…ç†å·²å®Œæˆæ¢å¾©å¤±æ•—: {e}")
    
    async def _snapshot_loop(self):
        """å¿«ç…§å¾ªç’°"""
        while self.is_running:
            try:
                # å‰µå»ºç³»çµ±å¿«ç…§
                snapshot_id = f"snapshot_{int(time.time())}"
                await self._create_snapshot(snapshot_id)
                
                # æ¸…ç†èˆŠå¿«ç…§
                await self._cleanup_old_snapshots()
                
                await asyncio.sleep(self.snapshot_interval)
                
            except Exception as e:
                self.logger.error(f"âŒ å¿«ç…§å¾ªç’°éŒ¯èª¤: {e}")
                await asyncio.sleep(30)
    
    async def _create_snapshot(self, snapshot_id: str):
        """å‰µå»ºç³»çµ±å¿«ç…§"""
        try:
            snapshot = SystemSnapshot(
                snapshot_id=snapshot_id,
                timestamp=datetime.now(),
                nodes=copy.deepcopy(self.monitored_nodes),
                tasks=copy.deepcopy(self.monitored_tasks),
                system_state={
                    "failure_events": len(self.failure_events),
                    "recovery_plans": len(self.recovery_plans),
                    "active_recoveries": len(self.active_recoveries)
                },
                checksum=""
            )
            
            snapshot.checksum = snapshot.calculate_checksum()
            
            # ä¿å­˜å¿«ç…§
            self.snapshots[snapshot_id] = snapshot
            self.snapshot_history.append(snapshot_id)
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            snapshot_file = os.path.join(self.snapshot_dir, f"{snapshot_id}.pkl")
            with open(snapshot_file, 'wb') as f:
                pickle.dump(snapshot, f)
            
            self.logger.debug(f"ğŸ“¸ ç³»çµ±å¿«ç…§å‰µå»º: {snapshot_id}")
            
        except Exception as e:
            self.logger.error(f"âŒ å‰µå»ºç³»çµ±å¿«ç…§å¤±æ•—: {e}")
    
    async def _cleanup_old_snapshots(self):
        """æ¸…ç†èˆŠå¿«ç…§"""
        try:
            # ä¿æŒæœ€å¤§å¿«ç…§æ•¸é‡
            while len(self.snapshot_history) > self.max_snapshots:
                oldest_snapshot = self.snapshot_history.pop(0)
                
                # å¾å…§å­˜ä¸­åˆªé™¤
                if oldest_snapshot in self.snapshots:
                    del self.snapshots[oldest_snapshot]
                
                # åˆªé™¤æ–‡ä»¶
                snapshot_file = os.path.join(self.snapshot_dir, f"{oldest_snapshot}.pkl")
                if os.path.exists(snapshot_file):
                    os.remove(snapshot_file)
                    
        except Exception as e:
            self.logger.error(f"âŒ æ¸…ç†èˆŠå¿«ç…§å¤±æ•—: {e}")
    
    async def restore_from_snapshot(self, snapshot_id: str) -> bool:
        """å¾å¿«ç…§æ¢å¾©"""
        try:
            snapshot = self.snapshots.get(snapshot_id)
            if not snapshot:
                # å˜—è©¦å¾æ–‡ä»¶è¼‰å…¥
                snapshot_file = os.path.join(self.snapshot_dir, f"{snapshot_id}.pkl")
                if os.path.exists(snapshot_file):
                    with open(snapshot_file, 'rb') as f:
                        snapshot = pickle.load(f)
                else:
                    self.logger.error(f"âŒ å¿«ç…§ä¸å­˜åœ¨: {snapshot_id}")
                    return False
            
            # é©—è­‰å¿«ç…§å®Œæ•´æ€§
            if snapshot.checksum != snapshot.calculate_checksum():
                self.logger.error(f"âŒ å¿«ç…§æ ¡é©—å¤±æ•—: {snapshot_id}")
                return False
            
            # æ¢å¾©ç³»çµ±ç‹€æ…‹
            self.monitored_nodes = snapshot.nodes
            self.monitored_tasks = snapshot.tasks
            
            self.logger.info(f"âœ… å¾å¿«ç…§æ¢å¾©ç³»çµ±ç‹€æ…‹: {snapshot_id}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ å¾å¿«ç…§æ¢å¾©å¤±æ•—: {e}")
            return False
    
    def get_failure_stats(self) -> Dict[str, Any]:
        """ç²å–æ•…éšœçµ±è¨ˆ"""
        uptime = (datetime.now() - self.stats["start_time"]).total_seconds()
        
        return {
            **self.stats,
            "uptime_seconds": uptime,
            "active_failures": len([f for f in self.failure_events.values() if not f.resolved]),
            "active_recoveries": len(self.active_recoveries),
            "recovery_plans": len(self.recovery_plans),
            "snapshots": len(self.snapshots),
            "failure_types": {
                failure_type.value: len([f for f in self.failure_events.values() 
                                       if f.failure_type == failure_type])
                for failure_type in FailureType
            }
        }
    
    def get_failure_history(self) -> List[Dict[str, Any]]:
        """ç²å–æ•…éšœæ­·å²"""
        return [failure.to_dict() for failure in self.failure_events.values()]
    
    def get_recovery_plans(self) -> List[Dict[str, Any]]:
        """ç²å–æ¢å¾©è¨ˆåŠƒ"""
        return [plan.to_dict() for plan in self.recovery_plans.values()]
    
    def get_snapshots(self) -> List[Dict[str, Any]]:
        """ç²å–å¿«ç…§åˆ—è¡¨"""
        return [
            {
                "snapshot_id": snapshot.snapshot_id,
                "timestamp": snapshot.timestamp.isoformat(),
                "nodes_count": len(snapshot.nodes),
                "tasks_count": len(snapshot.tasks),
                "checksum": snapshot.checksum
            }
            for snapshot in self.snapshots.values()
        ]
