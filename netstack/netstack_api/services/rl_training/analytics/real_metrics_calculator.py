"""
ğŸ§  çœŸå¯¦ RL è¨“ç·´æŒ‡æ¨™è¨ˆç®—æœå‹™
ç§»é™¤ç¡¬ç·¨ç¢¼æ¨¡æ“¬æ•¸æ“šï¼Œå¯¦ç¾åŸºæ–¼çœŸå¯¦è¨“ç·´éç¨‹çš„æŒ‡æ¨™è¨ˆç®—
"""

import asyncio
import logging
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import numpy as np
from dataclasses import dataclass

# å°å…¥çµ±ä¸€æ¥å£
from ..interfaces.metrics_interface import (
    IMetricsCalculator,
    StandardizedMetrics,
    MetricType,
    TrendType,
    MetricDefinition,
    MetricsStandardizer,
)

logger = logging.getLogger(__name__)


@dataclass
class TrainingMetrics:
    """è¨“ç·´æŒ‡æ¨™æ•¸æ“šçµæ§‹"""

    success_rate: float  # æˆåŠŸç‡ (0.0 - 1.0)
    stability: float  # ç©©å®šæ€§ (0.0 - 1.0)
    average_reward: float
    convergence_episode: Optional[int]
    learning_efficiency: float
    performance_trend: str  # "improving", "stable", "declining"
    confidence_score: float  # æŒ‡æ¨™å¯ä¿¡åº¦ (0.0 - 1.0)


@dataclass
class EpisodeData:
    """å–®å€‹å›åˆæ•¸æ“š"""

    episode_number: int
    total_reward: float
    steps: int
    success: bool
    handover_count: int
    average_latency: float
    timestamp: datetime


class RealMetricsCalculator(IMetricsCalculator):
    """çœŸå¯¦æŒ‡æ¨™è¨ˆç®—å™¨ - å¯¦ç¾çµ±ä¸€æ¥å£"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.standardizer = MetricsStandardizer()

        # æŒ‡æ¨™è¨ˆç®—åƒæ•¸
        self.success_reward_threshold = 100.0  # æˆåŠŸå›åˆçš„æœ€ä½çå‹µé–¾å€¼
        self.stability_window_size = 20  # ç©©å®šæ€§è¨ˆç®—çš„æ»‘å‹•çª—å£å¤§å°
        self.convergence_patience = 10  # æ”¶æ–‚åˆ¤æ–·çš„è€å¿ƒå€¼
        self.min_episodes_for_metrics = 5  # è¨ˆç®—æŒ‡æ¨™æ‰€éœ€çš„æœ€å°‘å›åˆæ•¸

    # å¯¦ç¾çµ±ä¸€æ¥å£æ–¹æ³•
    async def calculate_metrics(
        self, data: List[Dict[str, Any]], algorithm: str, session_id: str
    ) -> StandardizedMetrics:
        """å¯¦ç¾çµ±ä¸€æ¥å£çš„æŒ‡æ¨™è¨ˆç®—æ–¹æ³•"""
        # è½‰æ›æ•¸æ“šæ ¼å¼
        episodes_data = []
        for item in data:
            if hasattr(item, "episode_number"):  # EpisodeData å°è±¡
                episodes_data.append(item)
            else:  # å­—å…¸æ ¼å¼
                episode_data = EpisodeData(
                    episode_number=item.get("episode_number", 0),
                    total_reward=item.get("total_reward", 0.0),
                    steps=item.get("steps", 0),
                    success=item.get("success", False),
                    handover_count=item.get("handover_count", 0),
                    average_latency=item.get("average_latency", 0.0),
                    timestamp=datetime.fromisoformat(
                        item.get("timestamp", datetime.now().isoformat())
                    ),
                )
                episodes_data.append(episode_data)

        # ä½¿ç”¨åŸæœ‰çš„è¨ˆç®—é‚è¼¯
        legacy_metrics = await self.calculate_training_metrics(
            session_id, algorithm, episodes_data
        )

        # è½‰æ›ç‚ºæ¨™æº–åŒ–æ ¼å¼
        return StandardizedMetrics(
            success_rate=self.standardizer.normalize_value(legacy_metrics.success_rate),
            stability=self.standardizer.normalize_value(legacy_metrics.stability),
            learning_efficiency=self.standardizer.normalize_value(
                legacy_metrics.learning_efficiency
            ),
            confidence_score=self.standardizer.normalize_value(
                legacy_metrics.confidence_score
            ),
            average_reward=legacy_metrics.average_reward,
            convergence_episode=legacy_metrics.convergence_episode,
            performance_trend=(
                TrendType(legacy_metrics.performance_trend)
                if legacy_metrics.performance_trend in [t.value for t in TrendType]
                else TrendType.INSUFFICIENT_DATA
            ),
            calculation_timestamp=datetime.now().isoformat(),
            data_points_count=len(episodes_data),
            calculation_method="real_training_data",
        )

    def get_metric_definition(self, metric_type: MetricType) -> MetricDefinition:
        """ç²å–æŒ‡æ¨™å®šç¾©"""
        return self.standardizer.METRIC_DEFINITIONS.get(metric_type)

    def validate_metrics(self, metrics: StandardizedMetrics) -> bool:
        """é©—è­‰æŒ‡æ¨™æœ‰æ•ˆæ€§"""
        return self.standardizer._validate_all_metrics(metrics)

    async def calculate_training_metrics(
        self, session_id: str, algorithm: str, episodes_data: List[EpisodeData]
    ) -> TrainingMetrics:
        """
        è¨ˆç®—çœŸå¯¦çš„è¨“ç·´æŒ‡æ¨™

        Args:
            session_id: è¨“ç·´æœƒè©±ID
            algorithm: ç®—æ³•åç¨±
            episodes_data: å›åˆæ•¸æ“šåˆ—è¡¨

        Returns:
            TrainingMetrics: è¨ˆç®—å¾—å‡ºçš„çœŸå¯¦æŒ‡æ¨™
        """
        self.logger.info(
            f"ğŸ§® [æŒ‡æ¨™è¨ˆç®—] é–‹å§‹è¨ˆç®— {algorithm} æœƒè©± {session_id} çš„çœŸå¯¦æŒ‡æ¨™"
        )

        if len(episodes_data) < self.min_episodes_for_metrics:
            self.logger.warning(
                f"âš ï¸ [æŒ‡æ¨™è¨ˆç®—] å›åˆæ•¸ä¸è¶³ ({len(episodes_data)} < {self.min_episodes_for_metrics})ï¼Œè¿”å›åˆå§‹æŒ‡æ¨™"
            )
            return self._get_initial_metrics()

        try:
            # æå–åŸºç¤æ•¸æ“š
            rewards = [ep.total_reward for ep in episodes_data]
            successes = [ep.success for ep in episodes_data]
            latencies = [ep.average_latency for ep in episodes_data]

            # è¨ˆç®—å„é …æŒ‡æ¨™
            success_rate = await self._calculate_success_rate(successes, rewards)
            stability = await self._calculate_stability(rewards)
            average_reward = statistics.mean(rewards)
            convergence_episode = await self._detect_convergence(rewards)
            learning_efficiency = await self._calculate_learning_efficiency(rewards)
            performance_trend = await self._analyze_performance_trend(rewards)
            confidence_score = await self._calculate_confidence_score(episodes_data)

            metrics = TrainingMetrics(
                success_rate=success_rate,
                stability=stability,
                average_reward=average_reward,
                convergence_episode=convergence_episode,
                learning_efficiency=learning_efficiency,
                performance_trend=performance_trend,
                confidence_score=confidence_score,
            )

            self.logger.info(
                f"âœ… [æŒ‡æ¨™è¨ˆç®—] {algorithm} æŒ‡æ¨™è¨ˆç®—å®Œæˆ: æˆåŠŸç‡={success_rate:.3f}, ç©©å®šæ€§={stability:.3f}"
            )
            return metrics

        except Exception as e:
            self.logger.error(f"âŒ [æŒ‡æ¨™è¨ˆç®—] è¨ˆç®—å¤±æ•—: {e}")
            return self._get_error_metrics()

    async def _calculate_success_rate(
        self, successes: List[bool], rewards: List[float]
    ) -> float:
        """
        è¨ˆç®—çœŸå¯¦æˆåŠŸç‡
        çµåˆå¸ƒçˆ¾æˆåŠŸæ¨™è¨˜å’Œçå‹µé–¾å€¼
        """
        if not successes and not rewards:
            return 0.0

        # æ–¹æ³•1: åŸºæ–¼æ˜ç¢ºçš„æˆåŠŸæ¨™è¨˜
        boolean_success_rate = sum(successes) / len(successes) if successes else 0.0

        # æ–¹æ³•2: åŸºæ–¼çå‹µé–¾å€¼
        reward_success_count = sum(
            1 for r in rewards if r >= self.success_reward_threshold
        )
        reward_success_rate = reward_success_count / len(rewards) if rewards else 0.0

        # æ–¹æ³•3: åŸºæ–¼çå‹µè¶¨å‹¢ï¼ˆæœ€è¿‘å›åˆè¡¨ç¾ï¼‰
        if len(rewards) >= 10:
            recent_rewards = rewards[-10:]
            recent_avg = statistics.mean(recent_rewards)
            overall_avg = statistics.mean(rewards)
            trend_success_rate = min(1.0, max(0.0, recent_avg / max(overall_avg, 1.0)))
        else:
            trend_success_rate = reward_success_rate

        # åŠ æ¬Šå¹³å‡ä¸‰ç¨®æ–¹æ³•
        if successes:  # å¦‚æœæœ‰æ˜ç¢ºçš„æˆåŠŸæ¨™è¨˜
            final_success_rate = (
                boolean_success_rate * 0.5
                + reward_success_rate * 0.3
                + trend_success_rate * 0.2
            )
        else:  # åªåŸºæ–¼çå‹µè¨ˆç®—
            final_success_rate = reward_success_rate * 0.7 + trend_success_rate * 0.3

        return max(0.0, min(1.0, final_success_rate))

    async def _calculate_stability(self, rewards: List[float]) -> float:
        """
        è¨ˆç®—è¨“ç·´ç©©å®šæ€§
        åŸºæ–¼çå‹µçš„è®Šç•°ä¿‚æ•¸å’Œæ»‘å‹•çª—å£æ¨™æº–å·®
        """
        if len(rewards) < 3:
            return 0.0

        # æ–¹æ³•1: æ•´é«”è®Šç•°ä¿‚æ•¸
        mean_reward = statistics.mean(rewards)
        if mean_reward == 0:
            overall_cv = float("inf")
        else:
            std_reward = statistics.stdev(rewards)
            overall_cv = std_reward / abs(mean_reward)

        # æ–¹æ³•2: æ»‘å‹•çª—å£ç©©å®šæ€§
        window_size = min(self.stability_window_size, len(rewards) // 2)
        if window_size < 3:
            window_stability = 1.0 - min(1.0, overall_cv)
        else:
            window_stds = []
            for i in range(window_size, len(rewards)):
                window_rewards = rewards[i - window_size : i]
                window_std = statistics.stdev(window_rewards)
                window_stds.append(window_std)

            if window_stds:
                avg_window_std = statistics.mean(window_stds)
                window_stability = 1.0 - min(
                    1.0, avg_window_std / max(abs(mean_reward), 1.0)
                )
            else:
                window_stability = 1.0 - min(1.0, overall_cv)

        # æ–¹æ³•3: è¶¨å‹¢ç©©å®šæ€§ï¼ˆæœ€è¿‘æ˜¯å¦ç©©å®šï¼‰
        if len(rewards) >= 10:
            recent_rewards = rewards[-10:]
            recent_std = statistics.stdev(recent_rewards)
            recent_mean = statistics.mean(recent_rewards)
            trend_stability = 1.0 - min(1.0, recent_std / max(abs(recent_mean), 1.0))
        else:
            trend_stability = window_stability

        # åŠ æ¬Šå¹³å‡
        final_stability = (
            (1.0 - min(1.0, overall_cv)) * 0.3
            + window_stability * 0.4
            + trend_stability * 0.3
        )

        return max(0.0, min(1.0, final_stability))

    async def _detect_convergence(self, rewards: List[float]) -> Optional[int]:
        """
        æª¢æ¸¬æ”¶æ–‚é»
        åŸºæ–¼çå‹µè®ŠåŒ–ç‡å’Œç©©å®šæ€§
        """
        if len(rewards) < self.convergence_patience * 2:
            return None

        # è¨ˆç®—æ»‘å‹•å¹³å‡
        window_size = self.convergence_patience
        moving_averages = []

        for i in range(window_size, len(rewards)):
            window_avg = statistics.mean(rewards[i - window_size : i])
            moving_averages.append(window_avg)

        # æª¢æ¸¬æ”¶æ–‚ï¼šé€£çºŒå¤šå€‹çª—å£çš„è®ŠåŒ–ç‡å°æ–¼é–¾å€¼
        convergence_threshold = 0.05  # 5% è®ŠåŒ–ç‡é–¾å€¼
        stable_windows = 0

        for i in range(1, len(moving_averages)):
            if len(moving_averages) > i:
                change_rate = abs(moving_averages[i] - moving_averages[i - 1]) / max(
                    abs(moving_averages[i - 1]), 1.0
                )
                if change_rate < convergence_threshold:
                    stable_windows += 1
                    if stable_windows >= self.convergence_patience // 2:
                        convergence_episode = window_size + i - stable_windows
                        return convergence_episode
                else:
                    stable_windows = 0

        return None

    async def _calculate_learning_efficiency(self, rewards: List[float]) -> float:
        """
        è¨ˆç®—å­¸ç¿’æ•ˆç‡
        åŸºæ–¼çå‹µæ”¹å–„é€Ÿåº¦å’Œæ”¶æ–‚é€Ÿåº¦
        """
        if len(rewards) < 5:
            return 0.0

        # è¨ˆç®—æ”¹å–„é€Ÿåº¦
        initial_avg = statistics.mean(rewards[:5])
        final_avg = statistics.mean(rewards[-5:])
        improvement = final_avg - initial_avg

        # æ­£è¦åŒ–æ”¹å–„å¹…åº¦
        max_possible_improvement = max(abs(max(rewards)), abs(min(rewards)), 100.0)
        normalized_improvement = improvement / max_possible_improvement

        # è¨ˆç®—å­¸ç¿’é€Ÿåº¦ï¼ˆæ—©æœŸæ”¹å–„ï¼‰
        if len(rewards) >= 20:
            early_improvement = statistics.mean(rewards[10:15]) - statistics.mean(
                rewards[:5]
            )
            early_speed = early_improvement / max_possible_improvement
        else:
            early_speed = normalized_improvement

        # ç¶œåˆæ•ˆç‡åˆ†æ•¸
        efficiency = normalized_improvement * 0.6 + early_speed * 0.4
        return max(0.0, min(1.0, (efficiency + 1.0) / 2.0))  # è½‰æ›åˆ° [0,1] ç¯„åœ

    async def _analyze_performance_trend(self, rewards: List[float]) -> str:
        """
        åˆ†ææ€§èƒ½è¶¨å‹¢
        """
        if len(rewards) < 10:
            return "insufficient_data"

        # æ¯”è¼ƒå‰åŠæ®µå’Œå¾ŒåŠæ®µ
        mid_point = len(rewards) // 2
        first_half_avg = statistics.mean(rewards[:mid_point])
        second_half_avg = statistics.mean(rewards[mid_point:])

        # æ¯”è¼ƒæœ€è¿‘10å€‹å›åˆå’Œä¹‹å‰çš„å¹³å‡
        recent_avg = statistics.mean(rewards[-10:])
        earlier_avg = statistics.mean(rewards[:-10])

        improvement_threshold = 0.1  # 10% æ”¹å–„é–¾å€¼

        long_term_improvement = (second_half_avg - first_half_avg) / max(
            abs(first_half_avg), 1.0
        )
        recent_improvement = (recent_avg - earlier_avg) / max(abs(earlier_avg), 1.0)

        if (
            long_term_improvement > improvement_threshold
            and recent_improvement > improvement_threshold
        ):
            return "improving"
        elif (
            abs(long_term_improvement) <= improvement_threshold
            and abs(recent_improvement) <= improvement_threshold
        ):
            return "stable"
        elif (
            long_term_improvement < -improvement_threshold
            or recent_improvement < -improvement_threshold
        ):
            return "declining"
        else:
            return "mixed"

    async def _calculate_confidence_score(
        self, episodes_data: List[EpisodeData]
    ) -> float:
        """
        è¨ˆç®—æŒ‡æ¨™å¯ä¿¡åº¦åˆ†æ•¸
        åŸºæ–¼æ•¸æ“šé‡ã€æ•¸æ“šè³ªé‡å’Œæ™‚é–“è·¨åº¦
        """
        if not episodes_data:
            return 0.0

        # æ•¸æ“šé‡åˆ†æ•¸
        data_volume_score = min(1.0, len(episodes_data) / 50.0)  # 50å€‹å›åˆç‚ºæ»¿åˆ†

        # æ•¸æ“šè³ªé‡åˆ†æ•¸ï¼ˆåŸºæ–¼çå‹µåˆ†ä½ˆçš„åˆç†æ€§ï¼‰
        rewards = [ep.total_reward for ep in episodes_data]
        if len(rewards) > 1:
            reward_range = max(rewards) - min(rewards)
            reward_std = statistics.stdev(rewards)
            # åˆç†çš„è®Šç•°æ€§è¡¨ç¤ºçœŸå¯¦çš„å­¸ç¿’éç¨‹
            quality_score = min(1.0, reward_std / max(reward_range, 1.0))
        else:
            quality_score = 0.5

        # æ™‚é–“è·¨åº¦åˆ†æ•¸
        if len(episodes_data) > 1:
            time_span = (
                episodes_data[-1].timestamp - episodes_data[0].timestamp
            ).total_seconds()
            time_score = min(1.0, time_span / 3600.0)  # 1å°æ™‚ç‚ºæ»¿åˆ†
        else:
            time_score = 0.1

        # ç¶œåˆå¯ä¿¡åº¦
        confidence = data_volume_score * 0.5 + quality_score * 0.3 + time_score * 0.2
        return max(0.1, min(1.0, confidence))

    def _get_initial_metrics(self) -> TrainingMetrics:
        """è¿”å›åˆå§‹æŒ‡æ¨™ï¼ˆæ•¸æ“šä¸è¶³æ™‚ï¼‰"""
        return TrainingMetrics(
            success_rate=0.0,
            stability=0.0,
            average_reward=0.0,
            convergence_episode=None,
            learning_efficiency=0.0,
            performance_trend="insufficient_data",
            confidence_score=0.1,
        )

    def _get_error_metrics(self) -> TrainingMetrics:
        """è¿”å›éŒ¯èª¤æŒ‡æ¨™ï¼ˆè¨ˆç®—å¤±æ•—æ™‚ï¼‰"""
        return TrainingMetrics(
            success_rate=0.0,
            stability=0.0,
            average_reward=0.0,
            convergence_episode=None,
            learning_efficiency=0.0,
            performance_trend="error",
            confidence_score=0.0,
        )


# å…¨å±€å¯¦ä¾‹
real_metrics_calculator = RealMetricsCalculator()


async def get_real_metrics_calculator() -> RealMetricsCalculator:
    """ç²å–çœŸå¯¦æŒ‡æ¨™è¨ˆç®—å™¨å¯¦ä¾‹"""
    return real_metrics_calculator
