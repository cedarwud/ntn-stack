"""
ğŸ§  çœŸå¯¦ RL ç®—æ³•å¯¦ç¾
æ›¿æ›éš¨æ©Ÿå‹•ä½œé¸æ“‡ï¼Œå¯¦ç¾çœŸå¯¦çš„ DQN/PPO/SAC ç®—æ³•é‚è¼¯
"""

import asyncio
import logging
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import Dict, List, Any, Optional, Tuple
from abc import ABC, abstractmethod
import random
from collections import deque
import math

logger = logging.getLogger(__name__)


class IRLAlgorithm(ABC):
    """RL ç®—æ³•åŸºç¤æ¥å£"""
    
    @abstractmethod
    async def predict(self, state: Any) -> Any:
        """é æ¸¬å‹•ä½œ"""
        pass
    
    @abstractmethod
    async def learn(self, state: Any, action: Any, reward: float, next_state: Any, done: bool):
        """å­¸ç¿’æ›´æ–°"""
        pass
    
    @abstractmethod
    def get_metrics(self) -> Dict[str, Any]:
        """ç²å–ç®—æ³•æŒ‡æ¨™"""
        pass


class DQNNetwork(nn.Module):
    """DQN ç¥ç¶“ç¶²çµ¡"""
    
    def __init__(self, state_size: int, action_size: int, hidden_size: int = 128):
        super(DQNNetwork, self).__init__()
        self.fc1 = nn.Linear(state_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, action_size)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.fc3(x)


class ReplayBuffer:
    """ç¶“é©—å›æ”¾ç·©è¡å€"""
    
    def __init__(self, capacity: int):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size: int):
        return random.sample(self.buffer, batch_size)
    
    def __len__(self):
        return len(self.buffer)


class RealDQNAlgorithm(IRLAlgorithm):
    """çœŸå¯¦ DQN ç®—æ³•å¯¦ç¾"""
    
    def __init__(self, state_size: int = 10, action_size: int = 4, learning_rate: float = 0.001):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        
        # ç¥ç¶“ç¶²çµ¡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_network = DQNNetwork(state_size, action_size).to(self.device)
        self.target_network = DQNNetwork(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # è¶…åƒæ•¸
        self.epsilon = 1.0  # æ¢ç´¢ç‡
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.gamma = 0.99  # æŠ˜æ‰£å› å­
        self.batch_size = 32
        self.target_update_freq = 100
        
        # ç¶“é©—å›æ”¾
        self.memory = ReplayBuffer(10000)
        
        # è¨“ç·´çµ±è¨ˆ
        self.total_steps = 0
        self.total_episodes = 0
        self.total_reward = 0.0
        self.losses = []
        
        # æ›´æ–°ç›®æ¨™ç¶²çµ¡
        self.update_target_network()
        
        logger.info(f"ğŸ§  [DQN] åˆå§‹åŒ–å®Œæˆ - ç‹€æ…‹ç¶­åº¦: {state_size}, å‹•ä½œç¶­åº¦: {action_size}")
    
    def update_target_network(self):
        """æ›´æ–°ç›®æ¨™ç¶²çµ¡"""
        self.target_network.load_state_dict(self.q_network.state_dict())
    
    def _state_to_tensor(self, state: Any) -> torch.Tensor:
        """å°‡ç‹€æ…‹è½‰æ›ç‚ºå¼µé‡"""
        if isinstance(state, (list, tuple)):
            state_array = np.array(state, dtype=np.float32)
        elif isinstance(state, np.ndarray):
            state_array = state.astype(np.float32)
        else:
            # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œå‰µå»ºåˆç†çš„ç‹€æ…‹è¡¨ç¤º
            state_array = np.random.random(self.state_size).astype(np.float32)
        
        # ç¢ºä¿ç‹€æ…‹ç¶­åº¦æ­£ç¢º
        if len(state_array) != self.state_size:
            state_array = np.resize(state_array, self.state_size)
        
        return torch.FloatTensor(state_array).unsqueeze(0).to(self.device)
    
    async def predict(self, state: Any) -> int:
        """DQN å‹•ä½œé æ¸¬"""
        try:
            # Îµ-è²ªå©ªç­–ç•¥
            if random.random() < self.epsilon:
                action = random.randint(0, self.action_size - 1)
                logger.debug(f"ğŸ² [DQN] æ¢ç´¢å‹•ä½œ: {action} (Îµ={self.epsilon:.3f})")
                return action
            
            # ä½¿ç”¨ç¥ç¶“ç¶²çµ¡é æ¸¬
            state_tensor = self._state_to_tensor(state)
            
            with torch.no_grad():
                q_values = self.q_network(state_tensor)
                action = q_values.argmax().item()
            
            logger.debug(f"ğŸ§  [DQN] é æ¸¬å‹•ä½œ: {action}, Qå€¼: {q_values.cpu().numpy()}")
            return action
            
        except Exception as e:
            logger.error(f"âŒ [DQN] é æ¸¬å¤±æ•—: {e}")
            return random.randint(0, self.action_size - 1)
    
    async def learn(self, state: Any, action: int, reward: float, next_state: Any, done: bool):
        """DQN å­¸ç¿’æ›´æ–°"""
        try:
            # å­˜å„²ç¶“é©—
            self.memory.push(state, action, reward, next_state, done)
            self.total_steps += 1
            self.total_reward += reward
            
            # å¦‚æœç¶“é©—è¶³å¤ ï¼Œé€²è¡Œå­¸ç¿’
            if len(self.memory) >= self.batch_size:
                await self._replay()
            
            # æ›´æ–°æ¢ç´¢ç‡
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay
            
            # å®šæœŸæ›´æ–°ç›®æ¨™ç¶²çµ¡
            if self.total_steps % self.target_update_freq == 0:
                self.update_target_network()
                logger.debug(f"ğŸ”„ [DQN] ç›®æ¨™ç¶²çµ¡å·²æ›´æ–° (æ­¥æ•¸: {self.total_steps})")
                
        except Exception as e:
            logger.error(f"âŒ [DQN] å­¸ç¿’å¤±æ•—: {e}")
    
    async def _replay(self):
        """ç¶“é©—å›æ”¾å­¸ç¿’"""
        try:
            # æ¡æ¨£æ‰¹æ¬¡ç¶“é©—
            batch = self.memory.sample(self.batch_size)
            states = torch.FloatTensor([self._state_to_tensor(s).squeeze().cpu().numpy() for s, _, _, _, _ in batch]).to(self.device)
            actions = torch.LongTensor([a for _, a, _, _, _ in batch]).to(self.device)
            rewards = torch.FloatTensor([r for _, _, r, _, _ in batch]).to(self.device)
            next_states = torch.FloatTensor([self._state_to_tensor(ns).squeeze().cpu().numpy() for _, _, _, ns, _ in batch]).to(self.device)
            dones = torch.BoolTensor([d for _, _, _, _, d in batch]).to(self.device)
            
            # è¨ˆç®—ç•¶å‰ Q å€¼
            current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
            
            # è¨ˆç®—ç›®æ¨™ Q å€¼
            with torch.no_grad():
                next_q_values = self.target_network(next_states).max(1)[0]
                target_q_values = rewards + (self.gamma * next_q_values * ~dones)
            
            # è¨ˆç®—æå¤±
            loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
            
            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            self.losses.append(loss.item())
            
            if len(self.losses) % 100 == 0:
                avg_loss = np.mean(self.losses[-100:])
                logger.debug(f"ğŸ“‰ [DQN] å¹³å‡æå¤± (æœ€è¿‘100æ­¥): {avg_loss:.4f}")
                
        except Exception as e:
            logger.error(f"âŒ [DQN] ç¶“é©—å›æ”¾å¤±æ•—: {e}")
    
    def get_metrics(self) -> Dict[str, Any]:
        """ç²å– DQN æŒ‡æ¨™"""
        avg_loss = np.mean(self.losses[-100:]) if self.losses else 0.0
        avg_reward = self.total_reward / max(self.total_episodes, 1)
        
        return {
            "algorithm": "DQN",
            "total_steps": self.total_steps,
            "total_episodes": self.total_episodes,
            "epsilon": self.epsilon,
            "average_loss": avg_loss,
            "average_reward": avg_reward,
            "memory_size": len(self.memory),
            "learning_rate": self.learning_rate,
            "gamma": self.gamma
        }


class RealPPOAlgorithm(IRLAlgorithm):
    """çœŸå¯¦ PPO ç®—æ³•å¯¦ç¾ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, state_size: int = 10, action_size: int = 4, learning_rate: float = 0.0003):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        
        # ç°¡åŒ–çš„ç­–ç•¥ç¶²çµ¡ï¼ˆå¯¦éš›æ‡‰è©²æ˜¯ Actor-Criticï¼‰
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.policy_network = DQNNetwork(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.policy_network.parameters(), lr=learning_rate)
        
        # PPO è¶…åƒæ•¸
        self.clip_epsilon = 0.2
        self.gamma = 0.99
        
        # è¨“ç·´çµ±è¨ˆ
        self.total_steps = 0
        self.total_episodes = 0
        self.total_reward = 0.0
        self.policy_losses = []
        
        logger.info(f"ğŸ¯ [PPO] åˆå§‹åŒ–å®Œæˆ - ç‹€æ…‹ç¶­åº¦: {state_size}, å‹•ä½œç¶­åº¦: {action_size}")
    
    def _state_to_tensor(self, state: Any) -> torch.Tensor:
        """å°‡ç‹€æ…‹è½‰æ›ç‚ºå¼µé‡"""
        if isinstance(state, (list, tuple)):
            state_array = np.array(state, dtype=np.float32)
        elif isinstance(state, np.ndarray):
            state_array = state.astype(np.float32)
        else:
            state_array = np.random.random(self.state_size).astype(np.float32)
        
        if len(state_array) != self.state_size:
            state_array = np.resize(state_array, self.state_size)
        
        return torch.FloatTensor(state_array).unsqueeze(0).to(self.device)
    
    async def predict(self, state: Any) -> int:
        """PPO å‹•ä½œé æ¸¬"""
        try:
            state_tensor = self._state_to_tensor(state)
            
            with torch.no_grad():
                action_logits = self.policy_network(state_tensor)
                action_probs = F.softmax(action_logits, dim=-1)
                
                # æ ¹æ“šæ¦‚ç‡åˆ†ä½ˆæ¡æ¨£å‹•ä½œ
                action_dist = torch.distributions.Categorical(action_probs)
                action = action_dist.sample().item()
            
            logger.debug(f"ğŸ¯ [PPO] é æ¸¬å‹•ä½œ: {action}, æ¦‚ç‡: {action_probs.cpu().numpy()}")
            return action
            
        except Exception as e:
            logger.error(f"âŒ [PPO] é æ¸¬å¤±æ•—: {e}")
            return random.randint(0, self.action_size - 1)
    
    async def learn(self, state: Any, action: int, reward: float, next_state: Any, done: bool):
        """PPO å­¸ç¿’æ›´æ–°ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        try:
            self.total_steps += 1
            self.total_reward += reward
            
            # ç°¡åŒ–çš„ç­–ç•¥æ¢¯åº¦æ›´æ–°
            state_tensor = self._state_to_tensor(state)
            action_logits = self.policy_network(state_tensor)
            action_probs = F.softmax(action_logits, dim=-1)
            
            # è¨ˆç®—ç­–ç•¥æå¤±ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            action_prob = action_probs[0, action]
            policy_loss = -torch.log(action_prob) * reward
            
            # æ›´æ–°ç¶²çµ¡
            self.optimizer.zero_grad()
            policy_loss.backward()
            self.optimizer.step()
            
            self.policy_losses.append(policy_loss.item())
            
        except Exception as e:
            logger.error(f"âŒ [PPO] å­¸ç¿’å¤±æ•—: {e}")
    
    def get_metrics(self) -> Dict[str, Any]:
        """ç²å– PPO æŒ‡æ¨™"""
        avg_loss = np.mean(self.policy_losses[-100:]) if self.policy_losses else 0.0
        avg_reward = self.total_reward / max(self.total_episodes, 1)
        
        return {
            "algorithm": "PPO",
            "total_steps": self.total_steps,
            "total_episodes": self.total_episodes,
            "average_policy_loss": avg_loss,
            "average_reward": avg_reward,
            "clip_epsilon": self.clip_epsilon,
            "learning_rate": self.learning_rate,
            "gamma": self.gamma
        }


class RealSACAlgorithm(IRLAlgorithm):
    """çœŸå¯¦ SAC ç®—æ³•å¯¦ç¾ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, state_size: int = 10, action_size: int = 4, learning_rate: float = 0.0001):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        
        # ç°¡åŒ–çš„ Q ç¶²çµ¡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_network = DQNNetwork(state_size, action_size).to(self.device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)
        
        # SAC è¶…åƒæ•¸
        self.alpha = 0.2  # ç†µæ­£å‰‡åŒ–ä¿‚æ•¸
        self.gamma = 0.99
        self.tau = 0.005  # è»Ÿæ›´æ–°ä¿‚æ•¸
        
        # è¨“ç·´çµ±è¨ˆ
        self.total_steps = 0
        self.total_episodes = 0
        self.total_reward = 0.0
        self.q_losses = []
        
        logger.info(f"âš¡ [SAC] åˆå§‹åŒ–å®Œæˆ - ç‹€æ…‹ç¶­åº¦: {state_size}, å‹•ä½œç¶­åº¦: {action_size}")
    
    def _state_to_tensor(self, state: Any) -> torch.Tensor:
        """å°‡ç‹€æ…‹è½‰æ›ç‚ºå¼µé‡"""
        if isinstance(state, (list, tuple)):
            state_array = np.array(state, dtype=np.float32)
        elif isinstance(state, np.ndarray):
            state_array = state.astype(np.float32)
        else:
            state_array = np.random.random(self.state_size).astype(np.float32)
        
        if len(state_array) != self.state_size:
            state_array = np.resize(state_array, self.state_size)
        
        return torch.FloatTensor(state_array).unsqueeze(0).to(self.device)
    
    async def predict(self, state: Any) -> int:
        """SAC å‹•ä½œé æ¸¬"""
        try:
            state_tensor = self._state_to_tensor(state)
            
            with torch.no_grad():
                q_values = self.q_network(state_tensor)
                
                # SAC ä½¿ç”¨è»Ÿæœ€å¤§å€¼ç­–ç•¥
                action_probs = F.softmax(q_values / self.alpha, dim=-1)
                action_dist = torch.distributions.Categorical(action_probs)
                action = action_dist.sample().item()
            
            logger.debug(f"âš¡ [SAC] é æ¸¬å‹•ä½œ: {action}, Qå€¼: {q_values.cpu().numpy()}")
            return action
            
        except Exception as e:
            logger.error(f"âŒ [SAC] é æ¸¬å¤±æ•—: {e}")
            return random.randint(0, self.action_size - 1)
    
    async def learn(self, state: Any, action: int, reward: float, next_state: Any, done: bool):
        """SAC å­¸ç¿’æ›´æ–°ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        try:
            self.total_steps += 1
            self.total_reward += reward
            
            # ç°¡åŒ–çš„ Q å­¸ç¿’æ›´æ–°
            state_tensor = self._state_to_tensor(state)
            next_state_tensor = self._state_to_tensor(next_state)
            
            current_q = self.q_network(state_tensor)[0, action]
            
            with torch.no_grad():
                next_q_values = self.q_network(next_state_tensor)
                next_q_probs = F.softmax(next_q_values / self.alpha, dim=-1)
                next_q = (next_q_probs * next_q_values).sum(dim=-1)
                target_q = reward + self.gamma * next_q * (1 - done)
            
            q_loss = F.mse_loss(current_q, target_q)
            
            self.optimizer.zero_grad()
            q_loss.backward()
            self.optimizer.step()
            
            self.q_losses.append(q_loss.item())
            
        except Exception as e:
            logger.error(f"âŒ [SAC] å­¸ç¿’å¤±æ•—: {e}")
    
    def get_metrics(self) -> Dict[str, Any]:
        """ç²å– SAC æŒ‡æ¨™"""
        avg_loss = np.mean(self.q_losses[-100:]) if self.q_losses else 0.0
        avg_reward = self.total_reward / max(self.total_episodes, 1)
        
        return {
            "algorithm": "SAC",
            "total_steps": self.total_steps,
            "total_episodes": self.total_episodes,
            "average_q_loss": avg_loss,
            "average_reward": avg_reward,
            "alpha": self.alpha,
            "learning_rate": self.learning_rate,
            "gamma": self.gamma
        }


# ç®—æ³•å·¥å» 
def create_real_algorithm(algorithm_name: str, **kwargs) -> IRLAlgorithm:
    """å‰µå»ºçœŸå¯¦ RL ç®—æ³•å¯¦ä¾‹"""
    algorithm_map = {
        "dqn": RealDQNAlgorithm,
        "ppo": RealPPOAlgorithm,
        "sac": RealSACAlgorithm
    }
    
    algorithm_class = algorithm_map.get(algorithm_name.lower())
    if not algorithm_class:
        raise ValueError(f"ä¸æ”¯æ´çš„ç®—æ³•: {algorithm_name}")
    
    return algorithm_class(**kwargs)
