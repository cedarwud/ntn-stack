"""
ğŸ§  æ€§èƒ½ç›£æ§æ¥å£

ç ”ç©¶ç´šæ€§èƒ½ç›£æ§ç³»çµ±ï¼Œæ”¯æ´ï¼š
- å¯¦æ™‚æŒ‡æ¨™æ”¶é›†
- çµ±è¨ˆåˆ†æ
- åŸºæº–æ¯”è¼ƒ  
- è«–æ–‡æ•¸æ“šç”Ÿæˆ
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum

from .rl_algorithm import ScenarioType


class MetricType(str, Enum):
    """æŒ‡æ¨™é¡å‹"""
    REWARD = "reward"
    LATENCY = "latency"
    THROUGHPUT = "throughput" 
    SUCCESS_RATE = "success_rate"
    CONVERGENCE_TIME = "convergence_time"
    MEMORY_USAGE = "memory_usage"
    CPU_USAGE = "cpu_usage"
    GPU_USAGE = "gpu_usage"
    HANDOVER_COUNT = "handover_count"
    PACKET_LOSS = "packet_loss"


class AggregationType(str, Enum):
    """èšåˆé¡å‹"""
    MEAN = "mean"
    MEDIAN = "median"
    STD = "std"
    MIN = "min"
    MAX = "max"
    PERCENTILE_95 = "p95"
    PERCENTILE_99 = "p99"
    COUNT = "count"
    SUM = "sum"


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ¨™"""
    metric_name: str
    metric_type: MetricType
    value: float
    unit: str
    timestamp: datetime
    algorithm_name: str
    scenario_type: ScenarioType
    session_id: Optional[str] = None
    episode_number: Optional[int] = None
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class StatisticalSummary:
    """çµ±è¨ˆæ‘˜è¦"""
    metric_type: MetricType
    count: int
    mean: float
    std: float
    min_value: float
    max_value: float
    percentile_25: float
    percentile_50: float  # median
    percentile_75: float
    percentile_95: float
    percentile_99: float
    confidence_interval_95: Tuple[float, float]


@dataclass
class BaselineComparison:
    """åŸºæº–æ¯”è¼ƒçµæœ"""
    our_algorithm: str
    baseline_algorithm: str
    metric_type: MetricType
    our_performance: StatisticalSummary
    baseline_performance: StatisticalSummary
    improvement_percent: float
    statistical_significance: float  # p-value
    effect_size: float  # Cohen's d
    is_statistically_significant: bool
    comparison_notes: str


@dataclass
class ConvergenceAnalysis:
    """æ”¶æ–‚æ€§åˆ†æ"""
    algorithm_name: str
    scenario_type: ScenarioType
    convergence_episode: Optional[int]
    convergence_threshold: float
    final_performance: float
    convergence_rate: float  # episodes per unit improvement
    stability_score: float  # variance in final episodes
    plateau_detection: bool


class IPerformanceMonitor(ABC):
    """æ€§èƒ½ç›£æ§æ ¸å¿ƒæ¥å£
    
    æä¾›ç ”ç©¶ç´šçš„æ€§èƒ½ç›£æ§å’Œåˆ†æåŠŸèƒ½ï¼Œ
    æ”¯æ´å­¸è¡“è«–æ–‡æ•¸æ“šç”Ÿæˆå’ŒåŸºæº–æ¯”è¼ƒã€‚
    """
    
    @abstractmethod
    async def record_metric(self, metric: PerformanceMetric) -> bool:
        """è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
        
        Args:
            metric: æ€§èƒ½æŒ‡æ¨™
            
        Returns:
            bool: æ˜¯å¦è¨˜éŒ„æˆåŠŸ
        """
        pass
    
    @abstractmethod
    async def record_metrics_batch(self, metrics: List[PerformanceMetric]) -> int:
        """æ‰¹é‡è¨˜éŒ„æŒ‡æ¨™
        
        Args:
            metrics: æŒ‡æ¨™åˆ—è¡¨
            
        Returns:
            int: æˆåŠŸè¨˜éŒ„çš„æŒ‡æ¨™æ•¸é‡
        """
        pass
    
    @abstractmethod
    async def get_performance_summary(
        self, 
        algorithm_name: str,
        metric_types: Optional[List[MetricType]] = None,
        scenario_type: Optional[ScenarioType] = None,
        time_range: Optional[Tuple[datetime, datetime]] = None
    ) -> Dict[MetricType, StatisticalSummary]:
        """ç²å–æ€§èƒ½æ‘˜è¦
        
        Args:
            algorithm_name: ç®—æ³•åç¨±
            metric_types: æŒ‡æ¨™é¡å‹åˆ—è¡¨
            scenario_type: å ´æ™¯é¡å‹
            time_range: æ™‚é–“ç¯„åœ
            
        Returns:
            Dict[MetricType, StatisticalSummary]: çµ±è¨ˆæ‘˜è¦
        """
        pass
    
    @abstractmethod
    async def compare_with_baseline(
        self,
        our_algorithm: str,
        baseline_algorithm: str,
        metric_types: List[MetricType],
        scenario_type: Optional[ScenarioType] = None
    ) -> List[BaselineComparison]:
        """èˆ‡åŸºæº–ç®—æ³•æ¯”è¼ƒ
        
        Args:
            our_algorithm: æˆ‘å€‘çš„ç®—æ³•
            baseline_algorithm: åŸºæº–ç®—æ³•  
            metric_types: æ¯”è¼ƒçš„æŒ‡æ¨™é¡å‹
            scenario_type: å ´æ™¯é¡å‹
            
        Returns:
            List[BaselineComparison]: æ¯”è¼ƒçµæœ
        """
        pass
    
    @abstractmethod
    async def analyze_convergence(
        self,
        algorithm_name: str,
        session_ids: Optional[List[str]] = None,
        convergence_threshold: float = 0.95
    ) -> List[ConvergenceAnalysis]:
        """åˆ†ææ”¶æ–‚æ€§
        
        Args:
            algorithm_name: ç®—æ³•åç¨±
            session_ids: æœƒè©±IDåˆ—è¡¨
            convergence_threshold: æ”¶æ–‚é–¾å€¼
            
        Returns:
            List[ConvergenceAnalysis]: æ”¶æ–‚åˆ†æçµæœ
        """
        pass
    
    @abstractmethod
    async def get_real_time_metrics(
        self,
        algorithm_name: str,
        time_window_minutes: int = 5
    ) -> Dict[MetricType, List[PerformanceMetric]]:
        """ç²å–å¯¦æ™‚æŒ‡æ¨™
        
        Args:
            algorithm_name: ç®—æ³•åç¨±
            time_window_minutes: æ™‚é–“çª—å£ï¼ˆåˆ†é˜ï¼‰
            
        Returns:
            Dict[MetricType, List[PerformanceMetric]]: å¯¦æ™‚æŒ‡æ¨™
        """
        pass
    
    @abstractmethod
    async def export_metrics_for_paper(
        self,
        algorithm_names: List[str],
        metric_types: List[MetricType],
        format_type: str = "csv",  # csv, json, latex
        include_statistical_tests: bool = True
    ) -> str:
        """åŒ¯å‡ºè«–æ–‡æ•¸æ“š
        
        Args:
            algorithm_names: ç®—æ³•åç¨±åˆ—è¡¨
            metric_types: æŒ‡æ¨™é¡å‹åˆ—è¡¨
            format_type: åŒ¯å‡ºæ ¼å¼
            include_statistical_tests: æ˜¯å¦åŒ…å«çµ±è¨ˆæª¢é©—
            
        Returns:
            str: åŒ¯å‡ºçš„æª”æ¡ˆè·¯å¾‘
        """
        pass
    
    @abstractmethod
    async def generate_performance_report(
        self,
        algorithm_names: List[str],
        report_type: str = "comprehensive"  # comprehensive, summary, comparison
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½å ±å‘Š
        
        Args:
            algorithm_names: ç®—æ³•åç¨±åˆ—è¡¨
            report_type: å ±å‘Šé¡å‹
            
        Returns:
            Dict[str, Any]: æ€§èƒ½å ±å‘Š
        """
        pass
    
    @abstractmethod
    async def set_performance_alert(
        self,
        algorithm_name: str,
        metric_type: MetricType,
        threshold: float,
        comparison_operator: str = "less_than"  # less_than, greater_than, equals
    ) -> str:
        """è¨­å®šæ€§èƒ½è­¦å ±
        
        Args:
            algorithm_name: ç®—æ³•åç¨±
            metric_type: æŒ‡æ¨™é¡å‹
            threshold: é–¾å€¼
            comparison_operator: æ¯”è¼ƒé‹ç®—ç¬¦
            
        Returns:
            str: è­¦å ±ID
        """
        pass
    
    @abstractmethod
    async def get_trending_analysis(
        self,
        algorithm_name: str,
        metric_type: MetricType,
        time_period_days: int = 7
    ) -> Dict[str, Any]:
        """ç²å–è¶¨å‹¢åˆ†æ
        
        Args:
            algorithm_name: ç®—æ³•åç¨±
            metric_type: æŒ‡æ¨™é¡å‹
            time_period_days: æ™‚é–“å‘¨æœŸï¼ˆå¤©ï¼‰
            
        Returns:
            Dict[str, Any]: è¶¨å‹¢åˆ†æçµæœ
        """
        pass


class IMetricsCollector(ABC):
    """æŒ‡æ¨™æ”¶é›†å™¨æ¥å£"""
    
    @abstractmethod
    async def collect_system_metrics(self) -> List[PerformanceMetric]:
        """æ”¶é›†ç³»çµ±æŒ‡æ¨™"""
        pass
    
    @abstractmethod
    async def collect_algorithm_metrics(self, algorithm_name: str) -> List[PerformanceMetric]:
        """æ”¶é›†ç®—æ³•æŒ‡æ¨™"""
        pass
    
    @abstractmethod
    async def collect_network_metrics(self) -> List[PerformanceMetric]:
        """æ”¶é›†ç¶²è·¯æŒ‡æ¨™"""
        pass


class IMetricsAggregator(ABC):
    """æŒ‡æ¨™èšåˆå™¨æ¥å£"""
    
    @abstractmethod
    async def aggregate_by_time(
        self,
        metrics: List[PerformanceMetric],
        time_interval: timedelta,
        aggregation_type: AggregationType
    ) -> List[PerformanceMetric]:
        """æŒ‰æ™‚é–“èšåˆæŒ‡æ¨™"""
        pass
    
    @abstractmethod
    async def aggregate_by_scenario(
        self,
        metrics: List[PerformanceMetric],
        aggregation_type: AggregationType
    ) -> Dict[ScenarioType, PerformanceMetric]:
        """æŒ‰å ´æ™¯èšåˆæŒ‡æ¨™"""
        pass


class IAlertManager(ABC):
    """è­¦å ±ç®¡ç†å™¨æ¥å£"""
    
    @abstractmethod
    async def trigger_alert(self, alert_id: str, metric: PerformanceMetric):
        """è§¸ç™¼è­¦å ±"""
        pass
    
    @abstractmethod
    async def resolve_alert(self, alert_id: str):
        """è§£æ±ºè­¦å ±"""
        pass


# ç•°å¸¸å®šç¾©
class PerformanceMonitorError(Exception):
    """æ€§èƒ½ç›£æ§åŸºç¤ç•°å¸¸"""
    pass


class MetricRecordingError(PerformanceMonitorError):
    """æŒ‡æ¨™è¨˜éŒ„ç•°å¸¸"""
    pass


class StatisticalAnalysisError(PerformanceMonitorError):
    """çµ±è¨ˆåˆ†æç•°å¸¸"""
    pass


class BaselineNotFoundError(PerformanceMonitorError):
    """åŸºæº–æ•¸æ“šä¸å­˜åœ¨ç•°å¸¸"""
    pass


class InsufficientDataError(PerformanceMonitorError):
    """æ•¸æ“šä¸è¶³ç•°å¸¸"""
    pass