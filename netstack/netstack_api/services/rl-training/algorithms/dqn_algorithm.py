import gymnasium as gym
from typing import Dict, Any, List
import random
import time

from ..interfaces.rl_algorithm import IRLAlgorithm, ScenarioType


class DQNAlgorithm(IRLAlgorithm):
    """
    ä¸€å€‹DQNæ¼”ç®—æ³•çš„ç°¡å–®æ¨¡æ“¬å¯¦ä½œã€‚
    å®ƒä¸¦ä¸é€²è¡ŒçœŸæ­£çš„ç¥ç¶“ç¶²è·¯è¨“ç·´ï¼Œè€Œæ˜¯æ¨¡æ“¬ä¸€å€‹è¨“ç·´éç¨‹ï¼Œ
    ä»¥ä¾¿æˆ‘å€‘å¯ä»¥å°ˆæ³¨æ–¼æ‰“é€šæ•´å€‹ç³»çµ±çš„å‰å¾Œç«¯å’Œ API æµç¨‹ã€‚
    """

    def __init__(self, env_name: str, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–DQNæ¼”ç®—æ³•æ¨¡æ“¬å™¨

        Args:
            env_name (str): è¦ä½¿ç”¨çš„gymnasiumç’°å¢ƒåç¨±ã€‚
            config (Dict[str, Any]): æ¼”ç®—æ³•çš„è¶…åƒæ•¸ã€‚
        """
        self.env_name = env_name
        self.config = config
        self.is_training = False
        self._current_episode = 0
        self._total_episodes = config.get("total_episodes", 100)
        self._last_reward = 0.0
        self._average_reward = 0.0
        self._loss = 1.0  # Initial loss

        # å˜—è©¦åˆå§‹åŒ–ç’°å¢ƒä»¥ç¢ºä¿å…¶å­˜åœ¨
        try:
            self.env = gym.make(env_name)
        except Exception as e:
            # åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œé€™è£¡æ‡‰è©²æœ‰æ›´å¥å£¯çš„éŒ¯èª¤è™•ç†
            print(f"ç„¡æ³•å»ºç«‹ç’°å¢ƒ '{env_name}': {e}")
            raise

    def get_name(self) -> str:
        """ç²å–ç®—æ³•åç¨±"""
        return "DQN (Deep Q-Network)"

    def get_supported_scenarios(self) -> List[ScenarioType]:
        """ç²å–æ”¯æŒçš„å ´æ™¯é¡å‹"""
        return [
            ScenarioType.URBAN,
            ScenarioType.SUBURBAN,
            ScenarioType.LOW_LATENCY,
            ScenarioType.HIGH_MOBILITY
        ]

    async def predict(self, state: Any) -> Any:
        """åŸ·è¡Œé æ¸¬"""
        # ç°¡å–®çš„éš¨æ©Ÿé æ¸¬ä½œç‚ºæ¼”ç¤º
        return random.randint(0, 3)

    def train(self) -> None:
        """
        æ¨¡æ“¬ä¸€å€‹è¨“ç·´å¾ªç’°ã€‚
        åœ¨çœŸå¯¦çš„å¯¦ç¾ä¸­ï¼Œé€™è£¡æœƒæ˜¯èˆ‡ç’°å¢ƒäº’å‹•ã€æ›´æ–°ç¶²è·¯æ¬Šé‡ç­‰è¤‡é›œé‚è¼¯ã€‚
        """
        print(f"ğŸ” DEBUG [DQN]: é–‹å§‹è¨“ç·´æ­¥é©Ÿï¼Œç•¶å‰ç‹€æ…‹: is_training={self.is_training}, episode={self._current_episode}/{self._total_episodes}")
        
        if not self.is_training:
            print(f"ğŸ” DEBUG [DQN]: åˆå§‹åŒ–è¨“ç·´ç‹€æ…‹")
            self.is_training = True
            self._current_episode = 0

        if self._current_episode < self._total_episodes:
            print(f"ğŸ” DEBUG [DQN]: åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ {self._current_episode + 1}")
            
            # æ¨¡æ“¬ä¸€å€‹ step çš„è€—æ™‚ - ä½¿ç”¨é…ç½®çš„ step_time
            step_time = self.config.get("step_time", 1.0)  # é»˜è®¤1ç§’
            print(f"â±ï¸ DEBUG [DQN]: ç­‰å¾… {step_time} ç§’...")
            time.sleep(step_time)

            self._current_episode += 1
            # æ¨¡æ“¬çå‹µå’Œæå¤±çš„è®ŠåŒ–
            self._last_reward = random.uniform(-10, 10)
            self._average_reward = (
                self._average_reward * (self._current_episode - 1) + self._last_reward
            ) / self._current_episode
            self._loss *= 0.95  # æ¨¡æ“¬æå¤±ä¸‹é™
            
            print(f"ğŸ“Š DEBUG [DQN]: è¨“ç·´æ­¥é©Ÿå®Œæˆ {self._current_episode}/{self._total_episodes}, çå‹µ: {self._last_reward:.2f}")
        else:
            print(f"âœ… DEBUG [DQN]: è¨“ç·´å®Œæˆï¼Œä½†ä¿æŒç‹€æ…‹è®“å‰ç«¯é¡¯ç¤ºæœ€çµ‚çµæœ")
            # ä¸ç«‹å³è¨­ç½® is_training = Falseï¼Œè®“è¨“ç·´å¾ªç’°æ§åˆ¶æœ€çµ‚ç‹€æ…‹

    def get_status(self) -> Dict[str, Any]:
        """
        ç²å–ç›®å‰è¨“ç·´ç‹€æ…‹
        """
        return {
            "is_training": self.is_training,
            "algorithm": "DQN",
            "environment": self.env_name,
            "episode": self._current_episode,
            "total_episodes": self._total_episodes,
            "last_reward": self._last_reward,
            "average_reward": self._average_reward,
            "loss": self._loss,
            "progress": (
                (self._current_episode / self._total_episodes) * 100
                if self._total_episodes > 0
                else 0
            ),
        }

    def stop_training(self) -> None:
        """
        åœæ­¢è¨“ç·´éç¨‹
        """
        self.is_training = False

    def load_model(self, model_path: str) -> bool:
        """åŠ è¼‰æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æª”æ¡ˆè·¯å¾‘
            
        Returns:
            bool: æ˜¯å¦åŠ è¼‰æˆåŠŸ
        """
        # æ¨¡æ“¬æ¨¡å‹åŠ è¼‰
        print(f"æ¨¡æ“¬åŠ è¼‰æ¨¡å‹: {model_path}")
        return True

    def save_model(self, model_path: str) -> bool:
        """ä¿å­˜æ¨¡å‹
        
        Args:
            model_path: ä¿å­˜è·¯å¾‘
            
        Returns:
            bool: æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        # æ¨¡æ“¬æ¨¡å‹ä¿å­˜
        print(f"æ¨¡æ“¬ä¿å­˜æ¨¡å‹: {model_path}")
        return True

    def get_hyperparameters(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰è¶…åƒæ•¸"""
        return self.config.copy()

    def set_hyperparameters(self, params: Dict[str, Any]) -> bool:
        """è¨­å®šè¶…åƒæ•¸"""
        self.config.update(params)
        return True

    def get_training_metrics(self) -> Dict[str, Any]:
        """ç²å–è¨“ç·´æŒ‡æ¨™"""
        return {
            "episode": self._current_episode,
            "total_episodes": self._total_episodes,
            "last_reward": self._last_reward,
            "average_reward": self._average_reward,
            "loss": self._loss,
            "progress": (
                (self._current_episode / self._total_episodes) * 100
                if self._total_episodes > 0
                else 0
            ),
        }

    def is_trained(self) -> bool:
        """æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²è¨“ç·´"""
        return self._current_episode >= self._total_episodes

    def get_memory_usage(self) -> Dict[str, float]:
        """ç²å–è¨˜æ†¶é«”ä½¿ç”¨é‡"""
        # æ¨¡æ“¬è¨˜æ†¶é«”ä½¿ç”¨é‡
        return {
            "total_mb": 128.0,
            "used_mb": 64.0,
            "free_mb": 64.0,
            "usage_percent": 50.0
        }

    def validate_scenario(self, scenario: ScenarioType) -> bool:
        """é©—è­‰å ´æ™¯æ˜¯å¦æ”¯æ´"""
        # DQNæ”¯æ´æ‰€æœ‰å ´æ™¯é¡å‹
        return True
