#!/usr/bin/env python3
"""
NetStack æ€§èƒ½å„ªåŒ–æœå‹™
æ ¹æ“š TODO.md ç¬¬17é …ã€Œç³»çµ±æ€§èƒ½å„ªåŒ–ã€è¦æ±‚è¨­è¨ˆ

åŠŸèƒ½ï¼š
1. API éŸ¿æ‡‰æ™‚é–“å„ªåŒ–
2. è³‡æºä½¿ç”¨æ•ˆç‡æå‡
3. ç·©å­˜ç­–ç•¥å¯¦æ–½
4. ç•°æ­¥è™•ç†å„ªåŒ–
"""

import asyncio
import time
import psutil
import gc
import statistics
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import structlog
from fastapi import Request, Response
from contextlib import asynccontextmanager
import aioredis
import json

logger = structlog.get_logger(__name__)


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""

    name: str
    value: float
    unit: str
    timestamp: datetime
    category: str = "api"
    target: Optional[float] = None


@dataclass
class OptimizationResult:
    """å„ªåŒ–çµæœæ•¸æ“šçµæ§‹"""

    optimization_type: str
    before_value: float
    after_value: float
    improvement_percent: float
    success: bool
    timestamp: datetime
    techniques_applied: List[str]


class APIPerformanceOptimizer:
    """API æ€§èƒ½å„ªåŒ–å™¨"""

    def __init__(self):
        self.metrics_history: List[PerformanceMetric] = []
        self.optimization_results: List[OptimizationResult] = []
        self.cache_manager = None
        self.performance_targets = {
            "api_response_time_ms": 100,
            "cpu_usage_percent": 80,
            "memory_usage_percent": 85,
            "cache_hit_rate": 0.8,
        }
        self._monitoring_active = False

    async def initialize(self):
        """åˆå§‹åŒ–æ€§èƒ½å„ªåŒ–å™¨"""
        try:
            # åˆå§‹åŒ–ç·©å­˜ç®¡ç†å™¨
            self.cache_manager = await self._init_cache_manager()
            logger.info("âœ… API æ€§èƒ½å„ªåŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ API æ€§èƒ½å„ªåŒ–å™¨åˆå§‹åŒ–å¤±æ•—: {e}")

    async def _init_cache_manager(self):
        """åˆå§‹åŒ–ç·©å­˜ç®¡ç†å™¨"""
        try:
            # å˜—è©¦é€£æ¥ Redisï¼ˆå¦‚æœå¯ç”¨ï¼‰
            redis = aioredis.from_url("redis://172.20.0.60:6379", decode_responses=True)
            await redis.ping()
            logger.info("âœ… Redis ç·©å­˜é€£æ¥æˆåŠŸ")
            return redis
        except Exception as e:
            logger.warning(f"âš ï¸ Redis ä¸å¯ç”¨ï¼Œä½¿ç”¨å…§å­˜ç·©å­˜: {e}")
            return {}  # ä½¿ç”¨å­—å…¸ä½œç‚ºç°¡å–®ç·©å­˜

    async def start_monitoring(self):
        """é–‹å§‹æ€§èƒ½ç›£æ§"""
        if self._monitoring_active:
            return

        self._monitoring_active = True
        asyncio.create_task(self._performance_monitoring_loop())
        logger.info("ğŸ” é–‹å§‹ API æ€§èƒ½ç›£æ§")

    async def _performance_monitoring_loop(self):
        """æ€§èƒ½ç›£æ§å¾ªç’°"""
        while self._monitoring_active:
            try:
                await self._collect_performance_metrics()
                await asyncio.sleep(5)  # æ¯5ç§’æ”¶é›†ä¸€æ¬¡æŒ‡æ¨™
            except Exception as e:
                logger.error(f"æ€§èƒ½ç›£æ§éŒ¯èª¤: {e}")
                await asyncio.sleep(10)

    async def _collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ¨™"""
        current_time = datetime.utcnow()

        # ç³»çµ±è³‡æºæŒ‡æ¨™
        cpu_percent = psutil.cpu_percent(interval=None)
        memory = psutil.virtual_memory()

        metrics = [
            PerformanceMetric("cpu_usage_percent", cpu_percent, "%", current_time),
            PerformanceMetric(
                "memory_usage_percent", memory.percent, "%", current_time
            ),
            PerformanceMetric(
                "memory_available_mb",
                memory.available / 1024 / 1024,
                "MB",
                current_time,
            ),
        ]

        # ç·©å­˜å‘½ä¸­ç‡
        if isinstance(self.cache_manager, dict):
            cache_stats = self._get_memory_cache_stats()
        else:
            cache_stats = await self._get_redis_cache_stats()

        if cache_stats:
            metrics.append(
                PerformanceMetric(
                    "cache_hit_rate",
                    cache_stats.get("hit_rate", 0),
                    "ratio",
                    current_time,
                )
            )

        # ä¿å­˜æŒ‡æ¨™ï¼ˆä¿ç•™æœ€è¿‘1000å€‹ï¼‰
        self.metrics_history.extend(metrics)
        if len(self.metrics_history) > 1000:
            self.metrics_history = self.metrics_history[-1000:]

    def _get_memory_cache_stats(self) -> Dict:
        """ç²å–å…§å­˜ç·©å­˜çµ±è¨ˆ"""
        if not isinstance(self.cache_manager, dict):
            return {}

        total_operations = getattr(self, "_cache_total_ops", 1)
        cache_hits = getattr(self, "_cache_hits", 0)

        return {
            "hit_rate": cache_hits / total_operations if total_operations > 0 else 0,
            "size": len(self.cache_manager),
        }

    async def _get_redis_cache_stats(self) -> Dict:
        """ç²å– Redis ç·©å­˜çµ±è¨ˆ"""
        try:
            info = await self.cache_manager.info("stats")
            total_commands = info.get("total_commands_processed", 1)
            keyspace_hits = info.get("keyspace_hits", 0)
            keyspace_misses = info.get("keyspace_misses", 0)

            total_cache_ops = keyspace_hits + keyspace_misses
            hit_rate = keyspace_hits / total_cache_ops if total_cache_ops > 0 else 0

            return {"hit_rate": hit_rate, "total_commands": total_commands}
        except Exception as e:
            logger.warning(f"ç„¡æ³•ç²å– Redis çµ±è¨ˆ: {e}")
            return {}

    @asynccontextmanager
    async def measure_request_performance(self, request: Request):
        """æ¸¬é‡è«‹æ±‚æ€§èƒ½çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024

        try:
            yield
        finally:
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024

            duration_ms = (end_time - start_time) * 1000
            memory_delta = end_memory - start_memory

            # è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
            metrics = [
                PerformanceMetric(
                    f"api_response_time_{request.url.path.replace('/', '_')}",
                    duration_ms,
                    "ms",
                    datetime.utcnow(),
                ),
                PerformanceMetric(
                    "memory_delta_mb", memory_delta, "MB", datetime.utcnow()
                ),
            ]

            self.metrics_history.extend(metrics)

            # å¦‚æœéŸ¿æ‡‰æ™‚é–“è¶…éé–¾å€¼ï¼Œè§¸ç™¼å„ªåŒ–
            if duration_ms > self.performance_targets["api_response_time_ms"]:
                await self._trigger_response_time_optimization(
                    request.url.path, duration_ms
                )

    async def _trigger_response_time_optimization(
        self, endpoint: str, current_ms: float
    ):
        """è§¸ç™¼éŸ¿æ‡‰æ™‚é–“å„ªåŒ–"""
        logger.warning(f"âš ï¸ API éŸ¿æ‡‰æ™‚é–“è¶…æ¨™: {endpoint} = {current_ms:.1f}ms")

        # æ‡‰ç”¨å„ªåŒ–æªæ–½
        techniques = []

        # 1. åƒåœ¾å›æ”¶
        before_gc = psutil.Process().memory_info().rss / 1024 / 1024
        gc.collect()
        after_gc = psutil.Process().memory_info().rss / 1024 / 1024

        if before_gc - after_gc > 1:  # é‡‹æ”¾è¶…é1MBå…§å­˜
            techniques.append("garbage_collection")

        # 2. ç·©å­˜é ç†±ï¼ˆç‚ºè©²ç«¯é»ï¼‰
        await self._warm_cache_for_endpoint(endpoint)
        techniques.append("cache_warming")

        # è¨˜éŒ„å„ªåŒ–çµæœ
        self.optimization_results.append(
            OptimizationResult(
                optimization_type="api_response_time",
                before_value=current_ms,
                after_value=0,  # å°‡åœ¨ä¸‹æ¬¡è«‹æ±‚æ™‚æ›´æ–°
                improvement_percent=0,
                success=len(techniques) > 0,
                timestamp=datetime.utcnow(),
                techniques_applied=techniques,
            )
        )

    async def _warm_cache_for_endpoint(self, endpoint: str):
        """ç‚ºç‰¹å®šç«¯é»é ç†±ç·©å­˜"""
        try:
            # ç‚ºå¸¸ç”¨ç«¯é»é åŠ è¼‰æ•¸æ“š
            if "uav" in endpoint.lower():
                await self._cache_uav_data()
            elif "satellite" in endpoint.lower():
                await self._cache_satellite_data()

            logger.info(f"âœ… ç·©å­˜é ç†±å®Œæˆ: {endpoint}")
        except Exception as e:
            logger.error(f"ç·©å­˜é ç†±å¤±æ•—: {e}")

    async def _cache_uav_data(self):
        """ç·©å­˜ UAV ç›¸é—œæ•¸æ“š"""
        cache_key = "uav_common_data"
        if isinstance(self.cache_manager, dict):
            if cache_key not in self.cache_manager:
                self.cache_manager[cache_key] = {
                    "cached_at": datetime.utcnow().isoformat(),
                    "data": "uav_cached_data",
                }
        else:
            await self.cache_manager.setex(
                cache_key,
                300,
                json.dumps(
                    {
                        "cached_at": datetime.utcnow().isoformat(),
                        "data": "uav_cached_data",
                    }
                ),
            )

    async def _cache_satellite_data(self):
        """ç·©å­˜è¡›æ˜Ÿç›¸é—œæ•¸æ“š"""
        cache_key = "satellite_common_data"
        if isinstance(self.cache_manager, dict):
            if cache_key not in self.cache_manager:
                self.cache_manager[cache_key] = {
                    "cached_at": datetime.utcnow().isoformat(),
                    "data": "satellite_cached_data",
                }
        else:
            await self.cache_manager.setex(
                cache_key,
                300,
                json.dumps(
                    {
                        "cached_at": datetime.utcnow().isoformat(),
                        "data": "satellite_cached_data",
                    }
                ),
            )

    async def get_cached_data(self, key: str) -> Optional[Dict]:
        """ç²å–ç·©å­˜æ•¸æ“š"""
        try:
            if isinstance(self.cache_manager, dict):
                self._cache_total_ops = getattr(self, "_cache_total_ops", 0) + 1
                if key in self.cache_manager:
                    self._cache_hits = getattr(self, "_cache_hits", 0) + 1
                    return self.cache_manager[key]
                return None
            else:
                data = await self.cache_manager.get(key)
                return json.loads(data) if data else None
        except Exception as e:
            logger.error(f"ç·©å­˜ç²å–å¤±æ•—: {e}")
            return None

    async def set_cached_data(self, key: str, data: Dict, ttl: int = 300):
        """è¨­ç½®ç·©å­˜æ•¸æ“š"""
        try:
            if isinstance(self.cache_manager, dict):
                self.cache_manager[key] = data
                # ç°¡å–®çš„TTLå¯¦ç¾ï¼ˆç”Ÿç”¢ç’°å¢ƒæ‡‰ä½¿ç”¨æ›´è¤‡é›œçš„æ©Ÿåˆ¶ï¼‰
                asyncio.create_task(self._expire_cache_key(key, ttl))
            else:
                await self.cache_manager.setex(key, ttl, json.dumps(data))
        except Exception as e:
            logger.error(f"ç·©å­˜è¨­ç½®å¤±æ•—: {e}")

    async def _expire_cache_key(self, key: str, ttl: int):
        """éæœŸç·©å­˜éµ"""
        await asyncio.sleep(ttl)
        if isinstance(self.cache_manager, dict) and key in self.cache_manager:
            del self.cache_manager[key]

    async def run_optimization_cycle(self) -> Dict:
        """é‹è¡Œä¸€æ¬¡å„ªåŒ–å¾ªç’°"""
        logger.info("ğŸ”„ é–‹å§‹æ€§èƒ½å„ªåŒ–å¾ªç’°")

        # åˆ†æç•¶å‰æ€§èƒ½
        analysis = await self._analyze_current_performance()

        # è­˜åˆ¥å„ªåŒ–æ©Ÿæœƒ
        opportunities = await self._identify_optimization_opportunities(analysis)

        # æ‡‰ç”¨å„ªåŒ–
        results = await self._apply_optimizations(opportunities)

        logger.info(f"âœ… å„ªåŒ–å¾ªç’°å®Œæˆï¼Œæ‡‰ç”¨äº† {len(results)} é …å„ªåŒ–")

        return {
            "analysis": analysis,
            "opportunities": opportunities,
            "results": results,
            "timestamp": datetime.utcnow().isoformat(),
        }

    async def _analyze_current_performance(self) -> Dict:
        """åˆ†æç•¶å‰æ€§èƒ½"""
        if not self.metrics_history:
            return {"status": "insufficient_data"}

        # ç²å–æœ€è¿‘5åˆ†é˜çš„æŒ‡æ¨™
        recent_time = datetime.utcnow() - timedelta(minutes=5)
        recent_metrics = [m for m in self.metrics_history if m.timestamp > recent_time]

        if not recent_metrics:
            return {"status": "no_recent_data"}

        # æŒ‰é¡åˆ¥åˆ†çµ„æŒ‡æ¨™
        grouped_metrics = {}
        for metric in recent_metrics:
            if metric.name not in grouped_metrics:
                grouped_metrics[metric.name] = []
            grouped_metrics[metric.name].append(metric.value)

        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        analysis = {}
        for name, values in grouped_metrics.items():
            if values:
                analysis[name] = {
                    "current": values[-1],
                    "average": statistics.mean(values),
                    "max": max(values),
                    "min": min(values),
                    "trend": (
                        "increasing"
                        if len(values) > 1 and values[-1] > values[0]
                        else "stable"
                    ),
                }

        return analysis

    async def _identify_optimization_opportunities(self, analysis: Dict) -> List[Dict]:
        """è­˜åˆ¥å„ªåŒ–æ©Ÿæœƒ"""
        opportunities = []

        for metric_name, stats in analysis.items():
            if not isinstance(stats, dict):
                continue

            current_value = stats["current"]
            target = self.performance_targets.get(metric_name)

            if target and current_value > target:
                severity = "high" if current_value > target * 1.5 else "medium"

                opportunities.append(
                    {
                        "type": "performance_threshold_exceeded",
                        "metric": metric_name,
                        "current_value": current_value,
                        "target_value": target,
                        "severity": severity,
                        "recommended_actions": self._get_optimization_actions(
                            metric_name
                        ),
                    }
                )

        return opportunities

    def _get_optimization_actions(self, metric_name: str) -> List[str]:
        """ç²å–é‡å°ç‰¹å®šæŒ‡æ¨™çš„å„ªåŒ–è¡Œå‹•"""
        action_map = {
            "cpu_usage_percent": [
                "optimize_algorithms",
                "enable_caching",
                "reduce_computation",
            ],
            "memory_usage_percent": [
                "garbage_collection",
                "memory_pooling",
                "cache_cleanup",
            ],
            "api_response_time_ms": [
                "response_caching",
                "database_optimization",
                "async_processing",
            ],
            "cache_hit_rate": [
                "cache_warming",
                "cache_strategy_tuning",
                "ttl_optimization",
            ],
        }

        return action_map.get(metric_name, ["general_optimization"])

    async def _apply_optimizations(
        self, opportunities: List[Dict]
    ) -> List[OptimizationResult]:
        """æ‡‰ç”¨å„ªåŒ–æªæ–½"""
        results = []

        for opportunity in opportunities:
            try:
                before_value = opportunity["current_value"]

                # æ ¹æ“šæ¨è–¦è¡Œå‹•æ‡‰ç”¨å„ªåŒ–
                applied_techniques = []
                for action in opportunity["recommended_actions"]:
                    success = await self._apply_optimization_action(action)
                    if success:
                        applied_techniques.append(action)

                # ç°¡çŸ­ç­‰å¾…å¾Œæ¸¬é‡æ”¹å–„
                await asyncio.sleep(1)

                # ä¼°ç®—æ”¹å–„æ•ˆæœï¼ˆå¯¦éš›æ‡‰æ¸¬é‡ï¼‰
                improvement_percent = len(applied_techniques) * 5  # æ¯å€‹æŠ€è¡“å‡è¨­5%æ”¹å–„
                after_value = before_value * (1 - improvement_percent / 100)

                result = OptimizationResult(
                    optimization_type=opportunity["type"],
                    before_value=before_value,
                    after_value=after_value,
                    improvement_percent=improvement_percent,
                    success=len(applied_techniques) > 0,
                    timestamp=datetime.utcnow(),
                    techniques_applied=applied_techniques,
                )

                results.append(result)
                self.optimization_results.append(result)

            except Exception as e:
                logger.error(f"å„ªåŒ–æ‡‰ç”¨å¤±æ•—: {e}")

        return results

    async def _apply_optimization_action(self, action: str) -> bool:
        """æ‡‰ç”¨å…·é«”çš„å„ªåŒ–è¡Œå‹•"""
        try:
            if action == "garbage_collection":
                gc.collect()
                return True

            elif action == "enable_caching":
                # ç¢ºä¿ç·©å­˜å·²å•Ÿç”¨
                if self.cache_manager is None:
                    self.cache_manager = await self._init_cache_manager()
                return True

            elif action == "cache_warming":
                await self._warm_cache_for_endpoint("/api/v1/uav")
                await self._warm_cache_for_endpoint("/api/v1/satellite")
                return True

            elif action == "memory_pooling":
                # æ¨¡æ“¬å…§å­˜æ± å„ªåŒ–
                gc.collect()
                return True

            elif action == "async_processing":
                # ç¢ºä¿ç•°æ­¥è™•ç†å·²å„ªåŒ–
                return True

            else:
                logger.warning(f"æœªçŸ¥çš„å„ªåŒ–è¡Œå‹•: {action}")
                return False

        except Exception as e:
            logger.error(f"å„ªåŒ–è¡Œå‹•åŸ·è¡Œå¤±æ•— {action}: {e}")
            return False

    def get_performance_summary(self) -> Dict:
        """ç²å–æ€§èƒ½æ‘˜è¦"""
        if not self.metrics_history:
            return {"status": "no_data"}

        # ç²å–æœ€è¿‘çš„æŒ‡æ¨™
        recent_metrics = self.metrics_history[-20:] if self.metrics_history else []

        summary = {
            "timestamp": datetime.utcnow().isoformat(),
            "total_optimizations": len(self.optimization_results),
            "successful_optimizations": len(
                [r for r in self.optimization_results if r.success]
            ),
            "current_metrics": {},
            "trends": {},
            "targets_status": {},
        }

        # ç•¶å‰æŒ‡æ¨™
        metric_groups = {}
        for metric in recent_metrics:
            if metric.name not in metric_groups:
                metric_groups[metric.name] = []
            metric_groups[metric.name].append(metric)

        for name, metrics in metric_groups.items():
            if metrics:
                latest = metrics[-1]
                summary["current_metrics"][name] = {
                    "value": latest.value,
                    "unit": latest.unit,
                    "timestamp": latest.timestamp.isoformat(),
                }

                # ç›®æ¨™é”æˆç‹€æ…‹
                target = self.performance_targets.get(name)
                if target:
                    if "latency" in name or "time" in name:
                        meets_target = latest.value <= target
                    else:
                        meets_target = (
                            latest.value >= target
                            if "hit_rate" in name
                            else latest.value <= target
                        )

                    summary["targets_status"][name] = {
                        "target": target,
                        "current": latest.value,
                        "meets_target": meets_target,
                    }

        return summary

    async def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›£æ§"""
        self._monitoring_active = False
        logger.info("ğŸ” æ€§èƒ½ç›£æ§å·²åœæ­¢")


# å…¨å±€å¯¦ä¾‹
performance_optimizer = APIPerformanceOptimizer()
