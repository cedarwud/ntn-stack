"""
å…­éšæ®µæ•¸æ“šè™•ç†ç®¡é“çµ±è¨ˆ API
æä¾›æ¯å€‹éšæ®µçš„è¼¸å‡ºè¡›æ˜Ÿæ•¸æ“šçµ±è¨ˆä¿¡æ¯
"""

import os
import json
import logging
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

# å‰µå»ºè·¯ç”±å™¨
router = APIRouter(
    prefix="/api/v1/pipeline",
    tags=["Pipeline Statistics"],
    responses={
        404: {"description": "Not found"},
        500: {"description": "Internal server error"},
    },
)

logger = logging.getLogger(__name__)

# === Response Models ===

class StageStatistics(BaseModel):
    """å–®éšæ®µçµ±è¨ˆ"""
    stage: int
    stage_name: str
    status: str  # "success", "failed", "no_data"
    total_satellites: int
    starlink_count: int
    oneweb_count: int
    processing_time: Optional[str]
    output_file_size_mb: Optional[float]
    last_updated: Optional[str]
    error_message: Optional[str] = None

class PipelineStatisticsResponse(BaseModel):
    """ç®¡é“çµ±è¨ˆéŸ¿æ‡‰"""
    metadata: Dict[str, Any]
    stages: List[StageStatistics]
    summary: Dict[str, Any]

# === æ•¸æ“šè·¯å¾‘é…ç½® ===
DATA_PATHS = {
    1: "/app/data/tle_calculation_outputs/tle_orbital_calculation_output.json",
    2: "/app/data/intelligent_filtering_outputs/intelligent_filtered_output.json", 
    3: "/app/data/signal_analysis_outputs/signal_event_analysis_output.json",
    4: "/app/data/timeseries_preprocessing_outputs/conversion_statistics.json",
    5: "/app/data/data_integration_output.json",
    6: "/app/data/leo_outputs/dynamic_pool_planning_outputs/enhanced_dynamic_pools_output.json"
}

STAGE_NAMES = {
    1: "TLEè»Œé“è¨ˆç®—",
    2: "æ™ºèƒ½è¡›æ˜Ÿç¯©é¸", 
    3: "ä¿¡è™Ÿåˆ†æ",
    4: "æ™‚é–“åºåˆ—é è™•ç†",
    5: "æ•¸æ“šæ•´åˆ",
    6: "å‹•æ…‹æ± è¦åŠƒ"
}

def get_file_size_mb(file_path: str) -> Optional[float]:
    """ç²å–æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰"""
    try:
        if os.path.exists(file_path):
            size_bytes = os.path.getsize(file_path)
            return round(size_bytes / (1024 * 1024), 2)
        return None
    except Exception as e:
        logger.warning(f"ç²å–æ–‡ä»¶å¤§å°å¤±æ•— {file_path}: {e}")
        return None

def analyze_stage_data(stage: int, file_path: str) -> StageStatistics:
    """åˆ†æå–®éšæ®µæ•¸æ“š"""
    try:
        if not os.path.exists(file_path):
            return StageStatistics(
                stage=stage,
                stage_name=STAGE_NAMES[stage],
                status="no_data",
                total_satellites=0,
                starlink_count=0,
                oneweb_count=0,
                processing_time=None,
                output_file_size_mb=None,
                last_updated=None,
                error_message=f"è¼¸å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            )
        
        # è®€å– JSON æ•¸æ“š
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # çµ±è¨ˆè¡›æ˜Ÿæ•¸é‡
        total_satellites = 0
        starlink_count = 0
        oneweb_count = 0
        processing_time = None
        last_updated = None
        
        # æ ¹æ“šä¸åŒéšæ®µçš„æ•¸æ“šçµæ§‹è§£æ
        if stage == 1:
            # éšæ®µä¸€ï¼šTLEè»Œé“è¨ˆç®—
            metadata = data.get('metadata', {})
            total_satellites = metadata.get('total_satellites', 0)
            
            constellations = data.get('constellations', {})
            starlink_data = constellations.get('starlink', {})
            oneweb_data = constellations.get('oneweb', {})
            
            starlink_count = starlink_data.get('satellite_count', 0)
            oneweb_count = oneweb_data.get('satellite_count', 0)
            
            processing_time = metadata.get('processing_timestamp')
            
        elif stage == 2:
            # éšæ®µäºŒï¼šæ™ºèƒ½ç¯©é¸
            metadata = data.get('metadata', {})
            unified_results = metadata.get('unified_filtering_results', {})
            
            total_satellites = unified_results.get('total_selected', 0)
            starlink_count = unified_results.get('starlink_selected', 0)
            oneweb_count = unified_results.get('oneweb_selected', 0)
            
            processing_time = metadata.get('processing_timestamp')
            
        elif stage == 3:
            # éšæ®µä¸‰ï¼šä¿¡è™Ÿåˆ†æ
            metadata = data.get('metadata', {})
            satellites = data.get('satellites', [])
            
            total_satellites = len(satellites)
            starlink_count = len([s for s in satellites if s.get('constellation') == 'starlink'])
            oneweb_count = len([s for s in satellites if s.get('constellation') == 'oneweb'])
            
            processing_time = metadata.get('processing_timestamp')
            
        elif stage == 4:
            # éšæ®µå››ï¼šæ™‚é–“åºåˆ—é è™•ç† (ä½¿ç”¨conversion_statistics)
            total_satellites = data.get('total_processed', 0)
            successful_conversions = data.get('successful_conversions', 0)
            
            # å‡è¨­Starlink:OneWebæ¯”ä¾‹ç´„ç‚º86:14 (åŸºæ–¼éšæ®µ2çš„1029:167)
            starlink_count = int(successful_conversions * 0.86)
            oneweb_count = successful_conversions - starlink_count
            
            processing_time = None  # conversion_statisticsæ²’æœ‰æ™‚é–“æˆ³
            
        elif stage == 5:
            # éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆ
            total_satellites = data.get('total_satellites', 0)
            constellation_summary = data.get('constellation_summary', {})
            
            starlink_data = constellation_summary.get('starlink', {})
            oneweb_data = constellation_summary.get('oneweb', {})
            
            starlink_count = starlink_data.get('satellite_count', 0)
            oneweb_count = oneweb_data.get('satellite_count', 0)
            
            processing_time = data.get('start_time')
            
        elif stage == 6:
            # éšæ®µå…­ï¼šå‹•æ…‹æ± è¦åŠƒ
            metadata = data.get('metadata', {})
            pool_data = data.get('dynamic_satellite_pool', {})
            
            total_satellites = pool_data.get('total_selected', 0)
            selection_details = pool_data.get('selection_details', [])
            
            starlink_count = len([s for s in selection_details if s.get('constellation') == 'starlink'])
            oneweb_count = len([s for s in selection_details if s.get('constellation') == 'oneweb'])
            
            processing_time = metadata.get('timestamp')
        
        # ç²å–æ–‡ä»¶ä¿®æ”¹æ™‚é–“
        try:
            mtime = os.path.getmtime(file_path)
            last_updated = datetime.fromtimestamp(mtime, timezone.utc).isoformat()
        except:
            pass
        
        return StageStatistics(
            stage=stage,
            stage_name=STAGE_NAMES[stage],
            status="success",
            total_satellites=total_satellites,
            starlink_count=starlink_count,
            oneweb_count=oneweb_count,
            processing_time=processing_time,
            output_file_size_mb=get_file_size_mb(file_path),
            last_updated=last_updated
        )
        
    except Exception as e:
        logger.error(f"åˆ†æéšæ®µ {stage} æ•¸æ“šå¤±æ•—: {e}")
        return StageStatistics(
            stage=stage,
            stage_name=STAGE_NAMES[stage],
            status="failed",
            total_satellites=0,
            starlink_count=0,
            oneweb_count=0,
            processing_time=None,
            output_file_size_mb=get_file_size_mb(file_path),
            last_updated=None,
            error_message=str(e)
        )

@router.get("/statistics", response_model=PipelineStatisticsResponse)
async def get_pipeline_statistics():
    """ç²å–å…­éšæ®µç®¡é“çµ±è¨ˆä¿¡æ¯"""
    try:
        logger.info("ğŸ” é–‹å§‹åˆ†æå…­éšæ®µç®¡é“çµ±è¨ˆ...")
        
        stages = []
        
        # åˆ†ææ¯å€‹éšæ®µ
        for stage_num in range(1, 7):
            file_path = DATA_PATHS[stage_num]
            stage_stats = analyze_stage_data(stage_num, file_path)
            stages.append(stage_stats)
            
            logger.info(f"  éšæ®µ {stage_num} ({stage_stats.stage_name}): {stage_stats.status} - {stage_stats.total_satellites} é¡†è¡›æ˜Ÿ")
        
        # è¨ˆç®—åŒ¯ç¸½çµ±è¨ˆ
        successful_stages = [s for s in stages if s.status == "success"]
        failed_stages = [s for s in stages if s.status == "failed"]
        no_data_stages = [s for s in stages if s.status == "no_data"]
        
        # æ•¸æ“šæµçµ±è¨ˆ
        data_flow = []
        if successful_stages:
            for stage in sorted(successful_stages, key=lambda x: x.stage):
                data_flow.append({
                    'stage': stage.stage,
                    'satellites': stage.total_satellites,
                    'starlink': stage.starlink_count,
                    'oneweb': stage.oneweb_count
                })
        
        summary = {
            'total_stages': 6,
            'successful_stages': len(successful_stages),
            'failed_stages': len(failed_stages),
            'no_data_stages': len(no_data_stages),
            'data_flow': data_flow,
            'final_output': stages[-1].total_satellites if stages else 0,
            'pipeline_health': "healthy" if len(successful_stages) >= 4 else "degraded" if len(successful_stages) >= 2 else "critical"
        }
        
        # è¨ˆç®—æ•¸æ“šä¸Ÿå¤±ç‡
        if len(data_flow) >= 2:
            input_satellites = data_flow[0]['satellites']
            output_satellites = data_flow[-1]['satellites']
            if input_satellites > 0:
                retention_rate = (output_satellites / input_satellites) * 100
                summary['data_retention_rate'] = round(retention_rate, 2)
                summary['data_loss_rate'] = round(100 - retention_rate, 2)
        
        metadata = {
            'analysis_timestamp': datetime.now(timezone.utc).isoformat(),
            'analyzer_version': 'v1.0',
            'data_paths_analyzed': DATA_PATHS
        }
        
        logger.info(f"âœ… ç®¡é“åˆ†æå®Œæˆ: {summary['pipeline_health']} ç‹€æ…‹")
        logger.info(f"   æˆåŠŸéšæ®µ: {summary['successful_stages']}/6")
        logger.info(f"   æœ€çµ‚è¼¸å‡º: {summary['final_output']} é¡†è¡›æ˜Ÿ")
        
        return PipelineStatisticsResponse(
            metadata=metadata,
            stages=stages,
            summary=summary
        )
        
    except Exception as e:
        logger.error(f"âŒ ç®¡é“çµ±è¨ˆåˆ†æå¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=f"ç®¡é“çµ±è¨ˆåˆ†æå¤±æ•—: {str(e)}")

@router.get("/health")
async def pipeline_health_check():
    """ç®¡é“å¥åº·æª¢æŸ¥"""
    try:
        # å¿«é€Ÿæª¢æŸ¥é—œéµæ–‡ä»¶å­˜åœ¨æ€§
        stage_status = {}
        for stage, path in DATA_PATHS.items():
            stage_status[f"stage_{stage}"] = os.path.exists(path)
        
        healthy_stages = sum(stage_status.values())
        total_stages = len(stage_status)
        
        health_status = "healthy" if healthy_stages >= 4 else "degraded" if healthy_stages >= 2 else "critical"
        
        return {
            "status": health_status,
            "healthy_stages": healthy_stages,
            "total_stages": total_stages,
            "stage_status": stage_status,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
    except Exception as e:
        logger.error(f"å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }