"""
ğŸ“Š æ€§èƒ½åˆ†æå¼•æ“

çµ±ä¸€çš„ç®—æ³•æ€§èƒ½è©•ä¼°ã€æ¯”è¼ƒå’Œ A/B æ¸¬è©¦æ¡†æ¶ã€‚
"""

import asyncio
import logging
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import json
from pathlib import Path

from .interfaces import HandoverAlgorithm, HandoverContext, HandoverDecision
from .orchestrator import HandoverOrchestrator
from .registry import AlgorithmRegistry

logger = logging.getLogger(__name__)


class MetricType(Enum):
    """æŒ‡æ¨™é¡å‹"""
    LATENCY = "latency"
    SUCCESS_RATE = "success_rate"
    HANDOVER_FREQUENCY = "handover_frequency"
    NETWORK_EFFICIENCY = "network_efficiency"
    USER_SATISFACTION = "user_satisfaction"
    RESOURCE_UTILIZATION = "resource_utilization"
    DECISION_CONFIDENCE = "decision_confidence"
    ALGORITHM_RESPONSE_TIME = "algorithm_response_time"


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ¨™æ•¸æ“šçµæ§‹"""
    algorithm_name: str
    metric_type: MetricType
    value: float
    timestamp: datetime
    context_info: Dict[str, Any]
    scenario: str = "default"


@dataclass
class ComparisonResult:
    """ç®—æ³•æ¯”è¼ƒçµæœ"""
    algorithm_a: str
    algorithm_b: str
    metric_type: MetricType
    a_mean: float
    b_mean: float
    a_std: float
    b_std: float
    statistical_significance: float
    winner: Optional[str]
    improvement_percentage: float
    sample_size_a: int
    sample_size_b: int
    confidence_interval: Tuple[float, float]


@dataclass
class ABTestResult:
    """A/B æ¸¬è©¦çµæœ"""
    test_id: str
    algorithms: List[str]
    traffic_split: Dict[str, float]
    start_time: datetime
    end_time: Optional[datetime]
    total_requests: int
    results_by_algorithm: Dict[str, Dict[MetricType, float]]
    overall_winner: Optional[str]
    statistical_significance: Dict[str, float]
    recommendations: List[str]


class PerformanceAnalysisEngine:
    """æ€§èƒ½åˆ†æå¼•æ“
    
    æä¾›å…¨é¢çš„ç®—æ³•æ€§èƒ½åˆ†æåŠŸèƒ½ï¼š
    - å¯¦æ™‚æ€§èƒ½ç›£æ§
    - ç®—æ³•é–“æ¯”è¼ƒåˆ†æ
    - A/B æ¸¬è©¦ç®¡ç†
    - çµ±è¨ˆåˆ†æå’Œå ±å‘Šç”Ÿæˆ
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–æ€§èƒ½åˆ†æå¼•æ“
        
        Args:
            config: åˆ†æå¼•æ“é…ç½®
        """
        self.config = config or {}
        
        # æ€§èƒ½æ•¸æ“šå­˜å„²
        self.metrics_storage = []
        self.max_storage_size = self.config.get('max_storage_size', 10000)
        
        # A/B æ¸¬è©¦ç®¡ç†
        self.active_ab_tests = {}
        self.completed_ab_tests = {}
        
        # çµ±è¨ˆé…ç½®
        self.confidence_level = self.config.get('confidence_level', 0.95)
        self.min_sample_size = self.config.get('min_sample_size', 30)
        
        # çµæœå­˜å„²è·¯å¾‘
        self.results_dir = Path(self.config.get('results_dir', 'analysis_results'))
        self.results_dir.mkdir(exist_ok=True)
        
        # çµ„ä»¶å¼•ç”¨
        self.algorithm_registry = None
        self.orchestrator = None
        
        logger.info("æ€§èƒ½åˆ†æå¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def set_components(self, algorithm_registry: AlgorithmRegistry, orchestrator: HandoverOrchestrator):
        """è¨­ç½®çµ„ä»¶å¼•ç”¨
        
        Args:
            algorithm_registry: ç®—æ³•è¨»å†Šä¸­å¿ƒ
            orchestrator: å”èª¿å™¨
        """
        self.algorithm_registry = algorithm_registry
        self.orchestrator = orchestrator
    
    def record_metric(self, metric: PerformanceMetric) -> None:
        """è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™
        
        Args:
            metric: æ€§èƒ½æŒ‡æ¨™
        """
        self.metrics_storage.append(metric)
        
        # é™åˆ¶å­˜å„²å¤§å°
        if len(self.metrics_storage) > self.max_storage_size:
            self.metrics_storage = self.metrics_storage[-self.max_storage_size:]
        
        # è¨˜éŒ„åˆ°æ´»èºçš„ A/B æ¸¬è©¦
        for test_id, ab_test in self.active_ab_tests.items():
            if metric.algorithm_name in ab_test['algorithms']:
                self._update_ab_test_metrics(test_id, metric)
    
    def record_decision_metrics(self, algorithm_name: str, context: HandoverContext, 
                              decision: HandoverDecision) -> None:
        """è¨˜éŒ„æ±ºç­–ç›¸é—œæŒ‡æ¨™
        
        Args:
            algorithm_name: ç®—æ³•åç¨±
            context: æ›æ‰‹ä¸Šä¸‹æ–‡
            decision: æ›æ‰‹æ±ºç­–
        """
        timestamp = datetime.now()
        scenario = context.scenario_info.get('type', 'default') if context.scenario_info else 'default'
        
        # è¨˜éŒ„æ±ºç­–ä¿¡å¿ƒåº¦
        confidence_metric = PerformanceMetric(
            algorithm_name=algorithm_name,
            metric_type=MetricType.DECISION_CONFIDENCE,
            value=decision.confidence,
            timestamp=timestamp,
            context_info={
                'ue_id': context.ue_id,
                'current_satellite': context.current_satellite,
                'candidate_count': len(context.candidate_satellites) if context.candidate_satellites else 0
            },
            scenario=scenario
        )
        self.record_metric(confidence_metric)
        
        # è¨˜éŒ„ç®—æ³•éŸ¿æ‡‰æ™‚é–“
        if decision.decision_time > 0:
            response_time_metric = PerformanceMetric(
                algorithm_name=algorithm_name,
                metric_type=MetricType.ALGORITHM_RESPONSE_TIME,
                value=decision.decision_time,
                timestamp=timestamp,
                context_info={'decision_type': decision.handover_decision.name},
                scenario=scenario
            )
            self.record_metric(response_time_metric)
    
    def get_algorithm_performance(self, algorithm_name: str, 
                                metric_type: MetricType,
                                time_window: Optional[timedelta] = None,
                                scenario: Optional[str] = None) -> Dict[str, float]:
        """ç²å–ç®—æ³•æ€§èƒ½çµ±è¨ˆ
        
        Args:
            algorithm_name: ç®—æ³•åç¨±
            metric_type: æŒ‡æ¨™é¡å‹
            time_window: æ™‚é–“çª—å£
            scenario: å ´æ™¯éæ¿¾
            
        Returns:
            Dict[str, float]: æ€§èƒ½çµ±è¨ˆ
        """
        # éæ¿¾æŒ‡æ¨™
        filtered_metrics = self._filter_metrics(
            algorithm_name=algorithm_name,
            metric_type=metric_type,
            time_window=time_window,
            scenario=scenario
        )
        
        if not filtered_metrics:
            return {'count': 0, 'mean': 0.0, 'std': 0.0, 'min': 0.0, 'max': 0.0}
        
        values = [m.value for m in filtered_metrics]
        
        return {
            'count': len(values),
            'mean': np.mean(values),
            'std': np.std(values),
            'min': np.min(values),
            'max': np.max(values),
            'median': np.median(values),
            'percentile_95': np.percentile(values, 95),
            'percentile_99': np.percentile(values, 99)
        }
    
    def compare_algorithms(self, algorithm_a: str, algorithm_b: str,
                         metric_type: MetricType,
                         time_window: Optional[timedelta] = None,
                         scenario: Optional[str] = None) -> ComparisonResult:
        """æ¯”è¼ƒå…©å€‹ç®—æ³•çš„æ€§èƒ½
        
        Args:
            algorithm_a: ç®—æ³•Aåç¨±
            algorithm_b: ç®—æ³•Båç¨±
            metric_type: æ¯”è¼ƒæŒ‡æ¨™
            time_window: æ™‚é–“çª—å£
            scenario: å ´æ™¯éæ¿¾
            
        Returns:
            ComparisonResult: æ¯”è¼ƒçµæœ
        """
        # ç²å–å…©å€‹ç®—æ³•çš„æŒ‡æ¨™æ•¸æ“š
        metrics_a = self._filter_metrics(algorithm_a, metric_type, time_window, scenario)
        metrics_b = self._filter_metrics(algorithm_b, metric_type, time_window, scenario)
        
        values_a = [m.value for m in metrics_a]
        values_b = [m.value for m in metrics_b]
        
        if len(values_a) < self.min_sample_size or len(values_b) < self.min_sample_size:
            logger.warning(f"æ¨£æœ¬é‡ä¸è¶³ï¼Œç„¡æ³•é€²è¡Œå¯é çš„çµ±è¨ˆæ¯”è¼ƒ")
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        mean_a = np.mean(values_a) if values_a else 0.0
        mean_b = np.mean(values_b) if values_b else 0.0
        std_a = np.std(values_a) if values_a else 0.0
        std_b = np.std(values_b) if values_b else 0.0
        
        # çµ±è¨ˆé¡¯è‘—æ€§æª¢é©— (t-test)
        statistical_significance = 0.0
        winner = None
        improvement_percentage = 0.0
        confidence_interval = (0.0, 0.0)
        
        if len(values_a) >= self.min_sample_size and len(values_b) >= self.min_sample_size:
            try:
                from scipy import stats
                t_stat, p_value = stats.ttest_ind(values_a, values_b)
                statistical_significance = p_value
                
                # ç¢ºå®šå‹è€… (æ ¹æ“šæŒ‡æ¨™é¡å‹æ±ºå®šé«˜ä½å„ªåŠ£)
                if self._is_higher_better(metric_type):
                    if mean_a > mean_b and p_value < (1 - self.confidence_level):
                        winner = algorithm_a
                        improvement_percentage = ((mean_a - mean_b) / mean_b) * 100 if mean_b > 0 else 0
                    elif mean_b > mean_a and p_value < (1 - self.confidence_level):
                        winner = algorithm_b
                        improvement_percentage = ((mean_b - mean_a) / mean_a) * 100 if mean_a > 0 else 0
                else:
                    if mean_a < mean_b and p_value < (1 - self.confidence_level):
                        winner = algorithm_a
                        improvement_percentage = ((mean_b - mean_a) / mean_b) * 100 if mean_b > 0 else 0
                    elif mean_b < mean_a and p_value < (1 - self.confidence_level):
                        winner = algorithm_b
                        improvement_percentage = ((mean_a - mean_b) / mean_a) * 100 if mean_a > 0 else 0
                
                # è¨ˆç®—ç½®ä¿¡å€é–“
                confidence_interval = stats.t.interval(
                    self.confidence_level,
                    len(values_a) + len(values_b) - 2,
                    loc=mean_a - mean_b,
                    scale=np.sqrt(std_a**2/len(values_a) + std_b**2/len(values_b))
                )
                
            except ImportError:
                logger.warning("scipy ä¸å¯ç”¨ï¼Œè·³éçµ±è¨ˆæª¢é©—")
        
        return ComparisonResult(
            algorithm_a=algorithm_a,
            algorithm_b=algorithm_b,
            metric_type=metric_type,
            a_mean=mean_a,
            b_mean=mean_b,
            a_std=std_a,
            b_std=std_b,
            statistical_significance=statistical_significance,
            winner=winner,
            improvement_percentage=improvement_percentage,
            sample_size_a=len(values_a),
            sample_size_b=len(values_b),
            confidence_interval=confidence_interval
        )
    
    def start_ab_test(self, test_id: str, algorithms: List[str], 
                     traffic_split: Dict[str, float],
                     duration_hours: Optional[float] = None) -> bool:
        """å•Ÿå‹• A/B æ¸¬è©¦
        
        Args:
            test_id: æ¸¬è©¦ID
            algorithms: åƒèˆ‡æ¸¬è©¦çš„ç®—æ³•åˆ—è¡¨
            traffic_split: æµé‡åˆ†é…æ¯”ä¾‹
            duration_hours: æ¸¬è©¦æŒçºŒæ™‚é–“ï¼ˆå°æ™‚ï¼‰
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸå•Ÿå‹•
        """
        if test_id in self.active_ab_tests:
            logger.error(f"A/B æ¸¬è©¦ '{test_id}' å·²ç¶“åœ¨é‹è¡Œä¸­")
            return False
        
        # é©—è­‰æµé‡åˆ†é…
        total_split = sum(traffic_split.values())
        if abs(total_split - 1.0) > 0.01:
            logger.error(f"æµé‡åˆ†é…ç¸½å’Œå¿…é ˆç‚º 1.0ï¼Œç•¶å‰ç‚º {total_split}")
            return False
        
        # é©—è­‰ç®—æ³•å­˜åœ¨
        for algorithm in algorithms:
            if not self.algorithm_registry or not self.algorithm_registry.is_registered(algorithm):
                logger.error(f"ç®—æ³• '{algorithm}' æœªè¨»å†Š")
                return False
        
        # å‰µå»º A/B æ¸¬è©¦
        ab_test = {
            'test_id': test_id,
            'algorithms': algorithms,
            'traffic_split': traffic_split,
            'start_time': datetime.now(),
            'end_time': datetime.now() + timedelta(hours=duration_hours) if duration_hours else None,
            'total_requests': 0,
            'metrics_by_algorithm': {alg: {} for alg in algorithms}
        }
        
        self.active_ab_tests[test_id] = ab_test
        
        # æ›´æ–°å”èª¿å™¨çš„ A/B æ¸¬è©¦é…ç½®
        if self.orchestrator:
            self.orchestrator.set_ab_test_config(test_id, traffic_split)
        
        logger.info(f"A/B æ¸¬è©¦ '{test_id}' å·²å•Ÿå‹•ï¼Œç®—æ³•: {algorithms}ï¼Œåˆ†é…: {traffic_split}")
        return True
    
    def stop_ab_test(self, test_id: str) -> Optional[ABTestResult]:
        """åœæ­¢ A/B æ¸¬è©¦ä¸¦ç”Ÿæˆçµæœ
        
        Args:
            test_id: æ¸¬è©¦ID
            
        Returns:
            Optional[ABTestResult]: æ¸¬è©¦çµæœ
        """
        if test_id not in self.active_ab_tests:
            logger.error(f"A/B æ¸¬è©¦ '{test_id}' ä¸å­˜åœ¨æˆ–æœªé‹è¡Œ")
            return None
        
        ab_test = self.active_ab_tests.pop(test_id)
        ab_test['end_time'] = datetime.now()
        
        # ç”Ÿæˆæ¸¬è©¦çµæœ
        result = self._generate_ab_test_result(ab_test)
        
        # ä¿å­˜çµæœ
        self.completed_ab_tests[test_id] = result
        self._save_ab_test_result(result)
        
        # æ¸…é™¤å”èª¿å™¨çš„ A/B æ¸¬è©¦é…ç½®
        if self.orchestrator:
            self.orchestrator.clear_ab_test_config(test_id)
        
        logger.info(f"A/B æ¸¬è©¦ '{test_id}' å·²å®Œæˆ")
        return result
    
    def _filter_metrics(self, algorithm_name: Optional[str] = None,
                       metric_type: Optional[MetricType] = None,
                       time_window: Optional[timedelta] = None,
                       scenario: Optional[str] = None) -> List[PerformanceMetric]:
        """éæ¿¾æ€§èƒ½æŒ‡æ¨™"""
        filtered = self.metrics_storage.copy()
        
        if algorithm_name:
            filtered = [m for m in filtered if m.algorithm_name == algorithm_name]
        
        if metric_type:
            filtered = [m for m in filtered if m.metric_type == metric_type]
        
        if scenario:
            filtered = [m for m in filtered if m.scenario == scenario]
        
        if time_window:
            cutoff_time = datetime.now() - time_window
            filtered = [m for m in filtered if m.timestamp >= cutoff_time]
        
        return filtered
    
    def _is_higher_better(self, metric_type: MetricType) -> bool:
        """åˆ¤æ–·æŒ‡æ¨™æ˜¯å¦è¶Šé«˜è¶Šå¥½"""
        better_when_higher = {
            MetricType.SUCCESS_RATE,
            MetricType.NETWORK_EFFICIENCY,
            MetricType.USER_SATISFACTION,
            MetricType.RESOURCE_UTILIZATION,
            MetricType.DECISION_CONFIDENCE
        }
        return metric_type in better_when_higher
    
    def _update_ab_test_metrics(self, test_id: str, metric: PerformanceMetric) -> None:
        """æ›´æ–° A/B æ¸¬è©¦æŒ‡æ¨™"""
        if test_id not in self.active_ab_tests:
            return
        
        ab_test = self.active_ab_tests[test_id]
        
        if metric.algorithm_name not in ab_test['algorithms']:
            return
        
        # æ›´æ–°ç¸½è«‹æ±‚æ•¸
        ab_test['total_requests'] += 1
        
        # æ›´æ–°ç®—æ³•æŒ‡æ¨™
        alg_metrics = ab_test['metrics_by_algorithm'][metric.algorithm_name]
        
        if metric.metric_type not in alg_metrics:
            alg_metrics[metric.metric_type] = []
        
        alg_metrics[metric.metric_type].append(metric.value)
    
    def _generate_ab_test_result(self, ab_test: Dict[str, Any]) -> ABTestResult:
        """ç”Ÿæˆ A/B æ¸¬è©¦çµæœ"""
        algorithms = ab_test['algorithms']
        
        # è¨ˆç®—æ¯å€‹ç®—æ³•çš„å¹³å‡æŒ‡æ¨™
        results_by_algorithm = {}
        for algorithm in algorithms:
            alg_metrics = ab_test['metrics_by_algorithm'][algorithm]
            alg_results = {}
            
            for metric_type, values in alg_metrics.items():
                if values:
                    alg_results[metric_type] = np.mean(values)
            
            results_by_algorithm[algorithm] = alg_results
        
        # ç¢ºå®šæ•´é«”å‹è€…ï¼ˆåŸºæ–¼å¤šå€‹æŒ‡æ¨™çš„ç¶œåˆè©•åˆ†ï¼‰
        overall_winner = self._determine_overall_winner(results_by_algorithm)
        
        # è¨ˆç®—çµ±è¨ˆé¡¯è‘—æ€§
        statistical_significance = {}
        if len(algorithms) == 2:
            for metric_type in MetricType:
                comp_result = self.compare_algorithms(
                    algorithms[0], algorithms[1], metric_type,
                    time_window=ab_test['end_time'] - ab_test['start_time']
                )
                statistical_significance[f"{algorithms[0]}_vs_{algorithms[1]}_{metric_type.value}"] = comp_result.statistical_significance
        
        # ç”Ÿæˆå»ºè­°
        recommendations = self._generate_recommendations(results_by_algorithm, overall_winner)
        
        return ABTestResult(
            test_id=ab_test['test_id'],
            algorithms=algorithms,
            traffic_split=ab_test['traffic_split'],
            start_time=ab_test['start_time'],
            end_time=ab_test['end_time'],
            total_requests=ab_test['total_requests'],
            results_by_algorithm=results_by_algorithm,
            overall_winner=overall_winner,
            statistical_significance=statistical_significance,
            recommendations=recommendations
        )
    
    def _determine_overall_winner(self, results_by_algorithm: Dict[str, Dict[MetricType, float]]) -> Optional[str]:
        """ç¢ºå®šæ•´é«”å‹è€…"""
        if not results_by_algorithm:
            return None
        
        # ç°¡åŒ–çš„è©•åˆ†æ©Ÿåˆ¶ï¼šå°æ¯å€‹æŒ‡æ¨™é€²è¡Œæ¨™æº–åŒ–ä¸¦æ±‚å¹³å‡
        algorithm_scores = {}
        
        for algorithm, metrics in results_by_algorithm.items():
            score = 0.0
            metric_count = 0
            
            for metric_type, value in metrics.items():
                # æ ¹æ“šæŒ‡æ¨™é¡å‹èª¿æ•´åˆ†æ•¸
                if self._is_higher_better(metric_type):
                    score += value
                else:
                    score += (1.0 - value) if value <= 1.0 else 1.0 / (1.0 + value)
                metric_count += 1
            
            if metric_count > 0:
                algorithm_scores[algorithm] = score / metric_count
        
        if algorithm_scores:
            return max(algorithm_scores, key=algorithm_scores.get)
        
        return None
    
    def _generate_recommendations(self, results_by_algorithm: Dict[str, Dict[MetricType, float]], 
                                winner: Optional[str]) -> List[str]:
        """ç”Ÿæˆå»ºè­°"""
        recommendations = []
        
        if winner:
            recommendations.append(f"å»ºè­°ä½¿ç”¨ç®—æ³• '{winner}' ä½œç‚ºä¸»è¦ç®—æ³•")
        
        # åˆ†æå„ç®—æ³•çš„å¼·å¼±é …
        for algorithm, metrics in results_by_algorithm.items():
            strong_metrics = []
            weak_metrics = []
            
            for metric_type, value in metrics.items():
                if self._is_higher_better(metric_type) and value > 0.8:
                    strong_metrics.append(metric_type.value)
                elif not self._is_higher_better(metric_type) and value < 0.2:
                    strong_metrics.append(metric_type.value)
                else:
                    weak_metrics.append(metric_type.value)
            
            if strong_metrics:
                recommendations.append(f"ç®—æ³• '{algorithm}' åœ¨ä»¥ä¸‹æŒ‡æ¨™è¡¨ç¾å„ªç§€: {', '.join(strong_metrics)}")
            
            if weak_metrics:
                recommendations.append(f"ç®—æ³• '{algorithm}' åœ¨ä»¥ä¸‹æŒ‡æ¨™éœ€è¦æ”¹é€²: {', '.join(weak_metrics)}")
        
        return recommendations
    
    def _save_ab_test_result(self, result: ABTestResult) -> None:
        """ä¿å­˜ A/B æ¸¬è©¦çµæœ"""
        try:
            result_file = self.results_dir / f"ab_test_{result.test_id}_{result.start_time.strftime('%Y%m%d_%H%M%S')}.json"
            
            # å°‡çµæœè½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
            result_dict = {
                'test_id': result.test_id,
                'algorithms': result.algorithms,
                'traffic_split': result.traffic_split,
                'start_time': result.start_time.isoformat(),
                'end_time': result.end_time.isoformat() if result.end_time else None,
                'total_requests': result.total_requests,
                'results_by_algorithm': {
                    alg: {metric_type.value: value for metric_type, value in metrics.items()}
                    for alg, metrics in result.results_by_algorithm.items()
                },
                'overall_winner': result.overall_winner,
                'statistical_significance': result.statistical_significance,
                'recommendations': result.recommendations
            }
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result_dict, f, indent=2, ensure_ascii=False)
            
            logger.info(f"A/B æ¸¬è©¦çµæœå·²ä¿å­˜: {result_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ A/B æ¸¬è©¦çµæœå¤±æ•—: {e}")
    
    def generate_performance_report(self, algorithms: Optional[List[str]] = None,
                                  time_window: Optional[timedelta] = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½å ±å‘Š
        
        Args:
            algorithms: ç®—æ³•åˆ—è¡¨ï¼ˆå¯é¸ï¼‰
            time_window: æ™‚é–“çª—å£ï¼ˆå¯é¸ï¼‰
            
        Returns:
            Dict[str, Any]: æ€§èƒ½å ±å‘Š
        """
        report = {
            'generated_at': datetime.now().isoformat(),
            'time_window': str(time_window) if time_window else 'all_time',
            'algorithms': {},
            'comparisons': [],
            'summary': {}
        }
        
        # ç²å–è¦åˆ†æçš„ç®—æ³•åˆ—è¡¨
        if not algorithms:
            algorithms = list(set(m.algorithm_name for m in self.metrics_storage))
        
        # ç‚ºæ¯å€‹ç®—æ³•ç”Ÿæˆå ±å‘Š
        for algorithm in algorithms:
            alg_report = {}
            
            for metric_type in MetricType:
                stats = self.get_algorithm_performance(algorithm, metric_type, time_window)
                if stats['count'] > 0:
                    alg_report[metric_type.value] = stats
            
            report['algorithms'][algorithm] = alg_report
        
        # ç”Ÿæˆç®—æ³•é–“æ¯”è¼ƒ
        for i, alg_a in enumerate(algorithms):
            for alg_b in algorithms[i+1:]:
                for metric_type in MetricType:
                    comparison = self.compare_algorithms(alg_a, alg_b, metric_type, time_window)
                    if comparison.sample_size_a > 0 and comparison.sample_size_b > 0:
                        report['comparisons'].append({
                            'algorithm_a': comparison.algorithm_a,
                            'algorithm_b': comparison.algorithm_b,
                            'metric': metric_type.value,
                            'winner': comparison.winner,
                            'improvement_percentage': comparison.improvement_percentage,
                            'statistical_significance': comparison.statistical_significance
                        })
        
        # ç”Ÿæˆæ‘˜è¦
        if algorithms:
            best_by_metric = {}
            for metric_type in MetricType:
                best_alg = None
                best_value = None
                
                for algorithm in algorithms:
                    stats = self.get_algorithm_performance(algorithm, metric_type, time_window)
                    if stats['count'] > 0:
                        value = stats['mean']
                        if best_value is None:
                            best_alg = algorithm
                            best_value = value
                        elif self._is_higher_better(metric_type) and value > best_value:
                            best_alg = algorithm
                            best_value = value
                        elif not self._is_higher_better(metric_type) and value < best_value:
                            best_alg = algorithm
                            best_value = value
                
                if best_alg:
                    best_by_metric[metric_type.value] = {
                        'algorithm': best_alg,
                        'value': best_value
                    }
            
            report['summary']['best_by_metric'] = best_by_metric
        
        return report
    
    def get_active_ab_tests(self) -> Dict[str, Dict[str, Any]]:
        """ç²å–æ´»èºçš„ A/B æ¸¬è©¦ç‹€æ…‹"""
        status = {}
        
        for test_id, ab_test in self.active_ab_tests.items():
            duration = datetime.now() - ab_test['start_time']
            
            status[test_id] = {
                'algorithms': ab_test['algorithms'],
                'traffic_split': ab_test['traffic_split'],
                'start_time': ab_test['start_time'].isoformat(),
                'duration_hours': duration.total_seconds() / 3600,
                'total_requests': ab_test['total_requests'],
                'end_time': ab_test['end_time'].isoformat() if ab_test['end_time'] else None,
                'is_expired': ab_test['end_time'] and datetime.now() > ab_test['end_time']
            }
        
        return status