"""
ğŸš€ RL è¨“ç·´ç®¡ç·š

çµ±ä¸€ç®¡ç†å¼·åŒ–å­¸ç¿’ç®—æ³•çš„è¨“ç·´éç¨‹ï¼ŒåŒ…æ‹¬ç’°å¢ƒè¨­ç½®ã€è¨“ç·´ç›£æ§ã€æ¨¡å‹ä¿å­˜ç­‰ã€‚
"""

import asyncio
import logging
import numpy as np
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
import yaml

try:
    import gymnasium as gym
    GYMNASIUM_AVAILABLE = True
except ImportError:
    GYMNASIUM_AVAILABLE = False

from .interfaces import RLHandoverAlgorithm, HandoverContext
from .environment_manager import EnvironmentManager
from .registry import AlgorithmRegistry

logger = logging.getLogger(__name__)


class TrainingMetrics:
    """è¨“ç·´æŒ‡æ¨™æ”¶é›†å™¨"""
    
    def __init__(self):
        self.episode_rewards = []
        self.episode_lengths = []
        self.training_losses = []
        self.success_rates = []
        self.handover_counts = []
        self.start_time = None
        self.end_time = None
    
    def add_episode(self, reward: float, length: int, success: bool, handover_count: int):
        """æ·»åŠ ä¸€å€‹ episode çš„æŒ‡æ¨™"""
        self.episode_rewards.append(reward)
        self.episode_lengths.append(length)
        self.success_rates.append(1.0 if success else 0.0)
        self.handover_counts.append(handover_count)
    
    def add_training_loss(self, loss: Dict[str, float]):
        """æ·»åŠ è¨“ç·´æå¤±"""
        self.training_losses.append(loss)
    
    def get_recent_stats(self, window: int = 100) -> Dict[str, float]:
        """ç²å–æœ€è¿‘çš„çµ±è¨ˆæ•¸æ“š"""
        if not self.episode_rewards:
            return {}
        
        recent_rewards = self.episode_rewards[-window:]
        recent_lengths = self.episode_lengths[-window:]
        recent_success = self.success_rates[-window:]
        recent_handovers = self.handover_counts[-window:]
        
        return {
            'mean_reward': np.mean(recent_rewards),
            'std_reward': np.std(recent_rewards),
            'mean_length': np.mean(recent_lengths),
            'success_rate': np.mean(recent_success),
            'mean_handovers': np.mean(recent_handovers),
            'total_episodes': len(self.episode_rewards)
        }


class RLTrainingPipeline:
    """RL è¨“ç·´ç®¡ç·š
    
    è² è²¬ç®¡ç† RL ç®—æ³•çš„å®Œæ•´è¨“ç·´æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
    - ç’°å¢ƒåˆå§‹åŒ–å’Œé…ç½®
    - è¨“ç·´å¾ªç’°å’Œç›£æ§
    - æ¨¡å‹ä¿å­˜å’Œè¼‰å…¥
    - æ€§èƒ½è©•ä¼°å’Œåˆ†æ
    """
    
    def __init__(self, config_path: str = "algorithm_ecosystem_config.yml"):
        """åˆå§‹åŒ–è¨“ç·´ç®¡ç·š
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾‘
        """
        self.config_path = config_path
        self.config = self._load_config()
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.environment_manager = EnvironmentManager(self.config.get('environment', {}))
        self.algorithm_registry = AlgorithmRegistry()
        
        # è¨“ç·´ç‹€æ…‹
        self.is_training = False
        self.current_algorithm = None
        self.current_env = None
        self.training_metrics = TrainingMetrics()
        
        # æ¨¡å‹ä¿å­˜è·¯å¾‘
        self.model_save_dir = Path("models")
        self.model_save_dir.mkdir(exist_ok=True)
        
        logger.info("RL è¨“ç·´ç®¡ç·šåˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self) -> Dict[str, Any]:
        """è¼‰å…¥é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            logger.error(f"è¼‰å…¥é…ç½®æ–‡ä»¶å¤±æ•—: {e}")
            return {}
    
    async def setup_algorithm(self, algorithm_name: str) -> bool:
        """è¨­ç½®è¨“ç·´ç®—æ³•
        
        Args:
            algorithm_name: ç®—æ³•åç¨±
            
        Returns:
            bool: è¨­ç½®æ˜¯å¦æˆåŠŸ
        """
        try:
            # å¾è¨»å†Šä¸­å¿ƒç²å–ç®—æ³•
            algorithm = await self.algorithm_registry.get_algorithm(algorithm_name)
            if not algorithm or not isinstance(algorithm, RLHandoverAlgorithm):
                logger.error(f"ç®—æ³• '{algorithm_name}' ä¸æ˜¯æœ‰æ•ˆçš„ RL ç®—æ³•")
                return False
            
            self.current_algorithm = algorithm
            logger.info(f"ç®—æ³• '{algorithm_name}' è¨­ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"è¨­ç½®ç®—æ³•å¤±æ•—: {e}")
            return False
    
    async def setup_environment(self, env_config: Optional[Dict[str, Any]] = None) -> bool:
        """è¨­ç½®è¨“ç·´ç’°å¢ƒ
        
        Args:
            env_config: ç’°å¢ƒé…ç½®ï¼ˆå¯é¸ï¼‰
            
        Returns:
            bool: è¨­ç½®æ˜¯å¦æˆåŠŸ
        """
        try:
            if not GYMNASIUM_AVAILABLE:
                logger.error("Gymnasium ä¸å¯ç”¨ï¼Œç„¡æ³•å‰µå»ºè¨“ç·´ç’°å¢ƒ")
                return False
            
            env_config = env_config or self.config.get('environment', {})
            env_id = env_config.get('gymnasium', {}).get('env_name', 'LEOSatelliteHandoverEnv-v1')
            
            self.current_env = await self.environment_manager.create_environment(env_id, env_config)
            if self.current_env is None:
                logger.error("ç’°å¢ƒå‰µå»ºå¤±æ•—")
                return False
            
            logger.info(f"è¨“ç·´ç’°å¢ƒ '{env_id}' è¨­ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"è¨­ç½®ç’°å¢ƒå¤±æ•—: {e}")
            return False
    
    async def train_algorithm(self, algorithm_name: str, episodes: int = 1000, 
                            save_interval: int = 100, eval_interval: int = 50) -> Dict[str, Any]:
        """è¨“ç·´ç®—æ³•
        
        Args:
            algorithm_name: ç®—æ³•åç¨±
            episodes: è¨“ç·´å›åˆæ•¸
            save_interval: æ¨¡å‹ä¿å­˜é–“éš”
            eval_interval: è©•ä¼°é–“éš”
            
        Returns:
            Dict[str, Any]: è¨“ç·´çµæœ
        """
        if self.is_training:
            logger.warning("è¨“ç·´å·²åœ¨é€²è¡Œä¸­")
            return {'status': 'already_training'}
        
        try:
            self.is_training = True
            self.training_metrics = TrainingMetrics()
            self.training_metrics.start_time = datetime.now()
            
            # è¨­ç½®ç®—æ³•å’Œç’°å¢ƒ
            if not await self.setup_algorithm(algorithm_name):
                return {'status': 'algorithm_setup_failed'}
            
            if not await self.setup_environment():
                return {'status': 'environment_setup_failed'}
            
            logger.info(f"é–‹å§‹è¨“ç·´ç®—æ³• '{algorithm_name}'ï¼Œè¨ˆåŠƒ {episodes} å›åˆ")
            
            best_reward = -float('inf')
            
            for episode in range(episodes):
                try:
                    # åŸ·è¡Œä¸€å€‹è¨“ç·´å›åˆ
                    episode_result = await self._run_training_episode()
                    
                    # è¨˜éŒ„æŒ‡æ¨™
                    self.training_metrics.add_episode(
                        episode_result['reward'],
                        episode_result['length'],
                        episode_result['success'],
                        episode_result['handover_count']
                    )
                    
                    # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
                    if hasattr(self.current_algorithm, 'train_step'):
                        training_loss = await self.current_algorithm.train_step()
                        self.training_metrics.add_training_loss(training_loss)
                    
                    # è¨˜éŒ„æœ€ä½³æ€§èƒ½
                    if episode_result['reward'] > best_reward:
                        best_reward = episode_result['reward']
                        # ä¿å­˜æœ€ä½³æ¨¡å‹
                        await self._save_best_model(algorithm_name, episode, best_reward)
                    
                    # å®šæœŸä¿å­˜æ¨¡å‹
                    if (episode + 1) % save_interval == 0:
                        await self._save_checkpoint(algorithm_name, episode)
                    
                    # å®šæœŸè©•ä¼°
                    if (episode + 1) % eval_interval == 0:
                        eval_results = await self._evaluate_algorithm()
                        logger.info(f"Episode {episode + 1} è©•ä¼°çµæœ: {eval_results}")
                    
                    # è¨˜éŒ„é€²åº¦
                    if (episode + 1) % 10 == 0:
                        stats = self.training_metrics.get_recent_stats()
                        logger.info(f"Episode {episode + 1}/{episodes}: {stats}")
                
                except Exception as e:
                    logger.error(f"Episode {episode} è¨“ç·´å¤±æ•—: {e}")
                    continue
            
            self.training_metrics.end_time = datetime.now()
            training_duration = (self.training_metrics.end_time - self.training_metrics.start_time).total_seconds()
            
            # æœ€çµ‚ä¿å­˜
            await self._save_final_model(algorithm_name, episodes)
            
            final_stats = self.training_metrics.get_recent_stats()
            logger.info(f"è¨“ç·´å®Œæˆï¼æœ€çµ‚çµ±è¨ˆ: {final_stats}")
            
            return {
                'status': 'completed',
                'episodes': episodes,
                'duration_seconds': training_duration,
                'best_reward': best_reward,
                'final_stats': final_stats,
                'model_saved': True
            }
            
        except Exception as e:
            logger.error(f"è¨“ç·´éç¨‹å¤±æ•—: {e}")
            return {'status': 'training_failed', 'error': str(e)}
        
        finally:
            self.is_training = False
    
    async def _run_training_episode(self) -> Dict[str, Any]:
        """åŸ·è¡Œä¸€å€‹è¨“ç·´å›åˆ"""
        observation, _ = self.current_env.reset()
        total_reward = 0.0
        step_count = 0
        handover_count = 0
        done = False
        
        while not done:
            # å°‡è§€å¯Ÿè½‰æ›ç‚º HandoverContext
            context = self._observation_to_context(observation)
            
            # ç²å–ç®—æ³•æ±ºç­–
            decision = await self.current_algorithm.predict_handover(context)
            
            # å°‡æ±ºç­–è½‰æ›ç‚ºç’°å¢ƒå‹•ä½œ
            action = self._decision_to_action(decision)
            
            # åŸ·è¡Œå‹•ä½œ
            next_observation, reward, terminated, truncated, info = self.current_env.step(action)
            done = terminated or truncated
            
            # çµ±è¨ˆæ›æ‰‹æ¬¡æ•¸
            if hasattr(decision, 'handover_decision') and decision.handover_decision.name != 'NO_HANDOVER':
                handover_count += 1
            
            # å­˜å„²ç¶“é©— (å¦‚æœç®—æ³•æ”¯æŒ)
            if hasattr(self.current_algorithm, 'store_experience'):
                try:
                    import torch
                    self.current_algorithm.store_experience(
                        torch.tensor(observation, dtype=torch.float32),
                        action,
                        reward,
                        torch.tensor(next_observation, dtype=torch.float32),
                        done
                    )
                except ImportError:
                    # å¦‚æœ PyTorch ä¸å¯ç”¨ï¼Œè·³éç¶“é©—å­˜å„²
                    pass
            
            total_reward += reward
            step_count += 1
            observation = next_observation
        
        success = info.get('success', total_reward > 0)
        
        return {
            'reward': total_reward,
            'length': step_count,
            'success': success,
            'handover_count': handover_count
        }
    
    def _observation_to_context(self, observation: np.ndarray) -> HandoverContext:
        """å°‡ç’°å¢ƒè§€å¯Ÿè½‰æ›ç‚º HandoverContext"""
        # é€™è£¡éœ€è¦æ ¹æ“šå…·é«”çš„ç’°å¢ƒå¯¦ç¾
        # æš«æ™‚å‰µå»ºä¸€å€‹ç°¡åŒ–çš„ä¸Šä¸‹æ–‡
        from .interfaces import GeoCoordinate, SignalMetrics, SatelliteInfo
        
        return HandoverContext(
            ue_id="training_ue",
            ue_location=GeoCoordinate(latitude=0.0, longitude=0.0),
            current_satellite="satellite_1",
            current_signal_metrics=SignalMetrics(rsrp=-80.0, rsrq=-10.0, sinr=15.0, throughput=100.0, latency=50.0),
            candidate_satellites=[],
            timestamp=datetime.now()
        )
    
    def _decision_to_action(self, decision) -> int:
        """å°‡ HandoverDecision è½‰æ›ç‚ºç’°å¢ƒå‹•ä½œ"""
        # ç°¡åŒ–çš„æ±ºç­–åˆ°å‹•ä½œæ˜ å°„
        if hasattr(decision, 'handover_decision'):
            if decision.handover_decision.name == 'NO_HANDOVER':
                return 0
            elif decision.handover_decision.name == 'IMMEDIATE_HANDOVER':
                return 1
            elif decision.handover_decision.name == 'PREPARE_HANDOVER':
                return 2
        return 0
    
    async def _evaluate_algorithm(self, eval_episodes: int = 10) -> Dict[str, float]:
        """è©•ä¼°ç®—æ³•æ€§èƒ½"""
        eval_rewards = []
        eval_success = []
        
        for _ in range(eval_episodes):
            try:
                result = await self._run_training_episode()
                eval_rewards.append(result['reward'])
                eval_success.append(result['success'])
            except Exception as e:
                logger.warning(f"è©•ä¼°å›åˆå¤±æ•—: {e}")
                continue
        
        if eval_rewards:
            return {
                'mean_reward': np.mean(eval_rewards),
                'std_reward': np.std(eval_rewards),
                'success_rate': np.mean(eval_success),
                'episodes_evaluated': len(eval_rewards)
            }
        
        return {'mean_reward': 0.0, 'std_reward': 0.0, 'success_rate': 0.0, 'episodes_evaluated': 0}
    
    async def _save_checkpoint(self, algorithm_name: str, episode: int):
        """ä¿å­˜è¨“ç·´æª¢æŸ¥é»"""
        try:
            checkpoint_path = self.model_save_dir / f"{algorithm_name}_checkpoint_ep{episode}.pth"
            await self.current_algorithm.save_model(str(checkpoint_path))
            logger.info(f"æª¢æŸ¥é»å·²ä¿å­˜: {checkpoint_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æª¢æŸ¥é»å¤±æ•—: {e}")
    
    async def _save_best_model(self, algorithm_name: str, episode: int, reward: float):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        try:
            best_model_path = self.model_save_dir / f"{algorithm_name}_best.pth"
            await self.current_algorithm.save_model(str(best_model_path))
            logger.info(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {best_model_path} (Episode {episode}, Reward: {reward:.2f})")
        except Exception as e:
            logger.error(f"ä¿å­˜æœ€ä½³æ¨¡å‹å¤±æ•—: {e}")
    
    async def _save_final_model(self, algorithm_name: str, episodes: int):
        """ä¿å­˜æœ€çµ‚æ¨¡å‹"""
        try:
            final_model_path = self.model_save_dir / f"{algorithm_name}_final_ep{episodes}.pth"
            await self.current_algorithm.save_model(str(final_model_path))
            logger.info(f"æœ€çµ‚æ¨¡å‹å·²ä¿å­˜: {final_model_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜æœ€çµ‚æ¨¡å‹å¤±æ•—: {e}")
    
    def get_training_status(self) -> Dict[str, Any]:
        """ç²å–ç•¶å‰è¨“ç·´ç‹€æ…‹"""
        stats = self.training_metrics.get_recent_stats()
        
        return {
            'is_training': self.is_training,
            'current_algorithm': self.current_algorithm.name if self.current_algorithm else None,
            'training_metrics': stats,
            'start_time': self.training_metrics.start_time.isoformat() if self.training_metrics.start_time else None,
            'total_episodes': len(self.training_metrics.episode_rewards)
        }
    
    async def stop_training(self):
        """åœæ­¢è¨“ç·´"""
        if self.is_training:
            self.is_training = False
            logger.info("è¨“ç·´å·²åœæ­¢")
        else:
            logger.warning("æ²’æœ‰æ­£åœ¨é€²è¡Œçš„è¨“ç·´")
    
    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        if self.current_env:
            self.current_env.close()
            self.current_env = None
        
        self.current_algorithm = None
        logger.info("è¨“ç·´ç®¡ç·šè³‡æºå·²æ¸…ç†")