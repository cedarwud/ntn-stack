"""
ğŸ¤– åŸºç¤å¼·åŒ–å­¸ç¿’ç®—æ³•

ç‚ºå¼·åŒ–å­¸ç¿’æ›æ‰‹ç®—æ³•æä¾›é€šç”¨çš„åŸºç¤å¯¦ç¾ã€‚
"""

import logging
import numpy as np
from abc import abstractmethod
from typing import Dict, Any, Optional, Tuple
from datetime import datetime
import torch
import torch.nn as nn
from pathlib import Path

try:
    import gymnasium as gym
    GYMNASIUM_AVAILABLE = True
except ImportError:
    GYMNASIUM_AVAILABLE = False

from ..interfaces import (
    RLHandoverAlgorithm,
    HandoverContext,
    HandoverDecision,
    AlgorithmInfo,
    AlgorithmType,
    HandoverDecisionType,
    GeoCoordinate,
    SignalMetrics
)

logger = logging.getLogger(__name__)


class BaseRLHandoverAlgorithm(RLHandoverAlgorithm):
    """å¼·åŒ–å­¸ç¿’æ›æ‰‹ç®—æ³•åŸºé¡
    
    æä¾› RL ç®—æ³•çš„é€šç”¨åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
    - ä¸Šä¸‹æ–‡è½‰æ›
    - æ¨¡å‹ç®¡ç†
    - è¨“ç·´ç›£æ§
    - æ±ºç­–å¾Œè™•ç†
    """
    
    def __init__(self, name: str, config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ– RL ç®—æ³•
        
        Args:
            name: ç®—æ³•åç¨±
            config: ç®—æ³•é…ç½®
        """
        super().__init__(name, config)
        
        # æ¨¡å‹ç›¸é—œ
        self.model: Optional[nn.Module] = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_path: Optional[str] = None
        self.is_trained = False
        
        # è¨“ç·´ç›¸é—œ
        self.training_config = config.get('training_config', {}) if config else {}
        self.inference_config = config.get('inference_config', {}) if config else {}
        
        # ç’°å¢ƒé©é…
        self.observation_dim = config.get('observation_dim', 54) if config else 54  # é è¨­è§€å¯Ÿç¶­åº¦
        self.action_dim = config.get('action_dim', 11) if config else 11  # é è¨­å‹•ä½œç¶­åº¦
        
        # æ€§èƒ½çµ±è¨ˆ
        self._rl_statistics = {
            'total_predictions': 0,
            'average_confidence': 0.0,
            'model_inference_time': 0.0,
            'last_model_update': None,
            'training_episodes': 0,
            'best_reward': -float('inf')
        }
        
        logger.info(f"RL ç®—æ³• '{name}' åˆå§‹åŒ–å®Œæˆï¼Œè¨­å‚™: {self.device}")
    
    async def _initialize_algorithm(self) -> None:
        """åˆå§‹åŒ–ç®—æ³•ç‰¹å®šé‚è¼¯"""
        # å‰µå»ºæ¨¡å‹
        await self._create_model()
        
        # è¼‰å…¥é è¨“ç·´æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        model_path = self.config.get('model_path')
        if model_path and Path(model_path).exists():
            try:
                await self.load_model(model_path)
                logger.info(f"è¼‰å…¥é è¨“ç·´æ¨¡å‹: {model_path}")
            except Exception as e:
                logger.warning(f"è¼‰å…¥é è¨“ç·´æ¨¡å‹å¤±æ•—: {e}")
    
    @abstractmethod
    async def _create_model(self) -> None:
        """å‰µå»ºç¥ç¶“ç¶²è·¯æ¨¡å‹ - å­é¡å¿…é ˆå¯¦ç¾"""
        pass
    
    @abstractmethod
    async def _forward_pass(self, observation: torch.Tensor) -> torch.Tensor:
        """æ¨¡å‹å‰å‘å‚³æ’­ - å­é¡å¿…é ˆå¯¦ç¾
        
        Args:
            observation: è§€å¯Ÿå¼µé‡
            
        Returns:
            torch.Tensor: æ¨¡å‹è¼¸å‡º
        """
        pass
    
    async def predict_handover(self, context: HandoverContext) -> HandoverDecision:
        """åŸ·è¡Œæ›æ‰‹é æ¸¬
        
        Args:
            context: æ›æ‰‹ä¸Šä¸‹æ–‡
            
        Returns:
            HandoverDecision: æ›æ‰‹æ±ºç­–
        """
        try:
            if not self.model:
                logger.error("æ¨¡å‹æœªåˆå§‹åŒ–")
                return self._create_fallback_decision(context, "Model not initialized")
            
            # è½‰æ›ä¸Šä¸‹æ–‡ç‚ºè§€å¯Ÿ
            observation = self._context_to_observation(context)
            
            # åŸ·è¡Œæ¨¡å‹æ¨ç†
            start_time = datetime.now()
            decision_tensor = await self._forward_pass(observation)
            inference_time = (datetime.now() - start_time).total_seconds() * 1000
            
            # è½‰æ›æ¨¡å‹è¼¸å‡ºç‚ºæ±ºç­–
            decision = self._tensor_to_decision(decision_tensor, context)
            decision.algorithm_name = self.name
            decision.decision_time = inference_time
            
            # æ›´æ–°çµ±è¨ˆä¿¡æ¯
            self._update_rl_statistics(decision, inference_time)
            
            return decision
            
        except Exception as e:
            logger.error(f"RL é æ¸¬å¤±æ•—: {e}")
            return self._create_fallback_decision(context, f"Prediction error: {str(e)}")
    
    def _context_to_observation(self, context: HandoverContext) -> torch.Tensor:
        """å°‡ HandoverContext è½‰æ›ç‚ºè§€å¯Ÿå¼µé‡
        
        Args:
            context: æ›æ‰‹ä¸Šä¸‹æ–‡
            
        Returns:
            torch.Tensor: è§€å¯Ÿå¼µé‡
        """
        try:
            # åˆå§‹åŒ–è§€å¯Ÿå‘é‡
            observation = np.zeros(self.observation_dim)
            
            # UE ç‰¹å¾µ (ä½ç½®å’Œé€Ÿåº¦)
            observation[0] = context.ue_location.latitude / 90.0  # æ­£è¦åŒ–ç·¯åº¦
            observation[1] = context.ue_location.longitude / 180.0  # æ­£è¦åŒ–ç¶“åº¦
            
            if context.ue_velocity:
                observation[2] = np.clip(context.ue_velocity.latitude / 100.0, -1, 1)  # é€Ÿåº¦æ­£è¦åŒ–
                observation[3] = np.clip(context.ue_velocity.longitude / 100.0, -1, 1)
            
            # ç•¶å‰è¡›æ˜Ÿç‰¹å¾µ
            if context.current_satellite and context.current_signal_metrics:
                observation[4] = 1.0  # æœ‰ç•¶å‰è¡›æ˜Ÿ
                observation[5] = np.clip(context.current_signal_metrics.rsrp / 50.0 + 2, 0, 1)  # RSRP æ­£è¦åŒ–
                observation[6] = np.clip(context.current_signal_metrics.rsrq / 20.0 + 1, 0, 1)  # RSRQ æ­£è¦åŒ–
                observation[7] = np.clip(context.current_signal_metrics.sinr / 30.0, 0, 1)  # SINR æ­£è¦åŒ–
                observation[8] = np.clip(context.current_signal_metrics.throughput / 1000.0, 0, 1)  # ååé‡æ­£è¦åŒ–
                observation[9] = np.clip(1.0 - context.current_signal_metrics.latency / 1000.0, 0, 1)  # å»¶é²æ­£è¦åŒ–ï¼ˆåå‘ï¼‰
            
            # å€™é¸è¡›æ˜Ÿç‰¹å¾µ (æœ€å¤š 10 å€‹è¡›æ˜Ÿ)
            max_satellites = min(len(context.candidate_satellites), 10)
            for i, satellite in enumerate(context.candidate_satellites[:max_satellites]):
                start_idx = 10 + i * 4
                
                observation[start_idx] = 1.0  # è¡›æ˜Ÿå­˜åœ¨
                observation[start_idx + 1] = satellite.position.latitude / 90.0
                observation[start_idx + 2] = satellite.position.longitude / 180.0
                
                if satellite.signal_metrics:
                    observation[start_idx + 3] = np.clip(satellite.signal_metrics.sinr / 30.0, 0, 1)
            
            # ç¶²è·¯ç‹€æ…‹ç‰¹å¾µ
            network_start = 50
            network_state = context.network_state or {}
            observation[network_start] = np.clip(network_state.get('network_load', 0.0), 0, 1)
            observation[network_start + 1] = np.clip(network_state.get('interference_level', 0.0), 0, 1)
            observation[network_start + 2] = np.clip(network_state.get('total_ues', 0) / 1000.0, 0, 1)
            observation[network_start + 3] = np.clip(network_state.get('active_satellites', 0) / 20.0, 0, 1)
            
            # è½‰æ›ç‚º PyTorch å¼µé‡
            return torch.FloatTensor(observation).unsqueeze(0).to(self.device)
            
        except Exception as e:
            logger.error(f"ä¸Šä¸‹æ–‡è½‰æ›å¤±æ•—: {e}")
            # è¿”å›é›¶å‘é‡
            return torch.zeros((1, self.observation_dim)).to(self.device)
    
    def _tensor_to_decision(self, tensor: torch.Tensor, context: HandoverContext) -> HandoverDecision:
        """å°‡æ¨¡å‹è¼¸å‡ºå¼µé‡è½‰æ›ç‚º HandoverDecision
        
        Args:
            tensor: æ¨¡å‹è¼¸å‡ºå¼µé‡
            context: åŸå§‹ä¸Šä¸‹æ–‡
            
        Returns:
            HandoverDecision: æ›æ‰‹æ±ºç­–
        """
        try:
            # å‡è¨­æ¨¡å‹è¼¸å‡ºæ˜¯ [no_handover, immediate_handover, prepare_handover, satellite_probs...]
            output = tensor.cpu().numpy().flatten()
            
            # æ±ºç­–é¡å‹æ¦‚ç‡
            decision_probs = output[:3]
            decision_type_idx = np.argmax(decision_probs)
            confidence = float(np.max(decision_probs))
            
            # æ˜ å°„æ±ºç­–é¡å‹
            decision_type_map = {
                0: HandoverDecisionType.NO_HANDOVER,
                1: HandoverDecisionType.IMMEDIATE_HANDOVER,
                2: HandoverDecisionType.PREPARE_HANDOVER
            }
            decision_type = decision_type_map.get(decision_type_idx, HandoverDecisionType.NO_HANDOVER)
            
            # å¦‚æœéœ€è¦æ›æ‰‹ï¼Œé¸æ“‡ç›®æ¨™è¡›æ˜Ÿ
            target_satellite = None
            if decision_type != HandoverDecisionType.NO_HANDOVER and context.candidate_satellites:
                if len(output) > 3:
                    # ä½¿ç”¨æ¨¡å‹è¼¸å‡ºçš„è¡›æ˜Ÿæ¦‚ç‡
                    satellite_probs = output[3:3+len(context.candidate_satellites)]
                    best_satellite_idx = np.argmax(satellite_probs)
                    target_satellite = context.candidate_satellites[best_satellite_idx].satellite_id
                else:
                    # ä½¿ç”¨ä¿¡è™Ÿå¼·åº¦é¸æ“‡æœ€ä½³è¡›æ˜Ÿ
                    best_satellite = max(
                        context.candidate_satellites,
                        key=lambda s: s.signal_metrics.sinr if s.signal_metrics else -float('inf')
                    )
                    target_satellite = best_satellite.satellite_id
            
            return HandoverDecision(
                target_satellite=target_satellite,
                handover_decision=decision_type,
                confidence=confidence,
                timing=datetime.now() if decision_type != HandoverDecisionType.NO_HANDOVER else None,
                decision_reason=f"RL decision: {decision_type.name} (confidence: {confidence:.3f})",
                algorithm_name=self.name,
                decision_time=0.0,  # å°‡ç”±èª¿ç”¨è€…è¨­ç½®
                metadata={
                    'decision_probs': decision_probs.tolist(),
                    'model_output_shape': tensor.shape,
                    'device': str(self.device)
                }
            )
            
        except Exception as e:
            logger.error(f"å¼µé‡è½‰æ±ºç­–å¤±æ•—: {e}")
            return self._create_fallback_decision(context, f"Tensor conversion error: {str(e)}")
    
    def _create_fallback_decision(self, context: HandoverContext, reason: str) -> HandoverDecision:
        """å‰µå»ºå›é€€æ±ºç­–
        
        Args:
            context: ä¸Šä¸‹æ–‡
            reason: å›é€€åŸå› 
            
        Returns:
            HandoverDecision: å›é€€æ±ºç­–
        """
        return HandoverDecision(
            target_satellite=None,
            handover_decision=HandoverDecisionType.NO_HANDOVER,
            confidence=0.1,
            timing=None,
            decision_reason=f"Fallback: {reason}",
            algorithm_name=self.name,
            decision_time=0.0,
            metadata={'fallback': True, 'error': reason}
        )
    
    def _update_rl_statistics(self, decision: HandoverDecision, inference_time: float) -> None:
        """æ›´æ–° RL çµ±è¨ˆä¿¡æ¯
        
        Args:
            decision: æ±ºç­–çµæœ
            inference_time: æ¨ç†æ™‚é–“
        """
        self._rl_statistics['total_predictions'] += 1
        
        # æ›´æ–°å¹³å‡ä¿¡å¿ƒåº¦
        total_predictions = self._rl_statistics['total_predictions']
        prev_avg_confidence = self._rl_statistics['average_confidence']
        self._rl_statistics['average_confidence'] = (
            (prev_avg_confidence * (total_predictions - 1) + decision.confidence) / total_predictions
        )
        
        # æ›´æ–°æ¨ç†æ™‚é–“
        self._rl_statistics['model_inference_time'] = inference_time
    
    async def load_model(self, model_path: str) -> None:
        """è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æª”æ¡ˆè·¯å¾‘
        """
        try:
            if not self.model:
                await self._create_model()
            
            checkpoint = torch.load(model_path, map_location=self.device)
            
            if isinstance(checkpoint, dict):
                if 'model_state_dict' in checkpoint:
                    self.model.load_state_dict(checkpoint['model_state_dict'])
                    self._rl_statistics['training_episodes'] = checkpoint.get('episodes', 0)
                    self._rl_statistics['best_reward'] = checkpoint.get('best_reward', -float('inf'))
                else:
                    self.model.load_state_dict(checkpoint)
            else:
                self.model.load_state_dict(checkpoint)
            
            self.model.eval()
            self.model_path = model_path
            self.is_trained = True
            self._rl_statistics['last_model_update'] = datetime.now().isoformat()
            
            logger.info(f"æ¨¡å‹è¼‰å…¥æˆåŠŸ: {model_path}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise
    
    async def save_model(self, model_path: str) -> None:
        """ä¿å­˜æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹ä¿å­˜è·¯å¾‘
        """
        try:
            if not self.model:
                raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
            
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            Path(model_path).parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜æ¨¡å‹å’Œè¨“ç·´ä¿¡æ¯
            checkpoint = {
                'model_state_dict': self.model.state_dict(),
                'episodes': self._rl_statistics['training_episodes'],
                'best_reward': self._rl_statistics['best_reward'],
                'config': self.config,
                'algorithm_name': self.name,
                'saved_at': datetime.now().isoformat()
            }
            
            torch.save(checkpoint, model_path)
            self.model_path = model_path
            self._rl_statistics['last_model_update'] = datetime.now().isoformat()
            
            logger.info(f"æ¨¡å‹ä¿å­˜æˆåŠŸ: {model_path}")
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ä¿å­˜å¤±æ•—: {e}")
            raise
    
    def get_model_info(self) -> Dict[str, Any]:
        """ç²å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: æ¨¡å‹å…ƒæ•¸æ“š
        """
        model_info = {
            'model_type': self.__class__.__name__,
            'device': str(self.device),
            'is_trained': self.is_trained,
            'model_path': self.model_path,
            'observation_dim': self.observation_dim,
            'action_dim': self.action_dim,
            'parameters_count': sum(p.numel() for p in self.model.parameters()) if self.model else 0,
            'training_config': self.training_config,
            'inference_config': self.inference_config,
            'statistics': self._rl_statistics
        }
        
        if self.model:
            model_info['model_architecture'] = str(self.model)
        
        return model_info
    
    def get_algorithm_info(self) -> AlgorithmInfo:
        """ç²å–ç®—æ³•ä¿¡æ¯"""
        return AlgorithmInfo(
            name=self.name,
            version="1.0.0-rl",
            algorithm_type=AlgorithmType.REINFORCEMENT_LEARNING,
            description=f"å¼·åŒ–å­¸ç¿’æ›æ‰‹ç®—æ³•: {self.__class__.__name__}",
            parameters={
                **self.config,
                'model_info': self.get_model_info()
            },
            author="RL Implementation",
            created_at=datetime.now(),
            performance_metrics={
                'average_confidence': self._rl_statistics['average_confidence'],
                'total_predictions': self._rl_statistics['total_predictions'],
                'model_inference_time_ms': self._rl_statistics['model_inference_time']
            },
            supported_scenarios=["urban", "suburban", "complex", "dynamic"]
        )