"""
ğŸ¤– PPO æ›æ‰‹æ™ºèƒ½é«”

åŸºæ–¼è¿‘ç«¯ç­–ç•¥å„ªåŒ– (Proximal Policy Optimization) çš„è¡›æ˜Ÿæ›æ‰‹æ±ºç­–ç®—æ³•ã€‚
"""

import logging
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime

from .base_rl_algorithm import BaseRLHandoverAlgorithm
from ..interfaces import HandoverContext, HandoverDecision, AlgorithmInfo

logger = logging.getLogger(__name__)


class PPONetwork(nn.Module):
    """PPO æ¼”å“¡-è©•è«–å®¶ç¶²è·¯æ¶æ§‹"""
    
    def __init__(self, input_dim: int, hidden_dim: int = 256, action_dim: int = 11):
        """åˆå§‹åŒ– PPO ç¶²è·¯
        
        Args:
            input_dim: è¼¸å…¥ç¶­åº¦
            hidden_dim: éš±è—å±¤ç¶­åº¦
            action_dim: å‹•ä½œç©ºé–“å¤§å°
        """
        super(PPONetwork, self).__init__()
        
        # å…±äº«ç‰¹å¾µæå–å™¨
        self.shared_layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )
        
        # æ¼”å“¡ç¶²è·¯ (ç­–ç•¥ç¶²è·¯)
        self.actor = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, action_dim),
            nn.Softmax(dim=-1)
        )
        
        # è©•è«–å®¶ç¶²è·¯ (åƒ¹å€¼ç¶²è·¯)
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç¶²è·¯æ¬Šé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
                nn.init.constant_(module.bias, 0)
        
        # ç­–ç•¥ç¶²è·¯æœ€å¾Œä¸€å±¤ä½¿ç”¨è¼ƒå°çš„æ¬Šé‡
        nn.init.orthogonal_(self.actor[-2].weight, gain=0.01)
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥å¼µé‡
            
        Returns:
            Tuple[torch.Tensor, torch.Tensor]: (å‹•ä½œæ¦‚ç‡, ç‹€æ…‹åƒ¹å€¼)
        """
        shared_features = self.shared_layers(x)
        action_probs = self.actor(shared_features)
        state_values = self.critic(shared_features)
        return action_probs, state_values
    
    def get_action_and_value(self, x: torch.Tensor, action: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """ç²å–å‹•ä½œå’Œåƒ¹å€¼ï¼ˆç”¨æ–¼è¨“ç·´ï¼‰
        
        Args:
            x: ç‹€æ…‹å¼µé‡
            action: æ¡å–çš„å‹•ä½œ (å¯é¸ï¼Œç”¨æ–¼è¨ˆç®—å°æ•¸æ¦‚ç‡)
            
        Returns:
            Tuple: (æ¡æ¨£å‹•ä½œ, å°æ•¸æ¦‚ç‡, ç†µ, ç‹€æ…‹åƒ¹å€¼)
        """
        action_probs, state_values = self.forward(x)
        
        # å‰µå»ºå‹•ä½œåˆ†ä½ˆ
        dist = torch.distributions.Categorical(action_probs)
        
        if action is None:
            action = dist.sample()
        
        log_probs = dist.log_prob(action)
        entropy = dist.entropy()
        
        return action, log_probs, entropy, state_values.squeeze(-1)


class PPOHandoverAgent(BaseRLHandoverAlgorithm):
    """PPO æ›æ‰‹æ™ºèƒ½é«”
    
    å¯¦ç¾åŸºæ–¼ PPO çš„è¡›æ˜Ÿæ›æ‰‹æ±ºç­–ç®—æ³•ï¼ŒåŒ…æ‹¬ï¼š
    - æ¼”å“¡-è©•è«–å®¶æ¶æ§‹
    - å‰ªåˆ‡ç›®æ¨™å‡½æ•¸
    - å»£ç¾©å„ªå‹¢ä¼°è¨ˆ (GAE)
    - åƒ¹å€¼å‡½æ•¸å­¸ç¿’
    """
    
    def __init__(self, name: str = "ppo_handover", config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ– PPO æ™ºèƒ½é«”
        
        Args:
            name: ç®—æ³•åç¨±
            config: ç®—æ³•é…ç½®
        """
        super().__init__(name, config)
        
        # PPO ç‰¹å®šé…ç½®
        self.hidden_dim = self.training_config.get('hidden_dim', 256)
        self.learning_rate = self.training_config.get('learning_rate', 0.0003)
        self.clip_epsilon = self.training_config.get('clip_epsilon', 0.2)
        self.value_coefficient = self.training_config.get('value_coefficient', 0.5)
        self.entropy_coefficient = self.training_config.get('entropy_coefficient', 0.01)
        self.batch_size = self.training_config.get('batch_size', 256)
        self.minibatch_size = self.training_config.get('minibatch_size', 64)
        self.epochs_per_update = self.training_config.get('epochs_per_update', 4)
        self.gamma = self.training_config.get('gamma', 0.99)
        self.gae_lambda = self.training_config.get('gae_lambda', 0.95)
        self.max_grad_norm = self.training_config.get('max_grad_norm', 0.5)
        
        # æ¨ç†é…ç½®
        self.temperature = self.inference_config.get('temperature', 1.0)
        
        # å„ªåŒ–å™¨
        self.optimizer: Optional[optim.Adam] = None
        
        # ç¶“é©—ç·©è¡å€
        self.rollout_buffer = []
        self.episode_rewards = []
        
        # è¨“ç·´çµ±è¨ˆ
        self.update_count = 0
        self.total_timesteps = 0
        
        logger.info(f"PPO æ™ºèƒ½é«” '{name}' åˆå§‹åŒ–å®Œæˆ")
    
    async def _create_model(self) -> None:
        """å‰µå»º PPO æ¨¡å‹"""
        try:
            self.model = PPONetwork(
                input_dim=self.observation_dim,
                hidden_dim=self.hidden_dim,
                action_dim=self.action_dim
            ).to(self.device)
            
            # è¨­ç½®å„ªåŒ–å™¨
            self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, eps=1e-5)
            
            logger.info(f"PPO æ¨¡å‹å‰µå»ºå®Œæˆï¼Œåƒæ•¸é‡: {sum(p.numel() for p in self.model.parameters())}")
            
        except Exception as e:
            logger.error(f"PPO æ¨¡å‹å‰µå»ºå¤±æ•—: {e}")
            raise
    
    async def _forward_pass(self, observation: torch.Tensor) -> torch.Tensor:
        """PPO å‰å‘å‚³æ’­
        
        Args:
            observation: è§€å¯Ÿå¼µé‡
            
        Returns:
            torch.Tensor: æ±ºç­–è¼¸å‡º
        """
        self.model.eval()
        with torch.no_grad():
            action_probs, _ = self.model(observation)
            
            # æ‡‰ç”¨æº«åº¦ç¸®æ”¾
            if self.temperature != 1.0:
                action_probs = torch.pow(action_probs, 1.0 / self.temperature)
                action_probs = action_probs / action_probs.sum(dim=-1, keepdim=True)
            
            # è½‰æ›ç‚ºæ±ºç­–æ ¼å¼
            decision_tensor = self._action_probs_to_decision(action_probs)
            
            return decision_tensor
    
    def _action_probs_to_decision(self, action_probs: torch.Tensor) -> torch.Tensor:
        """å°‡å‹•ä½œæ¦‚ç‡è½‰æ›ç‚ºæ±ºç­–æ ¼å¼
        
        Args:
            action_probs: å‹•ä½œæ¦‚ç‡å¼µé‡ [batch_size, action_dim]
            
        Returns:
            torch.Tensor: æ±ºç­–å¼µé‡ [batch_size, decision_dim]
        """
        batch_size = action_probs.size(0)
        decision_dim = 3 + 8  # 3 å€‹æ±ºç­–é¡å‹ + 8 å€‹å€™é¸è¡›æ˜Ÿæ¦‚ç‡
        
        decision_tensor = torch.zeros(batch_size, decision_dim).to(self.device)
        
        # å‰ 3 å€‹å‹•ä½œå°æ‡‰æ±ºç­–é¡å‹
        decision_tensor[:, :3] = action_probs[:, :3]
        
        # å¾Œé¢çš„å‹•ä½œå°æ‡‰è¡›æ˜Ÿé¸æ“‡æ¦‚ç‡
        if action_probs.size(1) > 3:
            decision_tensor[:, 3:3+min(8, action_probs.size(1)-3)] = action_probs[:, 3:3+min(8, action_probs.size(1)-3)]
        
        return decision_tensor
    
    def store_experience(self, state: torch.Tensor, action: int, reward: float, 
                        log_prob: float, value: float, done: bool) -> None:
        """å­˜å„²ç¶“é©—åˆ°å›æ”¾ç·©è¡å€
        
        Args:
            state: ç•¶å‰ç‹€æ…‹
            action: æ¡å–çš„å‹•ä½œ
            reward: ç²å¾—çš„çå‹µ
            log_prob: å‹•ä½œå°æ•¸æ¦‚ç‡
            value: ç‹€æ…‹åƒ¹å€¼
            done: æ˜¯å¦çµæŸ
        """
        experience = {
            'state': state,
            'action': action,
            'reward': reward,
            'log_prob': log_prob,
            'value': value,
            'done': done
        }
        
        self.rollout_buffer.append(experience)
        self.total_timesteps += 1
    
    def compute_gae(self, rewards: List[float], values: List[float], 
                   dones: List[bool], next_value: float = 0.0) -> Tuple[List[float], List[float]]:
        """è¨ˆç®—å»£ç¾©å„ªå‹¢ä¼°è¨ˆ (GAE)
        
        Args:
            rewards: çå‹µåˆ—è¡¨
            values: åƒ¹å€¼åˆ—è¡¨
            dones: çµæŸæ¨™èªŒåˆ—è¡¨
            next_value: ä¸‹ä¸€å€‹ç‹€æ…‹çš„åƒ¹å€¼
            
        Returns:
            Tuple[List[float], List[float]]: (å„ªå‹¢, å›å ±)
        """
        advantages = []
        returns = []
        
        gae = 0
        next_value = next_value
        
        for step in reversed(range(len(rewards))):
            if step == len(rewards) - 1:
                next_non_terminal = 1.0 - dones[step]
                next_values = next_value
            else:
                next_non_terminal = 1.0 - dones[step]
                next_values = values[step + 1]
            
            delta = rewards[step] + self.gamma * next_values * next_non_terminal - values[step]
            gae = delta + self.gamma * self.gae_lambda * next_non_terminal * gae
            
            advantages.insert(0, gae)
            returns.insert(0, gae + values[step])
        
        return advantages, returns
    
    async def train_step(self) -> Dict[str, float]:
        """åŸ·è¡Œ PPO è¨“ç·´æ›´æ–°
        
        Returns:
            Dict[str, float]: è¨“ç·´æŒ‡æ¨™
        """
        if len(self.rollout_buffer) < self.batch_size:
            return {'policy_loss': 0.0, 'value_loss': 0.0, 'entropy_loss': 0.0}
        
        try:
            # æº–å‚™è¨“ç·´æ•¸æ“š
            states = torch.stack([exp['state'] for exp in self.rollout_buffer]).to(self.device)
            actions = torch.tensor([exp['action'] for exp in self.rollout_buffer], dtype=torch.long).to(self.device)
            old_log_probs = torch.tensor([exp['log_prob'] for exp in self.rollout_buffer], dtype=torch.float32).to(self.device)
            rewards = [exp['reward'] for exp in self.rollout_buffer]
            values = [exp['value'] for exp in self.rollout_buffer]
            dones = [exp['done'] for exp in self.rollout_buffer]
            
            # è¨ˆç®—å„ªå‹¢å’Œå›å ±
            advantages, returns = self.compute_gae(rewards, values, dones)
            advantages = torch.tensor(advantages, dtype=torch.float32).to(self.device)
            returns = torch.tensor(returns, dtype=torch.float32).to(self.device)
            
            # æ­£è¦åŒ–å„ªå‹¢
            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
            
            # å¤šè¼ªè¨“ç·´
            total_policy_loss = 0.0
            total_value_loss = 0.0
            total_entropy_loss = 0.0
            
            for epoch in range(self.epochs_per_update):
                # éš¨æ©Ÿæ’åˆ—æ•¸æ“š
                indices = torch.randperm(len(self.rollout_buffer))
                
                for start in range(0, len(self.rollout_buffer), self.minibatch_size):
                    end = start + self.minibatch_size
                    minibatch_indices = indices[start:end]
                    
                    # ç²å–å°æ‰¹æ¬¡æ•¸æ“š
                    mb_states = states[minibatch_indices]
                    mb_actions = actions[minibatch_indices]
                    mb_old_log_probs = old_log_probs[minibatch_indices]
                    mb_advantages = advantages[minibatch_indices]
                    mb_returns = returns[minibatch_indices]
                    
                    # å‰å‘å‚³æ’­
                    _, new_log_probs, entropy, new_values = self.model.get_action_and_value(mb_states, mb_actions)
                    
                    # è¨ˆç®—ç­–ç•¥æå¤±
                    ratio = torch.exp(new_log_probs - mb_old_log_probs)
                    surr1 = ratio * mb_advantages
                    surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * mb_advantages
                    policy_loss = -torch.min(surr1, surr2).mean()
                    
                    # è¨ˆç®—åƒ¹å€¼æå¤±
                    value_loss = nn.MSELoss()(new_values, mb_returns)
                    
                    # è¨ˆç®—ç†µæå¤±
                    entropy_loss = -entropy.mean()
                    
                    # ç¸½æå¤±
                    total_loss = policy_loss + self.value_coefficient * value_loss + self.entropy_coefficient * entropy_loss
                    
                    # åå‘å‚³æ’­
                    self.optimizer.zero_grad()
                    total_loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
                    self.optimizer.step()
                    
                    total_policy_loss += policy_loss.item()
                    total_value_loss += value_loss.item()
                    total_entropy_loss += entropy_loss.item()
            
            # æ¸…ç©ºç·©è¡å€
            self.rollout_buffer.clear()
            self.update_count += 1
            
            # æ›´æ–°çµ±è¨ˆä¿¡æ¯
            self._rl_statistics['training_episodes'] = self.update_count
            
            num_updates = self.epochs_per_update * (len(states) // self.minibatch_size)
            
            return {
                'policy_loss': total_policy_loss / num_updates,
                'value_loss': total_value_loss / num_updates,
                'entropy_loss': total_entropy_loss / num_updates,
                'total_timesteps': self.total_timesteps,
                'update_count': self.update_count
            }
            
        except Exception as e:
            logger.error(f"PPO è¨“ç·´æ­¥é©Ÿå¤±æ•—: {e}")
            return {'policy_loss': float('inf'), 'value_loss': float('inf'), 'entropy_loss': float('inf')}
    
    def get_algorithm_info(self) -> AlgorithmInfo:
        """ç²å– PPO ç®—æ³•ä¿¡æ¯"""
        base_info = super().get_algorithm_info()
        
        # æ·»åŠ  PPO ç‰¹å®šä¿¡æ¯
        base_info.description = "è¿‘ç«¯ç­–ç•¥å„ªåŒ–æ›æ‰‹æ™ºèƒ½é«” - åŸºæ–¼ç­–ç•¥æ¢¯åº¦çš„å¼·åŒ–å­¸ç¿’ç®—æ³•"
        base_info.parameters.update({
            'learning_rate': self.learning_rate,
            'clip_epsilon': self.clip_epsilon,
            'value_coefficient': self.value_coefficient,
            'entropy_coefficient': self.entropy_coefficient,
            'hidden_dim': self.hidden_dim,
            'gamma': self.gamma,
            'gae_lambda': self.gae_lambda,
            'update_count': self.update_count,
            'total_timesteps': self.total_timesteps
        })
        
        return base_info
    
    def get_training_info(self) -> Dict[str, Any]:
        """ç²å–è¨“ç·´ç›¸é—œä¿¡æ¯
        
        Returns:
            Dict[str, Any]: è¨“ç·´ä¿¡æ¯
        """
        return {
            'algorithm_type': 'PPO',
            'update_count': self.update_count,
            'total_timesteps': self.total_timesteps,
            'buffer_size': len(self.rollout_buffer),
            'learning_rate': self.learning_rate,
            'clip_epsilon': self.clip_epsilon,
            'model_parameters': sum(p.numel() for p in self.model.parameters()) if self.model else 0,
            'device': str(self.device),
            'is_training': self.training
        }