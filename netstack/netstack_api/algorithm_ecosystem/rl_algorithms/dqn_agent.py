"""
ğŸ¤– DQN æ›æ‰‹æ™ºèƒ½é«”

åŸºæ–¼æ·±åº¦ Q ç¶²è·¯ (Deep Q-Network) çš„è¡›æ˜Ÿæ›æ‰‹æ±ºç­–ç®—æ³•ã€‚
"""

import logging
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from typing import Dict, Any, Optional, Tuple
from datetime import datetime
from pathlib import Path

from .base_rl_algorithm import BaseRLHandoverAlgorithm
from ..interfaces import HandoverContext, HandoverDecision, AlgorithmInfo

logger = logging.getLogger(__name__)


class DQNNetwork(nn.Module):
    """DQN ç¥ç¶“ç¶²è·¯æ¶æ§‹"""
    
    def __init__(self, input_dim: int, hidden_dim: int = 256, output_dim: int = 11):
        """åˆå§‹åŒ– DQN ç¶²è·¯
        
        Args:
            input_dim: è¼¸å…¥ç¶­åº¦
            hidden_dim: éš±è—å±¤ç¶­åº¦
            output_dim: è¼¸å‡ºç¶­åº¦ (å‹•ä½œç©ºé–“å¤§å°)
        """
        super(DQNNetwork, self).__init__()
        
        self.feature_extractor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        
        # æ±ºç­–é ­ï¼šè¼¸å‡ºå‹•ä½œ Q å€¼
        self.decision_head = nn.Sequential(
            nn.Linear(hidden_dim // 2, output_dim),
            nn.Softmax(dim=-1)  # ç”¨æ–¼æ¦‚ç‡è¼¸å‡º
        )
        
        # åˆå§‹åŒ–æ¬Šé‡
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç¶²è·¯æ¬Šé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘å‚³æ’­
        
        Args:
            x: è¼¸å…¥å¼µé‡
            
        Returns:
            torch.Tensor: Q å€¼è¼¸å‡º
        """
        features = self.feature_extractor(x)
        q_values = self.decision_head(features)
        return q_values


class DQNHandoverAgent(BaseRLHandoverAlgorithm):
    """DQN æ›æ‰‹æ™ºèƒ½é«”
    
    å¯¦ç¾åŸºæ–¼ DQN çš„è¡›æ˜Ÿæ›æ‰‹æ±ºç­–ç®—æ³•ï¼ŒåŒ…æ‹¬ï¼š
    - ç¶“é©—å›æ”¾æ©Ÿåˆ¶
    - ç›®æ¨™ç¶²è·¯æ›´æ–°
    - Îµ-è²ªå©ªæ¢ç´¢ç­–ç•¥
    - å„ªå…ˆç¶“é©—å›æ”¾ (å¯é¸)
    """
    
    def __init__(self, name: str = "dqn_handover", config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ– DQN æ™ºèƒ½é«”
        
        Args:
            name: ç®—æ³•åç¨±
            config: ç®—æ³•é…ç½®
        """
        super().__init__(name, config)
        
        # DQN ç‰¹å®šé…ç½®
        self.hidden_dim = self.training_config.get('hidden_dim', 256)
        self.learning_rate = self.training_config.get('learning_rate', 0.0001)
        self.epsilon = self.training_config.get('epsilon_start', 1.0)
        self.epsilon_min = self.training_config.get('epsilon_min', 0.01)
        self.epsilon_decay = self.training_config.get('epsilon_decay', 0.995)
        self.target_update_frequency = self.training_config.get('target_update_frequency', 1000)
        self.batch_size = self.training_config.get('batch_size', 64)
        self.memory_size = self.training_config.get('memory_size', 100000)
        
        # æ¨ç†é…ç½®
        self.temperature = self.inference_config.get('temperature', 0.1)
        self.use_exploration = self.inference_config.get('use_exploration', False)
        
        # ç¶²è·¯å’Œå„ªåŒ–å™¨
        self.target_network: Optional[DQNNetwork] = None
        self.optimizer: Optional[optim.Adam] = None
        self.loss_fn = nn.MSELoss()
        
        # ç¶“é©—å›æ”¾
        self.memory = []
        self.memory_idx = 0
        
        # è¨“ç·´çµ±è¨ˆ
        self.training_step = 0
        self.target_update_count = 0
        
        logger.info(f"DQN æ™ºèƒ½é«” '{name}' åˆå§‹åŒ–å®Œæˆ")
    
    async def _create_model(self) -> None:
        """å‰µå»º DQN æ¨¡å‹"""
        try:
            self.model = DQNNetwork(
                input_dim=self.observation_dim,
                hidden_dim=self.hidden_dim,
                output_dim=self.action_dim
            ).to(self.device)
            
            # å‰µå»ºç›®æ¨™ç¶²è·¯
            self.target_network = DQNNetwork(
                input_dim=self.observation_dim,
                hidden_dim=self.hidden_dim,
                output_dim=self.action_dim
            ).to(self.device)
            
            # åˆå§‹åŒ–ç›®æ¨™ç¶²è·¯
            self._update_target_network()
            
            # è¨­ç½®å„ªåŒ–å™¨
            self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)
            
            logger.info(f"DQN æ¨¡å‹å‰µå»ºå®Œæˆï¼Œåƒæ•¸é‡: {sum(p.numel() for p in self.model.parameters())}")
            
        except Exception as e:
            logger.error(f"DQN æ¨¡å‹å‰µå»ºå¤±æ•—: {e}")
            raise
    
    async def _forward_pass(self, observation: torch.Tensor) -> torch.Tensor:
        """DQN å‰å‘å‚³æ’­
        
        Args:
            observation: è§€å¯Ÿå¼µé‡
            
        Returns:
            torch.Tensor: æ±ºç­–è¼¸å‡º
        """
        self.model.eval()
        with torch.no_grad():
            q_values = self.model(observation)
            
            if self.use_exploration and self.training:
                # è¨“ç·´æ™‚ä½¿ç”¨ Îµ-è²ªå©ªç­–ç•¥
                if np.random.random() < self.epsilon:
                    # éš¨æ©Ÿå‹•ä½œ
                    action_probs = torch.ones_like(q_values) / q_values.size(-1)
                else:
                    # è²ªå©ªå‹•ä½œ
                    action_probs = q_values
            else:
                # æ¨ç†æ™‚ä½¿ç”¨æº«åº¦è»Ÿæœ€å¤§åŒ–
                action_probs = torch.softmax(q_values / self.temperature, dim=-1)
            
            # è½‰æ›ç‚ºæ±ºç­–æ ¼å¼ï¼š[no_handover, immediate_handover, prepare_handover, satellite_probs...]
            decision_tensor = self._q_values_to_decision(action_probs)
            
            return decision_tensor
    
    def _q_values_to_decision(self, q_values: torch.Tensor) -> torch.Tensor:
        """å°‡ Q å€¼è½‰æ›ç‚ºæ±ºç­–æ ¼å¼
        
        Args:
            q_values: Q å€¼å¼µé‡ [batch_size, action_dim]
            
        Returns:
            torch.Tensor: æ±ºç­–å¼µé‡ [batch_size, decision_dim]
        """
        batch_size = q_values.size(0)
        decision_dim = 3 + 8  # 3 å€‹æ±ºç­–é¡å‹ + 8 å€‹å€™é¸è¡›æ˜Ÿæ¦‚ç‡
        
        decision_tensor = torch.zeros(batch_size, decision_dim).to(self.device)
        
        # å‰ 3 å€‹å‹•ä½œå°æ‡‰æ±ºç­–é¡å‹
        decision_tensor[:, :3] = q_values[:, :3]
        
        # å¾Œé¢çš„å‹•ä½œå°æ‡‰è¡›æ˜Ÿé¸æ“‡æ¦‚ç‡
        if q_values.size(1) > 3:
            # æ­¸ä¸€åŒ–è¡›æ˜Ÿé¸æ“‡æ¦‚ç‡
            satellite_probs = torch.softmax(q_values[:, 3:], dim=-1)
            decision_tensor[:, 3:3+satellite_probs.size(1)] = satellite_probs
        
        return decision_tensor
    
    def _update_target_network(self) -> None:
        """æ›´æ–°ç›®æ¨™ç¶²è·¯"""
        if self.target_network is not None:
            self.target_network.load_state_dict(self.model.state_dict())
            self.target_update_count += 1
            logger.debug(f"ç›®æ¨™ç¶²è·¯æ›´æ–° #{self.target_update_count}")
    
    def store_experience(self, state: torch.Tensor, action: int, reward: float, 
                        next_state: torch.Tensor, done: bool) -> None:
        """å­˜å„²ç¶“é©—åˆ°å›æ”¾ç·©è¡å€
        
        Args:
            state: ç•¶å‰ç‹€æ…‹
            action: æ¡å–çš„å‹•ä½œ
            reward: ç²å¾—çš„çå‹µ
            next_state: ä¸‹ä¸€å€‹ç‹€æ…‹
            done: æ˜¯å¦çµæŸ
        """
        experience = (state, action, reward, next_state, done)
        
        if len(self.memory) < self.memory_size:
            self.memory.append(experience)
        else:
            self.memory[self.memory_idx] = experience
            self.memory_idx = (self.memory_idx + 1) % self.memory_size
    
    async def train_step(self) -> Dict[str, float]:
        """åŸ·è¡Œä¸€æ­¥è¨“ç·´
        
        Returns:
            Dict[str, float]: è¨“ç·´æŒ‡æ¨™
        """
        if len(self.memory) < self.batch_size:
            return {'loss': 0.0, 'q_mean': 0.0}
        
        try:
            # æ¡æ¨£æ‰¹æ¬¡ç¶“é©—
            batch = self._sample_batch()
            states, actions, rewards, next_states, dones = zip(*batch)
            
            # è½‰æ›ç‚ºå¼µé‡
            states = torch.stack(states).to(self.device)
            actions = torch.tensor(actions, dtype=torch.long).to(self.device)
            rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)
            next_states = torch.stack(next_states).to(self.device)
            dones = torch.tensor(dones, dtype=torch.bool).to(self.device)
            
            # è¨ˆç®—ç•¶å‰ Q å€¼
            current_q_values = self.model(states).gather(1, actions.unsqueeze(1))
            
            # è¨ˆç®—ç›®æ¨™ Q å€¼
            with torch.no_grad():
                next_q_values = self.target_network(next_states).max(1)[0]
                target_q_values = rewards + (0.99 * next_q_values * ~dones)
            
            # è¨ˆç®—æå¤±
            loss = self.loss_fn(current_q_values.squeeze(), target_q_values)
            
            # åå‘å‚³æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            # æ›´æ–°çµ±è¨ˆ
            self.training_step += 1
            
            # æ›´æ–°ç›®æ¨™ç¶²è·¯
            if self.training_step % self.target_update_frequency == 0:
                self._update_target_network()
            
            # è¡°æ¸›æ¢ç´¢ç‡
            if self.epsilon > self.epsilon_min:
                self.epsilon *= self.epsilon_decay
            
            # æ›´æ–°çµ±è¨ˆä¿¡æ¯
            self._rl_statistics['training_episodes'] = self.training_step
            
            return {
                'loss': loss.item(),
                'q_mean': current_q_values.mean().item(),
                'epsilon': self.epsilon,
                'target_updates': self.target_update_count
            }
            
        except Exception as e:
            logger.error(f"DQN è¨“ç·´æ­¥é©Ÿå¤±æ•—: {e}")
            return {'loss': float('inf'), 'q_mean': 0.0}
    
    def _sample_batch(self) -> list:
        """å¾ç¶“é©—å›æ”¾ä¸­æ¡æ¨£æ‰¹æ¬¡
        
        Returns:
            list: æ¡æ¨£çš„ç¶“é©—æ‰¹æ¬¡
        """
        indices = np.random.choice(len(self.memory), self.batch_size, replace=False)
        return [self.memory[idx] for idx in indices]
    
    def get_algorithm_info(self) -> AlgorithmInfo:
        """ç²å– DQN ç®—æ³•ä¿¡æ¯"""
        base_info = super().get_algorithm_info()
        
        # æ·»åŠ  DQN ç‰¹å®šä¿¡æ¯
        base_info.description = "æ·±åº¦ Q ç¶²è·¯æ›æ‰‹æ™ºèƒ½é«” - åŸºæ–¼åƒ¹å€¼å‡½æ•¸çš„å¼·åŒ–å­¸ç¿’ç®—æ³•"
        base_info.parameters.update({
            'learning_rate': self.learning_rate,
            'epsilon': self.epsilon,
            'hidden_dim': self.hidden_dim,
            'memory_size': self.memory_size,
            'target_update_frequency': self.target_update_frequency,
            'training_step': self.training_step
        })
        
        return base_info
    
    def get_training_info(self) -> Dict[str, Any]:
        """ç²å–è¨“ç·´ç›¸é—œä¿¡æ¯
        
        Returns:
            Dict[str, Any]: è¨“ç·´ä¿¡æ¯
        """
        return {
            'algorithm_type': 'DQN',
            'training_step': self.training_step,
            'epsilon': self.epsilon,
            'memory_size': len(self.memory),
            'target_update_count': self.target_update_count,
            'learning_rate': self.learning_rate,
            'model_parameters': sum(p.numel() for p in self.model.parameters()) if self.model else 0,
            'device': str(self.device),
            'is_training': self.training
        }