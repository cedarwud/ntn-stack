"""
ğŸ¤– SAC æ›æ‰‹æ™ºèƒ½é«”

åŸºæ–¼è»Ÿæ¼”å“¡è©•è«–å®¶ (Soft Actor-Critic) çš„è¡›æ˜Ÿæ›æ‰‹æ±ºç­–ç®—æ³•ã€‚
"""

import logging
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import Dict, Any, Optional, Tuple
from datetime import datetime

from .base_rl_algorithm import BaseRLHandoverAlgorithm
from ..interfaces import HandoverContext, HandoverDecision, AlgorithmInfo

logger = logging.getLogger(__name__)


class SACNetwork(nn.Module):
    """SAC ç¶²è·¯åŸºé¡"""
    
    def __init__(self, input_dim: int, hidden_dim: int = 256):
        """åˆå§‹åŒ–ç¶²è·¯
        
        Args:
            input_dim: è¼¸å…¥ç¶­åº¦
            hidden_dim: éš±è—å±¤ç¶­åº¦
        """
        super(SACNetwork, self).__init__()
        
        self.shared_layers = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        self._initialize_weights()
    
    def _initialize_weights(self):
        """åˆå§‹åŒ–ç¶²è·¯æ¬Šé‡"""
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                nn.init.constant_(module.bias, 0)


class SACCritic(SACNetwork):
    """SAC è©•è«–å®¶ç¶²è·¯"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        """åˆå§‹åŒ–è©•è«–å®¶ç¶²è·¯
        
        Args:
            state_dim: ç‹€æ…‹ç¶­åº¦
            action_dim: å‹•ä½œç¶­åº¦
            hidden_dim: éš±è—å±¤ç¶­åº¦
        """
        super(SACCritic, self).__init__(state_dim + action_dim, hidden_dim)
        
        self.q_layer = nn.Linear(hidden_dim, 1)
    
    def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
        """å‰å‘å‚³æ’­
        
        Args:
            state: ç‹€æ…‹å¼µé‡
            action: å‹•ä½œå¼µé‡
            
        Returns:
            torch.Tensor: Q å€¼
        """
        x = torch.cat([state, action], dim=-1)
        x = self.shared_layers(x)
        q_value = self.q_layer(x)
        return q_value


class SACActor(SACNetwork):
    """SAC æ¼”å“¡ç¶²è·¯"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        """åˆå§‹åŒ–æ¼”å“¡ç¶²è·¯
        
        Args:
            state_dim: ç‹€æ…‹ç¶­åº¦
            action_dim: å‹•ä½œç¶­åº¦
            hidden_dim: éš±è—å±¤ç¶­åº¦
        """
        super(SACActor, self).__init__(state_dim, hidden_dim)
        
        self.mean_layer = nn.Linear(hidden_dim, action_dim)
        self.log_std_layer = nn.Linear(hidden_dim, action_dim)
        
        self.min_log_std = -20
        self.max_log_std = 2
    
    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """å‰å‘å‚³æ’­
        
        Args:
            state: ç‹€æ…‹å¼µé‡
            
        Returns:
            Tuple[torch.Tensor, torch.Tensor]: (å‡å€¼, å°æ•¸æ¨™æº–å·®)
        """
        x = self.shared_layers(state)
        mean = self.mean_layer(x)
        log_std = self.log_std_layer(x)
        log_std = torch.clamp(log_std, self.min_log_std, self.max_log_std)
        return mean, log_std
    
    def sample(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """æ¡æ¨£å‹•ä½œ
        
        Args:
            state: ç‹€æ…‹å¼µé‡
            
        Returns:
            Tuple[torch.Tensor, torch.Tensor]: (å‹•ä½œ, å°æ•¸æ¦‚ç‡)
        """
        mean, log_std = self.forward(state)
        std = log_std.exp()
        
        # é‡åƒæ•¸åŒ–æŠ€å·§
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()  # ç”¨æ–¼åå‘å‚³æ’­
        
        # æ‡‰ç”¨ tanh æ¿€æ´»ä¸¦è¨ˆç®—å°æ•¸æ¦‚ç‡
        action = torch.tanh(x_t)
        log_prob = normal.log_prob(x_t)
        
        # ä¿®æ­£ tanh è®Šæ›çš„é›…å¯æ¯”è¡Œåˆ—å¼
        log_prob -= torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(dim=-1, keepdim=True)
        
        return action, log_prob


class SACHandoverAgent(BaseRLHandoverAlgorithm):
    """SAC æ›æ‰‹æ™ºèƒ½é«”
    
    å¯¦ç¾åŸºæ–¼ SAC çš„è¡›æ˜Ÿæ›æ‰‹æ±ºç­–ç®—æ³•ï¼ŒåŒ…æ‹¬ï¼š
    - æœ€å¤§ç†µå¼·åŒ–å­¸ç¿’
    - é›™ Q ç¶²è·¯
    - è‡ªå‹•æº«åº¦èª¿ç¯€
    - é€£çºŒå‹•ä½œç©ºé–“è™•ç†
    """
    
    def __init__(self, name: str = "sac_handover", config: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ– SAC æ™ºèƒ½é«”
        
        Args:
            name: ç®—æ³•åç¨±
            config: ç®—æ³•é…ç½®
        """
        super().__init__(name, config)
        
        # SAC ç‰¹å®šé…ç½®
        self.hidden_dim = self.training_config.get('hidden_dim', 256)
        self.learning_rate = self.training_config.get('learning_rate', 0.0003)
        self.alpha = self.training_config.get('alpha', 0.2)  # æº«åº¦åƒæ•¸
        self.tau = self.training_config.get('tau', 0.005)  # è»Ÿæ›´æ–°ä¿‚æ•¸
        self.gamma = self.training_config.get('gamma', 0.99)
        self.batch_size = self.training_config.get('batch_size', 128)
        self.memory_size = self.training_config.get('memory_size', 100000)
        self.auto_alpha = self.training_config.get('auto_alpha', True)  # è‡ªå‹•èª¿ç¯€æº«åº¦
        
        # æ¨ç†é…ç½®
        self.temperature = self.inference_config.get('temperature', 1.0)
        
        # ç¶²è·¯çµ„ä»¶
        self.actor: Optional[SACActor] = None
        self.critic1: Optional[SACCritic] = None
        self.critic2: Optional[SACCritic] = None
        self.target_critic1: Optional[SACCritic] = None
        self.target_critic2: Optional[SACCritic] = None
        
        # å„ªåŒ–å™¨
        self.actor_optimizer: Optional[optim.Adam] = None
        self.critic1_optimizer: Optional[optim.Adam] = None
        self.critic2_optimizer: Optional[optim.Adam] = None
        self.alpha_optimizer: Optional[optim.Adam] = None
        
        # è‡ªå‹•æº«åº¦èª¿ç¯€
        if self.auto_alpha:
            self.target_entropy = -self.action_dim  # ç›®æ¨™ç†µ
            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)
        
        # ç¶“é©—å›æ”¾
        self.memory = []
        self.memory_idx = 0
        
        # è¨“ç·´çµ±è¨ˆ
        self.training_step = 0
        
        logger.info(f"SAC æ™ºèƒ½é«” '{name}' åˆå§‹åŒ–å®Œæˆ")
    
    async def _create_model(self) -> None:
        """å‰µå»º SAC æ¨¡å‹"""
        try:
            # å‰µå»ºç¶²è·¯
            self.actor = SACActor(
                state_dim=self.observation_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim
            ).to(self.device)
            
            self.critic1 = SACCritic(
                state_dim=self.observation_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim
            ).to(self.device)
            
            self.critic2 = SACCritic(
                state_dim=self.observation_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim
            ).to(self.device)
            
            # å‰µå»ºç›®æ¨™ç¶²è·¯
            self.target_critic1 = SACCritic(
                state_dim=self.observation_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim
            ).to(self.device)
            
            self.target_critic2 = SACCritic(
                state_dim=self.observation_dim,
                action_dim=self.action_dim,
                hidden_dim=self.hidden_dim
            ).to(self.device)
            
            # åˆå§‹åŒ–ç›®æ¨™ç¶²è·¯
            self._soft_update(self.target_critic1, self.critic1, tau=1.0)
            self._soft_update(self.target_critic2, self.critic2, tau=1.0)
            
            # è¨­ç½®å„ªåŒ–å™¨
            self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.learning_rate)
            self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.learning_rate)
            self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.learning_rate)
            
            if self.auto_alpha:
                self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.learning_rate)
            
            # å°‡ä¸»è¦æ¨¡å‹è¨­ç‚º actorï¼ˆç”¨æ–¼æ¨ç†ï¼‰
            self.model = self.actor
            
            total_params = sum(p.numel() for network in [self.actor, self.critic1, self.critic2] for p in network.parameters())
            logger.info(f"SAC æ¨¡å‹å‰µå»ºå®Œæˆï¼Œç¸½åƒæ•¸é‡: {total_params}")
            
        except Exception as e:
            logger.error(f"SAC æ¨¡å‹å‰µå»ºå¤±æ•—: {e}")
            raise
    
    async def _forward_pass(self, observation: torch.Tensor) -> torch.Tensor:
        """SAC å‰å‘å‚³æ’­
        
        Args:
            observation: è§€å¯Ÿå¼µé‡
            
        Returns:
            torch.Tensor: æ±ºç­–è¼¸å‡º
        """
        self.actor.eval()
        with torch.no_grad():
            if self.training:
                # è¨“ç·´æ™‚æ¡æ¨£å‹•ä½œ
                action, _ = self.actor.sample(observation)
            else:
                # æ¨ç†æ™‚ä½¿ç”¨ç¢ºå®šæ€§å‹•ä½œ
                mean, _ = self.actor.forward(observation)
                action = torch.tanh(mean)
            
            # æ‡‰ç”¨æº«åº¦ç¸®æ”¾
            if self.temperature != 1.0:
                action = action / self.temperature
            
            # è½‰æ›ç‚ºæ±ºç­–æ ¼å¼
            decision_tensor = self._action_to_decision(action)
            
            return decision_tensor
    
    def _action_to_decision(self, action: torch.Tensor) -> torch.Tensor:
        """å°‡é€£çºŒå‹•ä½œè½‰æ›ç‚ºæ±ºç­–æ ¼å¼
        
        Args:
            action: å‹•ä½œå¼µé‡ [batch_size, action_dim]
            
        Returns:
            torch.Tensor: æ±ºç­–å¼µé‡ [batch_size, decision_dim]
        """
        batch_size = action.size(0)
        decision_dim = 3 + 8  # 3 å€‹æ±ºç­–é¡å‹ + 8 å€‹å€™é¸è¡›æ˜Ÿæ¦‚ç‡
        
        decision_tensor = torch.zeros(batch_size, decision_dim).to(self.device)
        
        # å°‡é€£çºŒå‹•ä½œè½‰æ›ç‚ºæ¦‚ç‡åˆ†ä½ˆ
        action_probs = torch.softmax(action, dim=-1)
        
        # å‰ 3 å€‹å°æ‡‰æ±ºç­–é¡å‹
        decision_tensor[:, :3] = action_probs[:, :3]
        
        # å¾Œé¢çš„å°æ‡‰è¡›æ˜Ÿé¸æ“‡
        if action_probs.size(1) > 3:
            decision_tensor[:, 3:3+min(8, action_probs.size(1)-3)] = action_probs[:, 3:3+min(8, action_probs.size(1)-3)]
        
        return decision_tensor
    
    def _soft_update(self, target_network: nn.Module, source_network: nn.Module, tau: float) -> None:
        """è»Ÿæ›´æ–°ç›®æ¨™ç¶²è·¯
        
        Args:
            target_network: ç›®æ¨™ç¶²è·¯
            source_network: æºç¶²è·¯
            tau: æ›´æ–°ä¿‚æ•¸
        """
        for target_param, source_param in zip(target_network.parameters(), source_network.parameters()):
            target_param.data.copy_(tau * source_param.data + (1.0 - tau) * target_param.data)
    
    def store_experience(self, state: torch.Tensor, action: torch.Tensor, reward: float,
                        next_state: torch.Tensor, done: bool) -> None:
        """å­˜å„²ç¶“é©—åˆ°å›æ”¾ç·©è¡å€
        
        Args:
            state: ç•¶å‰ç‹€æ…‹
            action: æ¡å–çš„å‹•ä½œ
            reward: ç²å¾—çš„çå‹µ
            next_state: ä¸‹ä¸€å€‹ç‹€æ…‹
            done: æ˜¯å¦çµæŸ
        """
        experience = (state, action, reward, next_state, done)
        
        if len(self.memory) < self.memory_size:
            self.memory.append(experience)
        else:
            self.memory[self.memory_idx] = experience
            self.memory_idx = (self.memory_idx + 1) % self.memory_size
    
    async def train_step(self) -> Dict[str, float]:
        """åŸ·è¡Œ SAC è¨“ç·´æ­¥é©Ÿ
        
        Returns:
            Dict[str, float]: è¨“ç·´æŒ‡æ¨™
        """
        if len(self.memory) < self.batch_size:
            return {'actor_loss': 0.0, 'critic1_loss': 0.0, 'critic2_loss': 0.0, 'alpha_loss': 0.0}
        
        try:
            # æ¡æ¨£æ‰¹æ¬¡ç¶“é©—
            batch = self._sample_batch()
            states, actions, rewards, next_states, dones = zip(*batch)
            
            # è½‰æ›ç‚ºå¼µé‡
            states = torch.stack(states).to(self.device)
            actions = torch.stack(actions).to(self.device)
            rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device).unsqueeze(-1)
            next_states = torch.stack(next_states).to(self.device)
            dones = torch.tensor(dones, dtype=torch.bool).to(self.device).unsqueeze(-1)
            
            # æ›´æ–°è©•è«–å®¶ç¶²è·¯
            critic1_loss, critic2_loss = self._update_critics(states, actions, rewards, next_states, dones)
            
            # æ›´æ–°æ¼”å“¡ç¶²è·¯
            actor_loss = self._update_actor(states)
            
            # æ›´æ–°æº«åº¦åƒæ•¸
            alpha_loss = 0.0
            if self.auto_alpha:
                alpha_loss = self._update_alpha(states)
            
            # è»Ÿæ›´æ–°ç›®æ¨™ç¶²è·¯
            self._soft_update(self.target_critic1, self.critic1, self.tau)
            self._soft_update(self.target_critic2, self.critic2, self.tau)
            
            self.training_step += 1
            self._rl_statistics['training_episodes'] = self.training_step
            
            return {
                'actor_loss': actor_loss,
                'critic1_loss': critic1_loss,
                'critic2_loss': critic2_loss,
                'alpha_loss': alpha_loss,
                'alpha': self.alpha if not self.auto_alpha else self.log_alpha.exp().item(),
                'training_step': self.training_step
            }
            
        except Exception as e:
            logger.error(f"SAC è¨“ç·´æ­¥é©Ÿå¤±æ•—: {e}")
            return {'actor_loss': float('inf'), 'critic1_loss': float('inf'), 'critic2_loss': float('inf'), 'alpha_loss': float('inf')}
    
    def _update_critics(self, states: torch.Tensor, actions: torch.Tensor, rewards: torch.Tensor,
                       next_states: torch.Tensor, dones: torch.Tensor) -> Tuple[float, float]:
        """æ›´æ–°è©•è«–å®¶ç¶²è·¯"""
        with torch.no_grad():
            next_actions, next_log_probs = self.actor.sample(next_states)
            alpha = self.alpha if not self.auto_alpha else self.log_alpha.exp()
            
            target_q1 = self.target_critic1(next_states, next_actions)
            target_q2 = self.target_critic2(next_states, next_actions)
            target_q = torch.min(target_q1, target_q2) - alpha * next_log_probs
            target_q = rewards + self.gamma * target_q * (~dones)
        
        # æ›´æ–° Critic 1
        current_q1 = self.critic1(states, actions)
        critic1_loss = F.mse_loss(current_q1, target_q)
        
        self.critic1_optimizer.zero_grad()
        critic1_loss.backward()
        self.critic1_optimizer.step()
        
        # æ›´æ–° Critic 2
        current_q2 = self.critic2(states, actions)
        critic2_loss = F.mse_loss(current_q2, target_q)
        
        self.critic2_optimizer.zero_grad()
        critic2_loss.backward()
        self.critic2_optimizer.step()
        
        return critic1_loss.item(), critic2_loss.item()
    
    def _update_actor(self, states: torch.Tensor) -> float:
        """æ›´æ–°æ¼”å“¡ç¶²è·¯"""
        actions, log_probs = self.actor.sample(states)
        alpha = self.alpha if not self.auto_alpha else self.log_alpha.exp()
        
        q1 = self.critic1(states, actions)
        q2 = self.critic2(states, actions)
        min_q = torch.min(q1, q2)
        
        actor_loss = (alpha * log_probs - min_q).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        return actor_loss.item()
    
    def _update_alpha(self, states: torch.Tensor) -> float:
        """æ›´æ–°æº«åº¦åƒæ•¸"""
        with torch.no_grad():
            _, log_probs = self.actor.sample(states)
        
        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy)).mean()
        
        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.alpha_optimizer.step()
        
        return alpha_loss.item()
    
    def _sample_batch(self) -> list:
        """å¾ç¶“é©—å›æ”¾ä¸­æ¡æ¨£æ‰¹æ¬¡"""
        indices = np.random.choice(len(self.memory), self.batch_size, replace=False)
        return [self.memory[idx] for idx in indices]
    
    def get_algorithm_info(self) -> AlgorithmInfo:
        """ç²å– SAC ç®—æ³•ä¿¡æ¯"""
        base_info = super().get_algorithm_info()
        
        # æ·»åŠ  SAC ç‰¹å®šä¿¡æ¯
        base_info.description = "è»Ÿæ¼”å“¡è©•è«–å®¶æ›æ‰‹æ™ºèƒ½é«” - åŸºæ–¼æœ€å¤§ç†µçš„å¼·åŒ–å­¸ç¿’ç®—æ³•"
        base_info.parameters.update({
            'learning_rate': self.learning_rate,
            'alpha': self.alpha if not self.auto_alpha else self.log_alpha.exp().item(),
            'tau': self.tau,
            'gamma': self.gamma,
            'hidden_dim': self.hidden_dim,
            'auto_alpha': self.auto_alpha,
            'training_step': self.training_step
        })
        
        return base_info
    
    def get_training_info(self) -> Dict[str, Any]:
        """ç²å–è¨“ç·´ç›¸é—œä¿¡æ¯"""
        return {
            'algorithm_type': 'SAC',
            'training_step': self.training_step,
            'memory_size': len(self.memory),
            'alpha': self.alpha if not self.auto_alpha else self.log_alpha.exp().item(),
            'learning_rate': self.learning_rate,
            'model_parameters': sum(p.numel() for network in [self.actor, self.critic1, self.critic2] for p in network.parameters()) if self.actor else 0,
            'device': str(self.device),
            'is_training': self.training
        }