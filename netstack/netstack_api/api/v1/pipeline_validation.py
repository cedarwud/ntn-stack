#!/usr/bin/env python3
"""
ç®¡é“é©—è­‰ API ç«¯é»
==============================
ç‚ºå…­éšæ®µæ•¸æ“šé è™•ç†ç³»çµ±æä¾›é©—è­‰ç‹€æ…‹ REST API

Author: NTN Stack Team
Version: 1.0.0
Date: 2025-09-04

ç«¯é»åˆ—è¡¨:
- GET /api/v1/pipeline/validation/stage/{stage_number}  - ç²å–ç‰¹å®šéšæ®µé©—è­‰ç‹€æ…‹
- GET /api/v1/pipeline/validation/summary             - ç²å–ç®¡é“é©—è­‰ç¸½è¦½
- GET /api/v1/pipeline/statistics                    - ç²å–ç®¡é“çµ±è¨ˆä¿¡æ¯ï¼ˆå…¼å®¹æ€§ï¼‰
"""

import os
import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, Optional, List

from fastapi import APIRouter, HTTPException
from fastapi.responses import JSONResponse

logger = logging.getLogger(__name__)

# å‰µå»ºè·¯ç”±å™¨
router = APIRouter(prefix="/api/v1", tags=["pipeline", "validation"])

# é©—è­‰å¿«ç…§ç›®éŒ„ - æ™ºèƒ½é¸æ“‡
import os
if os.path.exists("/app"):
    VALIDATION_SNAPSHOTS_DIR = Path("/app/data/validation_snapshots")
else:
    VALIDATION_SNAPSHOTS_DIR = Path("/home/sat/ntn-stack/data/leo_outputs/validation_snapshots")


@router.get("/pipeline/validation/stage/{stage_number}")
async def get_stage_validation(stage_number: int) -> JSONResponse:
    """
    ç²å–ç‰¹å®šéšæ®µçš„é©—è­‰ç‹€æ…‹
    
    Args:
        stage_number: éšæ®µç·¨è™Ÿ (1-6)
        
    Returns:
        éšæ®µé©—è­‰ç‹€æ…‹ JSON
        
    Raises:
        HTTPException: éšæ®µç·¨è™Ÿç„¡æ•ˆæˆ–éšæ®µæœªåŸ·è¡Œ
    """
    if not (1 <= stage_number <= 6):
        raise HTTPException(
            status_code=400, 
            detail=f"Invalid stage number: {stage_number}. Must be between 1 and 6."
        )
    
    snapshot_file = VALIDATION_SNAPSHOTS_DIR / f"stage{stage_number}_validation.json"
    
    if not snapshot_file.exists():
        logger.warning(f"Stage {stage_number} validation snapshot not found: {snapshot_file}")
        raise HTTPException(
            status_code=404,
            detail={
                "error": "Stage not executed",
                "stage": stage_number,
                "message": f"Validation snapshot for Stage {stage_number} does not exist",
                "suggestion": "Execute the processing pipeline first"
            }
        )
    
    try:
        with open(snapshot_file, 'r', encoding='utf-8') as f:
            validation_data = json.load(f)
        
        logger.info(f"âœ… Loaded validation data for Stage {stage_number}")
        return JSONResponse(content=validation_data)
        
    except json.JSONDecodeError as e:
        logger.error(f"âŒ Invalid JSON in validation snapshot for Stage {stage_number}: {e}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Invalid validation data",
                "stage": stage_number,
                "message": "Validation snapshot file is corrupted"
            }
        )
    except Exception as e:
        logger.error(f"âŒ Error loading validation data for Stage {stage_number}: {e}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Internal server error",
                "stage": stage_number,
                "message": str(e)
            }
        )


@router.get("/pipeline/validation/summary")
async def get_pipeline_validation_summary() -> JSONResponse:
    """
    ç²å–æ•´å€‹ç®¡é“çš„é©—è­‰ç¸½è¦½
    
    Returns:
        ç®¡é“é©—è­‰ç¸½è¦½ JSONï¼ŒåŒ…å«æ‰€æœ‰éšæ®µç‹€æ…‹
    """
    stages = []
    
    for stage_num in range(1, 7):
        snapshot_file = VALIDATION_SNAPSHOTS_DIR / f"stage{stage_num}_validation.json"
        
        if snapshot_file.exists():
            try:
                with open(snapshot_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                stages.append({
                    "stage": stage_num,
                    "stageName": data.get("stageName", f"éšæ®µ {stage_num}"),
                    "status": data.get("status", "unknown"),
                    "passed": data.get("validation", {}).get("passed", False),
                    "timestamp": data.get("timestamp"),
                    "duration_seconds": data.get("duration_seconds", 0),
                    "checks_passed": data.get("validation", {}).get("passedChecks", 0),
                    "checks_total": data.get("validation", {}).get("totalChecks", 0),
                    "keyMetrics": data.get("keyMetrics", {}),
                    "nextStage": data.get("nextStage", {})
                })
                
            except Exception as e:
                logger.error(f"âŒ Error reading Stage {stage_num} snapshot: {e}")
                stages.append({
                    "stage": stage_num,
                    "stageName": f"éšæ®µ {stage_num}",
                    "status": "error",
                    "passed": False,
                    "timestamp": None,
                    "duration_seconds": 0,
                    "checks_passed": 0,
                    "checks_total": 0,
                    "keyMetrics": {},
                    "error_message": str(e)
                })
        else:
            stages.append({
                "stage": stage_num,
                "stageName": f"éšæ®µ {stage_num}",
                "status": "not_executed",
                "passed": False,
                "timestamp": None,
                "duration_seconds": 0,
                "checks_passed": 0,
                "checks_total": 0,
                "keyMetrics": {}
            })
    
    # è¨ˆç®—ç®¡é“å¥åº·åº¦
    executed_stages = [s for s in stages if s["status"] not in ["not_executed", "error"]]
    successful_stages = [s for s in executed_stages if s["passed"]]
    failed_stages = [s for s in executed_stages if not s["passed"]]
    
    if len(executed_stages) == 0:
        pipeline_health = "not_started"
    elif len(failed_stages) > 0:
        pipeline_health = "degraded"
    elif len(executed_stages) < 6:
        pipeline_health = "partial"
    else:
        pipeline_health = "healthy"
    
    # è¨ˆç®—ç¸½è™•ç†æ™‚é–“å’ŒæˆåŠŸç‡
    total_duration = sum(s["duration_seconds"] for s in executed_stages)
    success_rate = (len(successful_stages) / max(len(executed_stages), 1)) * 100
    
    summary = {
        "metadata": {
            "analysis_timestamp": datetime.now(timezone.utc).isoformat(),
            "analyzer_version": "pipeline_validation_api_v1.0.0",
            "snapshots_directory": str(VALIDATION_SNAPSHOTS_DIR)
        },
        "stages": stages,
        "summary": {
            "total_stages": 6,
            "executed_stages": len(executed_stages),
            "successful_stages": len(successful_stages),
            "failed_stages": len(failed_stages),
            "not_executed_stages": 6 - len(executed_stages),
            "pipeline_health": pipeline_health,
            "success_rate": round(success_rate, 1),
            "total_processing_time": total_duration,
            "average_stage_time": round(total_duration / max(len(executed_stages), 1), 2)
        },
        "healthStatus": {
            "status": pipeline_health,
            "message": _get_health_message(pipeline_health, len(successful_stages), len(failed_stages)),
            "recommendations": _get_health_recommendations(pipeline_health, failed_stages)
        }
    }
    
    logger.info(f"ğŸ“Š Pipeline validation summary generated: {pipeline_health}")
    return JSONResponse(content=summary)


@router.get("/pipeline/statistics")
async def get_pipeline_statistics() -> JSONResponse:
    """
    ç²å–ç®¡é“çµ±è¨ˆä¿¡æ¯ï¼ˆç‚ºå‰ç«¯å…¼å®¹æ€§æä¾›çš„ç«¯é»ï¼‰
    
    Returns:
        ç®¡é“çµ±è¨ˆä¿¡æ¯ï¼Œæ ¼å¼èˆ‡å‰ç«¯ SatelliteVisibilitySimplified.tsx å…¼å®¹
    """
    logger.info("ğŸ“Š Generating pipeline statistics (compatibility endpoint)")
    
    # é‡ç”¨é©—è­‰ç¸½è¦½çš„é‚è¼¯
    validation_summary_response = await get_pipeline_validation_summary()
    validation_data = json.loads(validation_summary_response.body)
    
    # è½‰æ›ç‚ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
    stages_statistics = []
    data_flow = []
    
    for stage_data in validation_data["stages"]:
        stage_num = stage_data["stage"]
        key_metrics = stage_data.get("keyMetrics", {})
        
        # æå–è¡›æ˜Ÿæ•¸é‡ä¿¡æ¯
        if "è¼¸å…¥TLEæ•¸é‡" in key_metrics:
            total_satellites = key_metrics["è¼¸å…¥TLEæ•¸é‡"]
            starlink_count = key_metrics.get("Starlinkè¡›æ˜Ÿ", 0)
            oneweb_count = key_metrics.get("OneWebè¡›æ˜Ÿ", 0)
        elif "è¼¸å…¥è¡›æ˜Ÿ" in key_metrics:
            total_satellites = key_metrics["è¼¸å‡ºè¡›æ˜Ÿ"]  # Stage 2 è¼¸å‡ºä½œç‚ºç¸½æ•¸
            starlink_count = key_metrics.get("Starlinkç¯©é¸", 0)
            oneweb_count = key_metrics.get("OneWebç¯©é¸", 0)
        else:
            total_satellites = 0
            starlink_count = 0
            oneweb_count = 0
        
        stages_statistics.append({
            "stage": stage_num,
            "stage_name": stage_data["stageName"],
            "status": "success" if stage_data["passed"] else ("failed" if stage_data["status"] != "not_executed" else "no_data"),
            "total_satellites": total_satellites,
            "starlink_count": starlink_count,
            "oneweb_count": oneweb_count,
            "processing_time": f"{stage_data['duration_seconds']}ç§’" if stage_data.get("duration_seconds") else None,
            "last_updated": stage_data.get("timestamp")
        })
        
        # æ·»åŠ åˆ°æ•¸æ“šæµ
        if stage_data["status"] != "not_executed" and total_satellites > 0:
            data_flow.append({
                "stage": stage_num,
                "satellites": total_satellites,
                "starlink": starlink_count,
                "oneweb": oneweb_count
            })
    
    # è¨ˆç®—æœ€çµ‚è¼¸å‡º
    final_output = data_flow[-1]["satellites"] if data_flow else 0
    
    # è¨ˆç®—æ•¸æ“šä¿ç•™ç‡
    initial_input = data_flow[0]["satellites"] if data_flow else 0
    data_retention_rate = (final_output / max(initial_input, 1)) * 100 if initial_input > 0 else 0
    
    statistics = {
        "metadata": {
            "analysis_timestamp": datetime.now(timezone.utc).isoformat(),
            "analyzer_version": "pipeline_statistics_api_v1.0.0"
        },
        "stages": stages_statistics,
        "summary": {
            "total_stages": 6,
            "successful_stages": len([s for s in stages_statistics if s["status"] == "success"]),
            "failed_stages": len([s for s in stages_statistics if s["status"] == "failed"]),
            "no_data_stages": len([s for s in stages_statistics if s["status"] == "no_data"]),
            "data_flow": data_flow,
            "final_output": final_output,
            "pipeline_health": validation_data["summary"]["pipeline_health"],
            "data_retention_rate": round(data_retention_rate, 1),
            "data_loss_rate": round(100 - data_retention_rate, 1)
        }
    }
    
    logger.info(f"ğŸ“ˆ Pipeline statistics generated: {len(data_flow)} stages with data")
    return JSONResponse(content=statistics)


def _get_health_message(health_status: str, successful: int, failed: int) -> str:
    """ç²å–å¥åº·ç‹€æ…‹æ¶ˆæ¯"""
    if health_status == "healthy":
        return f"æ‰€æœ‰ 6 å€‹éšæ®µæˆåŠŸå®Œæˆï¼Œç®¡é“é‹è¡Œæ­£å¸¸"
    elif health_status == "degraded":
        return f"{successful} å€‹éšæ®µæˆåŠŸï¼Œ{failed} å€‹éšæ®µå¤±æ•—ï¼Œéœ€è¦æª¢æŸ¥å¤±æ•—éšæ®µ"
    elif health_status == "partial":
        return f"{successful} å€‹éšæ®µå·²åŸ·è¡Œï¼Œç®¡é“éƒ¨åˆ†å®Œæˆ"
    elif health_status == "not_started":
        return "ç®¡é“å°šæœªé–‹å§‹åŸ·è¡Œ"
    else:
        return "ç®¡é“ç‹€æ…‹æœªçŸ¥"


def _get_health_recommendations(health_status: str, failed_stages: List[Dict]) -> List[str]:
    """ç²å–å¥åº·æ”¹å–„å»ºè­°"""
    recommendations = []
    
    if health_status == "not_started":
        recommendations.append("åŸ·è¡Œ LEO é è™•ç†ç®¡é“ä»¥é–‹å§‹æ•¸æ“šè™•ç†")
        recommendations.append("ç¢ºä¿ TLE æ•¸æ“šå¯ç”¨ä¸”æ ¼å¼æ­£ç¢º")
    
    elif health_status == "degraded":
        recommendations.append("æª¢æŸ¥å¤±æ•—éšæ®µçš„éŒ¯èª¤æ—¥èªŒ")
        recommendations.append("é©—è­‰è¼¸å…¥æ•¸æ“šçš„å®Œæ•´æ€§å’Œæ ¼å¼")
        for stage in failed_stages:
            recommendations.append(f"ä¿®å¾©éšæ®µ {stage['stage']} çš„å•é¡Œ")
    
    elif health_status == "partial":
        recommendations.append("ç¹¼çºŒåŸ·è¡Œå‰©é¤˜çš„ç®¡é“éšæ®µ")
        recommendations.append("æª¢æŸ¥ç³»çµ±è³‡æºæ˜¯å¦å……è¶³")
    
    return recommendations


# å¥åº·æª¢æŸ¥ç«¯é»
@router.get("/pipeline/health")
async def get_pipeline_health() -> JSONResponse:
    """
    å¿«é€Ÿå¥åº·æª¢æŸ¥ç«¯é»
    
    Returns:
        ç°¡åŒ–çš„ç®¡é“å¥åº·ç‹€æ…‹
    """
    try:
        summary_response = await get_pipeline_validation_summary()
        summary_data = json.loads(summary_response.body)
        
        health_status = summary_data["summary"]["pipeline_health"]
        executed_stages = summary_data["summary"]["executed_stages"]
        successful_stages = summary_data["summary"]["successful_stages"]
        
        return JSONResponse(content={
            "status": health_status,
            "healthy": health_status == "healthy",
            "executed_stages": executed_stages,
            "successful_stages": successful_stages,
            "timestamp": datetime.now(timezone.utc).isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ Health check error: {e}")
        return JSONResponse(
            content={
                "status": "error",
                "healthy": False,
                "error": str(e),
                "timestamp": datetime.now(timezone.utc).isoformat()
            },
            status_code=500
        )


# æ‰¹é‡æ“ä½œç«¯é»
@router.get("/pipeline/validation/stages")
async def get_all_stages_validation() -> JSONResponse:
    """
    æ‰¹é‡ç²å–æ‰€æœ‰éšæ®µçš„è©³ç´°é©—è­‰æ•¸æ“š
    
    Returns:
        æ‰€æœ‰éšæ®µçš„å®Œæ•´é©—è­‰æ•¸æ“š
    """
    all_stages = {}
    
    for stage_num in range(1, 7):
        try:
            stage_response = await get_stage_validation(stage_num)
            all_stages[f"stage{stage_num}"] = json.loads(stage_response.body)
        except HTTPException:
            all_stages[f"stage{stage_num}"] = {
                "stage": stage_num,
                "status": "not_executed",
                "error": "Stage not executed or validation snapshot missing"
            }
    
    return JSONResponse(content={
        "metadata": {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "total_stages": 6,
            "available_stages": len([s for s in all_stages.values() if "error" not in s])
        },
        "stages": all_stages
    })


# æ¸…ç†ç«¯é»ï¼ˆé–‹ç™¼/èª¿è©¦ç”¨ï¼‰
@router.delete("/pipeline/validation/cleanup")
async def cleanup_validation_snapshots() -> JSONResponse:
    """
    æ¸…ç†æ‰€æœ‰é©—è­‰å¿«ç…§ï¼ˆé–‹ç™¼/èª¿è©¦ç”¨ï¼‰
    
    Warning: æ­¤ç«¯é»å°‡åˆªé™¤æ‰€æœ‰é©—è­‰å¿«ç…§æ–‡ä»¶
    """
    try:
        deleted_files = []
        
        if VALIDATION_SNAPSHOTS_DIR.exists():
            for snapshot_file in VALIDATION_SNAPSHOTS_DIR.glob("stage*_validation.json"):
                snapshot_file.unlink()
                deleted_files.append(str(snapshot_file))
        
        logger.warning(f"ğŸ—‘ï¸ Validation snapshots cleanup: {len(deleted_files)} files deleted")
        
        return JSONResponse(content={
            "message": "Validation snapshots cleaned up",
            "deleted_files": deleted_files,
            "count": len(deleted_files),
            "timestamp": datetime.now(timezone.utc).isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ Cleanup error: {e}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "Cleanup failed",
                "message": str(e)
            }
        )