"""
ğŸ§  DQN ç®—æ³•å¯¦ç¾

å°‡ç¾æœ‰çš„ DQN ç®—æ³•é©é…åˆ°æ–°çš„æ¥å£æ¶æ§‹ä¸­ï¼Œ
æä¾›å®Œæ•´çš„ IRLAlgorithm æ¥å£å¯¦ç¾ã€‚
"""

import logging
import asyncio
import numpy as np
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path

from ..interfaces.rl_algorithm import (
    IRLAlgorithm, ScenarioType, TrainingConfig, TrainingResult,
    PredictionContext, HandoverDecision, ITrainingObserver
)
from ..core.algorithm_factory import algorithm_plugin

# å˜—è©¦å°å…¥ç¾æœ‰çš„ DQN å¯¦ç¾
try:
    from ...netstack_api.algorithm_ecosystem.rl_algorithms.dqn_agent import DQNHandoverAgent, DQNNetwork
    EXISTING_DQN_AVAILABLE = True
except ImportError:
    EXISTING_DQN_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("ç„¡æ³•å°å…¥ç¾æœ‰çš„ DQN å¯¦ç¾ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬å¯¦ç¾")

logger = logging.getLogger(__name__)


class DQNNetworkWrapper(nn.Module):
    """DQN ç¶²è·¯åŒ…è£å™¨"""
    
    def __init__(self, state_dim: int, action_dim: int, hidden_layers: List[int] = None):
        super().__init__()
        
        if hidden_layers is None:
            hidden_layers = [64, 64]
        
        layers = []
        prev_dim = state_dim
        
        for hidden_dim in hidden_layers:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.ReLU()
            ])
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, action_dim))
        self.network = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.network(x)


@algorithm_plugin(
    name="DQN",
    version="2.0.0",
    supported_scenarios=[ScenarioType.URBAN, ScenarioType.SUBURBAN, ScenarioType.LOW_LATENCY],
    description="Deep Q-Network ç®—æ³•å¯¦ç¾ï¼Œé©é…æ–°æ¶æ§‹",
    author="NetStack Team"
)
class DQNAlgorithmImpl(IRLAlgorithm):
    """DQN ç®—æ³•å¯¦ç¾
    
    åŸºæ–¼æ·±åº¦ Q ç¶²è·¯çš„å¼·åŒ–å­¸ç¿’ç®—æ³•ï¼Œ
    å°ˆé–€é‡å° LEO è¡›æ˜Ÿæ›æ‰‹æ±ºç­–å„ªåŒ–ã€‚
    """
    
    def __init__(self, config: Dict[str, Any], scenario_type: ScenarioType):
        self.config = config
        self.scenario_type = scenario_type
        self._name = "DQN"
        self._version = "2.0.0"
        
        # ç®—æ³•åƒæ•¸
        self.learning_rate = config.get('learning_rate', 0.001)
        self.batch_size = config.get('batch_size', 32)
        self.gamma = config.get('gamma', 0.99)
        self.epsilon = config.get('epsilon', 1.0)
        self.epsilon_min = config.get('epsilon_min', 0.01)
        self.epsilon_decay = config.get('epsilon_decay', 0.995)
        self.target_update_freq = config.get('target_update_freq', 100)
        self.memory_size = config.get('memory_size', 10000)
        
        # ç¶²è·¯åƒæ•¸
        self.state_dim = config.get('state_dim', 10)  # æ ¹æ“šè¡›æ˜Ÿç‹€æ…‹å®šç¾©
        self.action_dim = config.get('action_dim', 5)  # å€™é¸è¡›æ˜Ÿæ•¸é‡
        self.hidden_layers = config.get('hidden_layers', [64, 64])
        
        # åˆå§‹åŒ–ç¶²è·¯
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.q_network = DQNNetworkWrapper(self.state_dim, self.action_dim, self.hidden_layers).to(self.device)
        self.target_network = DQNNetworkWrapper(self.state_dim, self.action_dim, self.hidden_layers).to(self.device)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)
        \n        # ç¶“é©—å›æ”¾æ± \n        self.memory = []\n        self.memory_pointer = 0\n        \n        # è¨“ç·´ç‹€æ…‹\n        self._is_trained = False\n        self._training_metrics = {}\n        self._observers: List[ITrainingObserver] = []\n        \n        # æ ¹æ“šå ´æ™¯èª¿æ•´åƒæ•¸\n        self._adjust_scenario_parameters()\n        \n        logger.info(f\"DQN ç®—æ³•åˆå§‹åŒ–å®Œæˆ (å ´æ™¯: {scenario_type.value})\")\n    \n    def _adjust_scenario_parameters(self) -> None:\n        \"\"\"æ ¹æ“šå ´æ™¯èª¿æ•´ç®—æ³•åƒæ•¸\"\"\"\n        if self.scenario_type == ScenarioType.LOW_LATENCY:\n            # ä½å»¶é²å ´æ™¯éœ€è¦æ›´å¿«çš„æ±ºç­–\n            self.epsilon_decay = 0.99\n            self.target_update_freq = 50\n            logger.debug(\"èª¿æ•´åƒæ•¸ä»¥é©æ‡‰ä½å»¶é²å ´æ™¯\")\n        elif self.scenario_type == ScenarioType.URBAN:\n            # åŸå¸‚å ´æ™¯ç¶²è·¯è®ŠåŒ–è¼ƒå¿«\n            self.learning_rate *= 1.2\n            logger.debug(\"èª¿æ•´åƒæ•¸ä»¥é©æ‡‰åŸå¸‚å ´æ™¯\")\n        elif self.scenario_type == ScenarioType.SUBURBAN:\n            # éƒŠå€å ´æ™¯ç›¸å°ç©©å®š\n            self.epsilon_decay = 0.998\n            logger.debug(\"èª¿æ•´åƒæ•¸ä»¥é©æ‡‰éƒŠå€å ´æ™¯\")\n    \n    def get_name(self) -> str:\n        \"\"\"ç²å–ç®—æ³•åç¨±\"\"\"\n        return self._name\n    \n    def get_version(self) -> str:\n        \"\"\"ç²å–ç®—æ³•ç‰ˆæœ¬\"\"\"\n        return self._version\n    \n    def get_supported_scenarios(self) -> List[ScenarioType]:\n        \"\"\"ç²å–æ”¯æŒçš„å ´æ™¯é¡å‹\"\"\"\n        return [ScenarioType.URBAN, ScenarioType.SUBURBAN, ScenarioType.LOW_LATENCY]\n    \n    async def train(self, config: TrainingConfig) -> TrainingResult:\n        \"\"\"åŸ·è¡Œè¨“ç·´\"\"\"\n        start_time = datetime.now()\n        \n        try:\n            logger.info(f\"é–‹å§‹ DQN è¨“ç·´ (å›åˆæ•¸: {config.episodes})\")\n            \n            total_reward = 0\n            episode_rewards = []\n            convergence_episode = None\n            convergence_threshold = 0.8  # 80% æˆåŠŸç‡è¦–ç‚ºæ”¶æ–‚\n            \n            for episode in range(config.episodes):\n                # é€šçŸ¥è§€å¯Ÿè€…å›åˆé–‹å§‹\n                for observer in self._observers:\n                    await observer.on_episode_start(episode, self._name)\n                \n                episode_reward = await self._train_episode(config)\n                episode_rewards.append(episode_reward)\n                total_reward += episode_reward\n                \n                # è¨ˆç®—ç§»å‹•å¹³å‡æˆåŠŸç‡\n                if len(episode_rewards) >= 100:\n                    recent_success_rate = np.mean([r > 0 for r in episode_rewards[-100:]])\n                    if recent_success_rate >= convergence_threshold and convergence_episode is None:\n                        convergence_episode = episode\n                        logger.info(f\"ç®—æ³•åœ¨ç¬¬ {episode} å›åˆæ”¶æ–‚\")\n                \n                # æ›´æ–°ç›®æ¨™ç¶²è·¯\n                if episode % self.target_update_freq == 0:\n                    self.target_network.load_state_dict(self.q_network.state_dict())\n                \n                # è¡°æ¸›æ¢ç´¢ç‡\n                self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n                \n                # é€šçŸ¥è§€å¯Ÿè€…å›åˆçµæŸ\n                episode_metrics = {\n                    \"reward\": episode_reward,\n                    \"epsilon\": self.epsilon,\n                    \"success_rate\": recent_success_rate if len(episode_rewards) >= 100 else 0.0\n                }\n                \n                for observer in self._observers:\n                    await observer.on_episode_end(episode, episode_reward, episode_metrics)\n                \n                # æ¯100å›åˆè¨˜éŒ„é€²åº¦\n                if (episode + 1) % 100 == 0:\n                    avg_reward = np.mean(episode_rewards[-100:])\n                    logger.info(f\"å›åˆ {episode + 1}: å¹³å‡çå‹µ = {avg_reward:.3f}, Epsilon = {self.epsilon:.3f}\")\n            \n            # è¨“ç·´å®Œæˆ\n            end_time = datetime.now()\n            duration = (end_time - start_time).total_seconds()\n            \n            self._is_trained = True\n            final_score = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n            \n            # æ›´æ–°è¨“ç·´æŒ‡æ¨™\n            self._training_metrics = {\n                \"total_episodes\": config.episodes,\n                \"final_score\": final_score,\n                \"convergence_episode\": convergence_episode,\n                \"final_epsilon\": self.epsilon,\n                \"training_duration\": duration\n            }\n            \n            result = TrainingResult(\n                success=True,\n                final_score=final_score,\n                episodes_completed=config.episodes,\n                convergence_episode=convergence_episode,\n                metrics=self._training_metrics,\n                model_path=None,  # å°‡åœ¨ä¿å­˜æ¨¡å‹æ™‚è¨­ç½®\n                training_duration_seconds=duration,\n                memory_usage_mb=self._get_memory_usage_mb()\n            )\n            \n            # é€šçŸ¥è§€å¯Ÿè€…è¨“ç·´å®Œæˆ\n            for observer in self._observers:\n                await observer.on_training_complete(result)\n            \n            logger.info(f\"DQN è¨“ç·´å®Œæˆ: æœ€çµ‚åˆ†æ•¸ = {final_score:.3f}\")\n            return result\n            \n        except Exception as e:\n            logger.error(f\"DQN è¨“ç·´å¤±æ•—: {e}\")\n            return TrainingResult(\n                success=False,\n                final_score=0.0,\n                episodes_completed=0,\n                convergence_episode=None,\n                metrics={\"error\": str(e)},\n                model_path=None,\n                training_duration_seconds=(datetime.now() - start_time).total_seconds(),\n                memory_usage_mb=self._get_memory_usage_mb()\n            )\n    \n    async def _train_episode(self, config: TrainingConfig) -> float:\n        \"\"\"è¨“ç·´å–®å€‹å›åˆ\"\"\"\n        # æ¨¡æ“¬ç’°å¢ƒäº¤äº’\n        total_reward = 0\n        \n        for step in range(config.max_steps_per_episode):\n            # æ¨¡æ“¬ç‹€æ…‹è§€å¯Ÿ\n            state = self._generate_mock_state()\n            \n            # é¸æ“‡å‹•ä½œ\n            action = self._select_action(state)\n            \n            # åŸ·è¡Œå‹•ä½œä¸¦ç²å¾—åé¥‹\n            next_state, reward, done = self._simulate_environment_step(state, action)\n            \n            # å­˜å„²ç¶“é©—\n            self._store_experience(state, action, reward, next_state, done)\n            \n            # æ›´æ–°ç¶²è·¯\n            if len(self.memory) >= self.batch_size:\n                self._update_network()\n            \n            total_reward += reward\n            \n            if done:\n                break\n        \n        return total_reward\n    \n    def _generate_mock_state(self) -> np.ndarray:\n        \"\"\"ç”Ÿæˆæ¨¡æ“¬ç‹€æ…‹\"\"\"\n        # æ¨¡æ“¬è¡›æ˜Ÿç¶²è·¯ç‹€æ…‹ï¼šä½ç½®ã€ä¿¡è™Ÿå¼·åº¦ã€è² è¼‰ç­‰\n        return np.random.randn(self.state_dim)\n    \n    def _select_action(self, state: np.ndarray) -> int:\n        \"\"\"é¸æ“‡å‹•ä½œï¼ˆepsilon-greedyç­–ç•¥ï¼‰\"\"\"\n        if np.random.random() < self.epsilon:\n            return np.random.randint(self.action_dim)\n        \n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n            q_values = self.q_network(state_tensor)\n            return q_values.argmax().item()\n    \n    def _simulate_environment_step(self, state: np.ndarray, action: int) -> tuple:\n        \"\"\"æ¨¡æ“¬ç’°å¢ƒæ­¥é©Ÿ\"\"\"\n        # æ¨¡æ“¬æ›æ‰‹æ±ºç­–çš„çµæœ\n        next_state = self._generate_mock_state()\n        \n        # æ ¹æ“šå ´æ™¯ç”Ÿæˆä¸åŒçš„çå‹µ\n        if self.scenario_type == ScenarioType.LOW_LATENCY:\n            # ä½å»¶é²å ´æ™¯é‡è¦–å¿«é€Ÿæ±ºç­–\n            reward = np.random.normal(1.0, 0.5)\n        elif self.scenario_type == ScenarioType.URBAN:\n            # åŸå¸‚å ´æ™¯é‡è¦–ç©©å®šæ€§\n            reward = np.random.normal(0.8, 0.3)\n        else:\n            # éƒŠå€å ´æ™¯\n            reward = np.random.normal(0.9, 0.2)\n        \n        done = np.random.random() < 0.1  # 10% æ©Ÿç‡çµæŸ\n        \n        return next_state, reward, done\n    \n    def _store_experience(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool):\n        \"\"\"å­˜å„²ç¶“é©—åˆ°å›æ”¾æ± \"\"\"\n        experience = (state, action, reward, next_state, done)\n        \n        if len(self.memory) < self.memory_size:\n            self.memory.append(experience)\n        else:\n            self.memory[self.memory_pointer] = experience\n            self.memory_pointer = (self.memory_pointer + 1) % self.memory_size\n    \n    def _update_network(self):\n        \"\"\"æ›´æ–° Q ç¶²è·¯\"\"\"\n        batch = np.random.choice(len(self.memory), self.batch_size, replace=False)\n        batch_experiences = [self.memory[i] for i in batch]\n        \n        states = torch.FloatTensor([exp[0] for exp in batch_experiences]).to(self.device)\n        actions = torch.LongTensor([exp[1] for exp in batch_experiences]).to(self.device)\n        rewards = torch.FloatTensor([exp[2] for exp in batch_experiences]).to(self.device)\n        next_states = torch.FloatTensor([exp[3] for exp in batch_experiences]).to(self.device)\n        dones = torch.BoolTensor([exp[4] for exp in batch_experiences]).to(self.device)\n        \n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n        \n        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n        \n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n    \n    async def predict(self, context: PredictionContext) -> HandoverDecision:\n        \"\"\"åŸ·è¡Œæ›æ‰‹æ±ºç­–é æ¸¬\"\"\"\n        try:\n            # å°‡ä¸Šä¸‹æ–‡è½‰æ›ç‚ºç‹€æ…‹å‘é‡\n            state = self._context_to_state(context)\n            \n            # ä½¿ç”¨ç¶²è·¯é€²è¡Œé æ¸¬\n            with torch.no_grad():\n                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n                q_values = self.q_network(state_tensor)\n                action = q_values.argmax().item()\n            \n            # å°‡å‹•ä½œæ˜ å°„åˆ°å€™é¸è¡›æ˜Ÿ\n            target_satellite_id = context.candidate_satellites[action % len(context.candidate_satellites)]\n            confidence_score = float(torch.softmax(q_values, dim=1).max())\n            \n            # æ ¹æ“šå ´æ™¯ä¼°ç®—æ€§èƒ½æŒ‡æ¨™\n            estimated_latency = self._estimate_latency(context, target_satellite_id)\n            predicted_throughput = self._estimate_throughput(context, target_satellite_id)\n            \n            decision = HandoverDecision(\n                target_satellite_id=target_satellite_id,\n                confidence_score=confidence_score,\n                estimated_latency_ms=estimated_latency,\n                predicted_throughput_mbps=predicted_throughput,\n                decision_reasoning={\n                    \"algorithm\": self._name,\n                    \"q_values\": q_values.cpu().numpy().tolist(),\n                    \"selected_action\": action,\n                    \"scenario\": self.scenario_type.value\n                },\n                execution_priority=1\n            )\n            \n            logger.debug(f\"DQN é æ¸¬å®Œæˆ: ç›®æ¨™è¡›æ˜Ÿ {target_satellite_id}, ä¿¡å¿ƒåº¦ {confidence_score:.3f}\")\n            return decision\n            \n        except Exception as e:\n            logger.error(f\"DQN é æ¸¬å¤±æ•—: {e}\")\n            # è¿”å›é è¨­æ±ºç­–\n            return HandoverDecision(\n                target_satellite_id=context.candidate_satellites[0],\n                confidence_score=0.1,\n                estimated_latency_ms=100.0,\n                predicted_throughput_mbps=10.0,\n                decision_reasoning={\"error\": str(e)},\n                execution_priority=5\n            )\n    \n    def _context_to_state(self, context: PredictionContext) -> np.ndarray:\n        \"\"\"å°‡é æ¸¬ä¸Šä¸‹æ–‡è½‰æ›ç‚ºç‹€æ…‹å‘é‡\"\"\"\n        # ç°¡åŒ–çš„ç‹€æ…‹è¡¨ç¤º\n        state = np.zeros(self.state_dim)\n        \n        # UE ä½ç½® (2D)\n        state[0] = context.ue_position.get('lat', 0.0)\n        state[1] = context.ue_position.get('lon', 0.0)\n        \n        # ç•¶å‰æœå‹™è¡›æ˜Ÿè³‡è¨Š\n        state[2] = context.current_serving_satellite\n        \n        # å€™é¸è¡›æ˜Ÿæ•¸é‡\n        state[3] = len(context.candidate_satellites)\n        \n        # ç¶²è·¯æ¢ä»¶æŒ‡æ¨™\n        network_conditions = context.network_conditions\n        state[4] = network_conditions.get('signal_strength', 0.0)\n        state[5] = network_conditions.get('latency', 0.0)\n        state[6] = network_conditions.get('throughput', 0.0)\n        state[7] = network_conditions.get('packet_loss', 0.0)\n        \n        # æ™‚é–“ç‰¹å¾µï¼ˆå°æ™‚ï¼‰\n        state[8] = context.timestamp.hour / 24.0\n        \n        # å ´æ™¯ç‰¹å¾µ\n        scenario_encoding = {\n            ScenarioType.URBAN: 0.0,\n            ScenarioType.SUBURBAN: 0.5,\n            ScenarioType.LOW_LATENCY: 1.0\n        }\n        state[9] = scenario_encoding.get(self.scenario_type, 0.0)\n        \n        return state\n    \n    def _estimate_latency(self, context: PredictionContext, satellite_id: int) -> float:\n        \"\"\"ä¼°ç®—æ›æ‰‹å»¶é²\"\"\"\n        base_latency = 50.0  # åŸºç¤å»¶é² (ms)\n        \n        if self.scenario_type == ScenarioType.LOW_LATENCY:\n            return base_latency * 0.8\n        elif self.scenario_type == ScenarioType.URBAN:\n            return base_latency * 1.2\n        else:\n            return base_latency\n    \n    def _estimate_throughput(self, context: PredictionContext, satellite_id: int) -> float:\n        \"\"\"ä¼°ç®—ååé‡\"\"\"\n        base_throughput = 100.0  # åŸºç¤ååé‡ (Mbps)\n        \n        if self.scenario_type == ScenarioType.URBAN:\n            return base_throughput * 1.5  # åŸå¸‚å ´æ™¯å¸¶å¯¬è¼ƒé«˜\n        elif self.scenario_type == ScenarioType.LOW_LATENCY:\n            return base_throughput * 0.8  # ä½å»¶é²å ´æ™¯å¯èƒ½çŠ§ç‰²å¸¶å¯¬\n        else:\n            return base_throughput\n    \n    def load_model(self, model_path: str) -> bool:\n        \"\"\"åŠ è¼‰æ¨¡å‹\"\"\"\n        try:\n            model_path = Path(model_path)\n            if not model_path.exists():\n                logger.error(f\"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}\")\n                return False\n            \n            checkpoint = torch.load(model_path, map_location=self.device)\n            self.q_network.load_state_dict(checkpoint['q_network'])\n            self.target_network.load_state_dict(checkpoint['target_network'])\n            self.optimizer.load_state_dict(checkpoint['optimizer'])\n            self.epsilon = checkpoint.get('epsilon', self.epsilon_min)\n            \n            self._is_trained = True\n            logger.info(f\"æˆåŠŸåŠ è¼‰ DQN æ¨¡å‹: {model_path}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"åŠ è¼‰æ¨¡å‹å¤±æ•—: {e}\")\n            return False\n    \n    def save_model(self, model_path: str) -> bool:\n        \"\"\"ä¿å­˜æ¨¡å‹\"\"\"\n        try:\n            model_path = Path(model_path)\n            model_path.parent.mkdir(parents=True, exist_ok=True)\n            \n            checkpoint = {\n                'q_network': self.q_network.state_dict(),\n                'target_network': self.target_network.state_dict(),\n                'optimizer': self.optimizer.state_dict(),\n                'epsilon': self.epsilon,\n                'config': self.config,\n                'scenario_type': self.scenario_type.value,\n                'training_metrics': self._training_metrics\n            }\n            \n            torch.save(checkpoint, model_path)\n            logger.info(f\"æˆåŠŸä¿å­˜ DQN æ¨¡å‹: {model_path}\")\n            return True\n            \n        except Exception as e:\n            logger.error(f\"ä¿å­˜æ¨¡å‹å¤±æ•—: {e}\")\n            return False\n    \n    def get_hyperparameters(self) -> Dict[str, Any]:\n        \"\"\"ç²å–ç•¶å‰è¶…åƒæ•¸\"\"\"\n        return {\n            'learning_rate': self.learning_rate,\n            'batch_size': self.batch_size,\n            'gamma': self.gamma,\n            'epsilon': self.epsilon,\n            'epsilon_min': self.epsilon_min,\n            'epsilon_decay': self.epsilon_decay,\n            'target_update_freq': self.target_update_freq,\n            'memory_size': self.memory_size,\n            'hidden_layers': self.hidden_layers\n        }\n    \n    def set_hyperparameters(self, params: Dict[str, Any]) -> bool:\n        \"\"\"è¨­å®šè¶…åƒæ•¸\"\"\"\n        try:\n            for key, value in params.items():\n                if hasattr(self, key):\n                    setattr(self, key, value)\n                    logger.debug(f\"æ›´æ–°è¶…åƒæ•¸: {key} = {value}\")\n            return True\n        except Exception as e:\n            logger.error(f\"è¨­å®šè¶…åƒæ•¸å¤±æ•—: {e}\")\n            return False\n    \n    def get_training_metrics(self) -> Dict[str, Any]:\n        \"\"\"ç²å–è¨“ç·´æŒ‡æ¨™\"\"\"\n        return self._training_metrics.copy()\n    \n    def is_trained(self) -> bool:\n        \"\"\"æª¢æŸ¥æ¨¡å‹æ˜¯å¦å·²è¨“ç·´\"\"\"\n        return self._is_trained\n    \n    def get_memory_usage(self) -> Dict[str, float]:\n        \"\"\"ç²å–è¨˜æ†¶é«”ä½¿ç”¨é‡\"\"\"\n        return {\n            'total_mb': self._get_memory_usage_mb(),\n            'network_params': sum(p.numel() for p in self.q_network.parameters()) * 4 / 1024 / 1024,  # å‡è¨­ float32\n            'experience_replay_mb': len(self.memory) * self.state_dim * 4 / 1024 / 1024\n        }\n    \n    def _get_memory_usage_mb(self) -> float:\n        \"\"\"ç²å–ç¸½è¨˜æ†¶é«”ä½¿ç”¨é‡ (MB)\"\"\"\n        import psutil\n        import os\n        process = psutil.Process(os.getpid())\n        return process.memory_info().rss / 1024 / 1024\n    \n    def validate_scenario(self, scenario: ScenarioType) -> bool:\n        \"\"\"é©—è­‰å ´æ™¯æ˜¯å¦æ”¯æ´\"\"\"\n        return scenario in self.get_supported_scenarios()\n    \n    def add_training_observer(self, observer: ITrainingObserver) -> None:\n        \"\"\"æ·»åŠ è¨“ç·´è§€å¯Ÿè€…\"\"\"\n        self._observers.append(observer)\n    \n    def remove_training_observer(self, observer: ITrainingObserver) -> None:\n        \"\"\"ç§»é™¤è¨“ç·´è§€å¯Ÿè€…\"\"\"\n        if observer in self._observers:\n            self._observers.remove(observer)"