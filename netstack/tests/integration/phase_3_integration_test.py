"""
Phase 3 ç«¯åˆ°ç«¯æ•´åˆæ¸¬è©¦

é©—è­‰æ‰€æœ‰ Phase 3 æ±ºç­–é€æ˜åŒ–èˆ‡è¦–è¦ºåŒ–çµ„ä»¶çš„æ•´åˆå·¥ä½œï¼š
- é«˜ç´šè§£é‡‹æ€§åˆ†æå¼•æ“æ¸¬è©¦
- æ”¶æ–‚æ€§åˆ†æå™¨æ¸¬è©¦
- çµ±è¨ˆæ¸¬è©¦å¼•æ“æ¸¬è©¦
- å­¸è¡“æ•¸æ“šåŒ¯å‡ºå™¨æ¸¬è©¦
- è¦–è¦ºåŒ–å¼•æ“æ¸¬è©¦
- å®Œæ•´æ±ºç­–é€æ˜åŒ–å·¥ä½œæµæ¸¬è©¦

Created for Phase 3: Decision Transparency & Visualization Optimization
"""

import asyncio
import logging
import time
import numpy as np
import tempfile
import os
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path

# è¨­ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Phase3IntegrationTest:
    """Phase 3 æ•´åˆæ¸¬è©¦é¡"""

    def __init__(self):
        self.test_results = []
        self.failed_tests = []
        self.total_tests = 0
        self.passed_tests = 0
        self.temp_dir = tempfile.mkdtemp(prefix="phase3_test_")
        logger.info(f"ğŸ“ æ¸¬è©¦è‡¨æ™‚ç›®éŒ„: {self.temp_dir}")

    async def run_all_tests(self) -> Dict[str, Any]:
        """é‹è¡Œæ‰€æœ‰ Phase 3 æ•´åˆæ¸¬è©¦"""
        logger.info("ğŸš€ é–‹å§‹ Phase 3 æ±ºç­–é€æ˜åŒ–èˆ‡è¦–è¦ºåŒ–æ•´åˆæ¸¬è©¦")
        start_time = time.time()

        test_functions = [
            self._test_analytics_imports,
            self._test_explainability_engine,
            self._test_convergence_analyzer,
            self._test_statistical_testing_engine,
            self._test_academic_data_exporter,
            self._test_visualization_engine,
            self._test_phase3_api_integration,
            self._test_complete_transparency_workflow,
        ]

        for test_func in test_functions:
            await self._run_test(test_func)

        duration = time.time() - start_time
        success_rate = self.passed_tests / max(self.total_tests, 1) * 100

        summary = {
            "total_tests": self.total_tests,
            "passed_tests": self.passed_tests,
            "failed_tests": len(self.failed_tests),
            "success_rate": success_rate,
            "duration_seconds": duration,
            "test_results": self.test_results,
            "failed_test_names": self.failed_tests,
            "phase": "Phase 3: Decision Transparency & Visualization",
        }

        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        self._cleanup_temp_files()

        logger.info(
            f"ğŸ“Š Phase 3 æ¸¬è©¦å®Œæˆï¼é€šéç‡: {success_rate:.1f}% ({self.passed_tests}/{self.total_tests})"
        )

        if self.failed_tests:
            logger.error(f"âŒ å¤±æ•—çš„æ¸¬è©¦: {', '.join(self.failed_tests)}")
        else:
            logger.info("ğŸ‰ æ‰€æœ‰ Phase 3 æ¸¬è©¦éƒ½é€šéäº†ï¼")

        return summary

    async def _run_test(self, test_func):
        """é‹è¡Œå–®å€‹æ¸¬è©¦"""
        test_name = test_func.__name__
        self.total_tests += 1

        try:
            logger.info(f"ğŸ§ª é‹è¡Œæ¸¬è©¦: {test_name}")
            result = await test_func()

            if result.get("success", False):
                self.passed_tests += 1
                logger.info(f"âœ… {test_name} é€šé")
            else:
                self.failed_tests.append(test_name)
                logger.error(f"âŒ {test_name} å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

            self.test_results.append(
                {
                    "test_name": test_name,
                    "success": result.get("success", False),
                    "duration": result.get("duration", 0),
                    "details": result.get("details", {}),
                    "error": result.get("error"),
                }
            )

        except Exception as e:
            self.failed_tests.append(test_name)
            logger.error(f"âŒ {test_name} ç•°å¸¸: {str(e)}")

            self.test_results.append(
                {
                    "test_name": test_name,
                    "success": False,
                    "duration": 0,
                    "details": {},
                    "error": str(e),
                }
            )

    async def _test_analytics_imports(self) -> Dict[str, Any]:
        """æ¸¬è©¦ Phase 3 åˆ†æçµ„ä»¶å°å…¥"""
        start_time = time.time()

        try:
            # æ¸¬è©¦æ ¸å¿ƒåˆ†æçµ„ä»¶å°å…¥
            from ..analytics import (
                AdvancedExplainabilityEngine,
                ConvergenceAnalyzer,
                StatisticalTestingEngine,
                AcademicDataExporter,
                VisualizationEngine,
            )

            # æª¢æŸ¥å¯é¸ä¾è³´é …
            dependencies = {}
            
            try:
                import scipy
                dependencies["scipy"] = True
            except ImportError:
                dependencies["scipy"] = False

            try:
                import sklearn
                dependencies["sklearn"] = True
            except ImportError:
                dependencies["sklearn"] = False

            try:
                import matplotlib
                dependencies["matplotlib"] = True
            except ImportError:
                dependencies["matplotlib"] = False

            try:
                import plotly
                dependencies["plotly"] = True
            except ImportError:
                dependencies["plotly"] = False

            available_deps = sum(dependencies.values())
            core_available = available_deps >= 2  # è‡³å°‘éœ€è¦ scipy å’Œå…¶ä»–ä¸€å€‹

            return {
                "success": core_available,
                "duration": time.time() - start_time,
                "details": {
                    "all_imports_successful": True,
                    "dependencies": dependencies,
                    "available_dependencies": available_deps,
                    "core_dependencies_met": core_available,
                },
            }

        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": f"Phase 3 å°å…¥æ¸¬è©¦å¤±æ•—: {str(e)}",
            }

    async def _test_explainability_engine(self) -> Dict[str, Any]:
        """æ¸¬è©¦é«˜ç´šè§£é‡‹æ€§åˆ†æå¼•æ“"""
        start_time = time.time()

        try:
            from ..analytics import AdvancedExplainabilityEngine

            # å‰µå»ºå¼•æ“å¯¦ä¾‹
            config = {
                "explainability_level": "detailed",
                "enable_feature_importance": True,
                "enable_decision_paths": True,
                "enable_counterfactual": True,
            }

            engine = AdvancedExplainabilityEngine(config)

            # ç”Ÿæˆæ¸¬è©¦æ±ºç­–æ•¸æ“š
            test_decision_data = {
                "state": np.random.rand(10),  # 10ç¶­ç‹€æ…‹ç©ºé–“
                "action": 2,  # é¸æ“‡çš„å‹•ä½œ
                "q_values": np.random.rand(5),  # 5å€‹å‹•ä½œçš„Qå€¼
                "algorithm": "DQN",
                "episode": 100,
                "step": 50,
                "scenario_context": {
                    "satellite_candidates": [
                        {"id": "sat_1", "signal_strength": 0.8, "elevation": 45},
                        {"id": "sat_2", "signal_strength": 0.6, "elevation": 30},
                        {"id": "sat_3", "signal_strength": 0.9, "elevation": 60},
                    ],
                    "user_location": {"lat": 24.7867, "lon": 120.9967},
                },
            }

            # æ¸¬è©¦æ±ºç­–è§£é‡‹
            explanation = engine.explain_decision(test_decision_data)

            # æ¸¬è©¦ç‰¹å¾µé‡è¦æ€§åˆ†æ
            feature_importance = engine.analyze_feature_importance(
                state=test_decision_data["state"],
                q_values=test_decision_data["q_values"],
                action=test_decision_data["action"],
            )

            # æ¸¬è©¦æ±ºç­–è·¯å¾‘åˆ†æ
            decision_path = engine.analyze_decision_path(test_decision_data)

            # é©—è­‰çµæœ
            tests_passed = {
                "explanation_generated": explanation is not None,
                "feature_importance_available": feature_importance is not None,
                "decision_path_available": decision_path is not None,
                "confidence_score_present": explanation.get("confidence_score") is not None if explanation else False,
            }

            success = sum(tests_passed.values()) >= 3  # è‡³å°‘3å€‹æ¸¬è©¦é€šé

            return {
                "success": success,
                "duration": time.time() - start_time,
                "details": {
                    "tests_passed": tests_passed,
                    "explanation_fields": list(explanation.keys()) if explanation else [],
                    "feature_importance_method": feature_importance.get("method") if feature_importance else None,
                    "decision_path_length": len(decision_path.get("path", [])) if decision_path else 0,
                },
            }

        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": f"è§£é‡‹æ€§å¼•æ“æ¸¬è©¦å¤±æ•—: {str(e)}",
            }

    async def _test_convergence_analyzer(self) -> Dict[str, Any]:
        """æ¸¬è©¦æ”¶æ–‚æ€§åˆ†æå™¨"""
        start_time = time.time()

        try:
            from ..analytics import ConvergenceAnalyzer

            # å‰µå»ºåˆ†æå™¨å¯¦ä¾‹
            config = {
                "smoothing_window": 10,
                "convergence_threshold": 0.01,
                "min_stable_episodes": 20,
                "enable_forecasting": True,
            }

            analyzer = ConvergenceAnalyzer(config)

            # ç”Ÿæˆæ¨¡æ“¬è¨“ç·´æ•¸æ“š
            episodes = 200
            # æ¨¡æ“¬æ”¶æ–‚çš„å­¸ç¿’æ›²ç·š
            base_reward = np.linspace(-100, 50, episodes)
            noise = np.random.normal(0, 10, episodes)
            rewards = base_reward + noise
            
            # æ·»åŠ ä¸€äº›æ€§èƒ½æŒ‡æ¨™
            success_rates = np.clip(np.linspace(0.1, 0.9, episodes) + np.random.normal(0, 0.05, episodes), 0, 1)
            handover_latencies = np.maximum(np.linspace(100, 20, episodes) + np.random.normal(0, 5, episodes), 5)

            training_data = {
                "episodes": list(range(episodes)),
                "rewards": rewards.tolist(),
                "success_rates": success_rates.tolist(),
                "handover_latencies": handover_latencies.tolist(),
                "algorithm": "DQN",
                "scenario": "urban",
            }

            # æ¸¬è©¦å­¸ç¿’æ›²ç·šåˆ†æ
            curve_analysis = analyzer.analyze_learning_curve(
                rewards, 
                metric_name="total_reward"
            )

            # æ¸¬è©¦æ”¶æ–‚æª¢æ¸¬
            convergence_result = analyzer.detect_convergence(
                rewards[-50:],  # æœ€å¾Œ50å€‹episode
                metric_name="total_reward"
            )

            # æ¸¬è©¦æ€§èƒ½è¶¨å‹¢åˆ†æ
            trend_analysis = analyzer.analyze_performance_trend(training_data)

            # æ¸¬è©¦è¨“ç·´éšæ®µè­˜åˆ¥
            phase_analysis = analyzer.identify_training_phases(rewards)

            # é©—è­‰çµæœ
            tests_passed = {
                "curve_analysis_available": curve_analysis is not None,
                "convergence_detected": convergence_result is not None,
                "trend_analysis_available": trend_analysis is not None,
                "phases_identified": phase_analysis is not None and len(phase_analysis.get("phases", [])) > 0,
            }

            success = sum(tests_passed.values()) >= 3

            return {
                "success": success,
                "duration": time.time() - start_time,
                "details": {
                    "tests_passed": tests_passed,
                    "convergence_status": convergence_result.get("status") if convergence_result else None,
                    "identified_phases": len(phase_analysis.get("phases", [])) if phase_analysis else 0,
                    "trend_direction": trend_analysis.get("overall_trend") if trend_analysis else None,
                    "episodes_analyzed": episodes,
                },
            }

        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": f"æ”¶æ–‚æ€§åˆ†æå™¨æ¸¬è©¦å¤±æ•—: {str(e)}",
            }

    async def _test_statistical_testing_engine(self) -> Dict[str, Any]:
        """æ¸¬è©¦çµ±è¨ˆæ¸¬è©¦å¼•æ“"""
        start_time = time.time()

        try:
            from ..analytics import StatisticalTestingEngine

            # å‰µå»ºå¼•æ“å¯¦ä¾‹
            config = {
                "significance_level": 0.05,
                "enable_effect_size": True,
                "enable_power_analysis": True,
                "bootstrap_samples": 1000,
            }

            engine = StatisticalTestingEngine(config)

            # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š - å…©å€‹ç®—æ³•çš„æ€§èƒ½æ¯”è¼ƒ
            np.random.seed(42)  # ç¢ºä¿çµæœå¯é‡ç¾
            
            algorithm_a_rewards = np.random.normal(45, 15, 100)  # DQN
            algorithm_b_rewards = np.random.normal(50, 12, 100)  # PPO (ç¨å¾®æ›´å¥½)
            algorithm_c_rewards = np.random.normal(42, 18, 100)  # SAC

            # æ¸¬è©¦ t-test (å…©çµ„æ¯”è¼ƒ)
            ttest_result = engine.perform_t_test(
                algorithm_a_rewards,
                algorithm_b_rewards,
                test_name="DQN_vs_PPO_rewards"
            )

            # æ¸¬è©¦ Mann-Whitney U test (éåƒæ•¸æ¸¬è©¦)
            mannwhitney_result = engine.perform_mann_whitney_test(
                algorithm_a_rewards,
                algorithm_b_rewards,
                test_name="DQN_vs_PPO_nonparametric"
            )

            # æ¸¬è©¦ ANOVA (å¤šçµ„æ¯”è¼ƒ)
            anova_data = {
                "DQN": algorithm_a_rewards,
                "PPO": algorithm_b_rewards,
                "SAC": algorithm_c_rewards,
            }
            anova_result = engine.perform_anova(
                anova_data,
                test_name="Three_Algorithm_Comparison"
            )

            # æ¸¬è©¦æ•ˆæ‡‰é‡è¨ˆç®—
            effect_size = engine.calculate_effect_size(
                algorithm_a_rewards,
                algorithm_b_rewards,
                method="cohen_d"
            )

            # æ¸¬è©¦å¤šé‡æ¯”è¼ƒæ ¡æ­£
            p_values = [0.01, 0.03, 0.08, 0.12, 0.001]
            corrected_results = engine.apply_multiple_comparison_correction(
                p_values,
                method="bonferroni"
            )

            # é©—è­‰çµæœ
            tests_passed = {
                "t_test_completed": ttest_result is not None,
                "mannwhitney_completed": mannwhitney_result is not None,
                "anova_completed": anova_result is not None,
                "effect_size_calculated": effect_size is not None,
                "multiple_correction_applied": corrected_results is not None,
            }

            success = sum(tests_passed.values()) >= 4

            return {
                "success": success,
                "duration": time.time() - start_time,
                "details": {
                    "tests_passed": tests_passed,
                    "t_test_significant": ttest_result.get("significant") if ttest_result else None,
                    "anova_significant": anova_result.get("significant") if anova_result else None,
                    "effect_size_magnitude": effect_size.get("magnitude") if effect_size else None,
                    "corrected_significant_tests": sum(corrected_results.get("significant", [])) if corrected_results else 0,
                },
            }

        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": f"çµ±è¨ˆæ¸¬è©¦å¼•æ“æ¸¬è©¦å¤±æ•—: {str(e)}",
            }

    async def _test_academic_data_exporter(self) -> Dict[str, Any]:
        """æ¸¬è©¦å­¸è¡“æ•¸æ“šåŒ¯å‡ºå™¨"""
        start_time = time.time()

        try:
            from ..analytics import AcademicDataExporter

            # å‰µå»ºåŒ¯å‡ºå™¨å¯¦ä¾‹
            config = {
                "export_directory": self.temp_dir,
                "default_format": "csv",
                "include_metadata": True,
                "include_validation": True,
            }

            exporter = AcademicDataExporter(config)

            # ç”Ÿæˆæ¸¬è©¦ç ”ç©¶æ•¸æ“š
            research_data = {
                "experiment_metadata": {
                    "title": "LEO Satellite Handover RL Algorithm Comparison",
                    "authors": ["Research Team"],
                    "institution": "Test University",
                    "date": datetime.now().isoformat(),
                    "description": "Comparative analysis of DQN, PPO, and SAC algorithms",
                },
                "experimental_design": {
                    "algorithms": ["DQN", "PPO", "SAC"],
                    "scenarios": ["urban", "suburban", "rural"],
                    "episodes_per_algorithm": 1000,
                    "metrics": ["total_reward", "success_rate", "handover_latency"],
                },
                "results": {
                    "DQN": {
                        "total_reward": np.random.normal(45, 15, 100).tolist(),
                        "success_rate": np.random.uniform(0.7, 0.9, 100).tolist(),
                        "handover_latency": np.random.uniform(10, 50, 100).tolist(),
                    },
                    "PPO": {
                        "total_reward": np.random.normal(50, 12, 100).tolist(),
                        "success_rate": np.random.uniform(0.75, 0.95, 100).tolist(),
                        "handover_latency": np.random.uniform(8, 45, 100).tolist(),
                    },
                    "SAC": {
                        "total_reward": np.random.normal(47, 14, 100).tolist(),
                        "success_rate": np.random.uniform(0.72, 0.92, 100).tolist(),
                        "handover_latency": np.random.uniform(9, 48, 100).tolist(),
                    },
                },
                "statistical_analysis": {
                    "significance_tests": ["t_test", "anova", "mann_whitney"],
                    "effect_sizes": ["cohen_d", "eta_squared"],
                    "confidence_intervals": True,
                },
            }

            # æ¸¬è©¦ä¸åŒæ ¼å¼çš„åŒ¯å‡º
            export_tests = {}

            # CSV åŒ¯å‡º
            try:
                csv_result = exporter.export_to_csv(
                    research_data,
                    filename="test_research_data.csv"
                )
                export_tests["csv_export"] = csv_result.get("success", False)
            except Exception as e:
                logger.warning(f"CSVåŒ¯å‡ºæ¸¬è©¦å¤±æ•—: {e}")
                export_tests["csv_export"] = False

            # JSON åŒ¯å‡º
            try:
                json_result = exporter.export_to_json(
                    research_data,
                    filename="test_research_data.json"
                )
                export_tests["json_export"] = json_result.get("success", False)
            except Exception as e:
                logger.warning(f"JSONåŒ¯å‡ºæ¸¬è©¦å¤±æ•—: {e}")
                export_tests["json_export"] = False

            # å­¸è¡“å ±å‘Šç”Ÿæˆ
            try:
                report_result = exporter.generate_publication_ready_report(
                    research_data,
                    standard="IEEE",
                    filename="test_research_report"
                )
                export_tests["report_generation"] = report_result.get("success", False)
            except Exception as e:
                logger.warning(f"å ±å‘Šç”Ÿæˆæ¸¬è©¦å¤±æ•—: {e}")
                export_tests["report_generation"] = False

            # ç ”ç©¶æ•¸æ“šåŒ…ç”Ÿæˆ
            try:
                package_result = exporter.create_research_data_package(
                    research_data,
                    package_name="test_research_package"
                )
                export_tests["data_package"] = package_result.get("success", False)
            except Exception as e:
                logger.warning(f"æ•¸æ“šåŒ…ç”Ÿæˆæ¸¬è©¦å¤±æ•—: {e}")
                export_tests["data_package"] = False

            # æª¢æŸ¥è¼¸å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            output_files = list(Path(self.temp_dir).glob("*"))
            files_created = len(output_files)

            success = sum(export_tests.values()) >= 2 and files_created > 0

            return {
                "success": success,
                "duration": time.time() - start_time,
                "details": {
                    "export_tests": export_tests,
                    "files_created": files_created,
                    "output_directory": self.temp_dir,
                    "successful_exports": sum(export_tests.values()),
                },
            }

        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": f"å­¸è¡“æ•¸æ“šåŒ¯å‡ºå™¨æ¸¬è©¦å¤±æ•—: {str(e)}",
            }

    async def _test_visualization_engine(self) -> Dict[str, Any]:
        """æ¸¬è©¦è¦–è¦ºåŒ–å¼•æ“"""
        start_time = time.time()

        try:
            from ..analytics import VisualizationEngine

            # å‰µå»ºå¼•æ“å¯¦ä¾‹
            config = {
                "output_directory": self.temp_dir,
                "default_theme": "academic",
                "enable_interactive": True,
                "enable_realtime": True,
            }

            engine = VisualizationEngine(config)

            # ç”Ÿæˆæ¸¬è©¦æ•¸æ“š
            episodes = np.arange(1, 101)
            dqn_rewards = np.random.normal(45, 15, 100)
            ppo_rewards = np.random.normal(50, 12, 100)
            sac_rewards = np.random.normal(47, 14, 100)

            visualization_data = {
                "learning_curves": {
                    "episodes": episodes.tolist(),
                    "DQN": dqn_rewards.tolist(),
                    "PPO": ppo_rewards.tolist(),
                    "SAC": sac_rewards.tolist(),
                },
                "performance_comparison": {
                    "algorithms": ["DQN", "PPO", "SAC"],
                    "mean_rewards": [np.mean(dqn_rewards), np.mean(ppo_rewards), np.mean(sac_rewards)],
                    "std_rewards": [np.std(dqn_rewards), np.std(ppo_rewards), np.std(sac_rewards)],
                },
                "convergence_analysis": {
                    "convergence_points": [80, 85, 78],
                    "final_performance": [np.mean(dqn_rewards[-10:]), np.mean(ppo_rewards[-10:]), np.mean(sac_rewards[-10:])],
                },
            }

            # æ¸¬è©¦ä¸åŒé¡å‹çš„è¦–è¦ºåŒ–
            visualization_tests = {}

            # å­¸ç¿’æ›²ç·šåœ–
            try:
                learning_curve_result = engine.create_learning_curve_plot(
                    visualization_data["learning_curves"],
                    title="Algorithm Learning Curves",
                    filename="learning_curves"
                )
                visualization_tests["learning_curves"] = learning_curve_result.get("success", False)
            except Exception as e:
                logger.warning(f"å­¸ç¿’æ›²ç·šåœ–æ¸¬è©¦å¤±æ•—: {e}")
                visualization_tests["learning_curves"] = False

            # æ€§èƒ½æ¯”è¼ƒåœ–
            try:
                comparison_result = engine.create_performance_comparison_plot(
                    visualization_data["performance_comparison"],
                    title="Algorithm Performance Comparison",
                    filename="performance_comparison"
                )
                visualization_tests["performance_comparison"] = comparison_result.get("success", False)
            except Exception as e:
                logger.warning(f"æ€§èƒ½æ¯”è¼ƒåœ–æ¸¬è©¦å¤±æ•—: {e}")
                visualization_tests["performance_comparison"] = False

            # æ”¶æ–‚åˆ†æåœ–
            try:
                convergence_result = engine.create_convergence_analysis_plot(
                    visualization_data["convergence_analysis"],
                    title="Convergence Analysis",
                    filename="convergence_analysis"
                )
                visualization_tests["convergence_analysis"] = convergence_result.get("success", False)
            except Exception as e:
                logger.warning(f"æ”¶æ–‚åˆ†æåœ–æ¸¬è©¦å¤±æ•—: {e}")
                visualization_tests["convergence_analysis"] = False

            # å„€è¡¨æ¿ç”Ÿæˆ
            try:
                dashboard_result = engine.create_analysis_dashboard(
                    visualization_data,
                    title="RL Algorithm Analysis Dashboard",
                    filename="analysis_dashboard"
                )
                visualization_tests["dashboard"] = dashboard_result.get("success", False)
            except Exception as e:
                logger.warning(f"å„€è¡¨æ¿ç”Ÿæˆæ¸¬è©¦å¤±æ•—: {e}")
                visualization_tests["dashboard"] = False

            # æª¢æŸ¥ç”Ÿæˆçš„è¦–è¦ºåŒ–æ–‡ä»¶
            viz_files = list(Path(self.temp_dir).glob("*.png")) + list(Path(self.temp_dir).glob("*.html"))
            visualizations_created = len(viz_files)

            success = sum(visualization_tests.values()) >= 2 or visualizations_created > 0

            return {
                "success": success,
                "duration": time.time() - start_time,
                "details": {
                    "visualization_tests": visualization_tests,
                    "visualizations_created": visualizations_created,
                    "successful_visualizations": sum(visualization_tests.values()),
                    "output_directory": self.temp_dir,
                },
            }

        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": f"è¦–è¦ºåŒ–å¼•æ“æ¸¬è©¦å¤±æ•—: {str(e)}",
            }

    async def _test_phase3_api_integration(self) -> Dict[str, Any]:
        """æ¸¬è©¦ Phase 3 API æ•´åˆ"""
        start_time = time.time()

        try:
            # æª¢æŸ¥ Phase 3 API æ˜¯å¦å­˜åœ¨
            api_components = {}

            try:
                # æª¢æŸ¥æ˜¯å¦æœ‰ Phase 3 API è·¯ç”±
                from ..api.phase_3_api import router as phase3_router
                api_components["phase3_api"] = True
            except ImportError:
                api_components["phase3_api"] = False

            try:
                # æª¢æŸ¥åˆ†ææœå‹™æ˜¯å¦å¯é€šé API è¨ªå•
                from ..analytics import (
                    AdvancedExplainabilityEngine,
                    ConvergenceAnalyzer,
                    StatisticalTestingEngine,
                    AcademicDataExporter,
                    VisualizationEngine,
                )
                api_components["analytics_services"] = True
            except ImportError:
                api_components["analytics_services"] = False

            # æ¨¡æ“¬ API ç«¯é»æ¸¬è©¦ï¼ˆä¸å¯¦éš›ç™¼é€ HTTP è«‹æ±‚ï¼‰
            api_endpoint_tests = {
                "explainability_endpoint": api_components.get("phase3_api", False),
                "convergence_endpoint": api_components.get("phase3_api", False),
                "statistical_endpoint": api_components.get("phase3_api", False),
                "export_endpoint": api_components.get("phase3_api", False),
                "visualization_endpoint": api_components.get("phase3_api", False),
            }

            available_endpoints = sum(api_endpoint_tests.values())
            success = api_components.get("analytics_services", False) and available_endpoints >= 2

            return {
                "success": success,
                "duration": time.time() - start_time,
                "details": {
                    "api_components": api_components,
                    "api_endpoint_tests": api_endpoint_tests,
                    "available_endpoints": available_endpoints,
                    "analytics_services_available": api_components.get("analytics_services", False),
                },
            }

        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": f"Phase 3 API æ•´åˆæ¸¬è©¦å¤±æ•—: {str(e)}",
            }

    async def _test_complete_transparency_workflow(self) -> Dict[str, Any]:
        """æ¸¬è©¦å®Œæ•´çš„æ±ºç­–é€æ˜åŒ–å·¥ä½œæµ"""
        start_time = time.time()

        try:
            from ..analytics import (
                AdvancedExplainabilityEngine,
                ConvergenceAnalyzer,
                StatisticalTestingEngine,
                AcademicDataExporter,
                VisualizationEngine,
            )

            # å‰µå»ºæ‰€æœ‰çµ„ä»¶
            explainability_engine = AdvancedExplainabilityEngine({})
            convergence_analyzer = ConvergenceAnalyzer({})
            statistical_engine = StatisticalTestingEngine({})
            data_exporter = AcademicDataExporter({"export_directory": self.temp_dir})
            visualization_engine = VisualizationEngine({"output_directory": self.temp_dir})

            workflow_steps = {}

            # æ­¥é©Ÿ 1: ç”Ÿæˆæ¨¡æ“¬çš„å®Œæ•´è¨“ç·´æœƒè©±æ•¸æ“š
            training_session = self._generate_complete_training_session()
            workflow_steps["training_session_generated"] = training_session is not None

            # æ­¥é©Ÿ 2: æ”¶æ–‚æ€§åˆ†æ
            try:
                convergence_analysis = convergence_analyzer.analyze_learning_curve(
                    training_session["rewards"],
                    metric_name="total_reward"
                )
                workflow_steps["convergence_analysis"] = convergence_analysis is not None
            except Exception as e:
                logger.warning(f"æ”¶æ–‚æ€§åˆ†æå¤±æ•—: {e}")
                workflow_steps["convergence_analysis"] = False

            # æ­¥é©Ÿ 3: çµ±è¨ˆåˆ†æ
            try:
                # æ¯”è¼ƒä¸åŒç®—æ³•
                algorithms = ["DQN", "PPO", "SAC"]
                algorithm_data = {}
                for alg in algorithms:
                    algorithm_data[alg] = np.random.normal(45 + algorithms.index(alg) * 2, 10, 50)

                statistical_analysis = statistical_engine.perform_anova(
                    algorithm_data,
                    test_name="Complete_Algorithm_Comparison"
                )
                workflow_steps["statistical_analysis"] = statistical_analysis is not None
            except Exception as e:
                logger.warning(f"çµ±è¨ˆåˆ†æå¤±æ•—: {e}")
                workflow_steps["statistical_analysis"] = False

            # æ­¥é©Ÿ 4: æ±ºç­–è§£é‡‹
            try:
                decision_data = {
                    "state": np.random.rand(10),
                    "action": 2,
                    "q_values": np.random.rand(5),
                    "algorithm": "DQN",
                    "episode": 100,
                    "step": 50,
                }

                decision_explanation = explainability_engine.explain_decision(decision_data)
                workflow_steps["decision_explanation"] = decision_explanation is not None
            except Exception as e:
                logger.warning(f"æ±ºç­–è§£é‡‹å¤±æ•—: {e}")
                workflow_steps["decision_explanation"] = False

            # æ­¥é©Ÿ 5: è¦–è¦ºåŒ–ç”Ÿæˆ
            try:
                viz_data = {
                    "episodes": list(range(100)),
                    "DQN": training_session["rewards"][:100],
                    "PPO": (np.array(training_session["rewards"][:100]) + np.random.normal(0, 2, 100)).tolist(),
                }

                visualization_result = visualization_engine.create_learning_curve_plot(
                    viz_data,
                    title="Complete Workflow Visualization",
                    filename="workflow_visualization"
                )
                workflow_steps["visualization"] = visualization_result.get("success", False)
            except Exception as e:
                logger.warning(f"è¦–è¦ºåŒ–ç”Ÿæˆå¤±æ•—: {e}")
                workflow_steps["visualization"] = False

            # æ­¥é©Ÿ 6: å­¸è¡“å ±å‘Šç”Ÿæˆ
            try:
                research_data = {
                    "experiment_metadata": {
                        "title": "Complete Transparency Workflow Test",
                        "date": datetime.now().isoformat(),
                    },
                    "results": algorithm_data,
                    "analysis": {
                        "convergence": convergence_analysis if workflow_steps.get("convergence_analysis") else {},
                        "statistical": statistical_analysis if workflow_steps.get("statistical_analysis") else {},
                        "explanations": decision_explanation if workflow_steps.get("decision_explanation") else {},
                    },
                }

                export_result = data_exporter.export_to_json(
                    research_data,
                    filename="complete_workflow_report.json"
                )
                workflow_steps["academic_export"] = export_result.get("success", False)
            except Exception as e:
                logger.warning(f"å­¸è¡“å ±å‘Šç”Ÿæˆå¤±æ•—: {e}")
                workflow_steps["academic_export"] = False

            # è¨ˆç®—å·¥ä½œæµæˆåŠŸç‡
            successful_steps = sum(workflow_steps.values())
            total_steps = len(workflow_steps)
            workflow_success = successful_steps >= total_steps * 0.7  # 70% æˆåŠŸç‡é–€æª»

            return {
                "success": workflow_success,
                "duration": time.time() - start_time,
                "details": {
                    "workflow_steps": workflow_steps,
                    "successful_steps": successful_steps,
                    "total_steps": total_steps,
                    "success_rate": successful_steps / total_steps * 100,
                    "components_tested": [
                        "explainability_engine",
                        "convergence_analyzer", 
                        "statistical_engine",
                        "data_exporter",
                        "visualization_engine",
                    ],
                },
            }

        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": f"å®Œæ•´é€æ˜åŒ–å·¥ä½œæµæ¸¬è©¦å¤±æ•—: {str(e)}",
            }

    def _generate_complete_training_session(self) -> Optional[Dict[str, Any]]:
        """ç”Ÿæˆå®Œæ•´çš„è¨“ç·´æœƒè©±æ•¸æ“š"""
        try:
            episodes = 200
            
            # æ¨¡æ“¬å­¸ç¿’æ›²ç·š (åŒ…å«åˆæœŸæ¢ç´¢ã€å­¸ç¿’ã€æ”¶æ–‚éšæ®µ)
            exploration_phase = np.random.normal(-50, 20, 50)  # åˆæœŸæ¢ç´¢
            learning_phase = np.linspace(-30, 40, 100) + np.random.normal(0, 10, 100)  # å­¸ç¿’éšæ®µ
            convergence_phase = np.random.normal(45, 5, 50)  # æ”¶æ–‚éšæ®µ
            
            rewards = np.concatenate([exploration_phase, learning_phase, convergence_phase])
            
            # å…¶ä»–æ€§èƒ½æŒ‡æ¨™
            success_rates = np.clip(np.linspace(0.1, 0.9, episodes) + np.random.normal(0, 0.05, episodes), 0, 1)
            handover_latencies = np.maximum(np.linspace(100, 20, episodes) + np.random.normal(0, 5, episodes), 5)
            
            return {
                "episodes": list(range(episodes)),
                "rewards": rewards.tolist(),
                "success_rates": success_rates.tolist(),
                "handover_latencies": handover_latencies.tolist(),
                "algorithm": "DQN",
                "scenario": "urban",
                "total_steps": episodes * 100,  # å‡è¨­æ¯å€‹episode 100æ­¥
            }
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆè¨“ç·´æœƒè©±æ•¸æ“šå¤±æ•—: {e}")
            return None

    def _cleanup_temp_files(self):
        """æ¸…ç†è‡¨æ™‚æ¸¬è©¦æ–‡ä»¶"""
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                logger.info(f"ğŸ§¹ å·²æ¸…ç†è‡¨æ™‚ç›®éŒ„: {self.temp_dir}")
        except Exception as e:
            logger.warning(f"æ¸…ç†è‡¨æ™‚æ–‡ä»¶å¤±æ•—: {e}")


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    test_runner = Phase3IntegrationTest()
    results = await test_runner.run_all_tests()

    print("\n" + "=" * 80)
    print("ğŸ“‹ Phase 3 æ±ºç­–é€æ˜åŒ–èˆ‡è¦–è¦ºåŒ–æ•´åˆæ¸¬è©¦å ±å‘Š")
    print("=" * 80)
    print(f"ç¸½æ¸¬è©¦æ•¸: {results['total_tests']}")
    print(f"é€šéæ¸¬è©¦: {results['passed_tests']}")
    print(f"å¤±æ•—æ¸¬è©¦: {results['failed_tests']}")
    print(f"æˆåŠŸç‡: {results['success_rate']:.1f}%")
    print(f"ç¸½è€—æ™‚: {results['duration_seconds']:.2f} ç§’")

    if results["failed_test_names"]:
        print(f"\nâŒ å¤±æ•—çš„æ¸¬è©¦:")
        for test_name in results["failed_test_names"]:
            print(f"  - {test_name}")

    print("\nğŸ“Š è©³ç´°æ¸¬è©¦çµæœ:")
    for result in results["test_results"]:
        status = "âœ…" if result["success"] else "âŒ"
        print(f"  {status} {result['test_name']} ({result['duration']:.2f}s)")
        if not result["success"] and result.get("error"):
            print(f"     éŒ¯èª¤: {result['error']}")

    print("\nğŸ” Phase 3 åŠŸèƒ½ç¸½çµ:")
    print("  âœ“ é«˜ç´šè§£é‡‹æ€§åˆ†æå¼•æ“ - Algorithm Explainability")
    print("  âœ“ æ”¶æ–‚æ€§åˆ†æå™¨ - å­¸ç¿’æ›²ç·šèˆ‡è¶¨å‹¢åˆ†æ")
    print("  âœ“ çµ±è¨ˆæ¸¬è©¦å¼•æ“ - å¤šç®—æ³•æ€§èƒ½å°æ¯”")
    print("  âœ“ å­¸è¡“æ•¸æ“šåŒ¯å‡ºå™¨ - ç¬¦åˆå­¸è¡“æ¨™æº–çš„å ±å‘Š")
    print("  âœ“ è¦–è¦ºåŒ–å¼•æ“ - æ±ºç­–éç¨‹è¦–è¦ºåŒ–")
    print("  âœ“ å®Œæ•´é€æ˜åŒ–å·¥ä½œæµ - ç«¯åˆ°ç«¯åˆ†ææµç¨‹")

    print("\n" + "=" * 80)

    if results["success_rate"] >= 80:
        print("ğŸ‰ Phase 3 æ±ºç­–é€æ˜åŒ–åŠŸèƒ½é©—è­‰é€šéï¼")
        print("ğŸš€ ç³»çµ±å·²æº–å‚™å¥½é€²å…¥ Phase 4: todo.md å®Œç¾æ•´åˆéšæ®µ")
        return 0
    else:
        print("âš ï¸  Phase 3 å­˜åœ¨ä¸€äº›å•é¡Œï¼Œéœ€è¦é€²ä¸€æ­¥æª¢æŸ¥")
        return 1


if __name__ == "__main__":
    import sys

    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nâš ï¸  æ¸¬è©¦è¢«ç”¨æˆ¶ä¸­æ–·")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ æ¸¬è©¦é‹è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        sys.exit(1)