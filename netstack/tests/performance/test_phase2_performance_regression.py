"""
Phase 2 æ€§èƒ½å›æ­¸æ¸¬è©¦ - è‡ªå‹•åŒ–æ€§èƒ½åŸºæº–æ¸¬è©¦

æ¸¬è©¦ Phase 1 å’Œ Phase 2 å¯¦æ–½å¾Œçš„ç³»çµ±æ€§èƒ½ï¼š
- API éŸ¿æ‡‰æ™‚é–“å›æ­¸æ¸¬è©¦
- å…§å­˜ä½¿ç”¨é‡å›æ­¸æ¸¬è©¦
- ä¸¦ç™¼è™•ç†èƒ½åŠ›å›æ­¸æ¸¬è©¦
- æ•¸æ“šåº«æŸ¥è©¢æ€§èƒ½å›æ­¸æ¸¬è©¦
- ç³»çµ±è³‡æºä½¿ç”¨å›æ­¸æ¸¬è©¦
"""

import pytest
import sys
import os
import time
import requests
import psutil
import threading
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import List, Dict, Tuple
import json

# æ·»åŠ é …ç›®è·¯å¾‘
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)


@dataclass
class PerformanceBenchmark:
    """æ€§èƒ½åŸºæº–æ•¸æ“šçµæ§‹"""
    test_name: str
    response_time_ms: float
    memory_usage_mb: float
    cpu_usage_percent: float
    success_rate: float
    throughput_rps: float
    timestamp: str


# Global benchmarks storage
performance_benchmarks = []

# Performance baselines
PERFORMANCE_BASELINES = {
    'api_health_check': {
        'max_response_time_ms': 100,
        'max_memory_usage_mb': 50,
        'max_cpu_usage_percent': 10,
        'min_success_rate': 0.99
    },
    'concurrent_requests': {
        'max_response_time_ms': 500,
        'max_memory_usage_mb': 100,
        'max_cpu_usage_percent': 30,
        'min_success_rate': 0.95,
        'min_throughput_rps': 20
    }
}


def measure_system_resources() -> Tuple[float, float]:
    """æ¸¬é‡ç³»çµ±è³‡æºä½¿ç”¨æƒ…æ³"""
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    cpu_percent = process.cpu_percent(interval=0.1)
    return memory_mb, cpu_percent


def record_benchmark(benchmark: PerformanceBenchmark):
    """è¨˜éŒ„æ€§èƒ½åŸºæº–"""
    performance_benchmarks.append(benchmark)


def validate_performance(test_name: str, metrics: Dict) -> bool:
    """é©—è­‰æ€§èƒ½æ˜¯å¦ç¬¦åˆåŸºæº–"""
    if test_name not in PERFORMANCE_BASELINES:
        return True
    
    baseline = PERFORMANCE_BASELINES[test_name]
    
    # æª¢æŸ¥éŸ¿æ‡‰æ™‚é–“
    if 'response_time_ms' in metrics and 'max_response_time_ms' in baseline:
        if metrics['response_time_ms'] > baseline['max_response_time_ms']:
            print(f"âŒ {test_name} éŸ¿æ‡‰æ™‚é–“å›æ­¸: {metrics['response_time_ms']:.1f}ms > {baseline['max_response_time_ms']}ms")
            return False
    
    # æª¢æŸ¥æˆåŠŸç‡
    if 'success_rate' in metrics and 'min_success_rate' in baseline:
        if metrics['success_rate'] < baseline['min_success_rate']:
            print(f"âŒ {test_name} æˆåŠŸç‡å›æ­¸: {metrics['success_rate']:.2f} < {baseline['min_success_rate']}")
            return False
    
    return True


class TestAPIPerformanceRegression:
    """API æ€§èƒ½å›æ­¸æ¸¬è©¦"""
    
    def setup_method(self):
        """æ¸¬è©¦æ–¹æ³•è¨­ç½®"""
        self.api_base_url = "http://localhost:8080"
        self.simworld_url = "http://localhost:8888"
        self.session = requests.Session()
    
    def test_health_check_response_time_regression(self):
        """æ¸¬è©¦å¥åº·æª¢æŸ¥éŸ¿æ‡‰æ™‚é–“å›æ­¸"""
        response_times = []
        success_count = 0
        
        # åŸ·è¡Œå¤šæ¬¡æ¸¬è©¦ç²å¾—ç©©å®šæ•¸æ“š
        for _ in range(10):
            start_time = time.time()
            memory_before, cpu_before = measure_system_resources()
            
            try:
                response = self.session.get(f"{self.api_base_url}/health", timeout=2)
                end_time = time.time()
                
                if response.status_code == 200:
                    success_count += 1
                    response_time_ms = (end_time - start_time) * 1000
                    response_times.append(response_time_ms)
                    
            except requests.exceptions.RequestException:
                pass
            
            time.sleep(0.1)  # é¿å…éæ–¼é »ç¹çš„è«‹æ±‚
        
        if not response_times:
            pytest.skip("API å¥åº·æª¢æŸ¥ä¸å¯ç”¨")
        
        memory_after, cpu_after = measure_system_resources()
        
        # è¨ˆç®—æ€§èƒ½æŒ‡æ¨™
        avg_response_time = statistics.mean(response_times)
        max_response_time = max(response_times)
        success_rate = success_count / 10
        memory_usage = abs(memory_after - memory_before)
        cpu_usage = max(cpu_after, cpu_before)
        
        metrics = {
            'response_time_ms': avg_response_time,
            'memory_usage_mb': memory_usage,
            'cpu_usage_percent': cpu_usage,
            'success_rate': success_rate
        }
        
        # è¨˜éŒ„åŸºæº–
        benchmark = PerformanceBenchmark(
            test_name="api_health_check",
            response_time_ms=avg_response_time,
            memory_usage_mb=memory_usage,
            cpu_usage_percent=cpu_usage,
            success_rate=success_rate,
            throughput_rps=success_count / 1.0,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        record_benchmark(benchmark)
        
        # é©—è­‰æ€§èƒ½åŸºæº–
        performance_ok = validate_performance("api_health_check", metrics)
        
        print(f"âœ… API å¥åº·æª¢æŸ¥æ€§èƒ½: å¹³å‡ {avg_response_time:.1f}ms, æœ€å¤§ {max_response_time:.1f}ms, æˆåŠŸç‡ {success_rate:.1%}")
        
        assert success_rate >= 0.9, f"å¥åº·æª¢æŸ¥æˆåŠŸç‡ {success_rate:.1%} éä½"
        assert avg_response_time < 200, f"å¹³å‡éŸ¿æ‡‰æ™‚é–“ {avg_response_time:.1f}ms éé•·"
    
    def test_concurrent_requests_performance_regression(self):
        """æ¸¬è©¦ä¸¦ç™¼è«‹æ±‚æ€§èƒ½å›æ­¸"""
        def make_request():
            try:
                start_time = time.time()
                response = self.session.get(f"{self.api_base_url}/health", timeout=5)
                end_time = time.time()
                
                return {
                    'success': response.status_code == 200,
                    'response_time': (end_time - start_time) * 1000,
                    'status_code': response.status_code
                }
            except:
                return {'success': False, 'response_time': 5000, 'status_code': 0}
        
        # ä¸¦ç™¼æ¸¬è©¦é…ç½®
        concurrent_users = 10
        requests_per_user = 5
        
        memory_before, cpu_before = measure_system_resources()
        start_time = time.time()
        
        # åŸ·è¡Œä¸¦ç™¼è«‹æ±‚
        results = []
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = []
            for _ in range(concurrent_users * requests_per_user):
                futures.append(executor.submit(make_request))
            
            for future in as_completed(futures):
                results.append(future.result())
        
        end_time = time.time()
        memory_after, cpu_after = measure_system_resources()
        
        # åˆ†æçµæœ
        successful_requests = [r for r in results if r['success']]
        response_times = [r['response_time'] for r in successful_requests]
        
        if not response_times:
            pytest.skip("ä¸¦ç™¼è«‹æ±‚æ¸¬è©¦ç„¡æœ‰æ•ˆéŸ¿æ‡‰")
        
        total_time = end_time - start_time
        success_rate = len(successful_requests) / len(results)
        avg_response_time = statistics.mean(response_times)
        max_response_time = max(response_times)
        throughput = len(successful_requests) / total_time
        memory_usage = abs(memory_after - memory_before)
        cpu_usage = max(cpu_after - cpu_before, 0)
        
        metrics = {
            'response_time_ms': avg_response_time,
            'memory_usage_mb': memory_usage,
            'cpu_usage_percent': cpu_usage,
            'success_rate': success_rate,
            'throughput_rps': throughput
        }
        
        # è¨˜éŒ„åŸºæº–
        benchmark = PerformanceBenchmark(
            test_name="concurrent_requests",
            response_time_ms=avg_response_time,
            memory_usage_mb=memory_usage,
            cpu_usage_percent=cpu_usage,
            success_rate=success_rate,
            throughput_rps=throughput,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        record_benchmark(benchmark)
        
        print(f"âœ… ä¸¦ç™¼è«‹æ±‚æ€§èƒ½: {concurrent_users}ç”¨æˆ¶x{requests_per_user}è«‹æ±‚")
        print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_response_time:.1f}ms")
        print(f"   æœ€å¤§éŸ¿æ‡‰æ™‚é–“: {max_response_time:.1f}ms") 
        print(f"   æˆåŠŸç‡: {success_rate:.1%}")
        print(f"   ååé‡: {throughput:.1f} RPS")
        
        assert success_rate >= 0.8, f"ä¸¦ç™¼è«‹æ±‚æˆåŠŸç‡ {success_rate:.1%} éä½"
        assert avg_response_time < 1000, f"ä¸¦ç™¼å¹³å‡éŸ¿æ‡‰æ™‚é–“ {avg_response_time:.1f}ms éé•·"


class TestSystemResourceRegression:
    """ç³»çµ±è³‡æºä½¿ç”¨å›æ­¸æ¸¬è©¦"""
    
    def test_memory_usage_regression(self):
        """æ¸¬è©¦å…§å­˜ä½¿ç”¨å›æ­¸"""
        # ç²å–ç³»çµ±é€²ç¨‹ä¿¡æ¯
        netstack_processes = []
        simworld_processes = []
        
        for proc in psutil.process_iter(['pid', 'name', 'memory_info', 'cpu_percent']):
            try:
                proc_info = proc.info
                if 'python' in proc_info['name'].lower() or 'uvicorn' in proc_info['name'].lower():
                    # é€šéç«¯å£é€£æ¥åˆ¤æ–·æ˜¯å“ªå€‹æœå‹™
                    try:
                        connections = proc.connections()
                        for conn in connections:
                            if conn.laddr.port == 8080:
                                netstack_processes.append(proc_info)
                            elif conn.laddr.port == 8888:
                                simworld_processes.append(proc_info)
                    except:
                        pass
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
        
        # è¨ˆç®—å…§å­˜ä½¿ç”¨
        total_memory_mb = 0
        process_count = 0
        
        for proc_list, service_name in [(netstack_processes, "NetStack"), (simworld_processes, "SimWorld")]:
            service_memory = 0
            for proc in proc_list:
                if proc['memory_info']:
                    memory_mb = proc['memory_info'].rss / 1024 / 1024
                    service_memory += memory_mb
                    process_count += 1
            
            if service_memory > 0:
                print(f"ğŸ“Š {service_name} å…§å­˜ä½¿ç”¨: {service_memory:.1f}MB")
                total_memory_mb += service_memory
        
        print(f"ğŸ“Š ç¸½å…§å­˜ä½¿ç”¨: {total_memory_mb:.1f}MB ({process_count} å€‹é€²ç¨‹)")
        
        # è¨˜éŒ„åŸºæº–
        benchmark = PerformanceBenchmark(
            test_name="memory_usage",
            response_time_ms=0,
            memory_usage_mb=total_memory_mb,
            cpu_usage_percent=0,
            success_rate=1.0,
            throughput_rps=0,
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        record_benchmark(benchmark)
        
        assert total_memory_mb < 1000, f"ç³»çµ±å…§å­˜ä½¿ç”¨ {total_memory_mb:.1f}MB éé«˜"
    
    def test_response_time_stability_regression(self):
        """æ¸¬è©¦éŸ¿æ‡‰æ™‚é–“ç©©å®šæ€§å›æ­¸"""
        response_times = []
        
        # é•·æ™‚é–“ç©©å®šæ€§æ¸¬è©¦
        for i in range(30):  # 30æ¬¡è«‹æ±‚ï¼Œæ¸¬è©¦ç©©å®šæ€§
            try:
                start_time = time.time()
                response = requests.get("http://localhost:8080/health", timeout=3)
                end_time = time.time()
                
                if response.status_code == 200:
                    response_time_ms = (end_time - start_time) * 1000
                    response_times.append(response_time_ms)
                
            except requests.exceptions.RequestException:
                response_times.append(3000)  # è¶…æ™‚è¨˜ç‚º 3000ms
            
            time.sleep(0.2)  # æ¯200msä¸€æ¬¡è«‹æ±‚
        
        if not response_times:
            pytest.skip("éŸ¿æ‡‰æ™‚é–“ç©©å®šæ€§æ¸¬è©¦ç„¡æœ‰æ•ˆæ•¸æ“š")
        
        # è¨ˆç®—ç©©å®šæ€§æŒ‡æ¨™
        avg_time = statistics.mean(response_times)
        median_time = statistics.median(response_times)
        std_dev = statistics.stdev(response_times) if len(response_times) > 1 else 0
        min_time = min(response_times)
        max_time = max(response_times)
        
        # è¨ˆç®—è®Šç•°ä¿‚æ•¸ (æ¨™æº–å·®/å¹³å‡å€¼)
        coefficient_of_variation = (std_dev / avg_time) if avg_time > 0 else 0
        
        print(f"ğŸ“Š éŸ¿æ‡‰æ™‚é–“ç©©å®šæ€§åˆ†æ:")
        print(f"   å¹³å‡: {avg_time:.1f}ms")
        print(f"   ä¸­ä½æ•¸: {median_time:.1f}ms")
        print(f"   æ¨™æº–å·®: {std_dev:.1f}ms")
        print(f"   ç¯„åœ: {min_time:.1f}ms - {max_time:.1f}ms")
        print(f"   è®Šç•°ä¿‚æ•¸: {coefficient_of_variation:.3f}")
        
        # è¨˜éŒ„åŸºæº–
        benchmark = PerformanceBenchmark(
            test_name="response_time_stability",
            response_time_ms=avg_time,
            memory_usage_mb=0,
            cpu_usage_percent=0,
            success_rate=len([t for t in response_times if t < 1000]) / len(response_times),
            throughput_rps=len(response_times) / 6.0,  # 30æ¬¡è«‹æ±‚åœ¨6ç§’å…§
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S")
        )
        record_benchmark(benchmark)
        
        # ç©©å®šæ€§é©—è­‰
        assert avg_time < 200, f"å¹³å‡éŸ¿æ‡‰æ™‚é–“ {avg_time:.1f}ms éé•·"
        assert max_time < 1000, f"æœ€å¤§éŸ¿æ‡‰æ™‚é–“ {max_time:.1f}ms éé•·"
        assert coefficient_of_variation < 0.5, f"éŸ¿æ‡‰æ™‚é–“è®Šç•°éå¤§ {coefficient_of_variation:.3f}"


class TestPerformanceReporting:
    """æ€§èƒ½æ¸¬è©¦å ±å‘Š"""
    
    def test_generate_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æ¸¬è©¦å ±å‘Š"""
        # ç”Ÿæˆå ±å‘Š
        report = {
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
            'total_tests': len(performance_benchmarks),
            'benchmarks': [
                {
                    'test_name': b.test_name,
                    'response_time_ms': b.response_time_ms,
                    'memory_usage_mb': b.memory_usage_mb,
                    'cpu_usage_percent': b.cpu_usage_percent,
                    'success_rate': b.success_rate,
                    'throughput_rps': b.throughput_rps,
                    'timestamp': b.timestamp
                } for b in performance_benchmarks
            ]
        }
        
        # ä¿å­˜å ±å‘Šåˆ°æ–‡ä»¶
        report_file = "/home/sat/ntn-stack/netstack/tests/performance/performance_report.json"
        os.makedirs(os.path.dirname(report_file), exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š æ€§èƒ½æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ: {report_file}")
        print(f"ğŸ“Š ç¸½è¨ˆ {len(performance_benchmarks)} é …æ€§èƒ½åŸºæº–æ¸¬è©¦")
        
        # å ±å‘Šæ‘˜è¦
        if performance_benchmarks:
            response_times = [b.response_time_ms for b in performance_benchmarks if b.response_time_ms > 0]
            if response_times:
                avg_response_time = statistics.mean(response_times)
                print(f"ğŸ“Š å¹³å‡éŸ¿æ‡‰æ™‚é–“: {avg_response_time:.1f}ms")
            
            avg_success_rate = statistics.mean([b.success_rate for b in performance_benchmarks])
            print(f"ğŸ“Š å¹³å‡æˆåŠŸç‡: {avg_success_rate:.1%}")
        
        assert len(performance_benchmarks) > 0, "æ‡‰è©²ç”Ÿæˆè‡³å°‘ä¸€é …æ€§èƒ½åŸºæº–"


if __name__ == "__main__":
    # é‹è¡Œæ€§èƒ½å›æ­¸æ¸¬è©¦
    pytest.main([__file__, "-v", "--tb=short", "-s"])