"""
ğŸ§ª çœŸå¯¦æŒ‡æ¨™è¨ˆç®—ç³»çµ±æ¸¬è©¦
é©—è­‰æ‰€æœ‰æ”¹é€²çš„åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import asyncio
import logging
import json
from datetime import datetime
from typing import Dict, Any, List

# å°å…¥æ¸¬è©¦ç›®æ¨™
from ..analytics.real_metrics_calculator import (
    RealMetricsCalculator, EpisodeData, get_real_metrics_calculator
)
from ..interfaces.metrics_interface import (
    MetricsStandardizer, StandardizedMetrics, TrendType
)
from ..algorithms.real_rl_algorithms import (
    RealDQNAlgorithm, RealPPOAlgorithm, RealSACAlgorithm
)
from ..core.algorithm_integrator import (
    DQNAlgorithm, PPOAlgorithm, SACAlgorithm, RLAlgorithmIntegrator
)

logger = logging.getLogger(__name__)


class RealMetricsSystemTester:
    """çœŸå¯¦æŒ‡æ¨™è¨ˆç®—ç³»çµ±æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.test_results = {}
        self.logger = logging.getLogger(__name__)
    
    async def test_real_metrics_calculator(self) -> Dict[str, Any]:
        """æ¸¬è©¦çœŸå¯¦æŒ‡æ¨™è¨ˆç®—å™¨"""
        self.logger.info("ğŸ§® æ¸¬è©¦çœŸå¯¦æŒ‡æ¨™è¨ˆç®—å™¨...")
        
        try:
            # ç²å–è¨ˆç®—å™¨å¯¦ä¾‹
            calculator = await get_real_metrics_calculator()
            
            # å‰µå»ºæ¸¬è©¦æ•¸æ“š
            test_episodes = self._create_test_episodes()
            
            # è¨ˆç®—æŒ‡æ¨™
            metrics = await calculator.calculate_training_metrics(
                session_id="test_session_001",
                algorithm="DQN",
                episodes_data=test_episodes
            )
            
            # é©—è­‰çµæœ
            results = {
                "success": True,
                "metrics_calculated": True,
                "success_rate": metrics.success_rate,
                "stability": metrics.stability,
                "learning_efficiency": metrics.learning_efficiency,
                "confidence_score": metrics.confidence_score,
                "performance_trend": metrics.performance_trend,
                "convergence_episode": metrics.convergence_episode,
                "data_points": len(test_episodes)
            }
            
            # é©—è­‰æŒ‡æ¨™ç¯„åœ
            assert 0.0 <= metrics.success_rate <= 1.0, "Success rate è¶…å‡ºç¯„åœ"
            assert 0.0 <= metrics.stability <= 1.0, "Stability è¶…å‡ºç¯„åœ"
            assert 0.0 <= metrics.learning_efficiency <= 1.0, "Learning efficiency è¶…å‡ºç¯„åœ"
            assert 0.0 <= metrics.confidence_score <= 1.0, "Confidence score è¶…å‡ºç¯„åœ"
            
            self.logger.info(f"âœ… çœŸå¯¦æŒ‡æ¨™è¨ˆç®—å™¨æ¸¬è©¦é€šé: {results}")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ çœŸå¯¦æŒ‡æ¨™è¨ˆç®—å™¨æ¸¬è©¦å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    async def test_metrics_standardizer(self) -> Dict[str, Any]:
        """æ¸¬è©¦æŒ‡æ¨™æ¨™æº–åŒ–å™¨"""
        self.logger.info("ğŸ“ æ¸¬è©¦æŒ‡æ¨™æ¨™æº–åŒ–å™¨...")
        
        try:
            standardizer = MetricsStandardizer()
            
            # å‰µå»ºæ¸¬è©¦æŒ‡æ¨™
            test_metrics = StandardizedMetrics(
                success_rate=0.85,
                stability=0.92,
                learning_efficiency=0.78,
                confidence_score=0.65,
                average_reward=150.5,
                convergence_episode=45,
                performance_trend=TrendType.IMPROVING,
                calculation_timestamp=datetime.now().isoformat(),
                data_points_count=100,
                calculation_method="test"
            )
            
            # æ¸¬è©¦å‰ç«¯æ ¼å¼åŒ–
            frontend_format = standardizer.format_for_frontend(test_metrics)
            
            # æ¸¬è©¦å¾Œç«¯æ ¼å¼åŒ–
            backend_format = standardizer.format_for_backend(test_metrics)
            
            # é©—è­‰æ ¼å¼åŒ–çµæœ
            assert frontend_format["success_rate"] == 85.0, "å‰ç«¯æˆåŠŸç‡æ ¼å¼åŒ–éŒ¯èª¤"
            assert backend_format["success_rate"] == 0.85, "å¾Œç«¯æˆåŠŸç‡æ ¼å¼åŒ–éŒ¯èª¤"
            assert "trend_emoji" in frontend_format, "ç¼ºå°‘è¶¨å‹¢è¡¨æƒ…ç¬¦è™Ÿ"
            assert "confidence_emoji" in frontend_format, "ç¼ºå°‘å¯ä¿¡åº¦è¡¨æƒ…ç¬¦è™Ÿ"
            
            results = {
                "success": True,
                "frontend_format_valid": True,
                "backend_format_valid": True,
                "validation_passed": standardizer._validate_all_metrics(test_metrics),
                "confidence_level": standardizer.get_confidence_level(test_metrics.confidence_score),
                "quality_score": standardizer._calculate_quality_score(test_metrics)
            }
            
            self.logger.info(f"âœ… æŒ‡æ¨™æ¨™æº–åŒ–å™¨æ¸¬è©¦é€šé: {results}")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ æŒ‡æ¨™æ¨™æº–åŒ–å™¨æ¸¬è©¦å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    async def test_real_algorithms(self) -> Dict[str, Any]:
        """æ¸¬è©¦çœŸå¯¦ RL ç®—æ³•"""
        self.logger.info("ğŸ§  æ¸¬è©¦çœŸå¯¦ RL ç®—æ³•...")
        
        results = {}
        
        try:
            # æ¸¬è©¦ DQN ç®—æ³•
            dqn = RealDQNAlgorithm(state_size=10, action_size=4)
            test_state = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
            
            # æ¸¬è©¦é æ¸¬
            action = await dqn.predict(test_state)
            assert 0 <= action <= 3, "DQN å‹•ä½œè¶…å‡ºç¯„åœ"
            
            # æ¸¬è©¦å­¸ç¿’
            await dqn.learn(test_state, action, 10.0, test_state, False)
            
            # ç²å–æŒ‡æ¨™
            dqn_metrics = dqn.get_metrics()
            
            results["dqn"] = {
                "prediction_valid": True,
                "learning_executed": True,
                "metrics_available": bool(dqn_metrics),
                "total_steps": dqn_metrics.get("total_steps", 0),
                "epsilon": dqn_metrics.get("epsilon", 0)
            }
            
            # æ¸¬è©¦ PPO ç®—æ³•
            ppo = RealPPOAlgorithm(state_size=10, action_size=4)
            action = await ppo.predict(test_state)
            await ppo.learn(test_state, action, 15.0, test_state, False)
            ppo_metrics = ppo.get_metrics()
            
            results["ppo"] = {
                "prediction_valid": True,
                "learning_executed": True,
                "metrics_available": bool(ppo_metrics),
                "total_steps": ppo_metrics.get("total_steps", 0)
            }
            
            # æ¸¬è©¦ SAC ç®—æ³•
            sac = RealSACAlgorithm(state_size=10, action_size=4)
            action = await sac.predict(test_state)
            await sac.learn(test_state, action, 12.0, test_state, False)
            sac_metrics = sac.get_metrics()
            
            results["sac"] = {
                "prediction_valid": True,
                "learning_executed": True,
                "metrics_available": bool(sac_metrics),
                "total_steps": sac_metrics.get("total_steps", 0)
            }
            
            results["success"] = True
            self.logger.info(f"âœ… çœŸå¯¦ RL ç®—æ³•æ¸¬è©¦é€šé: {results}")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ çœŸå¯¦ RL ç®—æ³•æ¸¬è©¦å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    async def test_algorithm_integrator(self) -> Dict[str, Any]:
        """æ¸¬è©¦ç®—æ³•æ•´åˆå™¨"""
        self.logger.info("ğŸ”§ æ¸¬è©¦ç®—æ³•æ•´åˆå™¨...")
        
        try:
            integrator = RLAlgorithmIntegrator()
            
            # æ¸¬è©¦ç®—æ³•ç²å–
            dqn_algo = await integrator.get_algorithm("dqn")
            ppo_algo = await integrator.get_algorithm("ppo")
            sac_algo = await integrator.get_algorithm("sac")
            
            assert dqn_algo is not None, "DQN ç®—æ³•ç²å–å¤±æ•—"
            assert ppo_algo is not None, "PPO ç®—æ³•ç²å–å¤±æ•—"
            assert sac_algo is not None, "SAC ç®—æ³•ç²å–å¤±æ•—"
            
            # æ¸¬è©¦ç®—æ³•åˆ‡æ›
            switch_success = integrator.set_active_algorithm("dqn")
            assert switch_success, "ç®—æ³•åˆ‡æ›å¤±æ•—"
            
            active_algo = integrator.get_active_algorithm()
            assert active_algo is not None, "æ´»èºç®—æ³•ç²å–å¤±æ•—"
            
            # æ¸¬è©¦æŒ‡æ¨™ç²å–
            all_metrics = integrator.get_all_metrics()
            assert len(all_metrics) == 3, "æŒ‡æ¨™æ•¸é‡ä¸æ­£ç¢º"
            
            # æ¸¬è©¦ç‹€æ…‹ç²å–
            status = integrator.get_integration_status()
            
            results = {
                "success": True,
                "algorithms_available": integrator.get_available_algorithms(),
                "active_algorithm": integrator.active_algorithm,
                "total_algorithms": status["total_algorithms"],
                "status": status["status"]
            }
            
            self.logger.info(f"âœ… ç®—æ³•æ•´åˆå™¨æ¸¬è©¦é€šé: {results}")
            return results
            
        except Exception as e:
            self.logger.error(f"âŒ ç®—æ³•æ•´åˆå™¨æ¸¬è©¦å¤±æ•—: {e}")
            return {"success": False, "error": str(e)}
    
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """é‹è¡Œç¶œåˆæ¸¬è©¦"""
        self.logger.info("ğŸš€ é–‹å§‹ç¶œåˆæ¸¬è©¦...")
        
        test_results = {
            "test_timestamp": datetime.now().isoformat(),
            "tests": {}
        }
        
        # é‹è¡Œå„é …æ¸¬è©¦
        test_results["tests"]["real_metrics_calculator"] = await self.test_real_metrics_calculator()
        test_results["tests"]["metrics_standardizer"] = await self.test_metrics_standardizer()
        test_results["tests"]["real_algorithms"] = await self.test_real_algorithms()
        test_results["tests"]["algorithm_integrator"] = await self.test_algorithm_integrator()
        
        # è¨ˆç®—ç¸½é«”æˆåŠŸç‡
        successful_tests = sum(1 for test in test_results["tests"].values() if test.get("success", False))
        total_tests = len(test_results["tests"])
        success_rate = successful_tests / total_tests
        
        test_results["summary"] = {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "success_rate": success_rate,
            "overall_success": success_rate >= 0.75  # 75% é€šéç‡ç‚ºæˆåŠŸ
        }
        
        if test_results["summary"]["overall_success"]:
            self.logger.info(f"ğŸ‰ ç¶œåˆæ¸¬è©¦é€šé! æˆåŠŸç‡: {success_rate:.2%}")
        else:
            self.logger.warning(f"âš ï¸ ç¶œåˆæ¸¬è©¦éƒ¨åˆ†å¤±æ•—! æˆåŠŸç‡: {success_rate:.2%}")
        
        return test_results
    
    def _create_test_episodes(self) -> List[EpisodeData]:
        """å‰µå»ºæ¸¬è©¦å›åˆæ•¸æ“š"""
        episodes = []
        
        for i in range(50):
            # æ¨¡æ“¬å­¸ç¿’æ›²ç·šï¼šåˆæœŸè¼ƒå·®ï¼Œé€æ¼¸æ”¹å–„
            progress = i / 49.0
            base_reward = 50 + progress * 100  # å¾ 50 æå‡åˆ° 150
            
            # æ·»åŠ ä¸€äº›éš¨æ©Ÿè®ŠåŒ–
            import random
            noise = random.uniform(-10, 10)
            episode_reward = base_reward + noise
            
            # åˆ¤æ–·æˆåŠŸï¼ˆåŸºæ–¼çå‹µé–¾å€¼ï¼‰
            success = episode_reward > 100.0
            
            episode = EpisodeData(
                episode_number=i + 1,
                total_reward=episode_reward,
                steps=random.randint(50, 200),
                success=success,
                handover_count=random.randint(0, 5),
                average_latency=random.uniform(10, 100),
                timestamp=datetime.now()
            )
            episodes.append(episode)
        
        return episodes


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    tester = RealMetricsSystemTester()
    results = await tester.run_comprehensive_test()
    
    # è¼¸å‡ºæ¸¬è©¦çµæœ
    print("\n" + "="*60)
    print("ğŸ§ª çœŸå¯¦æŒ‡æ¨™è¨ˆç®—ç³»çµ±æ¸¬è©¦çµæœ")
    print("="*60)
    print(json.dumps(results, indent=2, ensure_ascii=False))
    print("="*60)
    
    return results["summary"]["overall_success"]


if __name__ == "__main__":
    success = asyncio.run(main())
