#!/usr/bin/env python3
"""
DataIntegrationProcessor - éšæ®µäº”æ•¸æ“šæ•´åˆè™•ç†å™¨
ç¬¦åˆ@docs/stages/stage5-integration.mdè¦æ±‚çš„æ··åˆå­˜å„²æ¶æ§‹å¯¦ç¾
"""

import os
import json
import time
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional

# å°å…¥æ··åˆå­˜å„²ç®¡ç†å™¨
from services.storage.hybrid_storage_manager import HybridStorageManager

logger = logging.getLogger(__name__)


class DataIntegrationProcessor:
    """
    æ•¸æ“šæ•´åˆè™•ç†å™¨ - ç¬¦åˆ@docsè¦æ±‚
    å¯¦ç¾éšæ®µä¸‰ä¿¡è™Ÿåˆ†æ + éšæ®µå››æ™‚é–“åºåˆ—çš„æ™ºèƒ½èåˆ
    ä½¿ç”¨PostgreSQL + Docker Volumeæ··åˆå­˜å„²æ¶æ§‹
    """
    
    def __init__(self, 
                 input_dir: str = "/app/data",
                 output_dir: str = "/app/data"):
        """
        åˆå§‹åŒ–æ•¸æ“šæ•´åˆè™•ç†å™¨
        
        Args:
            input_dir: è¼¸å…¥æ•¸æ“šç›®éŒ„
            output_dir: è¼¸å‡ºæ•¸æ“šç›®éŒ„
        """
        logger.info("ğŸ“ åˆå§‹åŒ–æ•¸æ“šæ•´åˆè™•ç†å™¨...")
        
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        
        # å‰µå»ºéšæ®µäº”å°ˆç”¨è¼¸å‡ºç›®éŒ„
        self.data_integration_dir = self.output_dir / "data_integration_outputs"
        self.data_integration_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ··åˆå­˜å„²ç®¡ç†å™¨ - ç¬¦åˆ@docsè¦æ±‚
        self.storage_manager = HybridStorageManager(
            volume_path=str(self.output_dir)
        )
        
        # è™•ç†çµ±è¨ˆ
        self.processing_stats = {
            "stage3_satellites_loaded": 0,
            "stage4_animations_loaded": 0,
            "integration_success_count": 0,
            "postgresql_records": 0,
            "volume_files": 0
        }
        
        logger.info("âœ… æ•¸æ“šæ•´åˆè™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   è¼¸å…¥ç›®éŒ„: {self.input_dir}")
        logger.info(f"   è¼¸å‡ºç›®éŒ„: {self.data_integration_dir}")
        logger.info("   ğŸ—„ï¸ æ··åˆå­˜å„²æ¶æ§‹: å·²å°±ç·’")
    
    def process_data_integration(self) -> Dict[str, Any]:
        """
        åŸ·è¡Œæ•¸æ“šæ•´åˆè™•ç† - ä¸»è¦å…¥å£é»
        
        Returns:
            Dict: æ•´åˆçµæœå’Œçµ±è¨ˆä¿¡æ¯
        """
        logger.info("ğŸš€ é–‹å§‹æ•¸æ“šæ•´åˆè™•ç†...")
        start_time = time.time()
        
        try:
            # 1. è¼‰å…¥éšæ®µä¸‰ä¿¡è™Ÿåˆ†æçµæœ
            stage3_data = self._load_stage3_signal_analysis()
            
            # 2. è¼‰å…¥éšæ®µå››æ™‚é–“åºåˆ—æ•¸æ“š
            stage4_data = self._load_stage4_timeseries()
            
            # 3. åŸ·è¡Œé‹è¡Œæ™‚æ¶æ§‹æª¢æŸ¥ - ç¬¦åˆ@docsè¦æ±‚
            self._validate_integration_requirements(stage3_data, stage4_data)
            
            # 4. åŸ·è¡Œæ•¸æ“šèåˆ
            integrated_data = self._integrate_cross_stage_data(stage3_data, stage4_data)
            
            # 5. ç”Ÿæˆåˆ†å±¤ä»°è§’æ•¸æ“š (5Â°/10Â°/15Â°)
            layered_data = self._generate_layered_elevation_data(integrated_data)
            
            # 6. å­˜å„²åˆ°æ··åˆæ¶æ§‹
            self._store_to_hybrid_architecture(integrated_data, layered_data)
            
            # 7. ç”Ÿæˆæœ€çµ‚è¼¸å‡º
            output_data = self._generate_integration_output(integrated_data, layered_data)
            
            processing_time = time.time() - start_time
            
            logger.info(f"âœ… æ•¸æ“šæ•´åˆè™•ç†å®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’")
            return output_data
            
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šæ•´åˆè™•ç†å¤±æ•—: {e}")
            raise
    
    def _load_stage3_signal_analysis(self) -> Dict[str, Any]:
        """è¼‰å…¥éšæ®µä¸‰ä¿¡è™Ÿåˆ†æçµæœ"""
        logger.info("ğŸ“Š è¼‰å…¥éšæ®µä¸‰ä¿¡è™Ÿåˆ†ææ•¸æ“š...")
        
        stage3_file = self.input_dir / "signal_analysis_outputs" / "signal_event_analysis_output.json"
        
        if not stage3_file.exists():
            raise FileNotFoundError(f"éšæ®µä¸‰æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {stage3_file}")
        
        with open(stage3_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # çµ±è¨ˆè¼‰å…¥çš„è¡›æ˜Ÿæ•¸é‡
        constellations = data.get('constellations', {})
        total_satellites = 0
        
        for const_name, const_data in constellations.items():
            satellites = const_data.get('satellites', [])
            total_satellites += len(satellites)
            logger.info(f"   è¼‰å…¥ {const_name}: {len(satellites)} é¡†è¡›æ˜Ÿ")
        
        self.processing_stats["stage3_satellites_loaded"] = total_satellites
        logger.info(f"âœ… éšæ®µä¸‰æ•¸æ“šè¼‰å…¥å®Œæˆï¼Œç¸½è¨ˆ: {total_satellites} é¡†è¡›æ˜Ÿ")
        
        return data
    
    def _load_stage4_timeseries(self) -> Dict[str, Any]:
        """è¼‰å…¥éšæ®µå››æ™‚é–“åºåˆ—æ•¸æ“š"""
        logger.info("â° è¼‰å…¥éšæ®µå››æ™‚é–“åºåˆ—æ•¸æ“š...")
        
        stage4_file = self.input_dir / "timeseries_preprocessing_outputs" / "oneweb_enhanced.json"
        
        if not stage4_file.exists():
            logger.warning(f"âš ï¸ éšæ®µå››æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {stage4_file}")
            return {}
        
        with open(stage4_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # çµ±è¨ˆå‹•ç•«æ•¸æ“š
        animation_count = 0
        if 'enhanced_format' in data:
            tracks = data['enhanced_format'].get('animation_data', {}).get('satellite_tracks', {})
            animation_count = len(tracks)
        
        self.processing_stats["stage4_animations_loaded"] = animation_count
        logger.info(f"âœ… éšæ®µå››æ•¸æ“šè¼‰å…¥å®Œæˆï¼Œå‹•ç•«è»Œè·¡: {animation_count} æ¢")
        
        return data
    
    def _validate_integration_requirements(self, stage3_data: Dict[str, Any], stage4_data: Dict[str, Any]):
        """åŸ·è¡Œé‹è¡Œæ™‚æ¶æ§‹æª¢æŸ¥ - ç¬¦åˆ@docsè¦æ±‚"""
        logger.info("ğŸ” åŸ·è¡Œé‹è¡Œæ™‚æ¶æ§‹å®Œæ•´æ€§æª¢æŸ¥...")
        
        # æª¢æŸ¥è™•ç†å™¨é¡å‹ - ç¬¦åˆ@docsè¦æ±‚
        assert isinstance(self, DataIntegrationProcessor), f"éŒ¯èª¤æ•¸æ“šæ•´åˆè™•ç†å™¨: {type(self)}"
        assert isinstance(self.storage_manager, HybridStorageManager), f"éŒ¯èª¤å­˜å„²ç®¡ç†å™¨: {type(self.storage_manager)}"
        
        # æª¢æŸ¥éšæ®µä¸‰æ•¸æ“šå®Œæ•´æ€§
        assert 'constellations' in stage3_data, "ç¼ºå°‘éšæ®µä¸‰ä¿¡è™Ÿåˆ†æçµæœ"
        
        constellations = stage3_data['constellations']
        starlink_count = len(constellations.get('starlink', {}).get('satellites', []))
        oneweb_count = len(constellations.get('oneweb', {}).get('satellites', []))
        
        assert starlink_count > 100, f"Starlinkä¿¡è™Ÿæ•¸æ“šä¸è¶³: {starlink_count}"
        assert oneweb_count > 50, f"OneWebä¿¡è™Ÿæ•¸æ“šä¸è¶³: {oneweb_count}"
        
        # æª¢æŸ¥éšæ®µå››æ•¸æ“šï¼ˆå¯é¸ï¼Œå› ç‚ºå¯èƒ½ä¸å­˜åœ¨ï¼‰
        if stage4_data:
            logger.info("   éšæ®µå››æ™‚é–“åºåˆ—æ•¸æ“š: å¯ç”¨")
        else:
            logger.warning("   âš ï¸ éšæ®µå››æ™‚é–“åºåˆ—æ•¸æ“š: ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨éšæ®µä¸‰æ•¸æ“š")
        
        # æª¢æŸ¥æ··åˆå­˜å„²æ¶æ§‹
        storage_config = self.storage_manager.get_storage_configuration()
        assert 'postgresql' in storage_config, "ç¼ºå°‘PostgreSQLå­˜å„²é…ç½®"
        assert 'volume_files' in storage_config, "ç¼ºå°‘Volumeæ–‡ä»¶å­˜å„²é…ç½®"
        
        volume_path = self.storage_manager.get_volume_path()
        assert os.path.exists(volume_path), f"Volumeå­˜å„²è·¯å¾‘ä¸å­˜åœ¨: {volume_path}"
        assert os.access(volume_path, os.W_OK), f"Volumeè·¯å¾‘ç„¡å¯«å…¥æ¬Šé™: {volume_path}"
        
        logger.info("âœ… é‹è¡Œæ™‚æ¶æ§‹æª¢æŸ¥é€šé")
    
    def _integrate_cross_stage_data(self, stage3_data: Dict[str, Any], stage4_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œè·¨éšæ®µæ•¸æ“šèåˆ"""
        logger.info("ğŸ”— åŸ·è¡Œè·¨éšæ®µæ•¸æ“šæ™ºèƒ½èåˆ...")
        
        integrated_data = {
            "metadata": {
                "processing_stage": "stage5_data_integration",
                "processing_timestamp": datetime.now(timezone.utc).isoformat(),
                "source_stages": ["stage3_signal_analysis", "stage4_timeseries"],
                "integration_method": "cross_stage_fusion"
            },
            "constellations": {}
        }
        
        # èåˆéšæ®µä¸‰å’Œéšæ®µå››æ•¸æ“š
        constellations = stage3_data.get('constellations', {})
        
        for const_name, const_data in constellations.items():
            logger.info(f"   èåˆ {const_name} æ˜Ÿåº§æ•¸æ“š...")
            
            satellites = const_data.get('satellites', [])
            integrated_satellites = []
            
            for satellite in satellites:
                # åŸºç¤ä¿¡æ¯ä¾†è‡ªéšæ®µä¸‰
                integrated_satellite = {
                    "satellite_id": satellite.get('satellite_id'),
                    "name": satellite.get('name', ''),
                    "constellation": const_name,
                    "norad_id": satellite.get('norad_id', 0),
                    
                    # è»Œé“æ•¸æ“š
                    "orbit_data": satellite.get('orbit_data', {}),
                    
                    # ä¿¡è™Ÿå“è³ªåˆ†æï¼ˆéšæ®µä¸‰ï¼‰
                    "signal_quality_analysis": satellite.get('signal_quality_analysis', {}),
                    
                    # ä½ç½®æ™‚é–“åºåˆ—ï¼ˆéšæ®µä¸‰æˆ–éšæ®µå››ï¼‰
                    "position_timeseries": satellite.get('position_timeseries', []),
                    
                    # äº‹ä»¶åˆ†æçµæœ
                    "event_analysis": satellite.get('event_analysis', {}),
                    
                    # æ•´åˆæ¨™è¨˜
                    "integration_metadata": {
                        "stage3_data_available": bool(satellite.get('signal_quality_analysis')),
                        "stage4_data_available": False,  # TODO: å¯¦éš›æª¢æŸ¥
                        "integration_timestamp": datetime.now(timezone.utc).isoformat()
                    }
                }
                
                integrated_satellites.append(integrated_satellite)
                self.processing_stats["integration_success_count"] += 1
            
            integrated_data["constellations"][const_name] = {
                "metadata": {
                    "constellation": const_name,
                    "total_satellites": len(integrated_satellites),
                    "integration_source": "stage3_primary"
                },
                "satellites": integrated_satellites
            }
        
        logger.info(f"âœ… æ•¸æ“šèåˆå®Œæˆï¼Œè™•ç† {self.processing_stats['integration_success_count']} é¡†è¡›æ˜Ÿ")
        return integrated_data
    
    def _generate_layered_elevation_data(self, integrated_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†å±¤ä»°è§’æ•¸æ“š (5Â°/10Â°/15Â°) - ç¬¦åˆ@docsè¦æ±‚"""
        logger.info("ğŸ“ ç”Ÿæˆåˆ†å±¤ä»°è§’æ•¸æ“š...")
        
        elevation_thresholds = [5.0, 10.0, 15.0]
        layered_data = {}
        
        for threshold in elevation_thresholds:
            threshold_key = f"elevation_{int(threshold)}deg"
            layered_data[threshold_key] = {
                "metadata": {
                    "elevation_threshold": threshold,
                    "description": f"ä»°è§’é–€æª» {threshold}Â° çš„è¡›æ˜Ÿå¯è¦‹æ€§æ•¸æ“š",
                    "academic_compliance": "Grade A - åŸºæ–¼çœŸå¯¦è»Œé“è¨ˆç®—"
                },
                "constellations": {}
            }
            
            # ç‚ºæ¯å€‹æ˜Ÿåº§ç”Ÿæˆåˆ†å±¤æ•¸æ“š
            for const_name, const_data in integrated_data["constellations"].items():
                filtered_satellites = []
                
                for satellite in const_data["satellites"]:
                    # æª¢æŸ¥è¡›æ˜Ÿæ˜¯å¦åœ¨æ­¤ä»°è§’é–€æª»ä¸‹å¯è¦‹
                    position_series = satellite.get("position_timeseries", [])
                    
                    visible_points = [
                        pos for pos in position_series
                        if pos.get("elevation_deg", -999) > threshold
                    ]
                    
                    if visible_points:
                        filtered_satellites.append({
                            **satellite,
                            "visibility_at_threshold": {
                                "elevation_threshold": threshold,
                                "visible_points": len(visible_points),
                                "total_points": len(position_series),
                                "visibility_ratio": len(visible_points) / len(position_series) if position_series else 0
                            }
                        })
                
                layered_data[threshold_key]["constellations"][const_name] = {
                    "metadata": {
                        "constellation": const_name,
                        "elevation_threshold": threshold,
                        "total_satellites": len(filtered_satellites)
                    },
                    "satellites": filtered_satellites
                }
                
                logger.info(f"   {threshold}Â°é–€æª» {const_name}: {len(filtered_satellites)} é¡†è¡›æ˜Ÿå¯è¦‹")
        
        logger.info("âœ… åˆ†å±¤ä»°è§’æ•¸æ“šç”Ÿæˆå®Œæˆ")
        return layered_data
    
    def _store_to_hybrid_architecture(self, integrated_data: Dict[str, Any], layered_data: Dict[str, Any]):
        """å­˜å„²åˆ°æ··åˆæ¶æ§‹ - PostgreSQL + Docker Volume"""
        logger.info("ğŸ’¾ å­˜å„²åˆ°æ··åˆæ¶æ§‹...")
        
        # æº–å‚™PostgreSQLçµæ§‹åŒ–æ•¸æ“š
        postgresql_data = self._prepare_postgresql_data(integrated_data)
        
        # æº–å‚™Volumeæª”æ¡ˆæ•¸æ“š
        volume_data = self._prepare_volume_data(integrated_data, layered_data)
        
        # å­˜å„²åˆ°PostgreSQL
        for data_type, data in postgresql_data.items():
            success = self.storage_manager.store_postgresql_data(data_type, data)
            if success:
                self.processing_stats["postgresql_records"] += len(data) if isinstance(data, dict) else 1
        
        # å­˜å„²åˆ°Volume
        for data_type, files in volume_data.items():
            for filename, data in files.items():
                success = self.storage_manager.store_volume_data(data_type, filename, data)
                if success:
                    self.processing_stats["volume_files"] += 1
        
        logger.info(f"âœ… æ··åˆæ¶æ§‹å­˜å„²å®Œæˆ")
        logger.info(f"   PostgreSQLè¨˜éŒ„: {self.processing_stats['postgresql_records']}")
        logger.info(f"   Volumeæª”æ¡ˆ: {self.processing_stats['volume_files']}")
    
    def _prepare_postgresql_data(self, integrated_data: Dict[str, Any]) -> Dict[str, Any]:
        """æº–å‚™PostgreSQLçµæ§‹åŒ–æ•¸æ“š"""
        postgresql_data = {}
        
        # è¡›æ˜Ÿå…ƒæ•¸æ“š
        satellite_metadata = {}
        for const_name, const_data in integrated_data["constellations"].items():
            for satellite in const_data["satellites"]:
                sat_id = satellite.get("satellite_id")
                if sat_id:
                    satellite_metadata[sat_id] = {
                        "name": satellite.get("name", ""),
                        "constellation": const_name,
                        "norad_id": satellite.get("norad_id", 0),
                        "altitude_km": satellite.get("orbit_data", {}).get("altitude", 0),
                        "inclination_deg": satellite.get("orbit_data", {}).get("inclination", 0)
                    }
        
        postgresql_data["satellite_metadata"] = satellite_metadata
        
        return postgresql_data
    
    def _prepare_volume_data(self, integrated_data: Dict[str, Any], layered_data: Dict[str, Any]) -> Dict[str, Any]:
        """æº–å‚™Volumeæª”æ¡ˆæ•¸æ“š"""
        volume_data = {
            "timeseries_data": {},
            "animation_resources": {}
        }
        
        # å®Œæ•´æ™‚é–“åºåˆ—æ•¸æ“š
        volume_data["timeseries_data"]["integrated_data_output.json"] = integrated_data
        
        # åˆ†å±¤ä»°è§’æ•¸æ“š
        for threshold_key, threshold_data in layered_data.items():
            volume_data["timeseries_data"][f"layered_{threshold_key}.json"] = threshold_data
        
        return volume_data
    
    def _generate_integration_output(self, integrated_data: Dict[str, Any], layered_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€çµ‚æ•´åˆè¼¸å‡º"""
        output_data = {
            "metadata": {
                "processing_stage": "stage5_data_integration",
                "processing_timestamp": datetime.now(timezone.utc).isoformat(),
                "processor_type": "DataIntegrationProcessor",
                "storage_architecture": "hybrid_postgresql_volume",
                "academic_compliance": "Grade A - çœŸå¯¦æ•¸æ“šæ•´åˆ",
                "processing_statistics": self.processing_stats
            },
            "integrated_data": integrated_data,
            "layered_elevation_data": layered_data,
            "storage_summary": self.storage_manager.get_storage_statistics()
        }
        
        # ä¿å­˜ä¸»è¦è¼¸å‡ºæ–‡ä»¶
        output_file = self.data_integration_dir / "integrated_data_output.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ’¾ æ•´åˆè¼¸å‡ºå·²ä¿å­˜: {output_file}")
        
        return output_data
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """ç²å–è™•ç†çµ±è¨ˆä¿¡æ¯"""
        return {
            "processor_type": "DataIntegrationProcessor",
            "storage_manager_type": "HybridStorageManager",
            "processing_stats": self.processing_stats,
            "storage_stats": self.storage_manager.get_storage_statistics()
        }