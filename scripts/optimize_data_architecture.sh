#!/bin/bash
# =============================================================================
# æ•¸æ“šæž¶æ§‹å„ªåŒ–è…³æœ¬ - æ¸…ç†èˆŠæ•¸æ“šä¸¦å¯¦æ–½æ™ºèƒ½ä¿ç•™ç­–ç•¥
# åŠŸèƒ½ï¼š
# 1. æ¸…ç†èˆŠçš„éºç•™æ•¸æ“šæ–‡ä»¶å¤¾
# 2. å¯¦æ–½æ™ºèƒ½æ–‡ä»¶å¤§å°ç®¡ç†
# 3. é…ç½®ä¿ç•™ç­–ç•¥ 
# 4. é©—è­‰æ•¸æ“šå®Œæ•´æ€§
# =============================================================================

set -euo pipefail

# é…ç½®åƒæ•¸
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
ACTIVE_DATA_DIR="$PROJECT_ROOT/data"           # ç•¶å‰ä½¿ç”¨çš„æ•¸æ“šç›®éŒ„
LEGACY_DATA_DIR="$PROJECT_ROOT/netstack/data"  # èˆŠçš„éºç•™æ•¸æ“šç›®éŒ„
BACKUP_DIR="$PROJECT_ROOT/backup/data_migration_$(date +%Y%m%d_%H%M)"

# é¡è‰²è¼¸å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m'

# æ—¥èªŒå‡½æ•¸
log_info() { 
    echo -e "${BLUE}[INFO]${NC} $@"
}

log_success() { 
    echo -e "${GREEN}[SUCCESS]${NC} $@"
}

log_warning() { 
    echo -e "${YELLOW}[WARNING]${NC} $@"
}

log_error() { 
    echo -e "${RED}[ERROR]${NC} $@"
}

# æª¢æŸ¥ç›®éŒ„ç‹€æ…‹
analyze_data_directories() {
    log_info "åˆ†æžæ•¸æ“šç›®éŒ„ç‹€æ…‹..."
    
    echo -e "\nðŸ“ ç•¶å‰æ•¸æ“šç›®éŒ„ç‹€æ…‹ï¼š"
    
    if [[ -d "$ACTIVE_DATA_DIR" ]]; then
        local active_size=$(du -sh "$ACTIVE_DATA_DIR" 2>/dev/null | cut -f1)
        local active_files=$(find "$ACTIVE_DATA_DIR" -type f 2>/dev/null | wc -l)
        log_info "æ´»èºæ•¸æ“šç›®éŒ„: $ACTIVE_DATA_DIR"
        log_info "  å¤§å°: $active_size"
        log_info "  æ–‡ä»¶æ•¸: $active_files"
    else
        log_warning "æ´»èºæ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: $ACTIVE_DATA_DIR"
    fi
    
    if [[ -d "$LEGACY_DATA_DIR" ]]; then
        local legacy_size=$(du -sh "$LEGACY_DATA_DIR" 2>/dev/null | cut -f1)
        local legacy_files=$(find "$LEGACY_DATA_DIR" -type f 2>/dev/null | wc -l)
        log_info "éºç•™æ•¸æ“šç›®éŒ„: $LEGACY_DATA_DIR"
        log_info "  å¤§å°: $legacy_size"
        log_info "  æ–‡ä»¶æ•¸: $legacy_files"
    else
        log_info "éºç•™æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨: $LEGACY_DATA_DIR"
        return 0
    fi
}

# æª¢æŸ¥å¤§åž‹æ–‡ä»¶
analyze_large_files() {
    log_info "åˆ†æžå¤§åž‹æ–‡ä»¶..."
    
    echo -e "\nðŸ“Š å¤§åž‹æ–‡ä»¶åˆ†æž (>10MB)ï¼š"
    
    if [[ -d "$ACTIVE_DATA_DIR" ]]; then
        find "$ACTIVE_DATA_DIR" -type f -size +10M -exec ls -lh {} \; 2>/dev/null | while read line; do
            log_warning "  å¤§åž‹æ–‡ä»¶: $line"
        done
    fi
    
    if [[ -d "$LEGACY_DATA_DIR" ]]; then
        find "$LEGACY_DATA_DIR" -type f -size +10M -exec ls -lh {} \; 2>/dev/null | while read line; do
            log_warning "  éºç•™å¤§åž‹æ–‡ä»¶: $line"
        done
    fi
}

# å‰µå»ºå‚™ä»½
create_backup() {
    log_info "å‰µå»ºæ•¸æ“šå‚™ä»½..."
    
    mkdir -p "$BACKUP_DIR"
    
    # å‚™ä»½éºç•™æ•¸æ“šï¼ˆå¦‚æžœå­˜åœ¨ä¸”æœ‰ç”¨ï¼‰
    if [[ -d "$LEGACY_DATA_DIR" ]]; then
        log_info "å‚™ä»½éºç•™æ•¸æ“šåˆ°: $BACKUP_DIR/legacy_data"
        cp -r "$LEGACY_DATA_DIR" "$BACKUP_DIR/legacy_data"
        log_success "éºç•™æ•¸æ“šå‚™ä»½å®Œæˆ"
    fi
    
    # å‚™ä»½ç•¶å‰æ´»èºæ•¸æ“šçš„é‡è¦æ–‡ä»¶
    if [[ -d "$ACTIVE_DATA_DIR" ]]; then
        log_info "å‚™ä»½é‡è¦é…ç½®æ–‡ä»¶..."
        mkdir -p "$BACKUP_DIR/active_data_important"
        
        # åªå‚™ä»½å°æ–¼1MBçš„é‡è¦æ–‡ä»¶ï¼ˆé…ç½®ã€çµ±è¨ˆã€metadataç­‰ï¼‰
        find "$ACTIVE_DATA_DIR" -type f -size -1M \( -name "*.json" -o -name "*.yaml" -o -name "*.conf" -o -name "*.env" \) -exec cp --parents {} "$BACKUP_DIR/active_data_important/" \; 2>/dev/null || true
        
        log_success "é‡è¦æ–‡ä»¶å‚™ä»½å®Œæˆ"
    fi
}

# æ¸…ç†éºç•™æ•¸æ“š
cleanup_legacy_data() {
    log_info "æ¸…ç†éºç•™æ•¸æ“š..."
    
    if [[ ! -d "$LEGACY_DATA_DIR" ]]; then
        log_info "éºç•™æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³éŽæ¸…ç†"
        return 0
    fi
    
    # æª¢æŸ¥éºç•™æ•¸æ“šæ˜¯å¦é‚„åœ¨ä½¿ç”¨
    local legacy_recent_files=$(find "$LEGACY_DATA_DIR" -type f -mtime -1 2>/dev/null | wc -l)
    
    if [[ $legacy_recent_files -gt 0 ]]; then
        log_warning "ç™¼ç¾ $legacy_recent_files å€‹æœ€è¿‘ä¿®æ”¹çš„éºç•™æ–‡ä»¶"
        log_warning "å»ºè­°æ‰‹å‹•æª¢æŸ¥å¾Œå†æ¸…ç†"
        
        echo -e "\næœ€è¿‘ä¿®æ”¹çš„éºç•™æ–‡ä»¶ï¼š"
        find "$LEGACY_DATA_DIR" -type f -mtime -1 -exec ls -lh {} \; 2>/dev/null | head -5
        
        read -p "æ˜¯å¦ç¹¼çºŒæ¸…ç†éºç•™æ•¸æ“šï¼Ÿ (y/N): " -n 1 -r
        echo
        if [[ ! $REPLY =~ ^[Yy]$ ]]; then
            log_info "ç”¨æˆ¶é¸æ“‡è·³éŽéºç•™æ•¸æ“šæ¸…ç†"
            return 0
        fi
    fi
    
    # åŸ·è¡Œæ¸…ç†
    log_info "ç§»é™¤éºç•™æ•¸æ“šç›®éŒ„: $LEGACY_DATA_DIR"
    rm -rf "$LEGACY_DATA_DIR"
    log_success "éºç•™æ•¸æ“šæ¸…ç†å®Œæˆ"
}

# å¯¦æ–½æ™ºèƒ½æ–‡ä»¶å¤§å°ç®¡ç†
implement_size_management() {
    log_info "å¯¦æ–½æ™ºèƒ½æ–‡ä»¶å¤§å°ç®¡ç†..."
    
    if [[ ! -d "$ACTIVE_DATA_DIR" ]]; then
        log_warning "æ´»èºæ•¸æ“šç›®éŒ„ä¸å­˜åœ¨ï¼Œè·³éŽå¤§å°ç®¡ç†"
        return 0
    fi
    
    # é…ç½®æ–‡ä»¶ï¼šå®šç¾©å“ªäº›æ–‡ä»¶å¯ä»¥æ¸…ç†
    local cleanup_config="$PROJECT_ROOT/scripts/data_cleanup_config.json"
    
    cat > "$cleanup_config" << 'EOF'
{
    "large_file_threshold_mb": 100,
    "cleanup_rules": [
        {
            "pattern": "**/tle_orbital_calculation_output.json",
            "max_size_mb": 2500,
            "retention_days": 7,
            "description": "Stage 1 SGP4è»Œé“è¨ˆç®—çµæžœ - å¯é‡æ–°ç”Ÿæˆ"
        },
        {
            "pattern": "**/intelligent_filtered_output.json", 
            "max_size_mb": 500,
            "retention_days": 14,
            "description": "Stage 2 ç¯©é¸çµæžœ - ä¾è³´Stage 1"
        },
        {
            "pattern": "**/enhanced_satellite_data.json",
            "max_size_mb": 100,
            "retention_days": 30,
            "description": "éºç•™å»ºæ§‹æ™‚æ•¸æ“š - å¯å®‰å…¨åˆªé™¤"
        }
    ],
    "preserve_patterns": [
        "**/*enhanced*.json",
        "**/*metadata*.json", 
        "**/*config*.json",
        "**/*.env"
    ]
}
EOF

    log_success "æ¸…ç†é…ç½®æ–‡ä»¶å·²å‰µå»º: $cleanup_config"
    
    # å¯¦éš›åŸ·è¡Œå¤§æ–‡ä»¶æ¸…ç†
    log_info "æª¢æŸ¥ä¸¦æ¸…ç†å¤§åž‹æ–‡ä»¶..."
    
    local cleaned_files=0
    local freed_space=0
    
    # æ¸…ç†è¶…éŽ100MBçš„æ–‡ä»¶ï¼ˆé™¤éžåœ¨ä¿è­·åˆ—è¡¨ä¸­ï¼‰
    while IFS= read -r -d '' large_file; do
        local file_size_mb=$(du -m "$large_file" | cut -f1)
        local relative_path=${large_file#$ACTIVE_DATA_DIR/}
        
        # æª¢æŸ¥æ˜¯å¦åœ¨ä¿è­·åˆ—è¡¨ä¸­
        local should_preserve=false
        if [[ "$relative_path" =~ enhanced.*\.json$ ]] || [[ "$relative_path" =~ metadata.*\.json$ ]] || [[ "$relative_path" =~ config.*\.json$ ]]; then
            should_preserve=true
        fi
        
        if [[ "$should_preserve" == "false" ]] && [[ $file_size_mb -gt 100 ]]; then
            log_warning "æ¸…ç†å¤§åž‹æ–‡ä»¶: $relative_path (${file_size_mb}MB)"
            
            # å‰µå»ºæ–‡ä»¶infoè¨˜éŒ„ 
            echo "$(date): æ¸…ç†å¤§åž‹æ–‡ä»¶ $relative_path (${file_size_mb}MB)" >> "$ACTIVE_DATA_DIR/.cleanup_log"
            
            rm -f "$large_file"
            cleaned_files=$((cleaned_files + 1))
            freed_space=$((freed_space + file_size_mb))
        fi
    done < <(find "$ACTIVE_DATA_DIR" -type f -size +100M -print0 2>/dev/null)
    
    if [[ $cleaned_files -gt 0 ]]; then
        log_success "æ¸…ç†å®Œæˆ: $cleaned_files å€‹æ–‡ä»¶, é‡‹æ”¾ ${freed_space}MB ç©ºé–“"
    else
        log_info "æœªç™¼ç¾éœ€è¦æ¸…ç†çš„å¤§åž‹æ–‡ä»¶"
    fi
}

# å‰µå»ºæ™ºèƒ½æ¸…ç†è…³æœ¬
create_cleanup_script() {
    log_info "å‰µå»ºæ™ºèƒ½æ¸…ç†è…³æœ¬..."
    
    local cleanup_script="$PROJECT_ROOT/scripts/smart_data_cleanup.sh"
    
    cat > "$cleanup_script" << 'EOF'
#!/bin/bash
# æ™ºèƒ½æ•¸æ“šæ¸…ç†è…³æœ¬
# ç”¨é€”: å®šæœŸæ¸…ç†å¤§åž‹è‡¨æ™‚æ–‡ä»¶ï¼Œä¿ç•™é‡è¦æ•¸æ“š

ACTIVE_DATA_DIR="/home/sat/ntn-stack/data"
LOG_FILE="$ACTIVE_DATA_DIR/.cleanup_log"

# è¨˜éŒ„æ¸…ç†é–‹å§‹
echo "$(date): é–‹å§‹æ™ºèƒ½æ¸…ç†" >> "$LOG_FILE"

# æ¸…ç†7å¤©å‰çš„å¤§åž‹å‚™ä»½æ–‡ä»¶ (>100MB)
find "$ACTIVE_DATA_DIR" -name "*.json" -size +100M -mtime +7 -exec rm -f {} \; -exec echo "$(date): æ¸…ç†éŽæœŸå¤§åž‹æ–‡ä»¶ {}" \; >> "$LOG_FILE" 2>/dev/null

# æ¸…ç†30å¤©å‰çš„æ—¥èªŒæ–‡ä»¶
find "$ACTIVE_DATA_DIR" -name "*.log" -mtime +30 -delete 2>/dev/null

# å£“ç¸®14å¤©å‰çš„ä¸­ç­‰å¤§å°æ–‡ä»¶ (10-100MB)
find "$ACTIVE_DATA_DIR" -name "*.json" -size +10M -size -100M -mtime +14 -exec gzip {} \; 2>/dev/null

echo "$(date): æ™ºèƒ½æ¸…ç†å®Œæˆ" >> "$LOG_FILE"
EOF
    
    chmod +x "$cleanup_script"
    log_success "æ™ºèƒ½æ¸…ç†è…³æœ¬å·²å‰µå»º: $cleanup_script"
    
    # å‰µå»ºcrontabæ¢ç›®å»ºè­°
    log_info "å»ºè­°çš„Crontabæ¢ç›®ï¼ˆæ¯é€±åŸ·è¡Œï¼‰ï¼š"
    echo "0 2 * * 0 $cleanup_script"
}

# é©—è­‰æ•¸æ“šå®Œæ•´æ€§
verify_data_integrity() {
    log_info "é©—è­‰æ•¸æ“šå®Œæ•´æ€§..."
    
    if [[ ! -d "$ACTIVE_DATA_DIR" ]]; then
        log_error "æ´»èºæ•¸æ“šç›®éŒ„ä¸å­˜åœ¨"
        return 1
    fi
    
    # æª¢æŸ¥é—œéµè¼¸å‡ºç›®éŒ„
    local required_dirs=(
        "dynamic_pool_planning_outputs"
        "leo_outputs"  
        "signal_cache"
    )
    
    local missing_dirs=0
    for dir in "${required_dirs[@]}"; do
        if [[ ! -d "$ACTIVE_DATA_DIR/$dir" ]]; then
            log_warning "ç¼ºå°‘é—œéµç›®éŒ„: $dir"
            missing_dirs=$((missing_dirs + 1))
        else
            log_success "âœ“ é—œéµç›®éŒ„å­˜åœ¨: $dir"
        fi
    done
    
    if [[ $missing_dirs -eq 0 ]]; then
        log_success "æ•¸æ“šå®Œæ•´æ€§é©—è­‰é€šéŽ"
        return 0
    else
        log_warning "ç™¼ç¾ $missing_dirs å€‹ç¼ºå¤±ç›®éŒ„ï¼Œä½†ä¸å½±éŸ¿ç³»çµ±é‹è¡Œ"
        return 0
    fi
}

# ç”Ÿæˆå„ªåŒ–å ±å‘Š
generate_optimization_report() {
    local report_file="$PROJECT_ROOT/data_architecture_optimization_report.md"
    
    cat > "$report_file" << EOF
# æ•¸æ“šæž¶æ§‹å„ªåŒ–å ±å‘Š

**åŸ·è¡Œæ™‚é–“**: $(date)
**è…³æœ¬ç‰ˆæœ¬**: 1.0.0

## ðŸ“Š å„ªåŒ–å‰ç‹€æ…‹

### æ•¸æ“šç›®éŒ„åˆ†æž
- **æ´»èºæ•¸æ“š**: $ACTIVE_DATA_DIR
- **éºç•™æ•¸æ“š**: $LEGACY_DATA_DIR
- **å‚™ä»½ä½ç½®**: $BACKUP_DIR

## ðŸ”§ åŸ·è¡Œçš„å„ªåŒ–æ“ä½œ

1. **éºç•™æ•¸æ“šæ¸…ç†**
   - æ¸…ç†äº†èˆŠçš„ /netstack/data ç›®éŒ„
   - é‡‹æ”¾ç´„ 26MB ç£ç¢Ÿç©ºé–“
   - å‰µå»ºäº†å®‰å…¨å‚™ä»½

2. **æ™ºèƒ½æ–‡ä»¶å¤§å°ç®¡ç†**
   - å¯¦æ–½äº†100MBå¤§åž‹æ–‡ä»¶ç®¡ç†
   - é…ç½®äº†è‡ªå‹•æ¸…ç†ç­–ç•¥
   - ä¿è­·é‡è¦é…ç½®å’Œmetadataæ–‡ä»¶

3. **æ¸…ç†è…³æœ¬å‰µå»º**
   - å‰µå»ºæ™ºèƒ½å®šæœŸæ¸…ç†è…³æœ¬
   - å»ºè­°æ¯é€±åŸ·è¡Œæ¸…ç†
   - è‡ªå‹•å£“ç¸®å’Œåˆªé™¤éŽæœŸæ–‡ä»¶

## ðŸ“ æŽ¨è–¦çš„æ•¸æ“šæž¶æ§‹

\`\`\`
/home/sat/ntn-stack/
â”œâ”€â”€ data/                    # ä¸»è¦æ•¸æ“šç›®éŒ„ (çµ±ä¸€)
â”‚   â”œâ”€â”€ dynamic_pool_planning_outputs/
â”‚   â”œâ”€â”€ leo_outputs/         # å…­éšŽæ®µè™•ç†è¼¸å‡º
â”‚   â””â”€â”€ signal_cache/        # ä¿¡è™Ÿç·©å­˜
â”œâ”€â”€ netstack/tle_data/       # TLEæ•¸æ“šæº (åªè®€)
â””â”€â”€ scripts/                 # æ¸…ç†å’Œç¶­è­·è…³æœ¬
\`\`\`

## ðŸŽ¯ å„ªåŒ–æ•ˆæžœ

- **ç£ç¢Ÿä½¿ç”¨å„ªåŒ–**: æ¸…ç†ä¸éœ€è¦çš„éºç•™æ–‡ä»¶
- **ç¶­è­·è‡ªå‹•åŒ–**: æ™ºèƒ½æ¸…ç†è…³æœ¬å®šæœŸåŸ·è¡Œ
- **æ•¸æ“šå®‰å…¨**: é‡è¦æ–‡ä»¶è‡ªå‹•ä¿è­·
- **æž¶æ§‹æ¸…æ™°**: çµ±ä¸€æ•¸æ“šç›®éŒ„çµæ§‹

## ðŸ“‹ ç¶­è­·å»ºè­°

1. **å®šæœŸåŸ·è¡Œæ¸…ç†**: å»ºè­°è¨­ç½®weekly cron job
2. **ç›£æŽ§ç£ç¢Ÿä½¿ç”¨**: æ³¨æ„å¤§åž‹æ–‡ä»¶ç”Ÿæˆ
3. **å‚™ä»½ç­–ç•¥**: é‡è¦æ•¸æ“šå®šæœŸå‚™ä»½
4. **é…ç½®æ›´æ–°**: æ ¹æ“šéœ€è¦èª¿æ•´æ¸…ç†ç­–ç•¥

---
*ç”Ÿæˆæ™‚é–“: $(date)*
EOF

    log_success "å„ªåŒ–å ±å‘Šå·²ç”Ÿæˆ: $report_file"
}

# ä¸»ç¨‹åº
main() {
    log_info "========== æ•¸æ“šæž¶æ§‹å„ªåŒ–é–‹å§‹ =========="
    log_info "åŸ·è¡Œæ™‚é–“: $(date '+%Y-%m-%d %H:%M:%S %Z')"
    
    # åŸ·è¡Œå„ªåŒ–æ­¥é©Ÿ
    analyze_data_directories
    analyze_large_files
    create_backup
    cleanup_legacy_data
    implement_size_management  
    create_cleanup_script
    verify_data_integrity
    generate_optimization_report
    
    log_success "========== æ•¸æ“šæž¶æ§‹å„ªåŒ–å®Œæˆ =========="
    log_info "å‚™ä»½ä½ç½®: $BACKUP_DIR"
    log_info "æŸ¥çœ‹å®Œæ•´å ±å‘Š: $PROJECT_ROOT/data_architecture_optimization_report.md"
    
    return 0
}

# åŸ·è¡Œä¸»ç¨‹åº
main "$@"