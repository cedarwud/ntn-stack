# Phase 2: æ¸¬è©¦é©—è­‰æŒ‡å—

**æ¸¬è©¦ç›®æ¨™**: é©—è­‰æ–‡æª”ã€æ¸¬è©¦æ¡†æ¶ã€é…ç½®ç®¡ç†å’Œç›£æ§ç³»çµ±
**æ¸¬è©¦æ·±åº¦**: æ·±åº¦æ¸¬è©¦ + é•·æœŸç©©å®šæ€§é©—è­‰

## ğŸ“‹ æ¸¬è©¦éšæ®µè¦åŠƒ

### éšæ®µ 1: æ¸¬è©¦æ¡†æ¶é©—è­‰ (Week 1-2)
- æ¸¬è©¦è¦†è“‹ç‡æå‡é©—è­‰
- æ–°å¢æ¸¬è©¦æ¡ˆä¾‹çš„æ­£ç¢ºæ€§
- æ¸¬è©¦åŸ·è¡Œæ•ˆç‡è©•ä¼°

### éšæ®µ 2: ç›£æ§ç³»çµ±æ¸¬è©¦ (Week 3-4)
- æ€§èƒ½ç›£æ§æº–ç¢ºæ€§
- å‘Šè­¦æ©Ÿåˆ¶æœ‰æ•ˆæ€§
- å„€è¡¨æ¿åŠŸèƒ½å®Œæ•´æ€§

### éšæ®µ 3: é…ç½®ç®¡ç†æ¸¬è©¦ (Week 5)
- å¤šç’°å¢ƒé…ç½®æ­£ç¢ºæ€§
- é…ç½®è®Šæ›´æµç¨‹é©—è­‰
- é…ç½®å®‰å…¨æ€§æª¢æŸ¥

### éšæ®µ 4: æ•´åˆç©©å®šæ€§æ¸¬è©¦ (Week 6)
- é•·æœŸé‹è¡Œç©©å®šæ€§
- è² è¼‰ä¸‹çš„ç³»çµ±è¡¨ç¾
- æ•…éšœæ¢å¾©èƒ½åŠ›

## ğŸ§ª è©³ç´°æ¸¬è©¦è¨ˆç•«

### 1. æ¸¬è©¦æ¡†æ¶é©—è­‰

#### 1.1 æ¸¬è©¦è¦†è“‹ç‡é©—è­‰
```bash
#!/bin/bash
# test_coverage_validation.sh

echo "ğŸ§ª é©—è­‰æ¸¬è©¦è¦†è“‹ç‡æå‡..."

# åŸ·è¡Œè¦†è“‹ç‡æ¸¬è©¦
python -m pytest --cov=config --cov=netstack --cov=simworld \
  --cov-report=html --cov-report=term-missing \
  --cov-report=json -v

# æª¢æŸ¥è¦†è“‹ç‡å ±å‘Š
coverage_file="coverage.json"
if [ -f "$coverage_file" ]; then
    # ä½¿ç”¨ jq è§£æè¦†è“‹ç‡
    total_coverage=$(cat $coverage_file | jq '.totals.percent_covered')
    
    echo "ç¸½è¦†è“‹ç‡: $total_coverage%"
    
    # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™ (90%)
    if (( $(echo "$total_coverage >= 90" | bc -l) )); then
        echo "âœ… è¦†è“‹ç‡é”æ¨™: $total_coverage%"
    else
        echo "âŒ è¦†è“‹ç‡æœªé”æ¨™: $total_coverage% (ç›®æ¨™: 90%)"
        exit 1
    fi
else
    echo "âŒ è¦†è“‹ç‡å ±å‘Šæª”æ¡ˆä¸å­˜åœ¨"
    exit 1
fi

# æª¢æŸ¥å„æ¨¡çµ„è¦†è“‹ç‡
echo "ğŸ“Š å„æ¨¡çµ„è¦†è“‹ç‡è©³æƒ…:"
cat $coverage_file | jq -r '.files | to_entries[] | "\(.key): \(.value.summary.percent_covered)%"' | sort
```

#### 1.2 æ–°å¢æ¸¬è©¦æ¡ˆä¾‹é©—è­‰
```bash
#!/bin/bash
# test_new_testcases.sh

echo "ğŸ” é©—è­‰æ–°å¢æ¸¬è©¦æ¡ˆä¾‹..."

# åŸ·è¡Œæ–°å¢çš„å–®å…ƒæ¸¬è©¦
echo "åŸ·è¡Œæ“´å±•é…ç½®æ¸¬è©¦..."
python -m pytest tests/unit/test_satellite_config_extended.py -v

# åŸ·è¡Œæ•´åˆæ¸¬è©¦
echo "åŸ·è¡Œé è™•ç†ç®¡é“æ¸¬è©¦..."
python -m pytest tests/integration/test_preprocessing_pipeline.py -v

# åŸ·è¡Œæ€§èƒ½æ¸¬è©¦
echo "åŸ·è¡Œæ€§èƒ½æ¸¬è©¦..."
python -m pytest tests/performance/test_system_performance.py -v -m performance

# æª¢æŸ¥æ¸¬è©¦çµæœ
if [ $? -eq 0 ]; then
    echo "âœ… æ‰€æœ‰æ–°å¢æ¸¬è©¦é€šé"
else
    echo "âŒ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—"
    exit 1
fi

# é©—è­‰æ¸¬è©¦åŸ·è¡Œæ™‚é–“
echo "â±ï¸ æ¸¬è©¦åŸ·è¡Œæ™‚é–“åˆ†æ..."
python -c "
import time
import subprocess

start_time = time.time()
result = subprocess.run(['python', '-m', 'pytest', 'tests/', '-q'], 
                       capture_output=True)
execution_time = time.time() - start_time

print(f'ç¸½æ¸¬è©¦åŸ·è¡Œæ™‚é–“: {execution_time:.2f}ç§’')

if execution_time > 600:  # 10åˆ†é˜
    print('âŒ æ¸¬è©¦åŸ·è¡Œæ™‚é–“éé•·')
    exit(1)
else:
    print('âœ… æ¸¬è©¦åŸ·è¡Œæ™‚é–“åˆç†')
"
```

#### 1.3 æ¸¬è©¦å“è³ªè©•ä¼°
```python
#!/usr/bin/env python3
# test_quality_assessment.py

"""
æ¸¬è©¦å“è³ªè©•ä¼°è…³æœ¬
åˆ†ææ¸¬è©¦çš„æœ‰æ•ˆæ€§å’Œå“è³ª
"""

import ast
import os
from pathlib import Path
from collections import defaultdict

class TestQualityAnalyzer:
    """æ¸¬è©¦å“è³ªåˆ†æå™¨"""
    
    def __init__(self, test_dir: Path):
        self.test_dir = test_dir
        self.quality_metrics = {
            'test_count': 0,
            'assertion_count': 0,
            'mock_usage': 0,
            'parametrized_tests': 0,
            'async_tests': 0,
            'performance_tests': 0
        }
    
    def analyze_test_file(self, file_path: Path):
        """åˆ†æå–®å€‹æ¸¬è©¦æª”æ¡ˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                # è¨ˆç®—æ¸¬è©¦å‡½æ•¸
                if isinstance(node, ast.FunctionDef) and node.name.startswith('test_'):
                    self.quality_metrics['test_count'] += 1
                    
                    # æª¢æŸ¥æ˜¯å¦ç‚ºç•°æ­¥æ¸¬è©¦
                    if any(isinstance(decorator, ast.Name) and decorator.id == 'pytest' 
                          for decorator in node.decorator_list):
                        for decorator in node.decorator_list:
                            if (isinstance(decorator, ast.Attribute) and 
                                decorator.attr == 'asyncio'):
                                self.quality_metrics['async_tests'] += 1
                    
                    # æª¢æŸ¥æ€§èƒ½æ¸¬è©¦æ¨™è¨˜
                    for decorator in node.decorator_list:
                        if (isinstance(decorator, ast.Attribute) and 
                            isinstance(decorator.value, ast.Attribute) and
                            decorator.value.attr == 'mark' and
                            decorator.attr == 'performance'):
                            self.quality_metrics['performance_tests'] += 1
                
                # è¨ˆç®—æ–·è¨€
                if isinstance(node, ast.Assert):
                    self.quality_metrics['assertion_count'] += 1
                
                # æª¢æŸ¥ Mock ä½¿ç”¨
                if isinstance(node, ast.Call):
                    if (isinstance(node.func, ast.Attribute) and 
                        'mock' in str(node.func)):
                        self.quality_metrics['mock_usage'] += 1
                        
        except Exception as e:
            print(f"åˆ†ææª”æ¡ˆå¤±æ•— {file_path}: {e}")
    
    def run_analysis(self):
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        test_files = list(self.test_dir.rglob("test_*.py"))
        
        print(f"ç™¼ç¾ {len(test_files)} å€‹æ¸¬è©¦æª”æ¡ˆ")
        
        for test_file in test_files:
            self.analyze_test_file(test_file)
        
        return self.generate_quality_report()
    
    def generate_quality_report(self):
        """ç”Ÿæˆå“è³ªå ±å‘Š"""
        report = {
            'summary': self.quality_metrics.copy(),
            'quality_score': self.calculate_quality_score(),
            'recommendations': self.generate_recommendations()
        }
        
        return report
    
    def calculate_quality_score(self):
        """è¨ˆç®—å“è³ªåˆ†æ•¸ (0-100)"""
        score = 0
        
        # æ¸¬è©¦æ•¸é‡ (30åˆ†)
        if self.quality_metrics['test_count'] >= 50:
            score += 30
        elif self.quality_metrics['test_count'] >= 30:
            score += 20
        elif self.quality_metrics['test_count'] >= 10:
            score += 10
        
        # æ–·è¨€å¯†åº¦ (20åˆ†) - å¹³å‡æ¯å€‹æ¸¬è©¦çš„æ–·è¨€æ•¸
        if self.quality_metrics['test_count'] > 0:
            assertion_density = (self.quality_metrics['assertion_count'] / 
                               self.quality_metrics['test_count'])
            if assertion_density >= 3:
                score += 20
            elif assertion_density >= 2:
                score += 15
            elif assertion_density >= 1:
                score += 10
        
        # Mock ä½¿ç”¨ (15åˆ†) - è¡¨ç¤ºéš”é›¢æ¸¬è©¦
        if self.quality_metrics['mock_usage'] >= 10:
            score += 15
        elif self.quality_metrics['mock_usage'] >= 5:
            score += 10
        
        # ç•°æ­¥æ¸¬è©¦ (10åˆ†)
        if self.quality_metrics['async_tests'] >= 5:
            score += 10
        elif self.quality_metrics['async_tests'] >= 2:
            score += 5
        
        # æ€§èƒ½æ¸¬è©¦ (15åˆ†)
        if self.quality_metrics['performance_tests'] >= 5:
            score += 15
        elif self.quality_metrics['performance_tests'] >= 2:
            score += 10
        
        # æ¸¬è©¦å¤šæ¨£æ€§ (10åˆ†)
        diversity_score = min(len([k for k, v in self.quality_metrics.items() if v > 0]), 6)
        score += int(diversity_score * 10 / 6)
        
        return min(score, 100)
    
    def generate_recommendations(self):
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        recommendations = []
        
        if self.quality_metrics['test_count'] < 30:
            recommendations.append("å¢åŠ æ›´å¤šæ¸¬è©¦æ¡ˆä¾‹ä»¥æå‡è¦†è“‹ç‡")
        
        if self.quality_metrics['assertion_count'] / max(self.quality_metrics['test_count'], 1) < 2:
            recommendations.append("å¢åŠ æ¸¬è©¦æ–·è¨€ä»¥æå‡æ¸¬è©¦å¼·åº¦")
        
        if self.quality_metrics['mock_usage'] < 5:
            recommendations.append("ä½¿ç”¨æ›´å¤š Mock ä¾†éš”é›¢æ¸¬è©¦ä¾è³´")
        
        if self.quality_metrics['performance_tests'] < 3:
            recommendations.append("å¢åŠ æ€§èƒ½å›æ­¸æ¸¬è©¦")
        
        if self.quality_metrics['async_tests'] < 3:
            recommendations.append("ç‚ºç•°æ­¥åŠŸèƒ½æ·»åŠ å°ˆé–€æ¸¬è©¦")
        
        return recommendations

if __name__ == "__main__":
    analyzer = TestQualityAnalyzer(Path("tests"))
    report = analyzer.run_analysis()
    
    print("\nğŸ“Š æ¸¬è©¦å“è³ªå ±å‘Š")
    print("=" * 40)
    
    print(f"æ¸¬è©¦ç¸½æ•¸: {report['summary']['test_count']}")
    print(f"æ–·è¨€ç¸½æ•¸: {report['summary']['assertion_count']}")
    print(f"Mock ä½¿ç”¨: {report['summary']['mock_usage']}")
    print(f"ç•°æ­¥æ¸¬è©¦: {report['summary']['async_tests']}")
    print(f"æ€§èƒ½æ¸¬è©¦: {report['summary']['performance_tests']}")
    
    print(f"\nå“è³ªåˆ†æ•¸: {report['quality_score']}/100")
    
    if report['quality_score'] >= 80:
        print("âœ… æ¸¬è©¦å“è³ªå„ªç§€")
    elif report['quality_score'] >= 60:
        print("âš ï¸ æ¸¬è©¦å“è³ªè‰¯å¥½ï¼Œæœ‰æ”¹é€²ç©ºé–“")
    else:
        print("âŒ æ¸¬è©¦å“è³ªéœ€è¦æå‡")
    
    if report['recommendations']:
        print("\nğŸ’¡ æ”¹é€²å»ºè­°:")
        for rec in report['recommendations']:
            print(f"  - {rec}")
```

### 2. ç›£æ§ç³»çµ±æ¸¬è©¦

#### 2.1 æ€§èƒ½ç›£æ§æº–ç¢ºæ€§æ¸¬è©¦
```bash
#!/bin/bash
# test_performance_monitoring.sh

echo "ğŸ“Š æ¸¬è©¦æ€§èƒ½ç›£æ§ç³»çµ±..."

# å•Ÿå‹•æ€§èƒ½ç›£æ§
python monitoring/performance_monitor.py --interval 5 &
monitor_pid=$!

echo "æ€§èƒ½ç›£æ§å·²å•Ÿå‹• (PID: $monitor_pid)"

# ç­‰å¾…æ”¶é›†ä¸€äº›æ•¸æ“š
sleep 30

# ç”Ÿæˆæ¸¬è©¦è² è¼‰
echo "ğŸ”„ ç”Ÿæˆæ¸¬è©¦è² è¼‰..."
for i in {1..10}; do
    curl -s http://localhost:8080/health > /dev/null &
    curl -s http://localhost:8888/health > /dev/null &
done

# ç­‰å¾…è² è¼‰å®Œæˆ
wait

# å†æ”¶é›†ä¸€äº›æ•¸æ“š
sleep 30

# åœæ­¢ç›£æ§
kill $monitor_pid

# æª¢æŸ¥ç›£æ§æ•¸æ“š
echo "ğŸ“‹ æª¢æŸ¥ç›£æ§æ•¸æ“š..."
if [ -d "monitoring/data" ]; then
    file_count=$(find monitoring/data -name "metrics_*.json" | wc -l)
    echo "ç”Ÿæˆçš„ç›£æ§æ–‡ä»¶æ•¸é‡: $file_count"
    
    if [ $file_count -gt 0 ]; then
        echo "âœ… ç›£æ§æ•¸æ“šæ”¶é›†æ­£å¸¸"
        
        # æª¢æŸ¥æœ€æ–°æ•¸æ“šçš„å®Œæ•´æ€§
        latest_file=$(find monitoring/data -name "metrics_*.json" -type f -exec ls -t {} + | head -1)
        echo "æª¢æŸ¥æœ€æ–°ç›£æ§æ•¸æ“š: $latest_file"
        
        # é©—è­‰JSONæ ¼å¼
        python -c "
import json
import sys

try:
    with open('$latest_file') as f:
        data = json.load(f)
    
    required_fields = ['timestamp', 'cpu_percent', 'memory_percent', 'api_response_times']
    missing_fields = [field for field in required_fields if field not in data]
    
    if missing_fields:
        print(f'âŒ ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}')
        sys.exit(1)
    else:
        print('âœ… ç›£æ§æ•¸æ“šæ ¼å¼æ­£ç¢º')
        
except Exception as e:
    print(f'âŒ ç›£æ§æ•¸æ“šæ ¼å¼éŒ¯èª¤: {e}')
    sys.exit(1)
"
        
    else
        echo "âŒ æœªç”Ÿæˆç›£æ§æ•¸æ“š"
        exit 1
    fi
else
    echo "âŒ ç›£æ§æ•¸æ“šç›®éŒ„ä¸å­˜åœ¨"
    exit 1
fi
```

#### 2.2 å‘Šè­¦æ©Ÿåˆ¶æ¸¬è©¦
```python
#!/usr/bin/env python3
# test_alerting_system.py

"""
å‘Šè­¦æ©Ÿåˆ¶æ¸¬è©¦
é©—è­‰å„ç¨®é–¾å€¼æ¢ä»¶ä¸‹çš„å‘Šè­¦è§¸ç™¼
"""

import time
import psutil
from monitoring.performance_monitor import PerformanceMonitor, PerformanceMetrics
from datetime import datetime, timezone

class AlertingSystemTester:
    """å‘Šè­¦ç³»çµ±æ¸¬è©¦å™¨"""
    
    def __init__(self):
        self.monitor = PerformanceMonitor()
        self.test_results = []
    
    def create_test_metrics(self, cpu_percent=10, memory_percent=20, 
                          api_response_times=None):
        """å‰µå»ºæ¸¬è©¦ç”¨çš„æ€§èƒ½æŒ‡æ¨™"""
        if api_response_times is None:
            api_response_times = {
                "http://localhost:8080/health": 0.05,
                "http://localhost:8888/health": 0.03
            }
        
        return PerformanceMetrics(
            timestamp=datetime.now(timezone.utc).isoformat(),
            cpu_percent=cpu_percent,
            memory_percent=memory_percent,
            memory_used_mb=1024,
            disk_usage_percent=50,
            api_response_times=api_response_times,
            active_connections=10,
            error_count=0
        )
    
    def test_cpu_alert(self):
        """æ¸¬è©¦ CPU å‘Šè­¦"""
        print("ğŸ§ª æ¸¬è©¦ CPU é«˜ä½¿ç”¨ç‡å‘Šè­¦...")
        
        # æ­£å¸¸æƒ…æ³ - ä¸æ‡‰è§¸ç™¼å‘Šè­¦
        normal_metrics = self.create_test_metrics(cpu_percent=50)
        alerts = self.monitor._generate_alerts([normal_metrics])
        
        if not any("CPU" in alert for alert in alerts):
            print("âœ… æ­£å¸¸ CPU ä½¿ç”¨ç‡æœªè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("CPUæ­£å¸¸", True))
        else:
            print("âŒ æ­£å¸¸ CPU ä½¿ç”¨ç‡éŒ¯èª¤è§¸ç™¼å‘Šè­¦")
            self.test_results.append(("CPUæ­£å¸¸", False))
        
        # é«˜ä½¿ç”¨ç‡ - æ‡‰è§¸ç™¼å‘Šè­¦
        high_cpu_metrics = self.create_test_metrics(cpu_percent=85)
        alerts = self.monitor._generate_alerts([high_cpu_metrics])
        
        if any("CPU" in alert for alert in alerts):
            print("âœ… é«˜ CPU ä½¿ç”¨ç‡æ­£ç¢ºè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("CPUé«˜ä½¿ç”¨ç‡", True))
        else:
            print("âŒ é«˜ CPU ä½¿ç”¨ç‡æœªè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("CPUé«˜ä½¿ç”¨ç‡", False))
    
    def test_memory_alert(self):
        """æ¸¬è©¦è¨˜æ†¶é«”å‘Šè­¦"""
        print("ğŸ§ª æ¸¬è©¦è¨˜æ†¶é«”é«˜ä½¿ç”¨ç‡å‘Šè­¦...")
        
        # æ­£å¸¸æƒ…æ³
        normal_metrics = self.create_test_metrics(memory_percent=70)
        alerts = self.monitor._generate_alerts([normal_metrics])
        
        if not any("è¨˜æ†¶é«”" in alert for alert in alerts):
            print("âœ… æ­£å¸¸è¨˜æ†¶é«”ä½¿ç”¨ç‡æœªè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("è¨˜æ†¶é«”æ­£å¸¸", True))
        else:
            print("âŒ æ­£å¸¸è¨˜æ†¶é«”ä½¿ç”¨ç‡éŒ¯èª¤è§¸ç™¼å‘Šè­¦")
            self.test_results.append(("è¨˜æ†¶é«”æ­£å¸¸", False))
        
        # é«˜ä½¿ç”¨ç‡
        high_memory_metrics = self.create_test_metrics(memory_percent=90)
        alerts = self.monitor._generate_alerts([high_memory_metrics])
        
        if any("è¨˜æ†¶é«”" in alert for alert in alerts):
            print("âœ… é«˜è¨˜æ†¶é«”ä½¿ç”¨ç‡æ­£ç¢ºè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("è¨˜æ†¶é«”é«˜ä½¿ç”¨ç‡", True))
        else:
            print("âŒ é«˜è¨˜æ†¶é«”ä½¿ç”¨ç‡æœªè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("è¨˜æ†¶é«”é«˜ä½¿ç”¨ç‡", False))
    
    def test_api_response_alert(self):
        """æ¸¬è©¦ API éŸ¿æ‡‰æ™‚é–“å‘Šè­¦"""
        print("ğŸ§ª æ¸¬è©¦ API éŸ¿æ‡‰æ™‚é–“å‘Šè­¦...")
        
        # æ­£å¸¸éŸ¿æ‡‰æ™‚é–“
        normal_api_times = {
            "http://localhost:8080/health": 0.05,
            "http://localhost:8888/health": 0.03
        }
        normal_metrics = self.create_test_metrics(api_response_times=normal_api_times)
        alerts = self.monitor._generate_alerts([normal_metrics])
        
        if not any("API éŸ¿æ‡‰" in alert for alert in alerts):
            print("âœ… æ­£å¸¸ API éŸ¿æ‡‰æ™‚é–“æœªè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("APIéŸ¿æ‡‰æ­£å¸¸", True))
        else:
            print("âŒ æ­£å¸¸ API éŸ¿æ‡‰æ™‚é–“éŒ¯èª¤è§¸ç™¼å‘Šè­¦")
            self.test_results.append(("APIéŸ¿æ‡‰æ­£å¸¸", False))
        
        # æ…¢éŸ¿æ‡‰æ™‚é–“
        slow_api_times = {
            "http://localhost:8080/health": 0.8,  # 800msï¼Œè¶…é500msé–¾å€¼
            "http://localhost:8888/health": 0.03
        }
        slow_metrics = self.create_test_metrics(api_response_times=slow_api_times)
        alerts = self.monitor._generate_alerts([slow_metrics])
        
        if any("API éŸ¿æ‡‰ç·©æ…¢" in alert for alert in alerts):
            print("âœ… æ…¢ API éŸ¿æ‡‰æ™‚é–“æ­£ç¢ºè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("APIéŸ¿æ‡‰ç·©æ…¢", True))
        else:
            print("âŒ æ…¢ API éŸ¿æ‡‰æ™‚é–“æœªè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("APIéŸ¿æ‡‰ç·©æ…¢", False))
        
        # API ç„¡éŸ¿æ‡‰
        failed_api_times = {
            "http://localhost:8080/health": -1,  # è¡¨ç¤ºå¤±æ•—
            "http://localhost:8888/health": 0.03
        }
        failed_metrics = self.create_test_metrics(api_response_times=failed_api_times)
        alerts = self.monitor._generate_alerts([failed_metrics])
        
        if any("API ç„¡éŸ¿æ‡‰" in alert for alert in alerts):
            print("âœ… API ç„¡éŸ¿æ‡‰æ­£ç¢ºè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("APIç„¡éŸ¿æ‡‰", True))
        else:
            print("âŒ API ç„¡éŸ¿æ‡‰æœªè§¸ç™¼å‘Šè­¦")
            self.test_results.append(("APIç„¡éŸ¿æ‡‰", False))
    
    def test_alert_threshold_accuracy(self):
        """æ¸¬è©¦å‘Šè­¦é–¾å€¼ç²¾ç¢ºæ€§"""
        print("ğŸ§ª æ¸¬è©¦å‘Šè­¦é–¾å€¼ç²¾ç¢ºæ€§...")
        
        # æ¸¬è©¦é‚Šç•Œå€¼
        boundary_tests = [
            (79.9, "CPU", False),  # æ¥è¿‘ä½†æœªé”åˆ°80%é–¾å€¼
            (80.1, "CPU", True),   # è¶…é80%é–¾å€¼
            (84.9, "è¨˜æ†¶é«”", False), # æ¥è¿‘ä½†æœªé”åˆ°85%é–¾å€¼
            (85.1, "è¨˜æ†¶é«”", True),  # è¶…é85%é–¾å€¼
        ]
        
        for value, metric_type, should_alert in boundary_tests:
            if metric_type == "CPU":
                test_metrics = self.create_test_metrics(cpu_percent=value)
                keyword = "CPU"
            else:
                test_metrics = self.create_test_metrics(memory_percent=value)
                keyword = "è¨˜æ†¶é«”"
            
            alerts = self.monitor._generate_alerts([test_metrics])
            has_alert = any(keyword in alert for alert in alerts)
            
            if has_alert == should_alert:
                print(f"âœ… {metric_type} {value}% é–¾å€¼æ¸¬è©¦é€šé")
                self.test_results.append((f"{metric_type}é–¾å€¼{value}%", True))
            else:
                print(f"âŒ {metric_type} {value}% é–¾å€¼æ¸¬è©¦å¤±æ•—")
                self.test_results.append((f"{metric_type}é–¾å€¼{value}%", False))
    
    def run_all_tests(self):
        """åŸ·è¡Œæ‰€æœ‰å‘Šè­¦æ¸¬è©¦"""
        print("ğŸš¨ é–‹å§‹å‘Šè­¦æ©Ÿåˆ¶æ¸¬è©¦...")
        
        self.test_cpu_alert()
        self.test_memory_alert()
        self.test_api_response_alert()
        self.test_alert_threshold_accuracy()
        
        # ç”Ÿæˆæ¸¬è©¦å ±å‘Š
        self.generate_test_report()
    
    def generate_test_report(self):
        """ç”Ÿæˆæ¸¬è©¦å ±å‘Š"""
        print("\nğŸ“‹ å‘Šè­¦æ¸¬è©¦å ±å‘Š")
        print("=" * 40)
        
        passed_tests = sum(1 for _, result in self.test_results if result)
        total_tests = len(self.test_results)
        
        print(f"ç¸½æ¸¬è©¦æ•¸: {total_tests}")
        print(f"é€šéæ¸¬è©¦: {passed_tests}")
        print(f"å¤±æ•—æ¸¬è©¦: {total_tests - passed_tests}")
        print(f"é€šéç‡: {passed_tests/total_tests*100:.1f}%")
        
        print("\nè©³ç´°çµæœ:")
        for test_name, result in self.test_results:
            status = "âœ…" if result else "âŒ"
            print(f"  {status} {test_name}")
        
        if passed_tests == total_tests:
            print("\nğŸ‰ æ‰€æœ‰å‘Šè­¦æ¸¬è©¦é€šéï¼")
            return True
        else:
            print(f"\nâš ï¸ {total_tests - passed_tests} å€‹æ¸¬è©¦å¤±æ•—")
            return False

if __name__ == "__main__":
    tester = AlertingSystemTester()
    success = tester.run_all_tests()
    exit(0 if success else 1)
```

### 3. é…ç½®ç®¡ç†æ¸¬è©¦

#### 3.1 å¤šç’°å¢ƒé…ç½®æ¸¬è©¦
```bash
#!/bin/bash
# test_environment_config.sh

echo "âš™ï¸ æ¸¬è©¦å¤šç’°å¢ƒé…ç½®ç®¡ç†..."

# æ¸¬è©¦ç’°å¢ƒåˆ—è¡¨
environments=("development" "testing" "staging" "production")

for env in "${environments[@]}"; do
    echo "æ¸¬è©¦ $env ç’°å¢ƒé…ç½®..."
    
    # è¨­ç½®ç’°å¢ƒè®Šæ•¸
    export NTN_ENVIRONMENT=$env
    
    # æ¸¬è©¦é…ç½®è¼‰å…¥
    python -c "
from config.environment_config import CONFIG_MANAGER
import sys

try:
    config = CONFIG_MANAGER.load_environment_config()
    print(f'âœ… {env} ç’°å¢ƒé…ç½®è¼‰å…¥æˆåŠŸ')
    
    # é©—è­‰é…ç½®çµæ§‹
    required_sections = ['database', 'api', 'satellite']
    for section in required_sections:
        if section not in config:
            print(f'âŒ {env} ç’°å¢ƒç¼ºå°‘ {section} é…ç½®')
            sys.exit(1)
    
    # é©—è­‰é…ç½®å€¼çš„åˆç†æ€§
    db_config = CONFIG_MANAGER.get_database_config()
    api_config = CONFIG_MANAGER.get_api_config()
    sat_config = CONFIG_MANAGER.get_satellite_config()
    
    if sat_config.max_candidates > 8:
        print(f'âŒ {env} ç’°å¢ƒå€™é¸è¡›æ˜Ÿæ•¸è¶…éé™åˆ¶')
        sys.exit(1)
    
    print(f'âœ… {env} ç’°å¢ƒé…ç½®é©—è­‰é€šé')
    
except Exception as e:
    print(f'âŒ {env} ç’°å¢ƒé…ç½®éŒ¯èª¤: {e}')
    sys.exit(1)
"
    
    if [ $? -ne 0 ]; then
        echo "âŒ $env ç’°å¢ƒé…ç½®æ¸¬è©¦å¤±æ•—"
        exit 1
    fi
done

echo "âœ… æ‰€æœ‰ç’°å¢ƒé…ç½®æ¸¬è©¦é€šé"

# æ¸¬è©¦é…ç½®é©—è­‰åŠŸèƒ½
echo "æ¸¬è©¦é…ç½®é©—è­‰åŠŸèƒ½..."
python -c "
from config.environment_config import CONFIG_MANAGER

if CONFIG_MANAGER.validate_current_config():
    print('âœ… é…ç½®é©—è­‰åŠŸèƒ½æ­£å¸¸')
else:
    print('âŒ é…ç½®é©—è­‰åŠŸèƒ½ç•°å¸¸')
    exit(1)
"
```

### 4. é•·æœŸç©©å®šæ€§æ¸¬è©¦

#### 4.1 æŒçºŒé‹è¡Œæ¸¬è©¦
```bash
#!/bin/bash
# test_long_term_stability.sh

echo "â³ é–‹å§‹é•·æœŸç©©å®šæ€§æ¸¬è©¦ (24å°æ™‚)..."

# å‰µå»ºæ¸¬è©¦æ—¥èªŒç›®éŒ„
mkdir -p stability_test_logs
cd stability_test_logs

# å•Ÿå‹•ç›£æ§
python ../monitoring/performance_monitor.py --interval 300 > monitor.log 2>&1 &  # 5åˆ†é˜é–“éš”
monitor_pid=$!

echo "æ€§èƒ½ç›£æ§å·²å•Ÿå‹• (PID: $monitor_pid)"

# æŒçºŒå£“åŠ›æ¸¬è©¦å‡½æ•¸
run_stress_test() {
    local duration=$1
    local end_time=$((SECONDS + duration))
    
    while [ $SECONDS -lt $end_time ]; do
        # API å£“åŠ›æ¸¬è©¦
        for i in {1..5}; do
            curl -s http://localhost:8080/health > /dev/null &
            curl -s http://localhost:8888/health > /dev/null &
            curl -s http://localhost:8080/api/v1/satellites/unified/health > /dev/null &
        done
        
        # ç­‰å¾…æ‰€æœ‰è«‹æ±‚å®Œæˆ
        wait
        
        # ä¼‘æ¯ä¸€æ®µæ™‚é–“
        sleep 60
    done
}

# è¨˜éŒ„é–‹å§‹æ™‚é–“
start_time=$(date)
echo "ç©©å®šæ€§æ¸¬è©¦é–‹å§‹æ™‚é–“: $start_time"

# é‹è¡Œ24å°æ™‚æ¸¬è©¦ (86400ç§’)
# ç‚ºäº†æ¼”ç¤ºï¼Œé€™è£¡ä½¿ç”¨è¼ƒçŸ­æ™‚é–“ (3600ç§’ = 1å°æ™‚)
test_duration=3600

echo "é‹è¡Œ $test_duration ç§’çš„ç©©å®šæ€§æ¸¬è©¦..."
run_stress_test $test_duration

# åœæ­¢ç›£æ§
kill $monitor_pid
echo "ç©©å®šæ€§æ¸¬è©¦å®Œæˆ"

# åˆ†æçµæœ
echo "åˆ†æç©©å®šæ€§æ¸¬è©¦çµæœ..."
python -c "
import json
import glob
from pathlib import Path
from datetime import datetime

# è¼‰å…¥æ‰€æœ‰ç›£æ§æ•¸æ“š
monitor_files = glob.glob('../monitoring/data/metrics_*.json')
monitor_files.sort()

if not monitor_files:
    print('âŒ æœªæ‰¾åˆ°ç›£æ§æ•¸æ“š')
    exit(1)

print(f'æ‰¾åˆ° {len(monitor_files)} å€‹ç›£æ§æ•¸æ“šæ–‡ä»¶')

# åˆ†ææ•¸æ“š
cpu_values = []
memory_values = []
api_failures = 0
total_api_checks = 0

for file_path in monitor_files:
    try:
        with open(file_path) as f:
            data = json.load(f)
        
        cpu_values.append(data['cpu_percent'])
        memory_values.append(data['memory_percent'])
        
        # æª¢æŸ¥ API éŸ¿æ‡‰
        for endpoint, response_time in data['api_response_times'].items():
            total_api_checks += 1
            if response_time == -1:  # å¤±æ•—
                api_failures += 1
                
    except Exception as e:
        print(f'è®€å–ç›£æ§æ•¸æ“šå¤±æ•—: {e}')

if cpu_values and memory_values:
    avg_cpu = sum(cpu_values) / len(cpu_values)
    max_cpu = max(cpu_values)
    avg_memory = sum(memory_values) / len(memory_values)
    max_memory = max(memory_values)
    
    api_success_rate = (total_api_checks - api_failures) / total_api_checks * 100 if total_api_checks > 0 else 0
    
    print(f'å¹³å‡ CPU ä½¿ç”¨ç‡: {avg_cpu:.2f}%')
    print(f'æœ€é«˜ CPU ä½¿ç”¨ç‡: {max_cpu:.2f}%')
    print(f'å¹³å‡è¨˜æ†¶é«”ä½¿ç”¨ç‡: {avg_memory:.2f}%')
    print(f'æœ€é«˜è¨˜æ†¶é«”ä½¿ç”¨ç‡: {max_memory:.2f}%')
    print(f'API æˆåŠŸç‡: {api_success_rate:.2f}%')
    
    # è©•ä¼°ç©©å®šæ€§
    stability_issues = []
    
    if max_cpu > 90:
        stability_issues.append('CPU ä½¿ç”¨ç‡éé«˜')
    
    if max_memory > 90:
        stability_issues.append('è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜')
    
    if api_success_rate < 99:
        stability_issues.append('API å¯ç”¨æ€§ä¸è¶³')
    
    if stability_issues:
        print(f'âŒ ç©©å®šæ€§å•é¡Œ: {stability_issues}')
        exit(1)
    else:
        print('âœ… ç³»çµ±ç©©å®šæ€§è‰¯å¥½')
else:
    print('âŒ ç„¡è¶³å¤ æ•¸æ“šé€²è¡Œåˆ†æ')
    exit(1)
"

echo "ç©©å®šæ€§æ¸¬è©¦å ±å‘Šå·²ç”Ÿæˆ"
```

## âœ… Phase 2 æ¸¬è©¦å®Œæˆæª¢æŸ¥æ¸…å–®

### å¿…é ˆé€šéçš„æ¸¬è©¦
- [ ] æ¸¬è©¦è¦†è“‹ç‡ â‰¥ 90%
- [ ] æ‰€æœ‰æ–°å¢æ¸¬è©¦æ¡ˆä¾‹é€šé
- [ ] æ€§èƒ½ç›£æ§æ•¸æ“šæ”¶é›†æ­£å¸¸
- [ ] å‘Šè­¦æ©Ÿåˆ¶è§¸ç™¼æº–ç¢º
- [ ] å¤šç’°å¢ƒé…ç½®æ­£ç¢ºè¼‰å…¥
- [ ] é•·æœŸç©©å®šæ€§æ¸¬è©¦ç„¡å•é¡Œ

### å“è³ªæŒ‡æ¨™æª¢æŸ¥
- [ ] æ¸¬è©¦åŸ·è¡Œæ™‚é–“ < 10åˆ†é˜
- [ ] æ¸¬è©¦å“è³ªåˆ†æ•¸ â‰¥ 80åˆ†
- [ ] API æˆåŠŸç‡ â‰¥ 99%
- [ ] ç³»çµ±å¯ç”¨æ€§ â‰¥ 99.9%

### æ–‡æª”é©—è­‰
- [ ] æ‰€æœ‰æ–‡æª”æ›´æ–°æº–ç¢º
- [ ] æ•…éšœæ’é™¤æŒ‡å—æœ‰æ•ˆ
- [ ] é–‹ç™¼è€…æŒ‡å—å¯ç”¨
- [ ] é…ç½®èªªæ˜å®Œæ•´

---

**æ³¨æ„**: Phase 2 çš„æ¸¬è©¦é‡é»æ˜¯é©—è­‰æ–°å»ºç«‹çš„åŸºç¤è¨­æ–½èƒ½å¤ æ”¯æ´é•·æœŸç©©å®šé‹è¡Œï¼Œç‚ºå¾ŒçºŒçš„ç³»çµ±å„ªåŒ–å’Œæ“´å±•å¥ å®šåŸºç¤ã€‚