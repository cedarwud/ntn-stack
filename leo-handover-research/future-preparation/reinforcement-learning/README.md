# ğŸ¤– å¼·åŒ–å­¸ç¿’ç³»çµ±æº–å‚™ (Phase 5+)

## ğŸ“‹ ç¸½è¦½

åŸºæ–¼å®Œæˆçš„ Phase 0-4 åŸºç¤æ¶æ§‹ï¼Œæº–å‚™å¯¦ç¾æ·±åº¦å¼·åŒ–å­¸ç¿’ (RL) é©…å‹•çš„æ™ºèƒ½æ›æ‰‹æ±ºç­–ç³»çµ±ã€‚

### ğŸ¯ RL ç³»çµ±ç›®æ¨™
- **æ™ºèƒ½æ±ºç­–**: åŸºæ–¼ç’°å¢ƒç‹€æ…‹çš„æœ€å„ªæ›æ‰‹ç­–ç•¥
- **å¤šç›®æ¨™å„ªåŒ–**: å¹³è¡¡å»¶é²ã€ååé‡ã€åˆ‡æ›æ¬¡æ•¸
- **è‡ªé©æ‡‰å­¸ç¿’**: æŒçºŒå„ªåŒ–æ±ºç­–å“è³ª
- **å¤šä»£ç†å”èª¿**: æ”¯æ´å¤šç”¨æˆ¶å ´æ™¯å”èª¿æ›æ‰‹

---

## ğŸ“ RL ç³»çµ±æ¶æ§‹

### **1. Deep Q-Network (DQN) å–®ç”¨æˆ¶å„ªåŒ–** ğŸ§ 
- **æ–‡æª”**: [dqn-single-user.md](dqn-single-user.md)
- **æ‡‰ç”¨**: å–®ç”¨æˆ¶æœ€å„ªæ›æ‰‹æ™‚æ©Ÿæ±ºç­–
- **ç‹€æ…‹ç©ºé–“**: RSRPã€è·é›¢ã€ä»°è§’ã€æ­·å²åˆ‡æ›
- **å„ªå…ˆç´š**: Phase 5 é¦–è¦å¯¦ç¾

### **2. Multi-Agent RL å¤šç”¨æˆ¶å”èª¿** ğŸ¤
- **æ–‡æª”**: [multi-agent-coordination.md](multi-agent-coordination.md)
- **æ‡‰ç”¨**: å¤šç”¨æˆ¶è¡çªé¿å…èˆ‡è² è¼‰å¹³è¡¡
- **æ¼”ç®—æ³•**: MA-PPOã€QMIXã€MADDPG
- **å„ªå…ˆç´š**: Phase 6 ä¸»è¦ç›®æ¨™

### **3. Graph Neural Network æ‹“æ’²æ„ŸçŸ¥** ğŸŒ
- **æ–‡æª”**: [gnn-topology-aware.md](gnn-topology-aware.md)
- **æ‡‰ç”¨**: æ˜Ÿåº§æ‹“æ’²è®ŠåŒ–çš„æ„ŸçŸ¥æ±ºç­–
- **æŠ€è¡“**: Graph Attention Networkã€GraphSAINT
- **å„ªå…ˆç´š**: Phase 7 é€²éšåŠŸèƒ½

### **4. é æ¸¬æ€§åˆ‡æ› LSTM/GRU** ğŸ”®
- **æ–‡æª”**: [predictive-handover.md](predictive-handover.md)  
- **æ‡‰ç”¨**: è»Œè·¡é æ¸¬èˆ‡é å…ˆæº–å‚™åˆ‡æ›
- **æŠ€è¡“**: LSTMã€GRUã€Transformer
- **å„ªå…ˆç´š**: Phase 6 è¼”åŠ©åŠŸèƒ½

---

## ğŸ—ï¸ RL åŸºç¤æ¶æ§‹è¨­è¨ˆ

### **ç‹€æ…‹ç©ºé–“å®šç¾©**
```python
class HandoverStateSpace:
    """
    æ›æ‰‹æ±ºç­–ç‹€æ…‹ç©ºé–“
    åŸºæ–¼ SIB19 å¢å¼·çš„ç³»çµ±ç‹€æ…‹
    """
    
    def __init__(self):
        self.state_dim = 32  # ç‹€æ…‹ç¶­åº¦
        
    def get_state_vector(self, timestamp, satellite_data, ue_context):
        """
        ç²å–ç•¶å‰ç‹€æ…‹å‘é‡
        """
        state = np.zeros(self.state_dim)
        
        # 1. æœå‹™è¡›æ˜Ÿç‹€æ…‹ (8ç¶­)
        serving_satellite = ue_context['serving_satellite']
        state[0] = serving_satellite['rsrp_dbm'] / 100.0  # æ­¸ä¸€åŒ– RSRP
        state[1] = serving_satellite['elevation_deg'] / 90.0
        state[2] = serving_satellite['distance_km'] / 2000.0
        state[3] = serving_satellite['doppler_offset_hz'] / 100000.0
        state[4] = serving_satellite['link_quality_index']  # 0-1
        state[5] = serving_satellite['load_factor']  # 0-1
        state[6] = serving_satellite['reliability_score']  # 0-1
        state[7] = serving_satellite['remaining_visibility_sec'] / 3600.0
        
        # 2. æœ€ä½³å€™é¸è¡›æ˜Ÿç‹€æ…‹ (8ç¶­)
        best_candidate = self._get_best_candidate(satellite_data)
        if best_candidate:
            state[8] = best_candidate['rsrp_dbm'] / 100.0
            state[9] = best_candidate['elevation_deg'] / 90.0
            state[10] = best_candidate['distance_km'] / 2000.0
            state[11] = best_candidate['doppler_offset_hz'] / 100000.0
            state[12] = best_candidate['link_quality_index']
            state[13] = best_candidate['load_factor']
            state[14] = best_candidate['reliability_score']
            state[15] = best_candidate['visibility_duration_sec'] / 3600.0
        
        # 3. ç¶²è·¯ç‹€æ…‹ (8ç¶­)
        network_state = ue_context['network_state']
        state[16] = network_state['total_load'] / 100.0
        state[17] = network_state['handover_rate'] / 10.0  # æ¯åˆ†é˜åˆ‡æ›æ¬¡æ•¸
        state[18] = network_state['packet_loss_rate']
        state[19] = network_state['average_latency_ms'] / 1000.0
        state[20] = network_state['throughput_mbps'] / 1000.0
        state[21] = network_state['congestion_level']  # 0-1
        state[22] = network_state['interference_level']  # 0-1
        state[23] = network_state['weather_impact_factor']  # 0-2
        
        # 4. æ­·å²æ›æ‰‹çµ±è¨ˆ (8ç¶­)
        handover_history = ue_context['handover_history']
        state[24] = handover_history['recent_handover_count'] / 10.0
        state[25] = handover_history['success_rate']
        state[26] = handover_history['average_handover_time_ms'] / 1000.0
        state[27] = handover_history['ping_pong_rate']
        state[28] = handover_history['last_handover_time_sec'] / 3600.0
        state[29] = handover_history['handover_urgency_score']  # 0-1
        state[30] = handover_history['user_mobility_pattern']  # 0-1
        state[31] = handover_history['service_continuity_score']  # 0-1
        
        return state
        
    def _get_best_candidate(self, satellite_data):
        """é¸æ“‡æœ€ä½³å€™é¸è¡›æ˜Ÿ"""
        candidates = [s for s in satellite_data if s['elevation_deg'] > 10]
        if not candidates:
            return None
        
        # åŸºæ–¼ç¶œåˆåˆ†æ•¸æ’åº
        scored_candidates = []
        for candidate in candidates:
            score = (
                candidate['rsrp_dbm'] / 100.0 * 0.4 +
                candidate['elevation_deg'] / 90.0 * 0.3 +
                (1 - candidate['load_factor']) * 0.2 +
                candidate['reliability_score'] * 0.1
            )
            scored_candidates.append((score, candidate))
        
        return max(scored_candidates, key=lambda x: x[0])[1]
```

### **å‹•ä½œç©ºé–“å®šç¾©**
```python
class HandoverActionSpace:
    """
    æ›æ‰‹æ±ºç­–å‹•ä½œç©ºé–“
    """
    
    def __init__(self):
        self.action_dim = 4
        self.actions = {
            0: 'MAINTAIN',      # ç¶­æŒç•¶å‰é€£æ¥
            1: 'HANDOVER',      # åŸ·è¡Œæ›æ‰‹
            2: 'PREPARE',       # æº–å‚™æ›æ‰‹ (é é…ç½®)
            3: 'EMERGENCY'      # ç·Šæ€¥æ›æ‰‹
        }
        
    def get_valid_actions(self, state_vector, satellite_data):
        """
        ç²å–ç•¶å‰ç‹€æ…‹ä¸‹çš„æœ‰æ•ˆå‹•ä½œ
        """
        valid_actions = [0]  # MAINTAIN ç¸½æ˜¯æœ‰æ•ˆ
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨å€™é¸è¡›æ˜Ÿ
        candidates = [s for s in satellite_data if s['elevation_deg'] > 10]
        if candidates:
            valid_actions.extend([1, 2])  # HANDOVER, PREPARE
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦ç·Šæ€¥æ›æ‰‹
        serving_rsrp = state_vector[0] * 100.0
        if serving_rsrp < -115:  # ä¿¡è™Ÿæ¥µå¼±
            valid_actions.append(3)  # EMERGENCY
        
        return valid_actions
```

### **çå‹µå‡½æ•¸è¨­è¨ˆ**
```python
class HandoverRewardFunction:
    """
    æ›æ‰‹æ±ºç­–çå‹µå‡½æ•¸
    å¤šç›®æ¨™å¹³è¡¡ï¼šQoS + æ•ˆç‡ + ç©©å®šæ€§
    """
    
    def __init__(self):
        self.weights = {
            'qos': 0.4,         # æœå‹™å“è³ª
            'efficiency': 0.3,   # åˆ‡æ›æ•ˆç‡
            'stability': 0.3     # é€£æ¥ç©©å®šæ€§
        }
        
    def calculate_reward(self, prev_state, action, new_state, handover_result):
        """
        è¨ˆç®—çå‹µå€¼
        """
        # 1. QoS çå‹µ
        qos_reward = self._calculate_qos_reward(prev_state, new_state)
        
        # 2. æ•ˆç‡çå‹µ
        efficiency_reward = self._calculate_efficiency_reward(
            action, handover_result)
        
        # 3. ç©©å®šæ€§çå‹µ
        stability_reward = self._calculate_stability_reward(
            prev_state, action, new_state)
        
        # åŠ æ¬Šç¸½çå‹µ
        total_reward = (
            self.weights['qos'] * qos_reward +
            self.weights['efficiency'] * efficiency_reward +
            self.weights['stability'] * stability_reward
        )
        
        return {
            'total_reward': total_reward,
            'qos_reward': qos_reward,
            'efficiency_reward': efficiency_reward,
            'stability_reward': stability_reward
        }
    
    def _calculate_qos_reward(self, prev_state, new_state):
        """
        QoS æ”¹å–„çå‹µ
        """
        # RSRP æ”¹å–„
        prev_rsrp = prev_state[0] * 100.0
        new_rsrp = new_state[0] * 100.0
        rsrp_improvement = (new_rsrp - prev_rsrp) / 10.0  # æ­¸ä¸€åŒ–åˆ° Â±10dB
        
        # å»¶é²æ”¹å–„
        prev_latency = prev_state[19] * 1000.0
        new_latency = new_state[19] * 1000.0
        latency_improvement = (prev_latency - new_latency) / 100.0  # æ­¸ä¸€åŒ–åˆ° Â±100ms
        
        # ååé‡æ”¹å–„
        prev_throughput = prev_state[20] * 1000.0
        new_throughput = new_state[20] * 1000.0
        throughput_improvement = (new_throughput - prev_throughput) / 100.0
        
        qos_reward = (
            rsrp_improvement * 0.5 +
            latency_improvement * 0.3 +
            throughput_improvement * 0.2
        )
        
        return np.clip(qos_reward, -1.0, 1.0)
    
    def _calculate_efficiency_reward(self, action, handover_result):
        """
        åˆ‡æ›æ•ˆç‡çå‹µ
        """
        if action == 0:  # MAINTAIN
            return 0.1  # å°æ­£çå‹µï¼šä¸å¿…è¦çš„åˆ‡æ›
        
        if action in [1, 3]:  # HANDOVER, EMERGENCY
            if handover_result and handover_result['success']:
                # æˆåŠŸåˆ‡æ›çå‹µ
                handover_time = handover_result.get('handover_time_ms', 500)
                time_efficiency = max(0, (1000 - handover_time) / 1000)
                return 0.5 + 0.5 * time_efficiency
            else:
                # åˆ‡æ›å¤±æ•—æ‡²ç½°
                return -0.8
        
        if action == 2:  # PREPARE
            # æº–å‚™å‹•ä½œå°çå‹µ
            return 0.2
        
        return 0.0
    
    def _calculate_stability_reward(self, prev_state, action, new_state):
        """
        é€£æ¥ç©©å®šæ€§çå‹µ
        """
        # Ping-pong æ‡²ç½°
        handover_count = new_state[24] * 10.0
        if handover_count > 5:  # 5åˆ†é˜å…§è¶…é5æ¬¡åˆ‡æ›
            ping_pong_penalty = -0.5 * (handover_count - 5) / 5
        else:
            ping_pong_penalty = 0.0
        
        # æœå‹™é€£çºŒæ€§çå‹µ
        service_continuity = new_state[31]
        continuity_reward = (service_continuity - 0.5) * 0.4
        
        # é æ¸¬æº–ç¢ºæ€§çå‹µ (å¦‚æœæ˜¯é å…ˆæº–å‚™çš„åˆ‡æ›)
        if action == 2 and prev_state[29] > 0.7:  # é«˜ç·Šæ€¥åº¦ä¸‹æº–å‚™
            prediction_reward = 0.3
        else:
            prediction_reward = 0.0
        
        stability_reward = ping_pong_penalty + continuity_reward + prediction_reward
        
        return np.clip(stability_reward, -1.0, 1.0)
```

---

## ğŸ”§ RL è¨“ç·´ç’°å¢ƒ

### **ä»¿çœŸç’°å¢ƒè¨­è¨ˆ**
```python
class LEOHandoverEnvironment(gym.Env):
    """
    LEO è¡›æ˜Ÿæ›æ‰‹ RL è¨“ç·´ç’°å¢ƒ
    """
    
    def __init__(self, scenario_config):
        super().__init__()
        
        # ç‹€æ…‹å’Œå‹•ä½œç©ºé–“
        self.state_space = HandoverStateSpace()
        self.action_space = HandoverActionSpace()
        self.reward_function = HandoverRewardFunction()
        
        # ä»¿çœŸçµ„ä»¶
        self.orbit_simulator = OrbitSimulator(scenario_config)
        self.network_simulator = NetworkSimulator(scenario_config)
        self.user_mobility = UserMobilityModel(scenario_config)
        
        # Gym ä»‹é¢
        self.observation_space = gym.spaces.Box(
            low=-1, high=1, shape=(32,), dtype=np.float32)
        self.action_space = gym.spaces.Discrete(4)
        
    def reset(self):
        """é‡ç½®ç’°å¢ƒ"""
        # é‡ç½®ä»¿çœŸå™¨
        self.orbit_simulator.reset()
        self.network_simulator.reset()
        self.user_mobility.reset()
        
        # åˆå§‹ç‹€æ…‹
        satellite_data = self.orbit_simulator.get_current_satellites()
        ue_context = self.network_simulator.get_ue_context()
        
        state = self.state_space.get_state_vector(
            time.time(), satellite_data, ue_context)
        
        return state
    
    def step(self, action):
        """åŸ·è¡Œå‹•ä½œä¸¦è¿”å›çµæœ"""
        # è¨˜éŒ„å‰ç‹€æ…‹
        prev_state = self._get_current_state()
        
        # åŸ·è¡Œå‹•ä½œ
        handover_result = self._execute_action(action)
        
        # æ¨é€²ä»¿çœŸ
        self.orbit_simulator.step()
        self.network_simulator.step()
        self.user_mobility.step()
        
        # ç²å–æ–°ç‹€æ…‹
        new_state = self._get_current_state()
        
        # è¨ˆç®—çå‹µ
        reward_info = self.reward_function.calculate_reward(
            prev_state, action, new_state, handover_result)
        
        # æª¢æŸ¥çµæŸæ¢ä»¶
        done = self._check_done()
        
        # æ§‹å»ºè³‡è¨Š
        info = {
            'handover_result': handover_result,
            'reward_breakdown': reward_info,
            'network_stats': self.network_simulator.get_stats()
        }
        
        return new_state, reward_info['total_reward'], done, info
```

---

## ğŸ“Š RL æ¼”ç®—æ³•é¸æ“‡

### **ç®—æ³•å„ªå…ˆç´šæ’åº**

| æ¼”ç®—æ³• | æ‡‰ç”¨å ´æ™¯ | å„ªå…ˆç´š | é–‹ç™¼éšæ®µ |
|--------|----------|--------|----------|
| **DQN** | å–®ç”¨æˆ¶é›¢æ•£å‹•ä½œ | â­â­â­â­â­ | Phase 5 |
| **PPO** | å–®ç”¨æˆ¶é€£çºŒæ§åˆ¶ | â­â­â­â­ | Phase 5 |
| **DDPG** | é€£çºŒå‹•ä½œç©ºé–“ | â­â­â­ | Phase 6 |
| **MA-PPO** | å¤šç”¨æˆ¶å”èª¿ | â­â­â­â­ | Phase 6 |
| **QMIX** | åƒ¹å€¼åˆ†è§£å”èª¿ | â­â­â­ | Phase 6 |
| **Graph RL** | æ‹“æ’²æ„ŸçŸ¥ | â­â­â­â­ | Phase 7 |

---

## ğŸ“… é–‹ç™¼æ™‚ç¨‹è¦åŠƒ

### **Phase 5 (3å€‹æœˆ): DQN åŸºç¤å¯¦ç¾**
- Month 1: ä»¿çœŸç’°å¢ƒæ§‹å»º
- Month 2: DQN æ¼”ç®—æ³•å¯¦ç¾èˆ‡è¨“ç·´
- Month 3: æ€§èƒ½è©•ä¼°èˆ‡å„ªåŒ–

### **Phase 6 (4å€‹æœˆ): å¤šä»£ç†å”èª¿**
- Month 1-2: Multi-Agent ç’°å¢ƒè¨­è¨ˆ
- Month 3: MA-PPO/QMIX å¯¦ç¾
- Month 4: å¤šç”¨æˆ¶å ´æ™¯é©—è­‰

### **Phase 7 (3å€‹æœˆ): é€²éšåŠŸèƒ½**
- Month 1: Graph Neural Network æ•´åˆ
- Month 2: é æ¸¬æ€§åˆ‡æ›å¯¦ç¾
- Month 3: ç³»çµ±æ•´åˆèˆ‡æœ€ä½³åŒ–

---

## ğŸ¯ æˆåŠŸæ¨™æº–

### **Phase 5 ç›®æ¨™**
- [ ] DQN æ”¶æ–‚ï¼šè¨“ç·´ 10M æ­¥å¾Œç©©å®š
- [ ] åˆ‡æ›æˆåŠŸç‡ >98%
- [ ] Ping-pong ç‡ <1%
- [ ] å¹³å‡ QoS æå‡ >20%

### **Phase 6 ç›®æ¨™**
- [ ] å¤šç”¨æˆ¶è¡çªç‡ <5%
- [ ] ç³»çµ±ç¸½ååé‡æå‡ >30%
- [ ] è² è¼‰å¹³è¡¡æ•ˆæœæ˜é¡¯
- [ ] å¯¦æ™‚æ±ºç­–å»¶é² <50ms

### **Phase 7 ç›®æ¨™**
- [ ] æ‹“æ’²è®ŠåŒ–é©æ‡‰æ€§ >95%
- [ ] é æ¸¬æº–ç¢ºç‡ >90%
- [ ] æ•´é«”ç³»çµ±ç©©å®šæ€§é”åˆ°å•†ç”¨ç´š
- [ ] å­¸è¡“è«–æ–‡ç™¼è¡¨å°±ç·’

---

*Reinforcement Learning System Preparation - Generated: 2025-08-01*