# ğŸ“ éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆèˆ‡æ··åˆå­˜å„²

[ğŸ”„ è¿”å›æ•¸æ“šæµç¨‹å°èˆª](../README.md) > éšæ®µäº”

## ğŸ“– éšæ®µæ¦‚è¿°

**ç›®æ¨™**ï¼šå°‡æ‰€æœ‰è™•ç†çµæœæ•´åˆä¸¦å»ºç«‹æ··åˆå­˜å„²æ¶æ§‹  
**è¼¸å…¥**ï¼šéšæ®µå››çš„å‰ç«¯æ™‚é–“åºåˆ—æ•¸æ“šï¼ˆ~60-75MBï¼‰  
**è¼¸å‡º**ï¼šPostgreSQLçµæ§‹åŒ–æ•¸æ“š + Docker Volumeæª”æ¡ˆå­˜å„²  
**å­˜å„²ç¸½é‡**ï¼š~365MB (PostgreSQL ~65MB + Volume ~300MB)  
**è™•ç†æ™‚é–“**ï¼šç´„ 2-3 åˆ†é˜

### ğŸ¯ @doc/todo.md å°æ‡‰å¯¦ç¾
æœ¬éšæ®µæ”¯æ´ä»¥ä¸‹éœ€æ±‚ï¼š
- ğŸ”§ **åˆ†å±¤æ•¸æ“šæº–å‚™**: ç”Ÿæˆ5Â°/10Â°/15Â°ä»°è§’åˆ†å±¤æ•¸æ“šï¼Œæ”¯æ´ä¸åŒä»°è§’é–€æª»éœ€æ±‚
- ğŸ’¾ **æ··åˆå­˜å„²**: PostgreSQLå¿«é€ŸæŸ¥è©¢ + Volumeå¤§å®¹é‡å­˜å„²ï¼Œæ”¯æ´å¼·åŒ–å­¸ç¿’æ•¸æ“šå­˜å–
- ğŸ”— **APIæ¥å£æº–å‚™**: ç‚ºå‹•æ…‹æ± è¦åŠƒå’Œæ›æ‰‹æ±ºç­–æä¾›é«˜æ•ˆæ•¸æ“šè¨ªå•æ¥å£

## ğŸ—ï¸ æ··åˆå­˜å„²æ¶æ§‹

### å­˜å„²ç­–ç•¥åˆ†å·¥
- **PostgreSQL**ï¼šçµæ§‹åŒ–æ•¸æ“šã€ç´¢å¼•æŸ¥è©¢ã€çµ±è¨ˆåˆ†æ
- **Docker Volume**ï¼šå¤§å‹æª”æ¡ˆã€æ™‚é–“åºåˆ—æ•¸æ“šã€å‰ç«¯è³‡æº

### æ•¸æ“šåˆ†é¡åŸå‰‡
```python
STORAGE_STRATEGY = {
    'postgresql': [
        'satellite_metadata',      # è¡›æ˜ŸåŸºæœ¬è³‡è¨Š
        'signal_statistics',       # ä¿¡è™Ÿçµ±è¨ˆæŒ‡æ¨™
        'event_summaries',         # 3GPPäº‹ä»¶æ‘˜è¦
        'performance_metrics'      # ç³»çµ±æ€§èƒ½æŒ‡æ¨™
    ],
    'volume_files': [
        'timeseries_data',         # å®Œæ•´æ™‚é–“åºåˆ—
        'animation_resources',     # å‰ç«¯å‹•ç•«æ•¸æ“š
        'signal_heatmaps',        # ä¿¡è™Ÿç†±åŠ›åœ–
        'orbit_trajectories'       # è»Œé“è»Œè·¡æ•¸æ“š
    ]
}
```

## ğŸ“Š PostgreSQL æ•¸æ“šçµæ§‹

### æ ¸å¿ƒè³‡æ–™è¡¨è¨­è¨ˆ

#### 1. satellite_metadata
```sql
CREATE TABLE satellite_metadata (
    satellite_id VARCHAR(50) PRIMARY KEY,
    constellation VARCHAR(20) NOT NULL,
    norad_id INTEGER UNIQUE,
    tle_epoch TIMESTAMP WITH TIME ZONE,
    orbital_period_minutes NUMERIC(8,3),
    inclination_deg NUMERIC(6,3),
    mean_altitude_km NUMERIC(8,3),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ç´¢å¼•å„ªåŒ–
CREATE INDEX idx_satellite_constellation ON satellite_metadata(constellation);
CREATE INDEX idx_satellite_norad ON satellite_metadata(norad_id);
```

#### 2. signal_quality_statistics
```sql
CREATE TABLE signal_quality_statistics (
    id SERIAL PRIMARY KEY,
    satellite_id VARCHAR(50) REFERENCES satellite_metadata(satellite_id),
    analysis_period_start TIMESTAMP WITH TIME ZONE,
    analysis_period_end TIMESTAMP WITH TIME ZONE,
    mean_rsrp_dbm NUMERIC(6,2),
    std_rsrp_db NUMERIC(5,2),
    max_elevation_deg NUMERIC(5,2),
    total_visible_time_minutes INTEGER,
    handover_event_count INTEGER,
    signal_quality_grade VARCHAR(10), -- 'high', 'medium', 'low'
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- è¤‡åˆç´¢å¼•
CREATE INDEX idx_signal_satellite_period ON signal_quality_statistics(satellite_id, analysis_period_start);
CREATE INDEX idx_signal_quality_grade ON signal_quality_statistics(signal_quality_grade);
```

#### 3. handover_events_summary
```sql
CREATE TABLE handover_events_summary (
    id SERIAL PRIMARY KEY,
    event_type VARCHAR(10) NOT NULL, -- 'A4', 'A5', 'D2'
    serving_satellite_id VARCHAR(50) REFERENCES satellite_metadata(satellite_id),
    neighbor_satellite_id VARCHAR(50) REFERENCES satellite_metadata(satellite_id),
    event_timestamp TIMESTAMP WITH TIME ZONE,
    trigger_rsrp_dbm NUMERIC(6,2),
    handover_decision VARCHAR(20), -- 'trigger', 'hold', 'reject'
    processing_latency_ms INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- äº‹ä»¶æŸ¥è©¢ç´¢å¼•
CREATE INDEX idx_handover_event_type ON handover_events_summary(event_type);
CREATE INDEX idx_handover_timestamp ON handover_events_summary(event_timestamp);
CREATE INDEX idx_handover_serving ON handover_events_summary(serving_satellite_id);
```

## ğŸ“ Docker Volume æª”æ¡ˆçµæ§‹

### Volume çµ„ç¹”æ¶æ§‹
```bash
/app/data/
â”œâ”€â”€ enhanced_timeseries/          # å‰ç«¯å‹•ç•«æ•¸æ“š (~60-75MB)
â”‚   â”œâ”€â”€ animation_enhanced_starlink.json
â”‚   â””â”€â”€ animation_enhanced_oneweb.json
â”‚
â”œâ”€â”€ layered_phase0_enhanced/      # åˆ†å±¤è™•ç†çµæœ (~85MB)
â”‚   â”œâ”€â”€ starlink_5deg_enhanced.json
â”‚   â”œâ”€â”€ starlink_10deg_enhanced.json
â”‚   â”œâ”€â”€ starlink_15deg_enhanced.json
â”‚   â”œâ”€â”€ oneweb_10deg_enhanced.json
â”‚   â”œâ”€â”€ oneweb_15deg_enhanced.json
â”‚   â””â”€â”€ oneweb_20deg_enhanced.json
â”‚
â”œâ”€â”€ handover_scenarios/           # æ›æ‰‹å ´æ™¯æ•¸æ“š (~55MB)
â”‚   â”œâ”€â”€ a4_events_enhanced.json
â”‚   â”œâ”€â”€ a5_events_enhanced.json
â”‚   â”œâ”€â”€ d2_events_enhanced.json
â”‚   â””â”€â”€ best_handover_windows.json
â”‚
â”œâ”€â”€ signal_quality_analysis/      # ä¿¡è™Ÿåˆ†æçµæœ (~65MB)
â”‚   â”œâ”€â”€ signal_heatmap_data.json
â”‚   â”œâ”€â”€ quality_metrics_summary.json
â”‚   â””â”€â”€ constellation_comparison.json
â”‚
â”œâ”€â”€ processing_cache/             # è™•ç†ç·©å­˜ (~35MB)
â”‚   â”œâ”€â”€ sgp4_calculation_cache.json
â”‚   â”œâ”€â”€ filtering_results_cache.json
â”‚   â””â”€â”€ gpp3_event_cache.json
â”‚
â””â”€â”€ status_files/                 # ç‹€æ…‹æ¨™è¨˜æª”æ¡ˆ (~1MB)
    â”œâ”€â”€ last_processing_time.txt
    â”œâ”€â”€ tle_checksum.txt
    â”œâ”€â”€ processing_status.json
    â””â”€â”€ health_check.json
```

## ğŸ”§ æ•´åˆè™•ç†å™¨å¯¦ç¾

### ä¸»è¦å¯¦ç¾ä½ç½®
```bash
# æ•¸æ“šæ•´åˆè™•ç†å™¨
/netstack/src/stages/data_integration_processor.py
â”œâ”€â”€ Stage5IntegrationProcessor.process_enhanced_timeseries()    # å¢å¼·æ™‚é–“åºåˆ—è™•ç†
â”œâ”€â”€ Stage5IntegrationProcessor._integrate_postgresql_data()     # PostgreSQLæ•¸æ“šæ•´åˆ
â”œâ”€â”€ Stage5IntegrationProcessor._generate_layered_data()         # åˆ†å±¤æ•¸æ“šç”Ÿæˆ
â”œâ”€â”€ Stage5IntegrationProcessor._generate_handover_scenarios()   # æ›æ‰‹å ´æ™¯ç”Ÿæˆ
â””â”€â”€ Stage5IntegrationProcessor._verify_mixed_storage_access()   # æ··åˆå­˜å„²é©—è­‰

# è³‡æ–™åº«é€£æ¥ç®¡ç†
/netstack/src/services/database/postgresql_manager.py
â”œâ”€â”€ PostgreSQLManager.setup_connection_pool()              # é€£æ¥æ± ç®¡ç†
â”œâ”€â”€ PostgreSQLManager.execute_batch_insert()               # æ‰¹æ¬¡æ’å…¥å„ªåŒ–
â””â”€â”€ PostgreSQLManager.create_indexes()                     # ç´¢å¼•å»ºç«‹
```

### æ ¸å¿ƒè™•ç†é‚è¼¯
```python
class Stage5IntegrationProcessor:
    
    async def process_enhanced_timeseries(self) -> Dict[str, Any]:
        """åŸ·è¡Œéšæ®µäº”å®Œæ•´æ•´åˆè™•ç†"""
        
        results = {
            "stage": "stage5_integration",
            "start_time": datetime.now(timezone.utc).isoformat(),
            "postgresql_integration": {},
            "layered_data_enhancement": {},
            "handover_scenarios": {},
            "signal_quality_analysis": {},
            "processing_cache": {},
            "status_files": {},
            "mixed_storage_verification": {}
        }
        
        try:
            # 1. è¼‰å…¥å¢å¼·æ™‚é–“åºåˆ—æ•¸æ“š
            enhanced_data = await self._load_enhanced_timeseries()
            
            # 2. PostgreSQL æ•¸æ“šæ•´åˆ
            results["postgresql_integration"] = await self._integrate_postgresql_data(enhanced_data)
            
            # 3. ç”Ÿæˆåˆ†å±¤æ•¸æ“šå¢å¼·
            results["layered_data_enhancement"] = await self._generate_layered_data(enhanced_data)
            
            # 4. ç”Ÿæˆæ›æ‰‹å ´æ™¯å°ˆç”¨æ•¸æ“š
            results["handover_scenarios"] = await self._generate_handover_scenarios(enhanced_data)
            
            # 5. å‰µå»ºä¿¡è™Ÿå“è³ªåˆ†æç›®éŒ„çµæ§‹
            results["signal_quality_analysis"] = await self._setup_signal_analysis_structure(enhanced_data)
            
            # 6. å‰µå»ºè™•ç†ç·©å­˜
            results["processing_cache"] = await self._create_processing_cache(enhanced_data)
            
            # 7. ç”Ÿæˆç‹€æ…‹æ–‡ä»¶
            results["status_files"] = await self._create_status_files()
            
            # 8. é©—è­‰æ··åˆå­˜å„²è¨ªå•æ¨¡å¼
            results["mixed_storage_verification"] = await self._verify_mixed_storage_access()
            
            results["success"] = True
            
        except Exception as e:
            logger.error(f"âŒ éšæ®µäº”è™•ç†å¤±æ•—: {e}")
            results["success"] = False
            results["error"] = str(e)
            
        return results
    
    async def _generate_layered_data(self, enhanced_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†å±¤æ•¸æ“šå¢å¼· - ä¿®æ­£å¾Œçš„ç‰ˆæœ¬"""
        
        self.logger.info("ğŸ”„ ç”Ÿæˆåˆ†å±¤ä»°è§’æ•¸æ“š")
        
        layered_results = {}
        
        for threshold in self.config.elevation_thresholds:
            threshold_dir = Path(self.config.output_layered_dir) / f"elevation_{threshold}deg"
            threshold_dir.mkdir(parents=True, exist_ok=True)
            
            layered_results[f"elevation_{threshold}deg"] = {}
            
            for constellation, data in enhanced_data.items():
                if not data:
                    continue
                
                # ç¯©é¸ç¬¦åˆä»°è§’é–€æª»çš„æ•¸æ“š
                filtered_satellites = []
                
                for satellite in data.get('satellites', []):
                    filtered_timeseries = []
                    
                    # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¢ºçš„æ™‚åºæ•¸æ“šæ¬„ä½åç¨±
                    timeseries_data = satellite.get('position_timeseries', satellite.get('timeseries', []))
                    
                    for point in timeseries_data:
                        if point.get('elevation_deg', 0) >= threshold:
                            filtered_timeseries.append(point)
                    
                    if filtered_timeseries:
                        filtered_satellites.append({
                            **satellite,
                            'position_timeseries': filtered_timeseries  # ä¿æŒåŸå§‹æ¬„ä½åç¨±
                        })
                
                # ç”Ÿæˆåˆ†å±¤æ•¸æ“šæª”æ¡ˆ
                layered_data = {
                    "metadata": {
                        **data.get('metadata', {}),
                        "elevation_threshold_deg": threshold,
                        "filtered_satellites_count": len(filtered_satellites),
                        "stage5_processing_time": datetime.now(timezone.utc).isoformat()
                    },
                    "satellites": filtered_satellites
                }
                
                output_file = threshold_dir / f"{constellation}_with_3gpp_events.json"
                
                with open(output_file, 'w') as f:
                    json.dump(layered_data, f, indent=2)
                
                file_size_mb = output_file.stat().st_size / (1024 * 1024)
                
                layered_results[f"elevation_{threshold}deg"][constellation] = {
                    "file_path": str(output_file),
                    "satellites_count": len(filtered_satellites),
                    "file_size_mb": round(file_size_mb, 2)
                }
                
                self.logger.info(f"âœ… {constellation} {threshold}åº¦: {len(filtered_satellites)} é¡†è¡›æ˜Ÿ, {file_size_mb:.1f}MB")
        
        return layered_results
```

## âš™ï¸ æ€§èƒ½æœ€ä½³åŒ–ç­–ç•¥

### PostgreSQL æœ€ä½³åŒ–
```python
# é€£æ¥æ± é…ç½®
POSTGRESQL_CONFIG = {
    'max_connections': 20,
    'connection_timeout': 30,
    'query_timeout': 60,
    'batch_insert_size': 100,
    'enable_connection_pooling': True
}

# ç´¢å¼•ç­–ç•¥
INDEX_STRATEGY = {
    'primary_indexes': ['satellite_id', 'norad_id'],
    'composite_indexes': [
        ('satellite_id', 'analysis_period_start'),
        ('constellation', 'signal_quality_grade')
    ],
    'partial_indexes': [
        'signal_quality_grade WHERE signal_quality_grade = \'high\''
    ]
}
```

### æª”æ¡ˆI/Oæœ€ä½³åŒ–
```python
# Volumeå¯«å…¥ç­–ç•¥
VOLUME_CONFIG = {
    'write_buffer_size': '64MB',
    'compression_enabled': True,
    'async_write_enabled': True,
    'file_integrity_check': True
}

# JSONåºåˆ—åŒ–æœ€ä½³åŒ–
JSON_CONFIG = {
    'ensure_ascii': False,
    'separators': (',', ':'),  # ç·Šæ¹Šæ ¼å¼
    'sort_keys': False,        # ä¿æŒåŸå§‹é †åº
    'indent': None            # ç„¡ç¸®æ’ç¯€çœç©ºé–“
}
```

## ğŸ“ˆ å­˜å„²çµ±è¨ˆèˆ‡ç›£æ§

### å­˜å„²ä½¿ç”¨åˆ†æ
```python
# é æœŸå­˜å„²åˆ†ä½ˆ
STORAGE_BREAKDOWN = {
    'postgresql_total_mb': 65,
    'postgresql_breakdown': {
        'satellite_metadata': 1.5,     # 391é¡†è¡›æ˜Ÿ Ã— åŸºæœ¬è³‡è¨Š
        'signal_statistics': 25,       # 391é¡† Ã— çµ±è¨ˆæ•¸æ“š
        'handover_events': 18,         # ~1,800å€‹æ›æ‰‹äº‹ä»¶
        'indexes_overhead': 9,         # ç´¢å¼•ç©ºé–“
        'system_metadata': 11.5        # PostgreSQLç³»çµ±é–‹éŠ·
    },
    'volume_total_mb': 300,
    'volume_breakdown': {
        'enhanced_timeseries': 75,     # å‰ç«¯å‹•ç•«æ•¸æ“š
        'layered_phase0': 85,          # åˆ†å±¤è™•ç†çµæœ  
        'handover_scenarios': 55,      # æ›æ‰‹å ´æ™¯
        'signal_analysis': 65,         # ä¿¡è™Ÿåˆ†æ
        'cache_files': 20             # ç·©å­˜æª”æ¡ˆ
    }
}
```

### å¥åº·æª¢æŸ¥æ©Ÿåˆ¶
```bash
# å­˜å„²å¥åº·æª¢æŸ¥è…³æœ¬
#!/bin/bash
echo "ğŸ“Š æ··åˆå­˜å„²å¥åº·æª¢æŸ¥"

# PostgreSQLé€£æ¥æª¢æŸ¥
docker exec netstack-rl-postgres psql -U rl_user -d rl_research -c "SELECT COUNT(*) FROM satellite_metadata;" > /dev/null
if [ $? -eq 0 ]; then
    echo "âœ… PostgreSQL: æ­£å¸¸"
else
    echo "âŒ PostgreSQL: ç•°å¸¸"
fi

# Volumeæª”æ¡ˆæª¢æŸ¥
if [ -f "/app/data/enhanced_timeseries/animation_enhanced_starlink.json" ]; then
    echo "âœ… Volumeæª”æ¡ˆ: æ­£å¸¸"
else
    echo "âŒ Volumeæª”æ¡ˆ: éºå¤±"
fi

# å­˜å„²ç©ºé–“æª¢æŸ¥
volume_usage=$(df -h /app/data | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $volume_usage -lt 80 ]; then
    echo "âœ… å­˜å„²ç©ºé–“: ${volume_usage}% (æ­£å¸¸)"
else
    echo "âš ï¸ å­˜å„²ç©ºé–“: ${volume_usage}% (è­¦å‘Š)"
fi
```

## ğŸ”§ é‡è¦ä¿®å¾©è¨˜éŒ„ (2025-08-18)

### å·²ä¿®å¾©çš„é—œéµå•é¡Œ

#### 1. PostgreSQLé€£æ¥é…ç½®éŒ¯èª¤
**å•é¡Œ**ï¼šStage5Config ä½¿ç”¨ `localhost` è€Œéå®¹å™¨ç¶²è·¯åç¨±  
**ç—‡ç‹€**ï¼šPostgreSQLæ•´åˆå¤±æ•—ï¼Œé€£æ¥è¢«æ‹’  
**ä¿®æ­£**ï¼š
```python
# ä¿®æ­£å‰
postgres_host: str = "localhost"

# ä¿®æ­£å¾Œ  
postgres_host: str = "netstack-postgres"
```

#### 2. æ™‚åºæ•¸æ“šæ¬„ä½åç¨±ä¸ä¸€è‡´
**å•é¡Œ**ï¼šä»£ç¢¼æŸ¥æ‰¾ `timeseries` ä½†æ•¸æ“šä½¿ç”¨ `position_timeseries`  
**ç—‡ç‹€**ï¼šåˆ†å±¤æ¿¾æ³¢ç”¢ç”Ÿ0é¡†è¡›æ˜Ÿ  
**ä¿®æ­£**ï¼š
```python
# ä¿®æ­£å‰
for point in satellite.get('timeseries', []):

# ä¿®æ­£å¾Œ
timeseries_data = satellite.get('position_timeseries', satellite.get('timeseries', []))
for point in timeseries_data:
```

#### 3. åˆ†å±¤æ¿¾æ³¢é‚è¼¯å®Œæ•´ä¿®æ­£
**æˆæœ**ï¼š
- elevation_5deg: 399é¡†è¡›æ˜Ÿ (100%ä¿ç•™)
- elevation_10deg: 351é¡†è¡›æ˜Ÿ (87.9%ä¿ç•™) 
- elevation_15deg: 277é¡†è¡›æ˜Ÿ (69.4%ä¿ç•™)

**æª”æ¡ˆå¤§å°**ï¼š
- Starlink: 4.9MB (5Â°) â†’ 3.5MB (10Â°) â†’ 2.5MB (15Â°)
- OneWeb: 560KB (5Â°) â†’ 477KB (10Â°) â†’ 339KB (15Â°)

### ä¿®å¾©é©—è­‰
```bash
# é©—è­‰åˆ†å±¤æ•¸æ“šç”Ÿæˆ
ls -lh /app/data/layered_phase0_enhanced/elevation_*/

# é©—è­‰PostgreSQLé…ç½®
python -c "from stages.data_integration_processor import Stage5Config; print(Stage5Config().postgres_host)"

# é©—è­‰æ•¸æ“šå®Œæ•´æ€§
python -c "import json; data=json.load(open('starlink_with_3gpp_events.json')); print(f'è¡›æ˜Ÿæ•¸: {len(data[\"satellites\"])}')"
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **PostgreSQLé€£æ¥å¤±æ•—**
   - æª¢æŸ¥ï¼šå®¹å™¨ç‹€æ…‹å’Œé€£æ¥å­—ä¸²
   - è§£æ±ºï¼šç¢ºèªä½¿ç”¨ `netstack-postgres` è€Œé `localhost`

2. **åˆ†å±¤æ¿¾æ³¢ç”¢ç”Ÿç©ºçµæœ** 
   - æª¢æŸ¥ï¼šæ™‚åºæ•¸æ“šæ¬„ä½åç¨±ä¸€è‡´æ€§
   - è§£æ±ºï¼šä½¿ç”¨ `position_timeseries` æ¬„ä½

3. **Volumeæª”æ¡ˆæ¬Šé™å•é¡Œ**
   - æª¢æŸ¥ï¼šæª”æ¡ˆæ‰€æœ‰æ¬Šå’Œæ¬Šé™
   - è§£æ±ºï¼š`chown -R app:app /app/data`

4. **æ··åˆæŸ¥è©¢æ€§èƒ½å·®**
   - æª¢æŸ¥ï¼šPostgreSQLç´¢å¼•ä½¿ç”¨
   - è§£æ±ºï¼šåˆ†ææŸ¥è©¢è¨ˆåŠƒä¸¦å„ªåŒ–ç´¢å¼•

### è¨ºæ–·æŒ‡ä»¤

```bash
# æª¢æŸ¥PostgreSQLæ•¸æ“š
docker exec netstack-rl-postgres psql -U rl_user -d rl_research -c "
SELECT 
    constellation,
    COUNT(*) as satellite_count,
    AVG(mean_rsrp_dbm) as avg_signal
FROM satellite_metadata sm 
LEFT JOIN signal_quality_statistics sqs ON sm.satellite_id = sqs.satellite_id
GROUP BY constellation;
"

# æª¢æŸ¥Volumeæª”æ¡ˆå®Œæ•´æ€§
find /app/data -name "*.json" -exec echo "æª¢æŸ¥: {}" \; -exec python -m json.tool {} > /dev/null \;

# æª¢æŸ¥æ•´åˆç‹€æ…‹
curl -s http://localhost:8080/api/v1/data-integration/status | jq
```

## âœ… éšæ®µé©—è­‰æ¨™æº–

### ğŸ¯ Stage 5 å®Œæˆé©—è­‰æª¢æŸ¥æ¸…å–®

#### 1. **è¼¸å…¥é©—è­‰**
- [ ] å¤šæºæ•¸æ“šå®Œæ•´æ€§
  - Stage 3ä¿¡è™Ÿåˆ†æçµæœ
  - Stage 4æ™‚é–“åºåˆ—æ•¸æ“š
  - åŸºç¤è¡›æ˜Ÿå…ƒæ•¸æ“š
- [ ] æ•¸æ“šæ™‚é–“æˆ³ä¸€è‡´æ€§
  - å„éšæ®µæ•¸æ“šæ™‚é–“å°é½Š
  - ç„¡æ™‚é–“å·®ç•°éŒ¯èª¤

#### 2. **åˆ†å±¤æ•¸æ“šç”Ÿæˆé©—è­‰**
- [ ] **ä»°è§’åˆ†å±¤æ­£ç¢ºæ€§**
  ```
  åˆ†å±¤é–€æª»:
  - 5åº¦å±¤: å…¨éƒ¨è¡›æ˜Ÿ
  - 10åº¦å±¤: ä»°è§’â‰¥10Â°çš„è¡›æ˜Ÿ
  - 15åº¦å±¤: ä»°è§’â‰¥15Â°çš„è¡›æ˜Ÿ
  æ•¸é‡éæ¸›é©—è­‰: 5åº¦ > 10åº¦ > 15åº¦
  ```
- [ ] **æ¯å±¤æ•¸æ“šå®Œæ•´æ€§**
  - æ™‚é–“åºåˆ—ä¿ç•™
  - ä¿¡è™ŸæŒ‡æ¨™å®Œæ•´
  - å¯è¦‹æ€§çª—å£æ­£ç¢º

#### 3. **PostgreSQLæ•´åˆé©—è­‰**
- [ ] **æ•¸æ“šåº«é€£æ¥**
  - é€£æ¥æˆåŠŸï¼ˆ172.20.0.51:5432ï¼‰
  - è³‡æ–™è¡¨å‰µå»ºå®Œæˆ
  - ç´¢å¼•å»ºç«‹æ­£ç¢º
- [ ] **æ•¸æ“šå¯«å…¥é©—è­‰**
  ```sql
  é æœŸè¨˜éŒ„æ•¸:
  - satellite_tle_data: 1,100+ç­†
  - satellite_signal_metrics: 200,000+ç­†
  - handover_events: 300+ç­†
  ```

#### 4. **è¼¸å‡ºé©—è­‰**
- [ ] **æ··åˆå­˜å„²çµæ§‹**
  ```json
  {
    "metadata": {
      "stage": "stage5_data_integration",
      "storage_mode": "hybrid",
      "postgresql_status": "connected",
      "volume_status": "active"
    },
    "integration_summary": {
      "elevation_5deg": {"count": 1196},
      "elevation_10deg": {"count": 900},
      "elevation_15deg": {"count": 600}
    }
  }
  ```
- [ ] **å­˜å„²åˆ†ä½ˆåˆç†**
  - PostgreSQL: < 50MBï¼ˆçµæ§‹åŒ–æ•¸æ“šï¼‰
  - Volume: < 450MBï¼ˆæ™‚é–“åºåˆ—ï¼‰
  - ç¸½è¨ˆ: < 500MB

#### 5. **æ€§èƒ½æŒ‡æ¨™**
- [ ] è™•ç†æ™‚é–“ < 1åˆ†é˜
- [ ] è³‡æ–™åº«å¯«å…¥é€Ÿåº¦ > 1000ç­†/ç§’
- [ ] è¨˜æ†¶é«”ä½¿ç”¨ < 500MB

#### 6. **è‡ªå‹•é©—è­‰è…³æœ¬**
```python
# åŸ·è¡Œéšæ®µé©—è­‰
python -c "
import json
import os
import psycopg2

# æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆ
output_file = '/app/data/data_integration_outputs/integrated_data_output.json'
if os.path.exists(output_file):
    with open(output_file, 'r') as f:
        data = json.load(f)
    
    metadata = data.get('metadata', {})
    summary = data.get('integration_summary', {})
    
    # æª¢æŸ¥åˆ†å±¤æ•¸æ“š
    elev_5 = summary.get('elevation_5deg', {}).get('count', 0)
    elev_10 = summary.get('elevation_10deg', {}).get('count', 0)
    elev_15 = summary.get('elevation_15deg', {}).get('count', 0)
else:
    elev_5 = elev_10 = elev_15 = 0

# æª¢æŸ¥PostgreSQL
try:
    conn = psycopg2.connect(
        host='172.20.0.51',
        database='rl_research',
        user='rl_user',
        password='rl_password'
    )
    cur = conn.cursor()
    cur.execute('SELECT COUNT(*) FROM satellite_tle_data')
    db_count = cur.fetchone()[0]
    conn.close()
    db_connected = True
except:
    db_count = 0
    db_connected = False

checks = {
    'output_exists': os.path.exists(output_file),
    'elevation_layers': elev_5 > elev_10 > elev_15 > 0,
    'layer_5deg_ok': elev_5 > 1000,
    'layer_10deg_ok': elev_10 > 800,
    'layer_15deg_ok': elev_15 > 500,
    'db_connected': db_connected,
    'db_has_data': db_count > 1000
}

passed = sum(checks.values())
total = len(checks)

print('ğŸ“Š Stage 5 é©—è­‰çµæœ:')
print(f'  åˆ†å±¤æ•¸æ“š: 5åº¦({elev_5}) > 10åº¦({elev_10}) > 15åº¦({elev_15})')
print(f'  è³‡æ–™åº«ç‹€æ…‹: {\"é€£æ¥æˆåŠŸ\" if db_connected else \"é€£æ¥å¤±æ•—\"}')
print(f'  è³‡æ–™åº«è¨˜éŒ„: {db_count}ç­†')

for check, result in checks.items():
    print(f'  {\"âœ…\" if result else \"âŒ\"} {check}')

if passed == total:
    print('âœ… Stage 5 é©—è­‰é€šéï¼')
else:
    print(f'âŒ Stage 5 é©—è­‰å¤±æ•— ({passed}/{total})')
    exit(1)
"
```

### ğŸš¨ é©—è­‰å¤±æ•—è™•ç†
1. **åˆ†å±¤æ•¸æ“šç•°å¸¸**: æª¢æŸ¥ä»°è§’é–€æª»è¨­å®š
2. **è³‡æ–™åº«é€£æ¥å¤±æ•—**: ç¢ºèªPostgreSQLæœå‹™ç‹€æ…‹
3. **å­˜å„²è¶…é™**: å„ªåŒ–æ•¸æ“šçµæ§‹ã€å¢åŠ å£“ç¸®

### ğŸ“Š é—œéµæŒ‡æ¨™
- **åˆ†å±¤æ­£ç¢ºæ€§**: 5åº¦ > 10åº¦ > 15åº¦éæ¸›
- **æ··åˆå­˜å„²**: PostgreSQL + Volumeå”åŒ
- **æ€§èƒ½å¹³è¡¡**: æŸ¥è©¢é€Ÿåº¦èˆ‡å­˜å„²æ•ˆç‡

---
**ä¸Šä¸€éšæ®µ**: [éšæ®µå››ï¼šæ™‚é–“åºåˆ—é è™•ç†](./stage4-timeseries.md)  
**ä¸‹ä¸€éšæ®µ**: [éšæ®µå…­ï¼šå‹•æ…‹æ± è¦åŠƒ](./stage6-dynamic-pool.md)  
**ç›¸é—œæ–‡æª”**: [PostgreSQLè¨­å®š](../system_architecture.md#postgresql-configuration)