# ğŸ“ éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆèˆ‡æ··åˆå­˜å„²

[ğŸ”„ è¿”å›æ•¸æ“šæµç¨‹å°èˆª](../data-flow-index.md) > éšæ®µäº”

## ğŸ“– éšæ®µæ¦‚è¿°

**ç›®æ¨™**ï¼šå°‡æ‰€æœ‰è™•ç†çµæœæ•´åˆä¸¦å»ºç«‹æ··åˆå­˜å„²æ¶æ§‹  
**è¼¸å…¥**ï¼šéšæ®µå››çš„å‰ç«¯æ™‚é–“åºåˆ—æ•¸æ“šï¼ˆ~85-100MBï¼‰  
**è¼¸å‡º**ï¼šPostgreSQLçµæ§‹åŒ–æ•¸æ“š + Docker Volumeæª”æ¡ˆå­˜å„²  
**å­˜å„²ç¸½é‡**ï¼š~486MB (PostgreSQL ~86MB + Volume ~400MB)  
**è™•ç†æ™‚é–“**ï¼šç´„ 2-3 åˆ†é˜

## ğŸ—ï¸ æ··åˆå­˜å„²æ¶æ§‹

### å­˜å„²ç­–ç•¥åˆ†å·¥
- **PostgreSQL**ï¼šçµæ§‹åŒ–æ•¸æ“šã€ç´¢å¼•æŸ¥è©¢ã€çµ±è¨ˆåˆ†æ
- **Docker Volume**ï¼šå¤§å‹æª”æ¡ˆã€æ™‚é–“åºåˆ—æ•¸æ“šã€å‰ç«¯è³‡æº

### æ•¸æ“šåˆ†é¡åŸå‰‡
```python
STORAGE_STRATEGY = {
    'postgresql': [
        'satellite_metadata',      # è¡›æ˜ŸåŸºæœ¬è³‡è¨Š
        'signal_statistics',       # ä¿¡è™Ÿçµ±è¨ˆæŒ‡æ¨™
        'event_summaries',         # 3GPPäº‹ä»¶æ‘˜è¦
        'performance_metrics'      # ç³»çµ±æ€§èƒ½æŒ‡æ¨™
    ],
    'volume_files': [
        'timeseries_data',         # å®Œæ•´æ™‚é–“åºåˆ—
        'animation_resources',     # å‰ç«¯å‹•ç•«æ•¸æ“š
        'signal_heatmaps',        # ä¿¡è™Ÿç†±åŠ›åœ–
        'orbit_trajectories'       # è»Œé“è»Œè·¡æ•¸æ“š
    ]
}
```

## ğŸ“Š PostgreSQL æ•¸æ“šçµæ§‹

### æ ¸å¿ƒè³‡æ–™è¡¨è¨­è¨ˆ

#### 1. satellite_metadata
```sql
CREATE TABLE satellite_metadata (
    satellite_id VARCHAR(50) PRIMARY KEY,
    constellation VARCHAR(20) NOT NULL,
    norad_id INTEGER UNIQUE,
    tle_epoch TIMESTAMP WITH TIME ZONE,
    orbital_period_minutes NUMERIC(8,3),
    inclination_deg NUMERIC(6,3),
    mean_altitude_km NUMERIC(8,3),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- ç´¢å¼•å„ªåŒ–
CREATE INDEX idx_satellite_constellation ON satellite_metadata(constellation);
CREATE INDEX idx_satellite_norad ON satellite_metadata(norad_id);
```

#### 2. signal_quality_statistics
```sql
CREATE TABLE signal_quality_statistics (
    id SERIAL PRIMARY KEY,
    satellite_id VARCHAR(50) REFERENCES satellite_metadata(satellite_id),
    analysis_period_start TIMESTAMP WITH TIME ZONE,
    analysis_period_end TIMESTAMP WITH TIME ZONE,
    mean_rsrp_dbm NUMERIC(6,2),
    std_rsrp_db NUMERIC(5,2),
    max_elevation_deg NUMERIC(5,2),
    total_visible_time_minutes INTEGER,
    handover_event_count INTEGER,
    signal_quality_grade VARCHAR(10), -- 'high', 'medium', 'low'
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- è¤‡åˆç´¢å¼•
CREATE INDEX idx_signal_satellite_period ON signal_quality_statistics(satellite_id, analysis_period_start);
CREATE INDEX idx_signal_quality_grade ON signal_quality_statistics(signal_quality_grade);
```

#### 3. handover_events_summary
```sql
CREATE TABLE handover_events_summary (
    id SERIAL PRIMARY KEY,
    event_type VARCHAR(10) NOT NULL, -- 'A4', 'A5', 'D2'
    serving_satellite_id VARCHAR(50) REFERENCES satellite_metadata(satellite_id),
    neighbor_satellite_id VARCHAR(50) REFERENCES satellite_metadata(satellite_id),
    event_timestamp TIMESTAMP WITH TIME ZONE,
    trigger_rsrp_dbm NUMERIC(6,2),
    handover_decision VARCHAR(20), -- 'trigger', 'hold', 'reject'
    processing_latency_ms INTEGER,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- äº‹ä»¶æŸ¥è©¢ç´¢å¼•
CREATE INDEX idx_handover_event_type ON handover_events_summary(event_type);
CREATE INDEX idx_handover_timestamp ON handover_events_summary(event_timestamp);
CREATE INDEX idx_handover_serving ON handover_events_summary(serving_satellite_id);
```

## ğŸ“ Docker Volume æª”æ¡ˆçµæ§‹

### Volume çµ„ç¹”æ¶æ§‹
```bash
/app/data/
â”œâ”€â”€ enhanced_timeseries/          # å‰ç«¯å‹•ç•«æ•¸æ“š (~85-100MB)
â”‚   â”œâ”€â”€ animation_enhanced_starlink.json
â”‚   â””â”€â”€ animation_enhanced_oneweb.json
â”‚
â”œâ”€â”€ layered_phase0_enhanced/      # åˆ†å±¤è™•ç†çµæœ (~120MB)
â”‚   â”œâ”€â”€ starlink_5deg_enhanced.json
â”‚   â”œâ”€â”€ starlink_10deg_enhanced.json
â”‚   â”œâ”€â”€ starlink_15deg_enhanced.json
â”‚   â”œâ”€â”€ oneweb_10deg_enhanced.json
â”‚   â”œâ”€â”€ oneweb_15deg_enhanced.json
â”‚   â””â”€â”€ oneweb_20deg_enhanced.json
â”‚
â”œâ”€â”€ handover_scenarios/           # æ›æ‰‹å ´æ™¯æ•¸æ“š (~80MB)
â”‚   â”œâ”€â”€ a4_events_enhanced.json
â”‚   â”œâ”€â”€ a5_events_enhanced.json
â”‚   â”œâ”€â”€ d2_events_enhanced.json
â”‚   â””â”€â”€ best_handover_windows.json
â”‚
â”œâ”€â”€ signal_quality_analysis/      # ä¿¡è™Ÿåˆ†æçµæœ (~90MB)
â”‚   â”œâ”€â”€ signal_heatmap_data.json
â”‚   â”œâ”€â”€ quality_metrics_summary.json
â”‚   â””â”€â”€ constellation_comparison.json
â”‚
â”œâ”€â”€ processing_cache/             # è™•ç†ç·©å­˜ (~50MB)
â”‚   â”œâ”€â”€ sgp4_calculation_cache.json
â”‚   â”œâ”€â”€ filtering_results_cache.json
â”‚   â””â”€â”€ gpp3_event_cache.json
â”‚
â””â”€â”€ status_files/                 # ç‹€æ…‹æ¨™è¨˜æª”æ¡ˆ (~1MB)
    â”œâ”€â”€ last_processing_time.txt
    â”œâ”€â”€ tle_checksum.txt
    â”œâ”€â”€ processing_status.json
    â””â”€â”€ health_check.json
```

## ğŸ”§ æ•´åˆè™•ç†å™¨å¯¦ç¾

### ä¸»è¦å¯¦ç¾ä½ç½®
```bash
# æ•´åˆè™•ç†å™¨
/netstack/src/stages/stage5_integration_processor.py
â”œâ”€â”€ Stage5IntegrationProcessor.setup_postgresql_schema()    # è³‡æ–™åº«æ¶æ§‹è¨­å®š
â”œâ”€â”€ Stage5IntegrationProcessor.populate_metadata_tables()   # å…ƒæ•¸æ“šå¡«å…¥
â”œâ”€â”€ Stage5IntegrationProcessor.generate_volume_files()      # Volumeæª”æ¡ˆç”Ÿæˆ
â”œâ”€â”€ Stage5IntegrationProcessor.verify_mixed_storage()       # æ··åˆå­˜å„²é©—è­‰
â””â”€â”€ Stage5IntegrationProcessor.process_stage5()             # å®Œæ•´æµç¨‹åŸ·è¡Œ

# è³‡æ–™åº«é€£æ¥ç®¡ç†
/netstack/src/services/database/postgresql_manager.py
â”œâ”€â”€ PostgreSQLManager.setup_connection_pool()              # é€£æ¥æ± ç®¡ç†
â”œâ”€â”€ PostgreSQLManager.execute_batch_insert()               # æ‰¹æ¬¡æ’å…¥å„ªåŒ–
â””â”€â”€ PostgreSQLManager.create_indexes()                     # ç´¢å¼•å»ºç«‹
```

### æ ¸å¿ƒè™•ç†é‚è¼¯
```python
class Stage5IntegrationProcessor:
    
    async def process_stage5(self) -> Dict[str, Any]:
        """åŸ·è¡Œéšæ®µäº”å®Œæ•´æ•´åˆè™•ç†"""
        
        results = {}
        
        # 1. è¨­å®šPostgreSQLæ¶æ§‹
        await self._setup_postgresql_schema()
        logger.info("âœ… PostgreSQLæ¶æ§‹è¨­å®šå®Œæˆ")
        
        # 2. å¡«å…¥è¡›æ˜Ÿå…ƒæ•¸æ“š
        satellite_count = await self._populate_metadata_tables()
        results['postgresql_satellites'] = satellite_count
        logger.info(f"âœ… PostgreSQLå…ƒæ•¸æ“š: {satellite_count}é¡†è¡›æ˜Ÿ")
        
        # 3. å¡«å…¥ä¿¡è™Ÿçµ±è¨ˆæ•¸æ“š
        signal_records = await self._populate_signal_statistics()
        results['postgresql_signal_records'] = signal_records
        logger.info(f"âœ… PostgreSQLä¿¡è™Ÿçµ±è¨ˆ: {signal_records}ç­†è¨˜éŒ„")
        
        # 4. å¡«å…¥æ›æ‰‹äº‹ä»¶æ‘˜è¦
        event_records = await self._populate_handover_events()
        results['postgresql_event_records'] = event_records
        logger.info(f"âœ… PostgreSQLæ›æ‰‹äº‹ä»¶: {event_records}ç­†è¨˜éŒ„")
        
        # 5. ç”ŸæˆVolumeæª”æ¡ˆ
        volume_files = await self._generate_all_volume_files()
        results['volume_files'] = volume_files
        logger.info(f"âœ… Volumeæª”æ¡ˆ: {len(volume_files)}å€‹æª”æ¡ˆ")
        
        # 6. æ··åˆå­˜å„²é©—è­‰
        verification = await self._verify_mixed_storage_access()
        results['storage_verification'] = verification
        logger.info(f"âœ… æ··åˆå­˜å„²é©—è­‰å®Œæˆ")
        
        return results
    
    async def _populate_metadata_tables(self) -> int:
        """æ‰¹æ¬¡å¡«å…¥è¡›æ˜Ÿå…ƒæ•¸æ“šåˆ°PostgreSQL"""
        
        satellites = self.get_all_processed_satellites()
        
        insert_data = []
        for satellite in satellites:
            insert_data.append({
                'satellite_id': satellite['satellite_id'],
                'constellation': satellite['constellation'],
                'norad_id': satellite['norad_id'],
                'tle_epoch': satellite['tle_epoch'],
                'orbital_period_minutes': satellite['orbital_period_minutes'],
                'inclination_deg': satellite['inclination_deg'],
                'mean_altitude_km': satellite['mean_altitude_km']
            })
        
        # æ‰¹æ¬¡æ’å…¥å„ªåŒ–
        await self.postgresql_manager.execute_batch_insert(
            'satellite_metadata',
            insert_data,
            batch_size=100
        )
        
        return len(insert_data)
```

## âš™ï¸ æ€§èƒ½æœ€ä½³åŒ–ç­–ç•¥

### PostgreSQL æœ€ä½³åŒ–
```python
# é€£æ¥æ± é…ç½®
POSTGRESQL_CONFIG = {
    'max_connections': 20,
    'connection_timeout': 30,
    'query_timeout': 60,
    'batch_insert_size': 100,
    'enable_connection_pooling': True
}

# ç´¢å¼•ç­–ç•¥
INDEX_STRATEGY = {
    'primary_indexes': ['satellite_id', 'norad_id'],
    'composite_indexes': [
        ('satellite_id', 'analysis_period_start'),
        ('constellation', 'signal_quality_grade')
    ],
    'partial_indexes': [
        'signal_quality_grade WHERE signal_quality_grade = \'high\''
    ]
}
```

### æª”æ¡ˆI/Oæœ€ä½³åŒ–
```python
# Volumeå¯«å…¥ç­–ç•¥
VOLUME_CONFIG = {
    'write_buffer_size': '64MB',
    'compression_enabled': True,
    'async_write_enabled': True,
    'file_integrity_check': True
}

# JSONåºåˆ—åŒ–æœ€ä½³åŒ–
JSON_CONFIG = {
    'ensure_ascii': False,
    'separators': (',', ':'),  # ç·Šæ¹Šæ ¼å¼
    'sort_keys': False,        # ä¿æŒåŸå§‹é †åº
    'indent': None            # ç„¡ç¸®æ’ç¯€çœç©ºé–“
}
```

## ğŸ“ˆ å­˜å„²çµ±è¨ˆèˆ‡ç›£æ§

### å­˜å„²ä½¿ç”¨åˆ†æ
```python
# é æœŸå­˜å„²åˆ†ä½ˆ
STORAGE_BREAKDOWN = {
    'postgresql_total_mb': 86,
    'postgresql_breakdown': {
        'satellite_metadata': 2,      # 563é¡†è¡›æ˜Ÿ Ã— åŸºæœ¬è³‡è¨Š
        'signal_statistics': 35,      # 563é¡† Ã— çµ±è¨ˆæ•¸æ“š
        'handover_events': 25,        # ~2,600å€‹æ›æ‰‹äº‹ä»¶
        'indexes_overhead': 12,       # ç´¢å¼•ç©ºé–“
        'system_metadata': 12         # PostgreSQLç³»çµ±é–‹éŠ·
    },
    'volume_total_mb': 400,
    'volume_breakdown': {
        'enhanced_timeseries': 100,   # å‰ç«¯å‹•ç•«æ•¸æ“š
        'layered_phase0': 120,        # åˆ†å±¤è™•ç†çµæœ  
        'handover_scenarios': 80,     # æ›æ‰‹å ´æ™¯
        'signal_analysis': 90,        # ä¿¡è™Ÿåˆ†æ
        'cache_files': 10            # ç·©å­˜æª”æ¡ˆ
    }
}
```

### å¥åº·æª¢æŸ¥æ©Ÿåˆ¶
```bash
# å­˜å„²å¥åº·æª¢æŸ¥è…³æœ¬
#!/bin/bash
echo "ğŸ“Š æ··åˆå­˜å„²å¥åº·æª¢æŸ¥"

# PostgreSQLé€£æ¥æª¢æŸ¥
docker exec netstack-rl-postgres psql -U rl_user -d rl_research -c "SELECT COUNT(*) FROM satellite_metadata;" > /dev/null
if [ $? -eq 0 ]; then
    echo "âœ… PostgreSQL: æ­£å¸¸"
else
    echo "âŒ PostgreSQL: ç•°å¸¸"
fi

# Volumeæª”æ¡ˆæª¢æŸ¥
if [ -f "/app/data/enhanced_timeseries/animation_enhanced_starlink.json" ]; then
    echo "âœ… Volumeæª”æ¡ˆ: æ­£å¸¸"
else
    echo "âŒ Volumeæª”æ¡ˆ: éºå¤±"
fi

# å­˜å„²ç©ºé–“æª¢æŸ¥
volume_usage=$(df -h /app/data | awk 'NR==2 {print $5}' | sed 's/%//')
if [ $volume_usage -lt 80 ]; then
    echo "âœ… å­˜å„²ç©ºé–“: ${volume_usage}% (æ­£å¸¸)"
else
    echo "âš ï¸ å­˜å„²ç©ºé–“: ${volume_usage}% (è­¦å‘Š)"
fi
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

1. **PostgreSQLé€£æ¥å¤±æ•—**
   - æª¢æŸ¥ï¼šå®¹å™¨ç‹€æ…‹å’Œé€£æ¥å­—ä¸²
   - è§£æ±ºï¼šé‡å•ŸPostgreSQLå®¹å™¨

2. **Volumeæª”æ¡ˆæ¬Šé™å•é¡Œ**
   - æª¢æŸ¥ï¼šæª”æ¡ˆæ‰€æœ‰æ¬Šå’Œæ¬Šé™
   - è§£æ±ºï¼š`chown -R app:app /app/data`

3. **æ··åˆæŸ¥è©¢æ€§èƒ½å·®**
   - æª¢æŸ¥ï¼šPostgreSQLç´¢å¼•ä½¿ç”¨
   - è§£æ±ºï¼šåˆ†ææŸ¥è©¢è¨ˆåŠƒä¸¦å„ªåŒ–ç´¢å¼•

### è¨ºæ–·æŒ‡ä»¤

```bash
# æª¢æŸ¥PostgreSQLæ•¸æ“š
docker exec netstack-rl-postgres psql -U rl_user -d rl_research -c "
SELECT 
    constellation,
    COUNT(*) as satellite_count,
    AVG(mean_rsrp_dbm) as avg_signal
FROM satellite_metadata sm 
LEFT JOIN signal_quality_statistics sqs ON sm.satellite_id = sqs.satellite_id
GROUP BY constellation;
"

# æª¢æŸ¥Volumeæª”æ¡ˆå®Œæ•´æ€§
find /app/data -name "*.json" -exec echo "æª¢æŸ¥: {}" \; -exec python -m json.tool {} > /dev/null \;

# æª¢æŸ¥æ•´åˆç‹€æ…‹
curl -s http://localhost:8080/api/v1/data-integration/status | jq
```

---
**ä¸Šä¸€éšæ®µ**: [éšæ®µå››ï¼šæ™‚é–“åºåˆ—é è™•ç†](./stage4-timeseries.md)  
**ä¸‹ä¸€éšæ®µ**: [éšæ®µå…­ï¼šå‹•æ…‹æ± è¦åŠƒ](./stage6-dynamic-pool.md)  
**ç›¸é—œæ–‡æª”**: [PostgreSQLè¨­å®š](../system_architecture.md#postgresql-configuration)