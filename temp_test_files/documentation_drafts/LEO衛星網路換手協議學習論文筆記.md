# LEO 衛星網路換手協議學習：存取延遲與碰撞最小化

**論文資訊**
- **標題**: Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization
- **作者**: Ju-Hyung Lee, Chanyoung Park, Soohyun Park, Andreas F. Molisch
- **期刊**: IEEE Transactions on Wireless Communications, Vol. 23, No. 7, July 2024
- **頁數**: 7624-7637
- **DOI**: 10.1109/TWC.2023.3342975

## 摘要 (Abstract)

### 研究背景與動機
本研究提出了一種新穎的基於深度強化學習（DRL）的換手（HO）協議，稱為 **DHO**（Deep reinforcement learning-based Handover），專門設計用於解決低軌道（LEO）衛星網路中長傳播延遲的持續挑戰。

### 核心技術創新
DHO 協議的關鍵創新在於：
- **跳過測量報告（MR）階段**：利用預訓練的預測能力，基於預先確定的LEO衛星軌道模式
- **簡化HO程序**：消除MR階段產生的傳播延遲，同時仍提供有效的HO決策
- **多目標優化**：同時最小化存取延遲、碰撞率並提高換手成功率

### 主要貢獻與成果
1. **協議設計**：重新設計傳統HO程序以適應LEO衛星網路需求
2. **演算法選擇**：採用重要性加權Actor-Learner架構的IMPALA演算法，確保大狀態和動作空間的穩定訓練
3. **性能提升**：相較於傳統HO協議，存取延遲降低高達6.86倍，相較於啟發式方法降低4.18倍

## 第一章：引言 (Introduction)

### 1.1 研究背景：超越5G與6G網路趨勢

#### 衛星星座的戰略重要性
在超越5G和6G網路領域中，利用低軌道（LEO）和中軌道（MEO）衛星的衛星星座已成為提供全球覆蓋的關鍵解決方案。這些星座具備以下特點：

**技術特徵**：
- **衛星數量**：由數千顆靠近地球的衛星組成
- **服務能力**：提供高速、低延遲的寬頻網路存取
- **覆蓋範圍**：為全球偏遠和服務不足地區提供連接

#### 大規模接入挑戰的產生
隨著mega-constellation網路中衛星數量持續增加，以及這些網路越來越多地用於各種應用，試圖接入網路的設備數量相應增加，稱為**大規模接入（massive access）**。

**挑戰分析**：
- **網路壅塞**：可能導致網路壅塞
- **延遲增加**：增加延遲，這是這些網路的重大關切
- **服務品質下降**：負面影響網路性能

### 1.2 現有解決方案的局限性

#### 透明型 vs 再生型LEO衛星
**透明型LEO衛星的限制**：
- **集中式保留方法**：依賴有限數量的地面站
- **功能管理**：管理各種功能，如換手和資源分配
- **可擴展性問題**：在大規模接入情境下可能不夠高效

**再生型LEO衛星的優勢**：
- **自主決策能力**：能夠做出決策並執行功能，無需依賴地面站
- **實時處理**：具備機載處理（OBP）能力
- **分散式架構**：更適合處理大規模接入情境

#### 傳統HO協議的不適應性
將地面網路使用的傳統HO協議應用於再生型LEO衛星網路可能不是最優的，因為：
- **設計考量不足**：這些協議並非為LEO衛星網路的高動態性設計
- **延遲敏感性**：未考慮長傳播延遲問題
- **資源消耗**：導致不必要的延遲和繁重HO程序造成的功耗

### 1.3 本研究的貢獻

#### 主要技術貢獻
**1. 創新協議設計**
- 提出專為LEO衛星網路設計的新型HO協議DHO
- 重新設計傳統HO程序以適應LEO衛星網路需求
- 利用可局部觀察的資訊，針對LEO衛星網路的獨特特性進行客製化

**2. 性能優化目標**
- **最小化存取延遲**：通過跳過測量報告（MR）簡化HO過程
- **降低碰撞率**：即使在較低功耗下也能獲得更好的性能
- **演算法選擇**：採用IMPALA演算法確保大狀態和動作空間的穩定訓練

**3. 實驗驗證**
- **性能對比**：相較於傳統HO協議，存取延遲降低高達6.86倍
- **啟發式比較**：相較於啟發式方法，存取延遲降低4.18倍
- **適應性展示**：證明DHO協議在各種條件下的優越性和適應性

#### 論文結構說明
- **第二章**：討論LEO衛星基礎非地面網路（NTN）HO過程的相關工作
- **第三章**：提供背景並涵蓋LEO衛星網路中HO的挑戰
- **第四章**：介紹提議的DHO演算法及其評估方法
- **第五章**：提供模擬結果和性能分析
- **第六章**：結論性評論

### 1.4 符號說明
論文使用以下符號約定：
- **標量**：使用正常字體表示
- **向量**：使用粗體字體表示
- **空間表示**：$\mathbb{R}^{D \times 1}$ 表示D維實值向量空間
- **梯度表示**：$\nabla_x f(x)$ 表示函數 $f(x)$ 的梯度向量

---

## 第二章：相關工作 (Related Works)

### 2.1 LEO衛星網路換手策略概覽

#### 圖論基礎方法
已有多種HO策略被提出來應對LEO衛星網路中HO的挑戰。其中一種策略採用**圖論基礎方法**：

**方法特徵**：
- **圖模型表示**：LEO衛星與用戶設備（UE）之間的關係以圖和節點表示
- **優化潛力**：具有找到有效HO決策的潛力
- **應用限制**：僅適用於透明型LEO衛星網路的集中保留方法
- **實時限制**：不適用於需要通過機載處理進行實時決策的再生型LEO衛星網路

#### 深度強化學習在無線通訊中的應用

**DRL應用領域廣泛性**：
DRL在無線通訊中的使用已被廣泛研究，主要應用包括：

1. **認知無線電中的動態頻譜感測/存取**
   - 應用DRL確定適當的頻譜存取策略
   - 提高頻譜使用效率
   - 適應動態頻譜環境

2. **LEO衛星網路中的隨機存取性能改善**
   - 擴展DRL應用至隨機存取性能提升
   - 針對LEO衛星網路特性進行優化
   - 處理大規模用戶接入問題

3. **資源分配與容量管理**
   - 主要專注於集中式決策
   - 改善網路資源利用率
   - 適應動態網路條件

### 2.2 機器學習在HO程序中的應用

#### 現有研究的技術路徑
已有多項研究調查了機器學習在HO程序中的應用：

**1. 基於DRL的HO程序優化**
- **集中式方法**：在某些HO過程中利用DRL方法論，但採用集中式方式
- **局限性分析**：集中式決策可能不適合分散式衛星網路環境

**2. 聯合優化方法**
- **多目標優化**：同時優化波束成形和HO
- **抽象層級**：僅在高層級處理HO過程
- **程序細節不足**：未充分考慮詳細的HO程序

**3. 監督學習預測方法**
- **預測能力增強**：採用監督學習方法增強特定HO程序的預測能力
- **改善限制**：產生的改善有限
- **數據依賴性**：嚴重依賴數據驅動方法

#### 現有方法的不足之處
雖然之前的研究展示了機器學習整合在HO過程中的潛力，但它們主要採用的方法存在以下問題：

**技術層面限制**：
- **過度高級化**：方法過於高層級
- **過度專業化**：過於專業化
- **環境考量不足**：未充分考慮NTN典型的動態環境

### 2.3 本研究的定位與創新

#### 研究目標與方向
考慮到上述方法，本研究旨在增強基於DRL的方法，特別專注於LEO衛星網路中的HO。

**創新要點**：
1. **專門化設計**：提出專為LEO衛星網路設計的新型DRL基礎HO協議
2. **實時決策能力**：實現高效的實時決策制定
3. **應用範圍擴展**：拓寬LEO衛星網路環境中DRL基礎解決方案的範圍

#### 與現有方法的差異化
**優勢特點**：
- **環境適應性**：專門針對LEO衛星網路的動態環境設計
- **分散式架構**：支援分散式決策制定
- **實時性能**：提供實時HO決策能力
- **全面考量**：充分考慮LEO衛星網路的獨特挑戰

---

## 第三章：LEO衛星基礎非地面網路的換手 (Handover for LEO SAT-Based Non-Terrestrial Networks)

### 3.1 HO過程概覽 (Overview of HO Process)

#### 衛星類型與運作機制對比

**透明型LEO衛星**：
- **運作機制**：主要依賴基於排程的HO機制
- **管理方式**：採用集中式保留方法
- **架構特點**：有限數量的地面站管理各種功能
- **管理範圍**：包括HO和所有連接到數千顆LEO衛星的UE資源分配

**再生型LEO衛星**（本論文重點）：
- **自主能力**：能夠做出自主決策
- **HO機制**：採用基於觸發的HO過程
- **對應標準**：對應於3GPP新無線電（NR）中使用的傳統HO過程
- **處理能力**：具備機載處理（OBP）能力

#### 3GPP NR中的HO過程結構

3GPP NR作為主要的5G蜂窩標準，其HO過程包含三個主要階段：

**階段一：準備階段（Preparation Phase）**
在準備階段，UE測量來自服務和目標節點B（gNBs）的信號，根據表I中列出的五個潛在條件決定是否需要HO事件。

**HO觸發條件分析**：
- **HOM（Handover Margin）**：換手邊距
- **TTT（Time-to-Trigger）**：觸發時間
- **條件評估**：如果滿足條件，UE向服務gNB發送MR
- **請求流程**：導致向目標gNB發送HO請求

**表格I：HO事件進入條件詳細分析**

| 事件 | 進入條件 |
|------|----------|
| A1 | 服務gNB信號變得比閾值更強（更好） |
| A2 | 服務gNB信號變得比閾值更弱（更差） |
| A3 | 目標gNB信號比服務gNB信號強（更好），偏移值 |
| A4 | 目標gNB信號變得比閾值更強（更好） |
| A5 | 服務gNB信號變得比閾值1更弱，目標gNB信號變得比閾值2更強 |

**數據走勢分析**：
- A1事件主要用於停止測量而非觸發HO
- A3事件是實際部署中最廣泛使用的，具有最高的發生概率
- A2、A4、A5事件用於特殊場景的HO觸發

**階段二：執行階段（Execution Phase）**
在執行階段，服務gNB將UE的數據和序號傳輸到目標gNB，UE通過隨機存取通道（RACH）過程連接到目標gNB。當連接成功時，UE發送HO確認訊息。

**階段三：完成階段（Completion Phase）**
完成階段包括網路實體請求將封包路徑從服務gNB切換到目標gNB，一旦傳輸HO完成訊息，就釋放來自服務gNB的資源。

#### HO過程的脆弱性分析
HO過程，特別是準備階段，在信號品質不佳的條件下容易失敗：

**失敗原因**：
- **傳輸/接收失敗**：HO相關訊息的傳輸/接收失敗
- **無線鏈路失敗（RLF）**：無線鏈路失敗

**LEO SAT-based NTN的特殊考量**：
考慮到LEO衛星基礎NTN在準備階段的脆弱性增加，這些網路通常面臨由於超長鏈路距離而導致的具有挑戰性的通道條件，本研究專注於增強HO過程的初始階段。

### 3.2 LEO衛星基礎NTN中的挑戰 (Challenges in LEO SAT-Based NTN)

#### 3.2.1 核心挑戰概述

LEO衛星基礎NTN中的HO面臨與地面蜂窩網路相比的獨特挑戰：

**主要挑戰領域**：
1. **長傳播延遲**：UE到衛星及返回地面站的信號需要大量時間
2. **大覆蓋區域**：衛星覆蓋區域遠大於地面蜂窩，增加目標gNB識別難度
3. **有限資源**：衛星上可用資源有限，特別是多個UE需要同時進行HO時

#### 3.2.2 挑戰一：過時、耗能且不可靠的MR

**傳播延遲的物理基礎**
LEO衛星網路中特定挑戰是過時、耗能且不可靠的MR。信令傳輸受到長傳播延遲的阻礙：

**延遲計算基礎**：
- **物理距離**：地面UE與提供網路服務的LEO衛星之間的物理距離
- **高度範圍**：500～2000公里的高度
- **傳播速度**：恆定（例如，$c = 2.997 \times 10^8$ m/s）
- **延遲公式**：傳播延遲與兩個終端之間的距離成正比

**單向傳播延遲範圍**：
如圖2a所示，單向傳播延遲可達1.6～6毫秒，這可能導致從地面UE發送到LEO衛星的MR變得過時。

#### 載波對雜訊比（CNR）計算與功耗分析

**CNR計算公式**：
對於NTN場景，CNR可以通過以下公式計算：

$$\text{CNR [dB]} = \text{EIRP [dBW]} - P_{FS} \text{[dB]} - P_A \text{[dB]} - P_{SM} \text{[dB]} - P_{SL} \text{[dB]} + G/T \text{[dB/K]} - k \text{[dBW/K/Hz]} - B \text{[dBHz]}$$

**參數詳細說明**：
- **EIRP**：發射器（TX）中的等效等向輻射功率
- **$P_{FS}$**：自由空間路徑損失 = $20 \log_{10}(f) + 20 \log_{10}(d) + 92.45$
  - $f$：載波頻率（GHz）
  - $d$：鏈路距離（km）
- **$P_A$**：由於氣體和雨衰在地面到太空之間的大氣路徑損失
- **$P_{SM}$**：陰影邊距
- **$P_{SL}$**：閃爍損失
- **$G/T$**：接收器（RX）中的天線增益對雜訊溫度比
- **$k$**：玻爾茲曼常數作為理論雜訊底限或接收器的最小敏感度
- **$B$**：通道頻寬

**頻段配置分析**：
- **手持型UE**：考慮S頻段（2 GHz），高度600公里的LEO衛星
- **VSAT型UE**：考慮Ka頻段（30 GHz）

**表格II：LEO衛星網路中UL傳輸參數分析**

根據表格II的參數配置，我們可以看到：

**S頻段（手持型UE）配置**：
- 載波頻率：2 GHz
- 頻寬：0.4 MHz
- TX功率：200 mW (23 dBm)
- TX天線增益：0 dBi

**Ka頻段（VSAT型UE）配置**：
- 載波頻率：30 GHz  
- 頻寬：400 MHz
- TX功率：2 W (33 dBm)
- TX天線增益：43.2 dBi

**數據趨勢洞察**：
1. **頻段效應**：Ka頻段雖然頻率更高，但透過更高的天線增益和功率補償路徑損失
2. **功耗權衡**：手持型設備功耗較低但需要在信號品質上做出妥協
3. **鏈路預算**：VSAT型設備具有更好的鏈路預算但複雜度和成本更高

#### 3.2.3 挑戰二：地面UE的高相關性和密度

**高密度用戶環境的問題**
在LEO衛星網路中，處理密集人口區域中UE的大量HO請求構成另一個挑戰，因為這些區域中的UE具有高度相關性。

**街道峽谷效應**：
例如，在地面UE位於街道峽谷的場景中：
- **同步現象**：由於衛星的開銷通過，它們可能遇到同時的視線損失和增益
- **碰撞概率增加**：當多個UE試圖同時執行HO時，碰撞概率增加

**大規模接入問題分析**：
- **同時請求**：來自UE的許多同時請求（例如，多個UE同時發生的A3事件）
- **高碰撞率**：導致高碰撞率和延長的存取延遲
- **性能影響**：對網路性能產生負面影響

**解決方案需求**：
為了解決這個挑戦，需要一個高效且有效的HO協議來：
- **管理大量請求**：管理如此大量的HO請求
- **最小化碰撞率和存取延遲**：確保高HO成功率

---

## 第四章：基於DRL的LEO衛星網路換手協議 (DRL-Based Handover Protocol for LEO Satellite Networks)

### 4.1 問題動機與解決方案概述

#### 現有HO方法的限制
現有的LEO衛星網路HO方法受到以下限制：
- **長傳播延遲**：導致上行信令功耗高
- **高功耗需求**：對上行信令的高功耗需求

#### 提出的解決方案
為了解決這些問題，我們提出了一個基於DRL的HO協議，其中**服務gNB代理**預測UE的網路信號資訊，並在不需要MR的情況下向目標gNB發送HO請求。

**關鍵優勢**：
1. **簡化HO準備階段**：省略MR步驟
2. **改善HO性能**：克服長傳播延遲
3. **節省信令功耗**：降低功耗需求

### 4.2 網路情境建模 (Network Scenario)

#### 4.2.1 LEO衛星和地面UE的配置

**軌道平面配置**：
- **軌道平面集合**：考慮圍繞地球的軌道平面集合 $\mathcal{K}$
- **衛星分佈**：對於每個軌道平面 $k \in \mathcal{K}$，存在一組在該平面上運行的LEO衛星 $\mathcal{I}_k$
- **地面用戶**：區域 $A$ 內部署的地面UE集合 $\mathcal{J}$

**空間座標系統**：
- **UE位置**：UE $j \in \mathcal{J}$ 的位置表示為笛卡爾座標系中的3維實向量：
  $$\mathbf{q}_j = (q_j^x, q_j^y, q_j^z) \in \mathbb{R}^3$$

- **衛星動態**：衛星 $i$ 在時間 $t \geq 0$ 的位置和速度分別表示為：
  $$\mathbf{q}_i(t) = (q_i^x(t), q_i^y(t), q_i^z(t)) \in \mathbb{R}^3$$
  $$\mathbf{v}_i(t) = (v_i^x(t), v_i^y(t), v_i^z(t)) \in \mathbb{R}^3$$

**系統假設**：
- **衛星數量均等**：每個軌道平面上的衛星數量相等：$|\mathcal{I}_k| = I$，$\forall k \in \mathcal{K}$
- **軌道運動**：所有衛星以相同的軌道週期 $T$ 進行均勻圓周運動
- **空間分佈**：同一軌道平面上任意兩個相鄰衛星之間的弧長相等

#### 4.2.2 離散時間動態模型

**時間離散化**：
時間被離散化為長度為 $\tau$ 的時隙。設 $\mathbf{q}_i[0]$ 為衛星 $i \in \bigcup_{k \in \mathcal{K}} \mathcal{I}_k$ 在 $t = 0$ 時的初始位置。

**狀態空間模型**：
根據離散時間狀態空間模型，衛星 $i$ 在時間 $t = m\tau$ 的位置可表示為：

$$\mathbf{q}_i[m] = \mathbf{q}_i[0] + \tau \sum_{m'=1}^m \mathbf{v}_i[m'\tau] \quad (2)$$

#### 4.2.3 HO程序建模

**場景設定**：
- **服務衛星**：cell $O$ 中的UE連接到服務衛星（作為服務gNB）
- **目標候選**：視野（FoV）中的其他衛星作為HO目標候選（目標gNBs或目標衛星）
- **HO觸發**：服務衛星可以向目標衛星發起HO請求

**HO時機建模**：
- **HO機會數量**：在間隔 $T$ 內有 $N$ 次HO機會
- **連接約束**：除非成功進行HO到目標衛星，否則UE失去與服務衛星的連接
- **時隙定義**：每個HO時隙的持續時間為 $\tau = \frac{T}{N}$，其中 $\tau \in \mathbb{Z}^+$

**動作空間定義**：
在每個HO機會，每個UE決定是否發送HO請求：

$$a_j[n] \in \{0, 1, \ldots, K-1\} \quad (3)$$

其中：
- $K := |\mathcal{K}|$ 是軌道平面的數量
- $k = 0$ 對應服務衛星
- $a_j[n] = 0$ 表示服務衛星不向任何目標衛星發送HO請求

### 4.3 評估指標 (Evaluation)

#### 4.3.1 碰撞類型與分析

LEO衛星HO情境中存在兩種類型的碰撞：

**類型一：由於缺乏資源塊（RBs）導致的NACK**
- **發生條件**：目標衛星接收到的HO請求數量超過RBs數量
- **結果**：發生碰撞

**類型二：物理隨機存取通道（PRACH）碰撞**
- **發生條件**：多個接收HO指令的UE試圖使用相同前導碼簽名存取同一目標衛星
- **觸發時機**：在接收到HO指令後進行隨機存取（RA）信令

#### 4.3.2 隨機存取程序

**5G-NR Release 16 RA信令**：
RA信令按照5G-NR Release 16指南進行兩步驟過程：

**步驟一：前導碼選擇**
UE從每個目標衛星的可用前導碼集合中均勻隨機選擇前導碼：
$$p_k[n] \in \{1, 2, \ldots, P\}, \quad \forall k \in \{1, \ldots, K-1\}$$

其中 $P$ 表示每個衛星在數據傳輸期間可以授予的資源數量。

**步驟二：碰撞檢測**
- **前導碼發送**：決定存取的UE向對應衛星發送前導碼
- **反饋確認**：衛星發送反饋確認每個選擇的前導碼是否有碰撞
- **成功標準**：選擇無碰撞前導碼的UE成功完成HO

#### 4.3.3 碰撞率計算

**資源塊不足碰撞率**：
定義請求指示器 $h_k,j^R[n]$：

$$h_{k,j}^R[n] = \begin{cases} 
1 & \text{if } a_j[n] > 0, a_j^{HO}[n] = 0 \\
0 & \text{otherwise}
\end{cases} \quad (4)$$

碰撞率 $C_k^R[n]$ 計算為：

$$C_k^R[n] = \begin{cases}
0 & \text{if } R_k[n] - \sum_{j=1}^J h_{k,j}^R[n] > 0 \\
\frac{\sum_{j=1}^J h_{k,j}^R[n] - R_k[n]}{J} & \text{otherwise}
\end{cases} \quad (5)$$

**PRACH碰撞率**：
定義碰撞指示器 $c_j^P[n]$：

$$c_j^P[n] = \begin{cases}
1 & \text{if } (h_j^C[n], p_j[n]) = (h_{j'}^C[n], p_{j'}[n]), a_j^{HO}[n] = 0 \\
0 & \text{otherwise}
\end{cases} \quad (6)$$

PRACH碰撞率為：
$$C^P[n] = \frac{1}{|\mathcal{J}|} \sum_{j \in \mathcal{J}} c_j^P[n] \quad (7)$$

**總碰撞率**：
$$C[n] = \sum_{k=1}^{K-1} C_k^R[n] + C^P[n] \quad (8)$$

#### 4.3.4 存取延遲計算

存取延遲測量UE成功HO到目標衛星所需的時間：

$$D[n] = \frac{1}{|\mathcal{J}|} \sum_{j \in \mathcal{J}} (1 - a_j^{HO}[n]) \quad (9)$$

#### 4.3.5 成功HO率

成功HO率測量在所有嘗試HO的UE中成功換手到目標衛星的UE比例：

$$H = \frac{1}{|\mathcal{J}|} \sum_{j \in \mathcal{J}} a_j^{HO}[N] \quad (10)$$

### 4.4 協議設計 (Protocol Design)

#### 4.4.1 DHO協議序列

提出的LEO衛星網路DHO協議包括以下步驟序列：

**步驟一：HO決策**
- **決策主體**：服務衛星做出HO請求決策
- **決策內容**：包括目標衛星選擇和每個UE的退避策略

**步驟二：HO准入**
- **資源檢查**：目標衛星檢查是否有可用RBs支援HO請求
- **ACK發送**：如有資源，目標衛星發送HO請求ACK
- **指令傳輸**：服務衛星向被允許換手的UE發送HO指令

**步驟三：隨機存取**
- **RACH存取**：接收HO指令的UE嘗試存取服務衛星指定的目標衛星
- **前導碼選擇**：隨機選擇RACH存取的前導碼

**步驟四：HO完成**
- **成功條件**：UE成功傳輸到服務衛星指定的目標衛星時完成HO

**關鍵創新**：
提出的DHO協議通過消除對MR的需求來簡化過程。

#### 4.4.2 馬爾可夫決策過程（MDP）建模

**優化問題表述**：
DHO中數學表述的優化問題是在考慮LEO衛星網路約束的情況下，同時最小化存取延遲和碰撞率：

$$\min_{a_j[n]} \sum_{n=1}^N D[n] + \nu C[n], \quad \text{s.t. } (2) \quad (11)$$

其中 $\nu$ 是平衡存取延遲和碰撞率權衡的正規化係數。

**MDP組件定義**：

**環境（Environment）**：
環境由地面UE、服務衛星和目標衛星相互作用組成，遵循MDP模型。

**狀態（State）**：
在MDP模型中，時間索引 $n$ 的狀態定義為：

$$s[n] = \{n, \mathbf{a}^{HO}[n], \mathbf{a}[n-1]\} \quad (12)$$

其中：
- $n$ 表示時間索引
- $\mathbf{a}^{HO}[n] = \{a_1^{HO}[n], a_2^{HO}[n], \ldots, a_J^{HO}[n]\}$
- $\mathbf{a}[n-1]$ 是UE採取的先前動作

**動作（Action）**：
動作空間 $\mathcal{A}$ 涉及HO決策。使用one-hot編碼表示：

$$a_j[n] = \{a_0, a_1, a_2, \ldots, a_{K-1}\}, \quad \text{s.t. } \sum_{k=0}^{K-1} a_k = 1 \quad (13)$$

完整動作集合：
$$\mathbf{a}[n] = \begin{bmatrix} \mathbf{a}_1[n] \\ \mathbf{a}_2[n] \\ \vdots \\ \mathbf{a}_J[n] \end{bmatrix} \quad (14)$$

**獎勵（Reward）**：
獎勵函數設計用於強化服務衛星做出最優存取決策，通過懲罰存取延遲和碰撞率：

$$r[n] = -D[n] - \nu C[n] \quad (15)$$

### 4.5 演算法細節 (Algorithm Details)

#### 4.5.1 IMPALA演算法選擇理由

DHO採用IMPALA演算法，該算法相對於其他DRL演算法（如DQN、A3C和PPO）提供了幾個優勢：

**主要優勢**：
1. **並行actor-learner**：利用並行actor-learner和重要性採樣
2. **採樣效率**：提高採樣效率和可擴展性
3. **off-policy學習**：off-policy強化學習演算法
4. **重要性採樣**：使用重要性採樣改善學習性能

#### 4.5.2 V-trace方法

**off-policy學習中的挑戰**：
在off-policy訓練中，方差很大程度上是由於用於生成行為的策略（行為策略）和正在改進的策略（目標策略）之間的差異。

**V-trace解決方案**：
IMPALA通過使用V-trace目標解決off-policy學習中的策略不匹配問題，這些目標使用截斷重要性採樣進行估計。

**V-trace目標函數**：
考慮軌跡 $(s[n], a[n], r[n]])_{n=s+k}^{n=s}$ 由actor遵循策略 $\mu$ 生成。狀態 $s[n]$ 的k步V-trace目標 $v[n]$ 定義為：

$$v[n] = V(s[n]) + \sum_{n=s}^{s+k-1} \gamma^{n-s} \prod_{i=s}^{n-1} c[i] \delta_n^V \quad (16)$$

其中：
- $\delta_n^V = \rho[n](r[n] + \gamma V(s[n+1]) - V(s[n]))$ 是 $V$ 的時間差分
- $\rho[n]$ 和 $c[n]$ 是截斷重要性採樣（IS）權重

**重要性採樣權重**：
$$\rho[n] = \min\left(\bar{\rho}, \frac{\pi(a[n]|s[n])}{\mu(a[n]|s[n])}\right)$$
$$c[n] = \min\left(\bar{c}, \frac{\pi(a[n]|s[n])}{\mu(a[n]|s[n])}\right)$$

#### 4.5.3 V-trace Actor-Critic演算法

**策略更新**：
在off-policy設定下，可以使用被評估策略 $\pi_{\bar{\rho}}$ 和行為策略 $\mu$ 之間的IS權重：

$$\mathbb{E}_{a[n] \sim \mu(\cdot|s[n])} \left[\frac{\pi_{\bar{\rho}}(a[n]|s[n])}{\mu(a[n]|s[n])} \cdot \nabla \log \pi_{\bar{\rho}}(a[n]|s[n])q[n]|s[n]\right]$$

其中 $q[n] := r[n] + \gamma v[n+1]$ 是 $Q^{\pi_{\bar{\rho}}}(s[n], a[n])$ 的近似。

**參數更新**：
- **價值參數更新**：使用梯度下降對l2損失進行更新
- **策略參數更新**：按照策略梯度方向更新

**完整訓練過程**：
詳見Algorithm 1中的訓練過程描述。

### 4.6 複雜度分析 (Complexity Analysis)

#### 4.6.1 訓練複雜度

**策略基礎DRL的優勢**：
利用策略梯度的策略基礎DRL已被證明相較於價值基礎DRL具有優越的獎勵收斂性。

**IMPALA的優勢特點**：
- **off-policy多actor-learner分散學習**：結合V-Trace提供更快的學習率和改善的性能
- **分散學習方法**：允許高效利用計算資源，使更快收斂成為可能
- **推理速度**：訓練後，DHO代理可以在NVIDIA GeForce RTX 3080 Ti GPU上在幾毫秒內輸出動作

#### 4.6.2 執行複雜度

**DHO的關鍵優勢**：
通過跳過MR步驟簡化HO程序，DHO具有以下優勢：

**複雜度降低**：
- **處理時間減少**：減少整體HO處理時間
- **功耗節省**：節省傳統HO協議中MR所需的UL信令功耗
- **預測能力**：服務衛星代理能夠直接向目標衛星發送HO請求，無需依賴MR

**效率提升**：
通過利用服務衛星代理學習的理解和對UE通道條件的預測，消除MR步驟顯著降低了執行複雜度。

---

## 第五章：數值結果 (Numerical Results)

### 5.1 模擬設置 (Simulation Setup)

#### 5.1.1 環境配置

**地面UE分佈**：
- **分佈區域**：除非另有規定，地面UE在1000×1000 [m²]區域內均勻分佈
- **衛星配置**：場景包括一個服務衛星（k = 0）和兩個目標衛星（k = 1或2）
- **軌道參數**：軌道高度為550 [km]，在三個不同的軌道平面上運行
- **軌道速度**：基於高度使用克卜勒定律確定LEO衛星的軌道速度

**視野（FoV）假設**：
特定興趣區域（A）中UE的FoV假設覆蓋每個軌道通道的一顆衛星。

#### 5.1.2 傳統HO決策參數

**測量更新機制**：
- **濾波測量更新**：$M_{L1}[n]$（即接收信號強度指示器RSSI）在UE每個HO測量週期（$T_M$）更新
- **L3無限脈衝響應（IIR）濾波**：測量$M_{L3}[n]$通過L3 IIR濾波進行評估

**L3濾波公式**：
每個HO決策更新週期（$T_U = T_M/\beta_{L3}$）的L3濾波測量為：

$$M_{L3}[n] = \beta_{L3}M_{L1}[n] + (1 - \beta_{L3})M_{L3}[n-1] \quad (17)$$

其中：
- **遺忘因子**：$\beta_{L3} = 1/2^{(k_{IIR}/4)}$
- **IIR濾波器階數**：$k_{IIR}$

**高動態環境考量**：
在高都卜勒位移和高速情境（如LEO衛星網路）中，對數正態陰影樣本可能不高度相關。在這種情況下，較短的濾波週期可能導致更準確的HO決策制定。

**A3事件標準**：
當滿足A3事件標準時，即目標細胞的L3濾波RSRP超過服務細胞的RSRP加上預定遲滯邊距（稱為事件A3偏移），UE向服務細胞發送通知並通過MR中繼此事件A3條件。

#### 5.1.3 基準方法

**1. HO（傳統）協議**：
- **方法特點**：4G-LTE和5G-NR網路中採用的傳統方法
- **觸發機制**：利用A3事件觸發HO
- **測量方式**：地面UE使用估計的RSRP進行HO測量
- **濾波應用**：應用濾波器以減輕衰落和測量不準確的影響
- **目標選擇**：在HO機會開始時，服務衛星根據有利的RSRP選擇目標衛星

**2. 隨機（啟發式）協議**：
- **動作選擇**：為環境中每個HO機會隨機選擇動作
- **結果多樣性**：每個UE接收特定目標衛星的HO指令或完全不接收
- **碰撞可能性**：如果多個UE嘗試使用相同前導碼簽名存取同一目標衛星時，可能發生碰撞
- **RACH過程**：假設進行兩步驟RACH過程進行RA信令

**3. DHO（提議）方法**：
- **訓練框架**：使用IMPALA框架訓練的HO方法
- **決策方式**：使每個服務衛星能夠為所有地面UE選擇最優HO動作
- **可觀察性**：在部分狀態可觀察性下運作
- **交互範圍**：每個服務衛星代理僅與地面UE交互
- **學習模型**：學習過程可視為部分可觀察馬爾可夫決策過程（POMDP）

### 5.2 HO性能分析

#### 5.2.1 資源塊（RBs）數量影響

**表格IV：不同RB數量下DHO與基準方法的存取延遲和碰撞率比較**

**充足RBs情況（Rₖ = J, k ∈ {1, 2}）**：

| 方法 | 平均存取延遲 | 平均碰撞率（RB不足） | 平均HO成功率 |
|------|-------------|-------------------|-------------|
| HO（傳統） | 0.789 | 0 | 1 |
| 隨機（啟發式） | 0.582 | 0 | 1 |
| DHO（提議） | 0.116 | 0 | 1 |

**數據趨勢洞察**：
1. **DHO顯著優勢**：DHO存取延遲比傳統HO低6.8倍，比隨機方法低5.02倍
2. **資源充足情況**：當RBs充足時，所有方法都能達到100%成功率
3. **延遲最小化**：DHO在延遲最小化方面表現卓越

**RBs不足情況（Rₖ = 0.3J, k ∈ {1, 2}）**：

| 方法 | 平均存取延遲 | 平均碰撞率（RB不足） | 平均HO成功率 |
|------|-------------|-------------------|-------------|
| HO（傳統） | 13.52 | 12.7 | 0.59 |
| 隨機（啟發式） | 8.156 | 5.22 | 0.60 |
| DHO（提議） | 8.053 | 4.15 | 0.60 |

**資源受限環境洞察**：
1. **性能差距縮小**：在資源受限情況下，各方法間性能差距縮小
2. **DHO仍優越**：DHO在存取延遲和碰撞率方面仍保持最佳性能
3. **成功率一致性**：所有方法在資源受限時成功率都降至約60%

#### 5.2.2 前導碼簽名數量影響

**表格V：不同前導碼簽名數量下的性能比較**

**充足前導碼簽名情況（P = 2J）**：

| 方法 | 平均存取延遲 | 平均碰撞率（PRACH碰撞） |
|------|-------------|----------------------|
| HO（傳統） | 1.231 | 3.31 |
| 隨機（啟發式） | 0.851 | 1.02 |
| DHO（提議） | 0.255 | 2.13 |

**前導碼簽名不足情況（P = 0.8J）**：

| 方法 | 平均存取延遲 | 平均碰撞率（PRACH碰撞） |
|------|-------------|----------------------|
| HO（傳統） | 1.740 | 9.41 |
| 隨機（啟發式） | 0.858 | 3.65 |
| DHO（提議） | 0.048 | 4.24 |

**前導碼分析洞察**：
1. **延遲優勢持續**：DHO在各種前導碼情況下都保持存取延遲優勢
2. **碰撞權衡**：DHO優先考慮存取延遲而非碰撞率，在某些情況下碰撞率略高
3. **適應性強**：DHO能夠適應不同的資源配置環境

#### 5.2.3 用戶數量影響

**圖7：用戶數量影響分析（J = 10, Rₖ = 0.5J, ∀k, P = 2J）**

從圖7可以觀察到：

**性能趨勢**：
1. **延遲隨用戶增加**：所有方法的存取延遲都隨用戶數量增加而上升
2. **DHO保持優勢**：即使在高用戶密度下，DHO仍保持最低的存取延遲
3. **可擴展性**：DHO展現了良好的可擴展性

### 5.3 DHO協議行為分析

#### 5.3.1 狀態資訊的消融研究

**圖8：狀態中每項資訊的影響**

消融研究顯示：
1. **時間索引重要性**：時間索引對DHO訓練有顯著影響
2. **存取用戶資訊**：存取的UE資訊是另一個關鍵因素
3. **最小狀態設計**：DHO的狀態設計實現了最小化的同時保持充分的決策資訊

#### 5.3.2 協議行為模式

**表格VI：各種情境下的DHO代理行為**

| 情境 | HO請求（目標衛星aⱼ = 1, 2） | 無HO請求（aⱼ = 0） |
|------|---------------------------|------------------|
| 充足RB和PRACH（案例1+3） | 93.7% | 6.33% |
| RB不足和PRACH（案例2+4） | 81.2% | 91.8% |

**行為模式洞察**：
1. **資源適應性**：DHO根據資源可用性調整行為
2. **智能退避**：在資源不足時，DHO會選擇性地避免發送HO請求
3. **優化策略**：展現了基於網路條件的適應性決策能力

### 5.4 存取延遲與碰撞率權衡分析

#### 5.4.1 權衡係數影響

**表格VII：存取延遲與碰撞率權衡**

| 目標 | 平均存取延遲 | 平均碰撞率（PRACH碰撞） |
|------|-------------|----------------------|
| 延遲感知（ν = 5） | 0.0875 | 0.0845 |
| 碰撞迴避（ν = 1/20） | 0.3461 | 0.0613 |

**權衡分析洞察**：
1. **可調節性**：通過調整ν係數，可以根據應用需求平衡延遲和碰撞
2. **URLLC應用**：對於超可靠低延遲通訊，可設置更高的ν值
3. **mMTC應用**：對於大規模機器類型通訊，可設置較低的ν值以減少碰撞

### 5.5 訓練與收斂分析

#### 5.5.1 不同DRL演算法比較

**圖9：DRL演算法比較**

**演算法性能排序**：
1. **IMPALA**：最佳收斂性能和穩定性
2. **PPO**：中等性能
3. **A3C**：較低性能
4. **DQN**：在大動作空間中面臨挑戰

**IMPALA優勢分析**：
1. **穩定收斂**：相較於其他演算法展現更穩定的收斂行為
2. **採樣效率**：更高的採樣效率和可擴展性
3. **大規模適用性**：特別適合大規模環境和複雜任務

#### 5.5.2 訓練複雜度分析

**圖7：用戶數量與訓練步驟關係**

**可擴展性挑戰**：
1. **維度詛咒**：狀態和動作空間維度增加使DRL學習更具挑戰性
2. **訓練時間**：100個UE的訓練時間約為10個UE的5倍
3. **獎勵下降**：隨著維度增加，學習獎勵趨於下降

**解決方案**：
- **轉移學習**：可使用轉移學習或分散訓練有效擴展到更多UE情況
- **分散訓練**：利用分散訓練技術提高可擴展性

---

## 第六章：結論 (Conclusion)

### 6.1 研究成果總結

本研究提出了一種名為**DHO**的新型HO協議，專門用於解決LEO衛星網路中大規模接入的挑戰。通過使用深度強化學習方法，DHO協議能夠最小化存取延遲和碰撞率，同時簡化HO過程。

#### 主要貢獻回顾

**1. 創新協議設計**：
- **跳過MR階段**：DHO通過跳過測量報告階段來簡化傳統HO程序
- **預測能力**：利用基於預先確定LEO衛星軌道模式的預測能力
- **延遲消除**：消除MR階段產生的傳播延遲

**2. 性能優越性**：
數值結果證實了DHO協議在各種網路條件下相較於傳統HO協議的優越性：
- **存取延遲降低**：相較於傳統HO方法降低高達**6.86倍**
- **與啟發式比較**：相較於啟發式方法降低**4.18倍**
- **廣泛適用性**：在不同的資源配置和網路條件下都展現優異性能

**3. 演算法創新**：
- **IMPALA架構**：採用重要性加權Actor-Learner架構的IMPALA演算法
- **穩定訓練**：確保在大狀態和動作空間中的穩定訓練
- **可擴展性**：展現良好的可擴展性和適應性

#### 實際應用價值

**DHO協議的實際適用性**：
1. **大規模部署**：適用於mega-constellation衛星網路的大規模部署
2. **資源優化**：在有限資源環境下提供優化的HO決策
3. **功耗節省**：通過消除MR顯著降低UE的功耗需求

### 6.2 技術洞察與發現

#### 6.2.1 權衡機制分析

**存取延遲vs碰撞率**：
研究發現存取延遲和碰撞率之間存在固有的權衡關係：
- **最小化碰撞率**：需要減少HO請求嘗試次數，但可能增加存取延遲
- **應用導向調整**：可根據特定應用需求調整權衡係數

**應用場景適配**：
1. **URLLC場景**：
   - **超可靠低延遲通訊**：如實時無線控制和監控系統
   - **優化目標**：最小化存取延遲至關重要
   - **係數設置**：設置較高的ν值優先考慮延遲

2. **mMTC場景**：
   - **大規模機器類型通訊**：連接大量設備的場景
   - **優化目標**：最小化碰撞率以最大化整體網路效率和容量
   - **係數設置**：設置較低的ν值優先考慮碰撞避免

#### 6.2.2 DHO代理行為洞察

**適應性決策機制**：
DHO代理展現了基於網路條件的智能適應能力：
- **資源充足時**：積極發送HO請求以最大化網路利用率
- **資源受限時**：選擇性退避以避免無效碰撞
- **動態調整**：根據實時網路狀況持續更新行為策略

### 6.3 未來發展方向

#### 6.3.1 技術擴展

**再生型LEO衛星網路的全面實現**：
為了完全實現再生型LEO衛星網路的潛力，未來需要：

1. **現有gNB功能優化**：
   - 針對NTN的獨特特性重新設計地面網路中的特定gNB功能
   - 考慮高動態特性和大規模接入需求

2. **系統架構創新**：
   - 開發專門針對LEO衛星網路特性的新架構
   - 整合機載處理能力與地面網路功能

#### 6.3.2 演算法改進

**可擴展性增強**：
1. **轉移學習應用**：
   - 開發轉移學習技術以適應新的網路條件
   - 減少重新訓練的時間和資源需求

2. **分散訓練技術**：
   - 實施分散訓練方法以處理更大規模的UE部署
   - 提高訓練效率和系統可擴展性

3. **多智能體協作**：
   - 探索多智能體DRL方法以處理複雜的衛星-地面協作場景
   - 實現更智能的分散式決策制定

#### 6.3.3 實際部署考量

**產業化挑戰**：
1. **標準化工作**：
   - 與3GPP等標準化組織合作制定NTN-HO標準
   - 確保與現有5G/6G架構的兼容性

2. **硬體實現**：
   - 開發支援DHO協議的衛星和地面設備
   - 優化機載處理硬體以支援實時DRL推理

3. **測試驗證**：
   - 進行大規模實地測試驗證DHO的實際性能
   - 評估在真實衛星網路環境中的可靠性

### 6.4 研究影響與貢獻

#### 學術貢獻

1. **方法論創新**：
   - 首次將DRL系統化應用於LEO衛星網路HO問題
   - 建立了衛星網路HO的MDP建模框架

2. **理論基礎**：
   - 提供了LEO衛星網路HO性能分析的理論基礎
   - 建立了存取延遲與碰撞率權衡的數學模型

#### 產業價值

1. **技術實用性**：
   - 為即將部署的大型衛星星座提供實用的HO解決方案
   - 顯著改善用戶體驗和網路效能

2. **成本效益**：
   - 通過功耗降低和性能提升帶來經濟效益
   - 為衛星通訊產業的可持續發展提供支持

---

## 附錄 (Appendices)

### 附錄A：狀態資訊選擇 (Selection of State Information)

#### A.1 狀態空間設計原則

代理的狀態空間應該包含足夠的決策資訊，同時最小化來自環境的額外數據收集開銷，以促進DRL訓練收斂。

#### A.2 消融研究方法

為了識別狀態空間中包含的重要資訊，我們進行了廣泛的研究，評估各種局部可觀察資訊對提議的DHO協議性能的影響。

**圖8消融研究結果**：

**1. DHO-集中化情況**（圖8a）：
- **狀態定義**：$s^c[n] = \{s[n], A3[n]\}$
- **包含資訊**：局部可觀察和非可觀察（集中化）資訊
- **性能表現**：作為提議DHO方案的上界結果

**2. 關鍵資訊識別**（圖8b）：
研究顯示DHO協議主要依賴兩項局部可觀察資訊：
- **時間索引**：對DHO訓練有重大影響
- **存取用戶資訊**：另一個關鍵因素

#### A.3 分散式潛力

消融研究的重要發現是DHO利用的資訊是局部可觀察的，這突顯了未來工作中分散式多智能體DRL方法的潛力。

### 附錄B：DRL演算法選擇 (Selection of DRL Algorithm)

#### B.1 演算法比較實驗

我們透過實證驗證了IMPALA在我們特定環境中相對於DQN、PPO和A3C的優越性能。

**圖9收斂行為分析**：

**1. DQN的挑戰**（圖9a）：
- **大動作空間問題**：由於大動作空間，DQN難以實現收斂
- **處理方法限制**：與IMPALA、A3C和PPO不同，DQN需要替代方法處理多離散類型動作
- **扁平化需求**：如扁平化動作空間以從所有可能動作中選擇動作

**2. IMPALA優勢**（圖9b）：
- **穩定收斂**：相較於A3C和PPO在穩定收斂方面表現更佳
- **可擴展性**：提供可擴展性和改善的採樣效率
- **性能優越**：在我們的研究中展現出優於其他DRL演算法的優勢

#### B.2 IMPALA技術優勢

**核心技術特點**：
1. **V-trace目標**：有效解決off-policy學習中的策略不匹配問題
2. **重要性採樣**：通過截斷重要性採樣提高學習穩定性
3. **並行架構**：支援多actor並行收集經驗，提高採樣效率
4. **分散式學習**：適合大規模分散式訓練環境

---

## 技術方程式總整理 (Complete Mathematical Formulations)

### 核心技術方程式清單

**1. 載波對雜訊比（CNR）計算** - 方程式(1)：
$$\text{CNR [dB]} = \text{EIRP [dBW]} - P_{FS} \text{[dB]} - P_A \text{[dB]} - P_{SM} \text{[dB]} - P_{SL} \text{[dB]} + G/T \text{[dB/K]} - k \text{[dBW/K/Hz]} - B \text{[dBHz]}$$

**2. 離散時間衛星位置模型** - 方程式(2)：
$$\mathbf{q}_i[m] = \mathbf{q}_i[0] + \tau \sum_{m'=1}^m \mathbf{v}_i[m'\tau]$$

**3. HO動作定義** - 方程式(3)：
$$a_j[n] \in \{0, 1, \ldots, K-1\}$$

**4-8. 碰撞率計算公式**：
- 資源請求指示器 - 方程式(4)：
$$h_{k,j}^R[n] = \begin{cases} 1 & \text{if } a_j[n] > 0, a_j^{HO}[n] = 0 \\ 0 & \text{otherwise} \end{cases}$$

- RB不足碰撞率 - 方程式(5)：
$$C_k^R[n] = \begin{cases} 0 & \text{if } R_k[n] - \sum_{j=1}^J h_{k,j}^R[n] > 0 \\ \frac{\sum_{j=1}^J h_{k,j}^R[n] - R_k[n]}{J} & \text{otherwise} \end{cases}$$

- PRACH碰撞指示器 - 方程式(6)：
$$c_j^P[n] = \begin{cases} 1 & \text{if } (h_j^C[n], p_j[n]) = (h_{j'}^C[n], p_{j'}[n]), a_j^{HO}[n] = 0 \\ 0 & \text{otherwise} \end{cases}$$

- PRACH碰撞率 - 方程式(7)：
$$C^P[n] = \frac{1}{|\mathcal{J}|} \sum_{j \in \mathcal{J}} c_j^P[n]$$

- 總碰撞率 - 方程式(8)：
$$C[n] = \sum_{k=1}^{K-1} C_k^R[n] + C^P[n]$$

**9. 存取延遲** - 方程式(9)：
$$D[n] = \frac{1}{|\mathcal{J}|} \sum_{j \in \mathcal{J}} (1 - a_j^{HO}[n])$$

**10. 成功HO率** - 方程式(10)：
$$H = \frac{1}{|\mathcal{J}|} \sum_{j \in \mathcal{J}} a_j^{HO}[N]$$

**11. 優化目標函數** - 方程式(11)：
$$\min_{a_j[n]} \sum_{n=1}^N D[n] + \nu C[n], \quad \text{s.t. } (2)$$

**12. MDP狀態定義** - 方程式(12)：
$$s[n] = \{n, \mathbf{a}^{HO}[n], \mathbf{a}[n-1]\}$$

**13. One-hot編碼動作** - 方程式(13)：
$$a_j[n] = \{a_0, a_1, a_2, \ldots, a_{K-1}\}, \quad \text{s.t. } \sum_{k=0}^{K-1} a_k = 1$$

**14. 完整動作向量** - 方程式(14)：
$$\mathbf{a}[n] = \begin{bmatrix} \mathbf{a}_1[n] \\ \mathbf{a}_2[n] \\ \vdots \\ \mathbf{a}_J[n] \end{bmatrix}$$

**15. 獎勵函數** - 方程式(15)：
$$r[n] = -D[n] - \nu C[n]$$

**16. V-trace目標函數** - 方程式(16)：
$$v[n] = V(s[n]) + \sum_{n=s}^{s+k-1} \gamma^{n-s} \prod_{i=s}^{n-1} c[i] \delta_n^V$$

**17. L3濾波測量** - 方程式(17)：
$$M_{L3}[n] = \beta_{L3}M_{L1}[n] + (1 - \beta_{L3})M_{L3}[n-1]$$

---

## 研究總結與未來展望

本論文通過提出DHO協議，為LEO衛星網路的換手問題提供了創新的解決方案。通過深度強化學習的應用，不僅解決了傳統方法面臨的延遲和功耗問題，也為未來6G非地面網路的發展奠定了重要的技術基礎。隨著全球衛星網路部署的加速，這項研究將對整個衛星通訊產業產生深遠的影響。

**論文完**

---

*本筆記根據 IEEE Transactions on Wireless Communications 期刊論文"Handover Protocol Learning for LEO Satellite Networks: Access Delay and Collision Minimization"完整整理而成，包含所有主要技術內容、數學公式、實驗數據分析和深度洞察。*