#!/usr/bin/env python3
"""
å‚™ä»½å’Œæ¢å¾©ç®¡ç†å™¨
æ”¯æ´æ•¸æ“šå‚™ä»½ã€å¢é‡å‚™ä»½å’Œç½é›£æ¢å¾©

æ ¹æ“š TODO.md ç¬¬18é …ã€Œéƒ¨ç½²æµç¨‹å„ªåŒ–èˆ‡è‡ªå‹•åŒ–ã€è¦æ±‚è¨­è¨ˆ
"""

import os
import shutil
import tarfile
import gzip
import hashlib
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from enum import Enum
import asyncio
import subprocess
import docker

from deployment.config_manager import ServiceType, DeploymentEnvironment

logger = logging.getLogger(__name__)


class BackupType(str, Enum):
    """å‚™ä»½é¡å‹"""

    FULL = "full"
    INCREMENTAL = "incremental"
    DIFFERENTIAL = "differential"


class BackupStatus(str, Enum):
    """å‚™ä»½ç‹€æ…‹"""

    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    SUCCESS = "success"
    FAILED = "failed"


@dataclass
class BackupRecord:
    """å‚™ä»½è¨˜éŒ„"""

    backup_id: str
    service_type: ServiceType
    backup_type: BackupType
    status: BackupStatus
    start_time: datetime
    end_time: Optional[datetime] = None
    file_path: Optional[str] = None
    file_size_bytes: int = 0
    checksum: Optional[str] = None
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

    @property
    def duration_seconds(self) -> Optional[float]:
        """å‚™ä»½æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰"""
        if self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return None

    @property
    def file_size_mb(self) -> float:
        """æ–‡ä»¶å¤§å°ï¼ˆMBï¼‰"""
        return self.file_size_bytes / (1024 * 1024)


class BackupManager:
    """å‚™ä»½ç®¡ç†å™¨"""

    def __init__(self, workspace_root: str = "."):
        self.workspace_root = Path(workspace_root)
        self.backup_dir = self.workspace_root / "deployment" / "backups"
        self.backup_dir.mkdir(parents=True, exist_ok=True)

        self.backup_history: List[BackupRecord] = []
        self.docker_client = docker.from_env()

        # å‚™ä»½é…ç½®
        self.backup_config = {
            ServiceType.NETSTACK: {
                "containers": ["netstack-mongo"],
                "volumes": ["mongo_data", "mongo_config"],
                "config_files": ["netstack/config", "netstack/compose"],
            },
            ServiceType.SIMWORLD: {
                "containers": ["simworld_postgis"],
                "volumes": ["postgres_data"],
                "config_files": ["simworld/backend", "simworld/frontend"],
            },
        }

        # ä¿ç•™æ”¿ç­–ï¼ˆå¤©æ•¸ï¼‰
        self.retention_policy = {
            BackupType.FULL: 30,
            BackupType.INCREMENTAL: 7,
            BackupType.DIFFERENTIAL: 14,
        }

    def generate_backup_id(
        self, service_type: ServiceType, backup_type: BackupType
    ) -> str:
        """ç”Ÿæˆå‚™ä»½ID"""
        timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        return f"backup_{service_type.value}_{backup_type.value}_{timestamp}"

    async def create_backup(
        self,
        service_type: ServiceType,
        backup_type: BackupType = BackupType.FULL,
        comment: str = "",
    ) -> BackupRecord:
        """å‰µå»ºå‚™ä»½"""
        backup_id = self.generate_backup_id(service_type, backup_type)

        record = BackupRecord(
            backup_id=backup_id,
            service_type=service_type,
            backup_type=backup_type,
            status=BackupStatus.PENDING,
            start_time=datetime.utcnow(),
            metadata={"comment": comment},
        )

        self.backup_history.append(record)

        try:
            logger.info(
                f"ğŸ—„ï¸ é–‹å§‹å‰µå»º {service_type.value} {backup_type.value} å‚™ä»½: {backup_id}"
            )
            record.status = BackupStatus.IN_PROGRESS

            # å‰µå»ºå‚™ä»½ç›®éŒ„
            backup_path = self.backup_dir / backup_id
            backup_path.mkdir(exist_ok=True)

            # æ ¹æ“šå‚™ä»½é¡å‹åŸ·è¡Œä¸åŒç­–ç•¥
            if backup_type == BackupType.FULL:
                success = await self._create_full_backup(
                    service_type, backup_path, record
                )
            elif backup_type == BackupType.INCREMENTAL:
                success = await self._create_incremental_backup(
                    service_type, backup_path, record
                )
            else:  # DIFFERENTIAL
                success = await self._create_differential_backup(
                    service_type, backup_path, record
                )

            if success:
                # å£“ç¸®å‚™ä»½
                compressed_file = await self._compress_backup(backup_path, record)
                if compressed_file:
                    record.file_path = str(compressed_file)
                    record.file_size_bytes = compressed_file.stat().st_size
                    record.checksum = await self._calculate_checksum(compressed_file)
                    record.status = BackupStatus.SUCCESS

                    # æ¸…ç†æœªå£“ç¸®çš„ç›®éŒ„
                    shutil.rmtree(backup_path)

                    logger.info(
                        f"âœ… å‚™ä»½å‰µå»ºæˆåŠŸ: {backup_id} ({record.file_size_mb:.1f}MB)"
                    )
                else:
                    record.status = BackupStatus.FAILED
                    record.error_message = "å£“ç¸®å¤±æ•—"
            else:
                record.status = BackupStatus.FAILED
                if not record.error_message:
                    record.error_message = "å‚™ä»½å‰µå»ºå¤±æ•—"

        except Exception as e:
            record.status = BackupStatus.FAILED
            record.error_message = str(e)
            logger.error(f"âŒ å‚™ä»½å‰µå»ºç•°å¸¸: {e}")

        finally:
            record.end_time = datetime.utcnow()
            await self._save_backup_metadata(record)

        return record

    async def _create_full_backup(
        self, service_type: ServiceType, backup_path: Path, record: BackupRecord
    ) -> bool:
        """å‰µå»ºå®Œæ•´å‚™ä»½"""
        try:
            config = self.backup_config[service_type]

            # 1. å‚™ä»½å®¹å™¨æ•¸æ“š
            for container_name in config["containers"]:
                success = await self._backup_container_data(container_name, backup_path)
                if not success:
                    record.error_message = f"å®¹å™¨æ•¸æ“šå‚™ä»½å¤±æ•—: {container_name}"
                    return False

            # 2. å‚™ä»½ Docker volumes
            for volume_name in config["volumes"]:
                success = await self._backup_docker_volume(volume_name, backup_path)
                if not success:
                    record.error_message = f"Volume å‚™ä»½å¤±æ•—: {volume_name}"
                    return False

            # 3. å‚™ä»½é…ç½®æ–‡ä»¶
            for config_path in config["config_files"]:
                success = await self._backup_config_files(config_path, backup_path)
                if not success:
                    record.error_message = f"é…ç½®æ–‡ä»¶å‚™ä»½å¤±æ•—: {config_path}"
                    return False

            # 4. ä¿å­˜å‚™ä»½å…ƒæ•¸æ“š
            metadata = {
                "backup_type": "full",
                "service_type": service_type.value,
                "timestamp": datetime.utcnow().isoformat(),
                "components": {
                    "containers": config["containers"],
                    "volumes": config["volumes"],
                    "config_files": config["config_files"],
                },
            }

            metadata_file = backup_path / "backup_metadata.json"
            with open(metadata_file, "w") as f:
                json.dump(metadata, f, indent=2)

            return True

        except Exception as e:
            logger.error(f"å®Œæ•´å‚™ä»½å¤±æ•—: {e}")
            return False

    async def _create_incremental_backup(
        self, service_type: ServiceType, backup_path: Path, record: BackupRecord
    ) -> bool:
        """å‰µå»ºå¢é‡å‚™ä»½"""
        try:
            # æ‰¾åˆ°æœ€è¿‘çš„å‚™ä»½ä½œç‚ºåŸºæº–
            last_backup = self._find_last_backup(service_type)
            if not last_backup:
                logger.info("æœªæ‰¾åˆ°åŸºæº–å‚™ä»½ï¼Œå‰µå»ºå®Œæ•´å‚™ä»½")
                return await self._create_full_backup(service_type, backup_path, record)

            logger.info(f"åŸºæ–¼å‚™ä»½å‰µå»ºå¢é‡å‚™ä»½: {last_backup.backup_id}")

            # åªå‚™ä»½è®Šæ›´çš„æ•¸æ“š
            config = self.backup_config[service_type]
            base_time = last_backup.start_time

            # å‚™ä»½è®Šæ›´çš„é…ç½®æ–‡ä»¶
            for config_path in config["config_files"]:
                success = await self._backup_changed_files(
                    config_path, backup_path, base_time
                )
                if not success:
                    record.error_message = f"å¢é‡å‚™ä»½å¤±æ•—: {config_path}"
                    return False

            # å‰µå»ºå¢é‡å‚™ä»½çš„å…ƒæ•¸æ“š
            metadata = {
                "backup_type": "incremental",
                "service_type": service_type.value,
                "timestamp": datetime.utcnow().isoformat(),
                "base_backup": last_backup.backup_id,
                "base_time": base_time.isoformat(),
            }

            metadata_file = backup_path / "backup_metadata.json"
            with open(metadata_file, "w") as f:
                json.dump(metadata, f, indent=2)

            return True

        except Exception as e:
            logger.error(f"å¢é‡å‚™ä»½å¤±æ•—: {e}")
            return False

    async def _create_differential_backup(
        self, service_type: ServiceType, backup_path: Path, record: BackupRecord
    ) -> bool:
        """å‰µå»ºå·®ç•°å‚™ä»½"""
        try:
            # æ‰¾åˆ°æœ€è¿‘çš„å®Œæ•´å‚™ä»½ä½œç‚ºåŸºæº–
            last_full_backup = self._find_last_full_backup(service_type)
            if not last_full_backup:
                logger.info("æœªæ‰¾åˆ°å®Œæ•´å‚™ä»½ï¼Œå‰µå»ºå®Œæ•´å‚™ä»½")
                return await self._create_full_backup(service_type, backup_path, record)

            logger.info(f"åŸºæ–¼å®Œæ•´å‚™ä»½å‰µå»ºå·®ç•°å‚™ä»½: {last_full_backup.backup_id}")

            # å‚™ä»½è‡ªå®Œæ•´å‚™ä»½ä»¥ä¾†çš„æ‰€æœ‰è®Šæ›´
            config = self.backup_config[service_type]
            base_time = last_full_backup.start_time

            # å‚™ä»½è®Šæ›´çš„æ•¸æ“š
            for config_path in config["config_files"]:
                success = await self._backup_changed_files(
                    config_path, backup_path, base_time
                )
                if not success:
                    record.error_message = f"å·®ç•°å‚™ä»½å¤±æ•—: {config_path}"
                    return False

            # å‰µå»ºå·®ç•°å‚™ä»½çš„å…ƒæ•¸æ“š
            metadata = {
                "backup_type": "differential",
                "service_type": service_type.value,
                "timestamp": datetime.utcnow().isoformat(),
                "base_backup": last_full_backup.backup_id,
                "base_time": base_time.isoformat(),
            }

            metadata_file = backup_path / "backup_metadata.json"
            with open(metadata_file, "w") as f:
                json.dump(metadata, f, indent=2)

            return True

        except Exception as e:
            logger.error(f"å·®ç•°å‚™ä»½å¤±æ•—: {e}")
            return False

    async def _backup_container_data(
        self, container_name: str, backup_path: Path
    ) -> bool:
        """å‚™ä»½å®¹å™¨æ•¸æ“š"""
        try:
            container = self.docker_client.containers.get(container_name)

            # å‰µå»ºå®¹å™¨æ•¸æ“šå°å‡º
            if "mongo" in container_name:
                # MongoDB æ•¸æ“šå°å‡º
                dump_cmd = [
                    "docker",
                    "exec",
                    container_name,
                    "mongodump",
                    "--out",
                    "/tmp/backup",
                    "--gzip",
                ]

                process = await asyncio.create_subprocess_exec(
                    *dump_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )

                stdout, stderr = await process.communicate()

                if process.returncode != 0:
                    logger.error(f"MongoDB å°å‡ºå¤±æ•—: {stderr.decode()}")
                    return False

                # è¤‡è£½å°å‡ºçš„æ•¸æ“š
                copy_cmd = [
                    "docker",
                    "cp",
                    f"{container_name}:/tmp/backup",
                    str(backup_path / f"{container_name}_data"),
                ]

                process = await asyncio.create_subprocess_exec(
                    *copy_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                )

                await process.communicate()
                return process.returncode == 0

            elif "postgis" in container_name:
                # PostgreSQL æ•¸æ“šå°å‡º
                dump_file = backup_path / f"{container_name}_dump.sql"

                dump_cmd = [
                    "docker",
                    "exec",
                    container_name,
                    "pg_dumpall",
                    "-U",
                    "postgres",
                ]

                with open(dump_file, "w") as f:
                    process = await asyncio.create_subprocess_exec(
                        *dump_cmd, stdout=f, stderr=asyncio.subprocess.PIPE
                    )

                    _, stderr = await process.communicate()

                if process.returncode != 0:
                    logger.error(f"PostgreSQL å°å‡ºå¤±æ•—: {stderr.decode()}")
                    return False

                return True

            return True

        except Exception as e:
            logger.error(f"å®¹å™¨æ•¸æ“šå‚™ä»½å¤±æ•—: {e}")
            return False

    async def _backup_docker_volume(self, volume_name: str, backup_path: Path) -> bool:
        """å‚™ä»½ Docker volume"""
        try:
            volume_backup_path = backup_path / f"volume_{volume_name}"
            volume_backup_path.mkdir(exist_ok=True)

            # ä½¿ç”¨è‡¨æ™‚å®¹å™¨è¤‡è£½ volume æ•¸æ“š
            copy_cmd = [
                "docker",
                "run",
                "--rm",
                "-v",
                f"{volume_name}:/source",
                "-v",
                f"{volume_backup_path}:/backup",
                "alpine",
                "cp",
                "-a",
                "/source/.",
                "/backup/",
            ]

            process = await asyncio.create_subprocess_exec(
                *copy_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
            )

            stdout, stderr = await process.communicate()

            if process.returncode != 0:
                logger.error(f"Volume å‚™ä»½å¤±æ•—: {stderr.decode()}")
                return False

            return True

        except Exception as e:
            logger.error(f"Docker volume å‚™ä»½å¤±æ•—: {e}")
            return False

    async def _backup_config_files(self, config_path: str, backup_path: Path) -> bool:
        """å‚™ä»½é…ç½®æ–‡ä»¶"""
        try:
            source_path = self.workspace_root / config_path
            target_path = backup_path / "config" / config_path

            if source_path.exists():
                target_path.parent.mkdir(parents=True, exist_ok=True)

                if source_path.is_dir():
                    shutil.copytree(source_path, target_path, dirs_exist_ok=True)
                else:
                    shutil.copy2(source_path, target_path)

                return True
            else:
                logger.warning(f"é…ç½®è·¯å¾‘ä¸å­˜åœ¨: {source_path}")
                return True  # ä¸å­˜åœ¨çš„é…ç½®ä¸ç®—éŒ¯èª¤

        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶å‚™ä»½å¤±æ•—: {e}")
            return False

    async def _backup_changed_files(
        self, config_path: str, backup_path: Path, since_time: datetime
    ) -> bool:
        """å‚™ä»½è®Šæ›´çš„æ–‡ä»¶"""
        try:
            source_path = self.workspace_root / config_path
            target_path = backup_path / "config" / config_path

            if not source_path.exists():
                return True

            changed_files = []

            if source_path.is_dir():
                for file_path in source_path.rglob("*"):
                    if file_path.is_file():
                        file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                        if file_mtime > since_time:
                            changed_files.append(file_path)
            else:
                file_mtime = datetime.fromtimestamp(source_path.stat().st_mtime)
                if file_mtime > since_time:
                    changed_files.append(source_path)

            # è¤‡è£½è®Šæ›´çš„æ–‡ä»¶
            for file_path in changed_files:
                relative_path = file_path.relative_to(self.workspace_root)
                target_file = backup_path / "config" / relative_path
                target_file.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(file_path, target_file)

            logger.info(f"å‚™ä»½äº† {len(changed_files)} å€‹è®Šæ›´æ–‡ä»¶")
            return True

        except Exception as e:
            logger.error(f"è®Šæ›´æ–‡ä»¶å‚™ä»½å¤±æ•—: {e}")
            return False

    async def _compress_backup(
        self, backup_path: Path, record: BackupRecord
    ) -> Optional[Path]:
        """å£“ç¸®å‚™ä»½"""
        try:
            compressed_file = backup_path.with_suffix(".tar.gz")

            with tarfile.open(compressed_file, "w:gz") as tar:
                tar.add(backup_path, arcname=backup_path.name)

            logger.info(f"å‚™ä»½å·²å£“ç¸®: {compressed_file}")
            return compressed_file

        except Exception as e:
            logger.error(f"å‚™ä»½å£“ç¸®å¤±æ•—: {e}")
            return None

    async def _calculate_checksum(self, file_path: Path) -> str:
        """è¨ˆç®—æ–‡ä»¶æ ¡é©—å’Œ"""
        try:
            sha256_hash = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(chunk)
            return sha256_hash.hexdigest()
        except Exception as e:
            logger.error(f"æ ¡é©—å’Œè¨ˆç®—å¤±æ•—: {e}")
            return ""

    def _find_last_backup(self, service_type: ServiceType) -> Optional[BackupRecord]:
        """æ‰¾åˆ°æœ€è¿‘çš„å‚™ä»½"""
        service_backups = [
            b
            for b in self.backup_history
            if b.service_type == service_type and b.status == BackupStatus.SUCCESS
        ]

        if service_backups:
            return max(service_backups, key=lambda x: x.start_time)
        return None

    def _find_last_full_backup(
        self, service_type: ServiceType
    ) -> Optional[BackupRecord]:
        """æ‰¾åˆ°æœ€è¿‘çš„å®Œæ•´å‚™ä»½"""
        full_backups = [
            b
            for b in self.backup_history
            if (
                b.service_type == service_type
                and b.backup_type == BackupType.FULL
                and b.status == BackupStatus.SUCCESS
            )
        ]

        if full_backups:
            return max(full_backups, key=lambda x: x.start_time)
        return None

    async def restore_backup(
        self,
        backup_id: str,
        target_environment: DeploymentEnvironment = DeploymentEnvironment.DEVELOPMENT,
    ) -> bool:
        """æ¢å¾©å‚™ä»½"""
        logger.info(f"ğŸ”„ é–‹å§‹æ¢å¾©å‚™ä»½: {backup_id}")

        # æ‰¾åˆ°å‚™ä»½è¨˜éŒ„
        backup_record = None
        for record in self.backup_history:
            if record.backup_id == backup_id:
                backup_record = record
                break

        if not backup_record:
            logger.error(f"âŒ æ‰¾ä¸åˆ°å‚™ä»½è¨˜éŒ„: {backup_id}")
            return False

        if backup_record.status != BackupStatus.SUCCESS:
            logger.error(f"âŒ å‚™ä»½ç‹€æ…‹ç„¡æ•ˆ: {backup_record.status}")
            return False

        try:
            # è§£å£“å‚™ä»½æ–‡ä»¶
            backup_file = Path(backup_record.file_path)
            if not backup_file.exists():
                logger.error(f"âŒ å‚™ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
                return False

            # é©—è­‰æ ¡é©—å’Œ
            if backup_record.checksum:
                current_checksum = await self._calculate_checksum(backup_file)
                if current_checksum != backup_record.checksum:
                    logger.error("âŒ å‚™ä»½æ–‡ä»¶æ ¡é©—å’Œä¸åŒ¹é…")
                    return False

            # è§£å£“åˆ°è‡¨æ™‚ç›®éŒ„
            temp_dir = self.backup_dir / f"restore_{backup_id}"
            temp_dir.mkdir(exist_ok=True)

            with tarfile.open(backup_file, "r:gz") as tar:
                tar.extractall(temp_dir)

            # è®€å–å‚™ä»½å…ƒæ•¸æ“š
            metadata_file = temp_dir / backup_id / "backup_metadata.json"
            if metadata_file.exists():
                with open(metadata_file, "r") as f:
                    metadata = json.load(f)
            else:
                logger.error("âŒ æ‰¾ä¸åˆ°å‚™ä»½å…ƒæ•¸æ“š")
                return False

            # æ ¹æ“šå‚™ä»½é¡å‹åŸ·è¡Œæ¢å¾©
            if metadata["backup_type"] == "full":
                success = await self._restore_full_backup(
                    temp_dir / backup_id, backup_record, metadata
                )
            else:
                # å¢é‡å’Œå·®ç•°å‚™ä»½éœ€è¦å…ˆæ¢å¾©åŸºæº–å‚™ä»½
                success = await self._restore_incremental_backup(
                    temp_dir / backup_id, backup_record, metadata
                )

            # æ¸…ç†è‡¨æ™‚ç›®éŒ„
            shutil.rmtree(temp_dir)

            if success:
                logger.info(f"âœ… å‚™ä»½æ¢å¾©æˆåŠŸ: {backup_id}")
            else:
                logger.error(f"âŒ å‚™ä»½æ¢å¾©å¤±æ•—: {backup_id}")

            return success

        except Exception as e:
            logger.error(f"âŒ å‚™ä»½æ¢å¾©ç•°å¸¸: {e}")
            return False

    async def _restore_full_backup(
        self, backup_path: Path, record: BackupRecord, metadata: Dict
    ) -> bool:
        """æ¢å¾©å®Œæ•´å‚™ä»½"""
        try:
            service_type = record.service_type

            # 1. åœæ­¢ç›¸é—œæœå‹™
            logger.info("ğŸ›‘ åœæ­¢æœå‹™...")
            if service_type == ServiceType.NETSTACK:
                stop_cmd = ["make", "-C", str(self.workspace_root), "netstack-stop"]
            else:
                stop_cmd = ["make", "-C", str(self.workspace_root), "simworld-stop"]

            process = await asyncio.create_subprocess_exec(*stop_cmd)
            await process.communicate()

            # 2. æ¢å¾©é…ç½®æ–‡ä»¶
            config_path = backup_path / "config"
            if config_path.exists():
                for item in config_path.rglob("*"):
                    if item.is_file():
                        relative_path = item.relative_to(config_path)
                        target_path = self.workspace_root / relative_path
                        target_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(item, target_path)

            # 3. æ¢å¾©å®¹å™¨æ•¸æ“š
            for container_data in backup_path.glob("*_data"):
                container_name = container_data.name.replace("_data", "")
                await self._restore_container_data(container_name, container_data)

            # 4. æ¢å¾© Docker volumes
            for volume_dir in backup_path.glob("volume_*"):
                volume_name = volume_dir.name.replace("volume_", "")
                await self._restore_docker_volume(volume_name, volume_dir)

            return True

        except Exception as e:
            logger.error(f"å®Œæ•´å‚™ä»½æ¢å¾©å¤±æ•—: {e}")
            return False

    async def _restore_incremental_backup(
        self, backup_path: Path, record: BackupRecord, metadata: Dict
    ) -> bool:
        """æ¢å¾©å¢é‡å‚™ä»½"""
        try:
            # å…ˆæ¢å¾©åŸºæº–å‚™ä»½
            base_backup_id = metadata.get("base_backup")
            if base_backup_id:
                base_success = await self.restore_backup(base_backup_id)
                if not base_success:
                    logger.error("âŒ åŸºæº–å‚™ä»½æ¢å¾©å¤±æ•—")
                    return False

            # å†æ‡‰ç”¨å¢é‡è®Šæ›´
            config_path = backup_path / "config"
            if config_path.exists():
                for item in config_path.rglob("*"):
                    if item.is_file():
                        relative_path = item.relative_to(config_path)
                        target_path = self.workspace_root / relative_path
                        target_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(item, target_path)

            return True

        except Exception as e:
            logger.error(f"å¢é‡å‚™ä»½æ¢å¾©å¤±æ•—: {e}")
            return False

    async def _restore_container_data(
        self, container_name: str, data_path: Path
    ) -> bool:
        """æ¢å¾©å®¹å™¨æ•¸æ“š"""
        try:
            if "mongo" in container_name:
                # æ¢å¾© MongoDB æ•¸æ“š
                restore_cmd = [
                    "docker",
                    "exec",
                    container_name,
                    "mongorestore",
                    "--gzip",
                    "/tmp/backup",
                ]

                # å…ˆè¤‡è£½æ•¸æ“šåˆ°å®¹å™¨
                copy_cmd = ["docker", "cp", str(data_path), f"{container_name}:/tmp/"]

                process = await asyncio.create_subprocess_exec(*copy_cmd)
                await process.communicate()

                if process.returncode == 0:
                    process = await asyncio.create_subprocess_exec(*restore_cmd)
                    await process.communicate()
                    return process.returncode == 0

            elif "postgis" in container_name:
                # æ¢å¾© PostgreSQL æ•¸æ“š
                dump_file = data_path.parent / f"{container_name}_dump.sql"
                if dump_file.exists():
                    restore_cmd = [
                        "docker",
                        "exec",
                        "-i",
                        container_name,
                        "psql",
                        "-U",
                        "postgres",
                    ]

                    with open(dump_file, "r") as f:
                        process = await asyncio.create_subprocess_exec(
                            *restore_cmd,
                            stdin=f,
                            stdout=asyncio.subprocess.PIPE,
                            stderr=asyncio.subprocess.PIPE,
                        )

                        await process.communicate()
                        return process.returncode == 0

            return True

        except Exception as e:
            logger.error(f"å®¹å™¨æ•¸æ“šæ¢å¾©å¤±æ•—: {e}")
            return False

    async def _restore_docker_volume(self, volume_name: str, volume_path: Path) -> bool:
        """æ¢å¾© Docker volume"""
        try:
            restore_cmd = [
                "docker",
                "run",
                "--rm",
                "-v",
                f"{volume_name}:/target",
                "-v",
                f"{volume_path}:/source",
                "alpine",
                "cp",
                "-a",
                "/source/.",
                "/target/",
            ]

            process = await asyncio.create_subprocess_exec(*restore_cmd)
            await process.communicate()

            return process.returncode == 0

        except Exception as e:
            logger.error(f"Docker volume æ¢å¾©å¤±æ•—: {e}")
            return False

    async def _save_backup_metadata(self, record: BackupRecord):
        """ä¿å­˜å‚™ä»½å…ƒæ•¸æ“š"""
        try:
            metadata_file = self.backup_dir / f"{record.backup_id}_metadata.json"

            metadata = {
                "backup_id": record.backup_id,
                "service_type": record.service_type.value,
                "backup_type": record.backup_type.value,
                "status": record.status.value,
                "start_time": record.start_time.isoformat(),
                "end_time": record.end_time.isoformat() if record.end_time else None,
                "duration_seconds": record.duration_seconds,
                "file_path": record.file_path,
                "file_size_bytes": record.file_size_bytes,
                "checksum": record.checksum,
                "error_message": record.error_message,
                "metadata": record.metadata,
            }

            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

        except Exception as e:
            logger.error(f"ä¿å­˜å‚™ä»½å…ƒæ•¸æ“šå¤±æ•—: {e}")

    async def cleanup_old_backups(self):
        """æ¸…ç†éæœŸå‚™ä»½"""
        logger.info("ğŸ§¹ é–‹å§‹æ¸…ç†éæœŸå‚™ä»½...")

        current_time = datetime.utcnow()
        cleaned_count = 0

        for record in self.backup_history[:]:  # è¤‡è£½åˆ—è¡¨é¿å…ä¿®æ”¹å•é¡Œ
            retention_days = self.retention_policy.get(record.backup_type, 30)
            expiry_time = record.start_time + timedelta(days=retention_days)

            if current_time > expiry_time:
                # åˆªé™¤å‚™ä»½æ–‡ä»¶
                if record.file_path and Path(record.file_path).exists():
                    Path(record.file_path).unlink()

                # åˆªé™¤å…ƒæ•¸æ“šæ–‡ä»¶
                metadata_file = self.backup_dir / f"{record.backup_id}_metadata.json"
                if metadata_file.exists():
                    metadata_file.unlink()

                # å¾æ­·å²è¨˜éŒ„ä¸­ç§»é™¤
                self.backup_history.remove(record)
                cleaned_count += 1

                logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç†éæœŸå‚™ä»½: {record.backup_id}")

        logger.info(f"âœ… æ¸…ç†å®Œæˆï¼Œå…±æ¸…ç† {cleaned_count} å€‹éæœŸå‚™ä»½")

    def list_backups(
        self, service_type: Optional[ServiceType] = None, limit: int = 20
    ) -> List[BackupRecord]:
        """åˆ—å‡ºå‚™ä»½è¨˜éŒ„"""
        filtered_backups = self.backup_history

        if service_type:
            filtered_backups = [
                b for b in filtered_backups if b.service_type == service_type
            ]

        # æŒ‰æ™‚é–“å€’åºæ’åˆ—
        sorted_backups = sorted(
            filtered_backups, key=lambda x: x.start_time, reverse=True
        )

        return sorted_backups[:limit]

    def get_backup_info(self, backup_id: str) -> Optional[BackupRecord]:
        """ç²å–å‚™ä»½ä¿¡æ¯"""
        for record in self.backup_history:
            if record.backup_id == backup_id:
                return record
        return None


# å…¨å±€å‚™ä»½ç®¡ç†å™¨å¯¦ä¾‹
backup_manager = BackupManager()
