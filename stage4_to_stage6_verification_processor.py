#!/usr/bin/env python3
"""
éšæ®µå››åˆ°å…­å®Œæ•´é©—è­‰è™•ç†å™¨

ä½¿ç”¨ä¿®å¾©å¾Œçš„éšæ®µä¸‰ç”¢å‡ºæ•¸æ“šï¼ŒåŸ·è¡Œéšæ®µå››ã€äº”ã€å…­çš„å®Œæ•´é©—è­‰
é‡é»é—œæ³¨éšæ®µå…­æ™‚åºä¿å­˜ç‡0%å•é¡Œçš„ä¿®å¾©ç‹€æ³
"""

import os
import sys
import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional

# è¨­å®š Python è·¯å¾‘
sys.path.insert(0, '/home/sat/ntn-stack/netstack')
sys.path.insert(0, '/home/sat/ntn-stack/netstack/src')

# å¼•ç”¨è™•ç†å™¨
from src.stages.tle_orbital_calculation_processor import Stage1TLEProcessor
from src.stages.intelligent_satellite_filter_processor import IntelligentSatelliteFilterProcessor
from src.stages.signal_quality_analysis_processor import SignalQualityAnalysisProcessor
from src.stages.timeseries_preprocessing_processor import TimeseriesPreprocessingProcessor
from src.stages.data_integration_processor import DataIntegrationProcessor
from src.stages.enhanced_dynamic_pool_planner import EnhancedDynamicPoolPlanner

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

logger = logging.getLogger(__name__)

class Stage4To6VerificationProcessor:
    """éšæ®µå››åˆ°å…­å®Œæ•´é©—è­‰è™•ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–é©—è­‰è™•ç†å™¨"""
        logger.info("ğŸ” éšæ®µå››åˆ°å…­é©—è­‰è™•ç†å™¨åˆå§‹åŒ–")
        
        # åˆå§‹åŒ–æ‰€æœ‰è™•ç†å™¨
        self.stage1_processor = Stage1TLEProcessor(
            tle_data_dir="/app/tle_data",
            output_dir="/app/data",
            sample_mode=False  # å…¨é‡è™•ç†æ¨¡å¼
        )
        
        self.stage2_processor = IntelligentSatelliteFilterProcessor(
            input_dir="/app/data",
            output_dir="/app/data"
        )
        
        self.stage3_processor = SignalQualityAnalysisProcessor(
            input_dir="/app/data",
            output_dir="/app/data"
        )
        
        self.stage4_processor = TimeseriesPreprocessingProcessor(
            input_dir="/app/data",
            output_dir="/app/data"
        )
        
        self.stage5_processor = DataIntegrationProcessor(
            input_dir="/app/data",
            output_dir="/app/data"
        )
        
        self.stage6_processor = EnhancedDynamicPoolPlanner(
            input_dir="/app/data",
            output_dir="/app/data"
        )
        
        logger.info("âœ… æ‰€æœ‰è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        
    def analyze_data_structure(self, data: Dict[str, Any], stage_name: str) -> Dict[str, Any]:
        """åˆ†ææ•¸æ“šçµæ§‹ - ä¿®å¾©ç‰ˆæœ¬"""
        logger.info(f"ğŸ” åˆ†æ {stage_name} æ•¸æ“šçµæ§‹...")
        
        analysis = {
            'stage': stage_name,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'satellites_count': 0,
            'constellations': {},
            'total_constellations': 0,
            'data_structure': 'unknown'
        }
        
        try:
            # æª¢æŸ¥metadata
            metadata = data.get('metadata', {})
            analysis['has_metadata'] = bool(metadata)
            if metadata:
                analysis['metadata_satellites'] = metadata.get('total_satellites', 0)
                analysis['processing_stage'] = metadata.get('processing_stage', 'unknown')
            
            # æª¢æŸ¥constellations
            constellations = data.get('constellations', {})
            analysis['total_constellations'] = len(constellations)
            
            total_satellites = 0
            for const_name, const_data in constellations.items():
                logger.info(f"  ğŸ” åˆ†ææ˜Ÿåº§: {const_name}")
                logger.info(f"    æ•¸æ“šéµ: {list(const_data.keys()) if isinstance(const_data, dict) else 'not_dict'}")
                
                satellites_count = 0
                
                # æª¢æŸ¥orbit_data.satellites (å­—å…¸æ ¼å¼)
                if 'orbit_data' in const_data and 'satellites' in const_data['orbit_data']:
                    satellites = const_data['orbit_data']['satellites']
                    if isinstance(satellites, dict):
                        satellites_count = len(satellites)
                        logger.info(f"    âœ… orbit_data.satellites: å­—å…¸æ ¼å¼ï¼Œ{satellites_count} é¡†è¡›æ˜Ÿ")
                    elif isinstance(satellites, list):
                        satellites_count = len(satellites)
                        logger.info(f"    âœ… orbit_data.satellites: åˆ—è¡¨æ ¼å¼ï¼Œ{satellites_count} é¡†è¡›æ˜Ÿ")
                    else:
                        logger.info(f"    âš ï¸ orbit_data.satellites: {type(satellites).__name__} æ ¼å¼")
                        
                # æª¢æŸ¥ç›´æ¥satellitesæ¬„ä½
                elif 'satellites' in const_data:
                    satellites = const_data['satellites']
                    if isinstance(satellites, (dict, list)):
                        satellites_count = len(satellites)
                        logger.info(f"    âœ… satellites: {type(satellites).__name__} æ ¼å¼ï¼Œ{satellites_count} é¡†è¡›æ˜Ÿ")
                    else:
                        logger.info(f"    âš ï¸ satellites: {type(satellites).__name__} æ ¼å¼")
                        
                # æª¢æŸ¥å…¶ä»–å¯èƒ½çš„çµæ§‹
                else:
                    # æª¢æŸ¥æ˜¯å¦æœ‰å…¶ä»–åŒ…å«è¡›æ˜Ÿæ•¸æ“šçš„æ¬„ä½
                    for key, value in const_data.items():
                        if isinstance(value, (dict, list)) and len(str(key).lower()) > 3:
                            if 'satellite' in key.lower() or 'data' in key.lower():
                                if isinstance(value, (dict, list)) and len(value) > 0:
                                    satellites_count = len(value)
                                    logger.info(f"    âœ… æ‰¾åˆ° {key}: {type(value).__name__} æ ¼å¼ï¼Œ{satellites_count} é …ç›®")
                                    break
                
                analysis['constellations'][const_name] = satellites_count
                total_satellites += satellites_count
                
            analysis['satellites_count'] = total_satellites
            
        except Exception as e:
            logger.error(f"åˆ†ææ•¸æ“šçµæ§‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            analysis['error'] = str(e)
            
        logger.info(f"  ğŸ“Š {stage_name} ç¸½è¨ˆ: {analysis['satellites_count']} é¡†è¡›æ˜Ÿ")
        return analysis
        
    def check_file_cleanup(self, stage_name: str, expected_files: List[str]) -> Dict[str, Any]:
        """æª¢æŸ¥æª”æ¡ˆæ¸…ç†æ©Ÿåˆ¶"""
        logger.info(f"ğŸ—‘ï¸ æª¢æŸ¥ {stage_name} æª”æ¡ˆæ¸…ç†æ©Ÿåˆ¶...")
        
        cleanup_result = {
            'stage': stage_name,
            'files_checked': [],
            'files_cleaned': 0,
            'cleanup_success': True
        }
        
        base_path = Path("/app/data")
        
        for filename in expected_files:
            file_path = base_path / filename
            
            file_info = {
                'filename': filename,
                'exists_before': file_path.exists(),
                'size_before': 0,
                'cleaned': False
            }
            
            if file_path.exists():
                file_info['size_before'] = file_path.stat().st_size
                logger.info(f"  ğŸ“ ç™¼ç¾èˆŠæª”æ¡ˆ: {filename} ({file_info['size_before']} bytes)")
                
                # æ¨¡æ“¬æ¸…ç†æª¢æŸ¥ - æª¢æŸ¥æ˜¯å¦æœƒè¢«è™•ç†å™¨æ¸…ç†
                file_info['should_be_cleaned'] = True
            else:
                logger.info(f"  âœ… ç„¡èˆŠæª”æ¡ˆ: {filename}")
                file_info['should_be_cleaned'] = False
                
            cleanup_result['files_checked'].append(file_info)
            
        return cleanup_result
        
    def execute_stage4_verification(self, stage3_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œéšæ®µå››é©—è­‰"""
        logger.info("================================================================================")
        logger.info("ğŸ” éšæ®µå››ï¼šæ™‚åºé è™•ç†é©—è­‰")
        logger.info("================================================================================")
        
        start_time = datetime.now()
        
        # åˆ†æè¼¸å…¥æ•¸æ“š
        input_analysis = self.analyze_data_structure(stage3_data, "éšæ®µå››è¼¸å…¥")
        
        # æª¢æŸ¥æª”æ¡ˆæ¸…ç†
        cleanup_check = self.check_file_cleanup("éšæ®µå››", [
            "starlink_enhanced.json",
            "oneweb_enhanced.json", 
            "conversion_statistics.json"
        ])
        
        # åŸ·è¡Œéšæ®µå››è™•ç†
        logger.info("ğŸš€ åŸ·è¡Œéšæ®µå››æ™‚åºé è™•ç†...")
        try:
            stage4_output = self.stage4_processor.process_timeseries_preprocessing(stage3_data)
            processing_success = True
            error_message = None
        except Exception as e:
            logger.error(f"éšæ®µå››è™•ç†å¤±æ•—: {e}")
            stage4_output = None
            processing_success = False
            error_message = str(e)
        
        # åˆ†æè¼¸å‡ºæ•¸æ“š
        if stage4_output:
            output_analysis = self.analyze_data_structure(stage4_output, "éšæ®µå››è¼¸å‡º")
        else:
            output_analysis = {'satellites_count': 0, 'error': error_message}
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        result = {
            'stage': 'stage4',
            'processing_time': processing_time,
            'processing_success': processing_success,
            'input_analysis': input_analysis,
            'output_analysis': output_analysis,
            'cleanup_check': cleanup_check,
            'data_flow': f"{input_analysis['satellites_count']} â†’ {output_analysis['satellites_count']}",
            'error_message': error_message
        }
        
        logger.info(f"âœ… éšæ®µå››é©—è­‰å®Œæˆ")
        logger.info(f"  â±ï¸  è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
        logger.info(f"  ğŸ“Š æ•¸æ“šæµ: {result['data_flow']}")
        
        return result, stage4_output
        
    def execute_stage5_verification(self, stage4_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œéšæ®µäº”é©—è­‰"""
        logger.info("================================================================================")
        logger.info("ğŸ” éšæ®µäº”ï¼šæ•¸æ“šæ•´åˆé©—è­‰")
        logger.info("================================================================================")
        
        start_time = datetime.now()
        
        # åˆ†æè¼¸å…¥æ•¸æ“š
        input_analysis = self.analyze_data_structure(stage4_data, "éšæ®µäº”è¼¸å…¥")
        
        # æª¢æŸ¥æª”æ¡ˆæ¸…ç†
        cleanup_check = self.check_file_cleanup("éšæ®µäº”", [
            "data_integration_output.json",
            "integration_statistics.json"
        ])
        
        # åŸ·è¡Œéšæ®µäº”è™•ç†
        logger.info("ğŸš€ åŸ·è¡Œéšæ®µäº”æ•¸æ“šæ•´åˆ...")
        try:
            stage5_output = self.stage5_processor.process_data_integration(stage4_data)
            processing_success = True
            error_message = None
        except Exception as e:
            logger.error(f"éšæ®µäº”è™•ç†å¤±æ•—: {e}")
            stage5_output = None
            processing_success = False
            error_message = str(e)
        
        # åˆ†æè¼¸å‡ºæ•¸æ“š
        if stage5_output:
            output_analysis = self.analyze_data_structure(stage5_output, "éšæ®µäº”è¼¸å‡º")
        else:
            output_analysis = {'satellites_count': 0, 'error': error_message}
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        result = {
            'stage': 'stage5',
            'processing_time': processing_time,
            'processing_success': processing_success,
            'input_analysis': input_analysis,
            'output_analysis': output_analysis,
            'cleanup_check': cleanup_check,
            'data_flow': f"{input_analysis['satellites_count']} â†’ {output_analysis['satellites_count']}",
            'error_message': error_message
        }
        
        logger.info(f"âœ… éšæ®µäº”é©—è­‰å®Œæˆ")
        logger.info(f"  â±ï¸  è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
        logger.info(f"  ğŸ“Š æ•¸æ“šæµ: {result['data_flow']}")
        
        return result, stage5_output
        
    def execute_stage6_verification(self, stage5_data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸ·è¡Œéšæ®µå…­é©—è­‰ - é‡é»é—œæ³¨æ™‚åºä¿å­˜ç‡å•é¡Œ"""
        logger.info("================================================================================")
        logger.info("ğŸ” éšæ®µå…­ï¼šå‹•æ…‹æ± è¦åŠƒé©—è­‰ (é‡é»æª¢æŸ¥æ™‚åºä¿å­˜ç‡)")
        logger.info("================================================================================")
        
        start_time = datetime.now()
        
        # åˆ†æè¼¸å…¥æ•¸æ“š
        input_analysis = self.analyze_data_structure(stage5_data, "éšæ®µå…­è¼¸å…¥")
        
        # æª¢æŸ¥æª”æ¡ˆæ¸…ç†
        cleanup_check = self.check_file_cleanup("éšæ®µå…­", [
            "enhanced_dynamic_pools_output.json",
            "dynamic_planning_statistics.json"
        ])
        
        # ç‰¹åˆ¥æª¢æŸ¥æ™‚åºæ•¸æ“šçµæ§‹
        logger.info("ğŸ” ç‰¹åˆ¥æª¢æŸ¥æ™‚åºæ•¸æ“šçµæ§‹...")
        timeseries_analysis = self.analyze_timeseries_structure(stage5_data)
        
        # åŸ·è¡Œéšæ®µå…­è™•ç†
        logger.info("ğŸš€ åŸ·è¡Œéšæ®µå…­å‹•æ…‹æ± è¦åŠƒ...")
        try:
            stage6_output = self.stage6_processor.process_enhanced_dynamic_pool_planning(stage5_data)
            processing_success = True
            error_message = None
        except Exception as e:
            logger.error(f"éšæ®µå…­è™•ç†å¤±æ•—: {e}")
            stage6_output = None
            processing_success = False
            error_message = str(e)
        
        # åˆ†æè¼¸å‡ºæ•¸æ“š
        if stage6_output:
            output_analysis = self.analyze_data_structure(stage6_output, "éšæ®µå…­è¼¸å‡º")
            # ç‰¹åˆ¥æª¢æŸ¥æ™‚åºä¿å­˜ç‡
            timeseries_preservation = self.check_timeseries_preservation(stage5_data, stage6_output)
        else:
            output_analysis = {'satellites_count': 0, 'error': error_message}
            timeseries_preservation = {'preservation_rate': 0.0, 'error': error_message}
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        result = {
            'stage': 'stage6',
            'processing_time': processing_time,
            'processing_success': processing_success,
            'input_analysis': input_analysis,
            'output_analysis': output_analysis,
            'cleanup_check': cleanup_check,
            'timeseries_analysis': timeseries_analysis,
            'timeseries_preservation': timeseries_preservation,
            'data_flow': f"{input_analysis['satellites_count']} â†’ {output_analysis['satellites_count']}",
            'error_message': error_message
        }
        
        logger.info(f"âœ… éšæ®µå…­é©—è­‰å®Œæˆ")
        logger.info(f"  â±ï¸  è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
        logger.info(f"  ğŸ“Š æ•¸æ“šæµ: {result['data_flow']}")
        logger.info(f"  ğŸ“ˆ æ™‚åºä¿å­˜ç‡: {timeseries_preservation.get('preservation_rate', 0):.1%}")
        
        return result, stage6_output
        
    def analyze_timeseries_structure(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ™‚åºæ•¸æ“šçµæ§‹"""
        logger.info("ğŸ“Š åˆ†ææ™‚åºæ•¸æ“šçµæ§‹...")
        
        timeseries_info = {
            'has_timeseries': False,
            'timeseries_fields': [],
            'total_timestamps': 0,
            'constellations_with_timeseries': {}
        }
        
        try:
            constellations = data.get('constellations', {})
            
            for const_name, const_data in constellations.items():
                const_timeseries = {
                    'has_timeseries': False,
                    'timestamps': 0,
                    'satellites_with_timeseries': 0
                }
                
                # æª¢æŸ¥å„ç¨®å¯èƒ½çš„æ™‚åºçµæ§‹
                satellites_data = None
                
                if 'orbit_data' in const_data and 'satellites' in const_data['orbit_data']:
                    satellites_data = const_data['orbit_data']['satellites']
                elif 'satellites' in const_data:
                    satellites_data = const_data['satellites']
                
                if satellites_data and isinstance(satellites_data, dict):
                    for sat_id, sat_data in satellites_data.items():
                        if isinstance(sat_data, dict):
                            # æª¢æŸ¥å¸¸è¦‹çš„æ™‚åºæ¬„ä½
                            timeseries_fields = ['positions', 'orbit_data', 'trajectory', 'timestamps']
                            for field in timeseries_fields:
                                if field in sat_data:
                                    field_data = sat_data[field]
                                    if isinstance(field_data, list) and len(field_data) > 0:
                                        const_timeseries['has_timeseries'] = True
                                        const_timeseries['timestamps'] = max(const_timeseries['timestamps'], len(field_data))
                                        const_timeseries['satellites_with_timeseries'] += 1
                                        if field not in timeseries_info['timeseries_fields']:
                                            timeseries_info['timeseries_fields'].append(field)
                                        break
                
                timeseries_info['constellations_with_timeseries'][const_name] = const_timeseries
                if const_timeseries['has_timeseries']:
                    timeseries_info['has_timeseries'] = True
                    timeseries_info['total_timestamps'] += const_timeseries['timestamps']
                
                logger.info(f"    {const_name}: {const_timeseries['satellites_with_timeseries']} è¡›æ˜Ÿæœ‰æ™‚åºæ•¸æ“šï¼Œ{const_timeseries['timestamps']} æ™‚é–“é»")
                
        except Exception as e:
            logger.error(f"åˆ†ææ™‚åºçµæ§‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            timeseries_info['error'] = str(e)
        
        logger.info(f"  ğŸ“Š æ™‚åºåˆ†æ: æœ‰æ™‚åºæ•¸æ“š={timeseries_info['has_timeseries']}, ç¸½æ™‚é–“é»={timeseries_info['total_timestamps']}")
        
        return timeseries_info
        
    def check_timeseries_preservation(self, input_data: Dict[str, Any], output_data: Dict[str, Any]) -> Dict[str, Any]:
        """æª¢æŸ¥æ™‚åºä¿å­˜ç‡"""
        logger.info("ğŸ“ˆ æª¢æŸ¥æ™‚åºä¿å­˜ç‡...")
        
        preservation = {
            'preservation_rate': 0.0,
            'input_timeseries_count': 0,
            'output_timeseries_count': 0,
            'details': {}
        }
        
        try:
            input_ts = self.analyze_timeseries_structure(input_data)
            output_ts = self.analyze_timeseries_structure(output_data)
            
            preservation['input_timeseries_count'] = input_ts['total_timestamps']
            preservation['output_timeseries_count'] = output_ts['total_timestamps']
            
            if preservation['input_timeseries_count'] > 0:
                preservation['preservation_rate'] = preservation['output_timeseries_count'] / preservation['input_timeseries_count']
            
            preservation['details'] = {
                'input_analysis': input_ts,
                'output_analysis': output_ts
            }
            
        except Exception as e:
            logger.error(f"æª¢æŸ¥æ™‚åºä¿å­˜ç‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            preservation['error'] = str(e)
        
        logger.info(f"  ğŸ“ˆ æ™‚åºä¿å­˜ç‡: {preservation['preservation_rate']:.1%} ({preservation['output_timeseries_count']}/{preservation['input_timeseries_count']})")
        
        return preservation
        
    def execute_full_verification(self) -> Dict[str, Any]:
        """åŸ·è¡Œå®Œæ•´çš„éšæ®µå››åˆ°å…­é©—è­‰"""
        logger.info("ğŸš€ é–‹å§‹å®Œæ•´çš„éšæ®µå››åˆ°å…­é©—è­‰åŸ·è¡Œ")
        logger.info("================================================================================")
        
        start_time = datetime.now()
        
        # é¦–å…ˆåŸ·è¡Œéšæ®µä¸€åˆ°ä¸‰ç²å–æ•¸æ“š
        logger.info("ğŸ”„ é‡æ–°åŸ·è¡Œéšæ®µä¸€åˆ°ä¸‰ç²å–åŸºç¤æ•¸æ“š...")
        stage1_data = self.stage1_processor.process_tle_orbital_calculation()
        stage2_data = self.stage2_processor.process_intelligent_satellite_filter(stage1_data)
        stage3_data = self.stage3_processor.process_signal_quality_analysis(stage2_data)
        
        logger.info("âœ… åŸºç¤æ•¸æ“šæº–å‚™å®Œæˆï¼Œé–‹å§‹éšæ®µå››åˆ°å…­é©—è­‰")
        
        # åŸ·è¡Œéšæ®µå››é©—è­‰
        stage4_result, stage4_data = self.execute_stage4_verification(stage3_data)
        
        # åŸ·è¡Œéšæ®µäº”é©—è­‰
        if stage4_data:
            stage5_result, stage5_data = self.execute_stage5_verification(stage4_data)
        else:
            logger.error("éšæ®µå››å¤±æ•—ï¼Œè·³ééšæ®µäº”")
            stage5_result = {'stage': 'stage5', 'processing_success': False, 'error_message': 'Stage4 failed'}
            stage5_data = None
        
        # åŸ·è¡Œéšæ®µå…­é©—è­‰
        if stage5_data:
            stage6_result, stage6_data = self.execute_stage6_verification(stage5_data)
        else:
            logger.error("éšæ®µäº”å¤±æ•—ï¼Œè·³ééšæ®µå…­")
            stage6_result = {'stage': 'stage6', 'processing_success': False, 'error_message': 'Stage5 failed'}
            stage6_data = None
        
        total_time = (datetime.now() - start_time).total_seconds()
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        verification_report = {
            'verification_timestamp': datetime.now(timezone.utc).isoformat(),
            'total_processing_time': total_time,
            'stages_results': {
                'stage4': stage4_result,
                'stage5': stage5_result,
                'stage6': stage6_result
            },
            'overall_success': all([
                stage4_result.get('processing_success', False),
                stage5_result.get('processing_success', False),
                stage6_result.get('processing_success', False)
            ]),
            'data_flow_summary': f"S4:{stage4_result.get('data_flow', 'failed')} | S5:{stage5_result.get('data_flow', 'failed')} | S6:{stage6_result.get('data_flow', 'failed')}",
            'timeseries_preservation_rate': stage6_result.get('timeseries_preservation', {}).get('preservation_rate', 0.0)
        }
        
        logger.info("================================================================================")
        logger.info("ğŸ¯ éšæ®µå››åˆ°å…­é©—è­‰åŸ·è¡Œå®Œæˆ")
        logger.info("================================================================================")
        logger.info(f"â±ï¸  ç¸½è™•ç†æ™‚é–“: {total_time:.2f} ç§’")
        logger.info(f"ğŸ“Š æ•´é«”æˆåŠŸç‡: {verification_report['overall_success']}")
        logger.info(f"ğŸ“ˆ æ™‚åºä¿å­˜ç‡: {verification_report['timeseries_preservation_rate']:.1%}")
        logger.info(f"ğŸ”„ æ•¸æ“šæµæ‘˜è¦: {verification_report['data_flow_summary']}")
        
        return verification_report

def main():
    """ä¸»å‡½æ•¸"""
    try:
        processor = Stage4To6VerificationProcessor()
        report = processor.execute_full_verification()
        
        # ä¿å­˜é©—è­‰å ±å‘Š
        report_path = Path("/app/data/stage4_to_6_verification_report.json")
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“„ é©—è­‰å ±å‘Šå·²ä¿å­˜: {report_path}")
        
        return report
        
    except Exception as e:
        logger.error(f"é©—è­‰åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()