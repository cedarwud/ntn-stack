#!/usr/bin/env python3
"""
éšæ®µå››æ™‚é–“åºåˆ—é è™•ç†å™¨

ç›´æ¥ä½¿ç”¨éšæ®µä¸‰ç”¢ç”Ÿçš„ä¿¡è™Ÿåˆ†ææª”æ¡ˆä¾†åŸ·è¡Œéšæ®µå››æ™‚é–“åºåˆ—é è™•ç†
æ¸¬è©¦æª”æ¡ˆæ¸…ç†æ©Ÿåˆ¶ï¼Œç¢ºèªéšæ®µå››æœƒå…ˆåˆªé™¤èˆŠæª”æ¡ˆå†é‡æ–°ç”¢ç”Ÿæ–°æª”æ¡ˆ
"""

import os
import sys
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Any, Optional

# è¨­å®š Python è·¯å¾‘
sys.path.insert(0, '/home/sat/ntn-stack/netstack')
sys.path.insert(0, '/home/sat/ntn-stack/netstack/src')

# å¼•ç”¨è™•ç†å™¨
from src.stages.timeseries_preprocessing_processor import TimeseriesPreprocessingProcessor

logger = logging.getLogger(__name__)

class Stage4TimeseriesProcessor:
    """éšæ®µå››æ™‚é–“åºåˆ—é è™•ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–éšæ®µå››è™•ç†å™¨"""
        logger.info("ğŸš€ éšæ®µå››æ™‚é–“åºåˆ—é è™•ç†å™¨åˆå§‹åŒ–")
        
        # åˆå§‹åŒ–æ™‚é–“åºåˆ—é è™•ç†å™¨
        self.timeseries_processor = TimeseriesPreprocessingProcessor(
            input_dir="/app/data",
            output_dir="/app/data"
        )
        
        logger.info("âœ… éšæ®µå››è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info("  ğŸ“Š æ™‚é–“åºåˆ—é è™•ç†ï¼šå¢å¼·å‰ç«¯å‹•ç•«æ•¸æ“š")
        
    def check_existing_files(self) -> Dict[str, Any]:
        """æª¢æŸ¥ç¾æœ‰æª”æ¡ˆç‹€æ…‹"""
        logger.info("ğŸ” æª¢æŸ¥ç¾æœ‰éšæ®µå››è¼¸å‡ºæª”æ¡ˆ...")
        
        output_dir = self.timeseries_processor.enhanced_dir
        existing_files = {}
        
        for filename in ["starlink_enhanced.json", "oneweb_enhanced.json", "conversion_statistics.json"]:
            file_path = output_dir / filename
            if file_path.exists():
                file_stat = file_path.stat()
                existing_files[filename] = {
                    "exists": True,
                    "size_mb": round(file_stat.st_size / (1024*1024), 2),
                    "modified_time": datetime.fromtimestamp(file_stat.st_mtime).isoformat(),
                    "path": str(file_path)
                }
                logger.info(f"  ğŸ“ ç™¼ç¾èˆŠæª”æ¡ˆ: {filename} ({existing_files[filename]['size_mb']} MB)")
            else:
                existing_files[filename] = {"exists": False}
        
        return existing_files
        
    def delete_old_files(self) -> Dict[str, bool]:
        """ä¸»å‹•åˆªé™¤èˆŠæª”æ¡ˆä»¥æ¸¬è©¦æ¸…ç†æ©Ÿåˆ¶"""
        logger.info("ğŸ—‘ï¸ æ¸¬è©¦éšæ®µå››æª”æ¡ˆæ¸…ç†æ©Ÿåˆ¶...")
        
        output_dir = self.timeseries_processor.enhanced_dir
        deletion_results = {}
        
        for filename in ["starlink_enhanced.json", "oneweb_enhanced.json", "conversion_statistics.json"]:
            file_path = output_dir / filename
            try:
                if file_path.exists():
                    old_size = file_path.stat().st_size / (1024*1024)
                    file_path.unlink()
                    deletion_results[filename] = True
                    logger.info(f"  âœ… å·²åˆªé™¤èˆŠæª”æ¡ˆ: {filename} ({old_size:.1f} MB)")
                else:
                    deletion_results[filename] = False
                    logger.info(f"  âšª æª”æ¡ˆä¸å­˜åœ¨: {filename}")
            except Exception as e:
                deletion_results[filename] = False
                logger.error(f"  âŒ åˆªé™¤å¤±æ•—: {filename} - {e}")
        
        return deletion_results
        
    def execute_stage4_processing(self, clean_regeneration: bool = True) -> Dict[str, Any]:
        """åŸ·è¡Œéšæ®µå››æ™‚é–“åºåˆ—é è™•ç†"""
        logger.info("=" * 80)
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œéšæ®µå››æ™‚é–“åºåˆ—é è™•ç†")
        logger.info("=" * 80)
        
        # 1. æª¢æŸ¥ç¾æœ‰æª”æ¡ˆ
        existing_files_before = self.check_existing_files()
        
        # 2. å¦‚æœå•Ÿç”¨æ¸…ç†æ¨¡å¼ï¼Œå…ˆåˆªé™¤èˆŠæª”æ¡ˆ
        deletion_results = {}
        if clean_regeneration:
            logger.info("ğŸ§¹ å•Ÿç”¨æ¸…ç†é‡æ–°ç”Ÿæˆæ¨¡å¼")
            deletion_results = self.delete_old_files()
        
        # 3. åŸ·è¡Œæ™‚é–“åºåˆ—é è™•ç†
        logger.info("ğŸ“Š éšæ®µå››ï¼šåŸ·è¡Œæ™‚é–“åºåˆ—é è™•ç†...")
        stage4_start_time = datetime.now()
        
        try:
            # æŒ‡å®šä½¿ç”¨éšæ®µä¸‰çš„ä¿¡è™Ÿåˆ†æè¼¸å‡ºæª”æ¡ˆ
            signal_file = "/app/data/signal_analysis_outputs/signal_event_analysis_output.json"
            
            stage4_data = self.timeseries_processor.process_timeseries_preprocessing(
                signal_file=signal_file,
                save_output=True
            )
            stage4_end_time = datetime.now()
            stage4_duration = (stage4_end_time - stage4_start_time).total_seconds()
            
            # é©—è­‰éšæ®µå››æ•¸æ“š
            total_processed = stage4_data['conversion_statistics']['total_processed']
            total_successful = stage4_data['conversion_statistics']['successful_conversions']
            conversion_rate = (total_successful / total_processed * 100) if total_processed > 0 else 0
            
            logger.info("âœ… éšæ®µå››è™•ç†å®Œæˆ")
            logger.info(f"  â±ï¸  è™•ç†æ™‚é–“: {stage4_duration:.1f} ç§’")
            logger.info(f"  ğŸ“Š è™•ç†è¡›æ˜Ÿæ•¸: {total_processed}")
            logger.info(f"  ğŸ¯ æˆåŠŸè½‰æ›: {total_successful}")
            logger.info(f"  ğŸ“ˆ è½‰æ›ç‡: {conversion_rate:.1f}%")
            
        except Exception as e:
            logger.error(f"âŒ éšæ®µå››è™•ç†å¤±æ•—: {e}")
            raise
        
        # 4. æª¢æŸ¥æ–°ç”Ÿæˆçš„æª”æ¡ˆ
        existing_files_after = self.check_existing_files()
        
        # 5. é©—è­‰æª”æ¡ˆæ¸…ç†å’Œé‡æ–°ç”Ÿæˆ
        file_management_verification = self.verify_file_management(
            existing_files_before, existing_files_after, deletion_results
        )
        
        # ç¸½çµè™•ç†çµæœ
        logger.info("=" * 80)
        logger.info("ğŸ‰ éšæ®µå››æ™‚é–“åºåˆ—é è™•ç†å®Œæˆ")
        logger.info("=" * 80)
        logger.info(f"â±ï¸  è™•ç†æ™‚é–“: {stage4_duration:.1f} ç§’")
        logger.info(f"ğŸ“Š æ•¸æ“šè½‰æ›: {total_processed} â†’ {total_successful} é¡†è¡›æ˜Ÿ")
        logger.info(f"ğŸ¯ è½‰æ›æ•ˆç‡: {conversion_rate:.1f}%")
        logger.info("ğŸ’¾ æª”æ¡ˆç®¡ç†: æ¸…ç†é‡æ–°ç”Ÿæˆæ¨¡å¼")
        
        # è¿”å›å®Œæ•´çµæœ
        result = {
            'stage4_data': stage4_data,
            'processing_metadata': {
                'processing_time_seconds': stage4_duration,
                'processing_timestamp': datetime.now(timezone.utc).isoformat(),
                'clean_regeneration_mode': clean_regeneration,
                'processing_version': '1.0.0'
            },
            'file_management': file_management_verification,
            'performance_metrics': {
                'total_processed': total_processed,
                'successful_conversions': total_successful,
                'conversion_rate_percent': conversion_rate,
                'processing_efficiency': 'excellent' if conversion_rate >= 95 else 'good' if conversion_rate >= 80 else 'needs_improvement'
            }
        }
        
        return result
        
    def verify_file_management(self, before: Dict, after: Dict, deletions: Dict) -> Dict[str, Any]:
        """é©—è­‰æª”æ¡ˆç®¡ç†æ©Ÿåˆ¶"""
        logger.info("ğŸ” é©—è­‰æª”æ¡ˆç®¡ç†æ©Ÿåˆ¶...")
        
        verification_results = {
            "old_files_cleanup": {},
            "new_files_generation": {},
            "file_size_comparison": {},
            "cleanup_success": True,
            "regeneration_success": True
        }
        
        for filename in ["starlink_enhanced.json", "oneweb_enhanced.json"]:
            # æª¢æŸ¥èˆŠæª”æ¡ˆæ¸…ç†
            was_deleted = deletions.get(filename, False)
            verification_results["old_files_cleanup"][filename] = was_deleted
            
            # æª¢æŸ¥æ–°æª”æ¡ˆç”Ÿæˆ
            new_file_exists = after.get(filename, {}).get("exists", False)
            verification_results["new_files_generation"][filename] = new_file_exists
            
            if not new_file_exists:
                verification_results["regeneration_success"] = False
            
            # æª”æ¡ˆå¤§å°æ¯”è¼ƒ
            if before.get(filename, {}).get("exists") and after.get(filename, {}).get("exists"):
                old_size = before[filename]["size_mb"]
                new_size = after[filename]["size_mb"]
                size_change = new_size - old_size
                verification_results["file_size_comparison"][filename] = {
                    "old_size_mb": old_size,
                    "new_size_mb": new_size,
                    "size_change_mb": round(size_change, 2),
                    "size_change_percent": round((size_change / old_size * 100), 1) if old_size > 0 else 0
                }
                
                logger.info(f"  ğŸ“Š {filename}: {old_size:.1f}MB â†’ {new_size:.1f}MB ({size_change:+.1f}MB)")
        
        # ç¸½çµé©—è­‰çµæœ
        if verification_results["cleanup_success"] and verification_results["regeneration_success"]:
            logger.info("âœ… æª”æ¡ˆç®¡ç†æ©Ÿåˆ¶é©—è­‰é€šéï¼šæ¸…ç†èˆŠæª”æ¡ˆ âœ“ ç”Ÿæˆæ–°æª”æ¡ˆ âœ“")
        else:
            logger.warning("âš ï¸ æª”æ¡ˆç®¡ç†æ©Ÿåˆ¶é©—è­‰ç™¼ç¾å•é¡Œ")
        
        return verification_results
        
    def generate_stage4_execution_report(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”¢ç”Ÿéšæ®µå››åŸ·è¡Œå ±å‘Š"""
        logger.info("ğŸ“ ç”¢ç”Ÿéšæ®µå››åŸ·è¡Œå ±å‘Š...")
        
        stage4_data = result['stage4_data']
        processing_meta = result['processing_metadata']
        file_management = result['file_management']
        performance_metrics = result['performance_metrics']
        
        # è¼¸å‡ºæª”æ¡ˆè©³ç´°ä¿¡æ¯
        output_files_info = {}
        for const_name in ['starlink', 'oneweb']:
            constellation_data = stage4_data['constellation_data'][const_name]
            if constellation_data['output_file']:
                file_path = Path(constellation_data['output_file'])
                if file_path.exists():
                    file_size = file_path.stat().st_size / (1024*1024)
                    output_files_info[const_name] = {
                        "filename": file_path.name,
                        "full_path": str(file_path),
                        "size_mb": round(file_size, 2),
                        "satellites_count": constellation_data['satellites_processed']
                    }
        
        # æª¢æŸ¥æ˜¯å¦ç¬¦åˆå¾ŒçºŒéšæ®µéœ€æ±‚ï¼ˆéšæ®µäº”ï¼‰
        stage5_readiness = {
            "ready_for_data_integration": True,
            "enhanced_timeseries_available": len(output_files_info) > 0,
            "data_format_compliance": True,
            "file_accessibility": all(Path(info["full_path"]).exists() for info in output_files_info.values())
        }
        
        # ç”¢ç”Ÿå®Œæ•´å ±å‘Š
        execution_report = {
            'report_metadata': {
                'report_type': 'stage4_execution_report',
                'report_timestamp': datetime.now(timezone.utc).isoformat(),
                'processing_version': processing_meta['processing_version']
            },
            'execution_summary': {
                'processing_time': f"{processing_meta['processing_time_seconds']:.1f} ç§’",
                'processing_timestamp': processing_meta['processing_timestamp'],
                'clean_regeneration_mode': processing_meta['clean_regeneration_mode'],
                'total_satellites_processed': performance_metrics['total_processed'],
                'successful_conversions': performance_metrics['successful_conversions'],
                'conversion_rate': f"{performance_metrics['conversion_rate_percent']:.1f}%",
                'processing_efficiency': performance_metrics['processing_efficiency']
            },
            'file_management_verification': {
                'old_files_cleanup_success': file_management['cleanup_success'],
                'new_files_generation_success': file_management['regeneration_success'],
                'file_size_changes': file_management['file_size_comparison'],
                'clean_regeneration_confirmed': file_management['cleanup_success'] and file_management['regeneration_success']
            },
            'output_files_analysis': output_files_info,
            'stage5_readiness_check': stage5_readiness,
            'data_quality_metrics': {
                'conversion_success_rate': performance_metrics['conversion_rate_percent'],
                'data_completeness': 'complete' if performance_metrics['conversion_rate_percent'] >= 95 else 'partial',
                'frontend_animation_ready': True,
                'timeseries_optimization_applied': True
            }
        }
        
        logger.info("âœ… éšæ®µå››åŸ·è¡Œå ±å‘Šç”¢ç”Ÿå®Œæˆ")
        return execution_report

def main():
    """ä¸»å‡½æ•¸"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    try:
        processor = Stage4TimeseriesProcessor()
        result = processor.execute_stage4_processing(clean_regeneration=True)
        
        # ç”¢ç”ŸåŸ·è¡Œå ±å‘Š
        report = processor.generate_stage4_execution_report(result)
        
        logger.info("ğŸŠ éšæ®µå››æ™‚é–“åºåˆ—é è™•ç†æˆåŠŸå®Œæˆï¼")
        logger.info("ğŸ“ åŸ·è¡Œå ±å‘Šå·²ç”¢ç”Ÿ")
        
        return True, report
        
    except Exception as e:
        logger.error(f"ğŸ’¥ éšæ®µå››è™•ç†å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return False, None

if __name__ == "__main__":
    success, report = main()
    sys.exit(0 if success else 1)